<!-- Page 1 -->
# (3)WILEY

ELEMENTS OF INFORMATION THEORY

SECOND EDITION

Thomas M. Cover
Joy A. Thomas
<!-- Page 2 -->
# 정보 이론의 기초

## 제2판

THOMAS M. COVER<br>JOY A. THOMAS

## WILEY- <br> INTERSCIENCE

A JOHN WILEY & SONS, INC., PUBLICATION

<!-- Page 3 -->
# 정보 이론의 기초
<!-- Page 4 -->
.
<!-- Page 5 -->
# 정보 이론의 기초

## 제2판

THOMAS M. COVER<br>JOY A. THOMAS

## WILEY- <br> INTERSCIENCE

A JOHN WILEY & SONS, INC., PUBLICATION

<!-- Page 6 -->
Copyright © 2006 by John Wiley & Sons, Inc. 모든 권리가 있습니다.

John Wiley & Sons, Inc., Hoboken, New Jersey에서 출판되었습니다.
동시에 캐나다에서도 출판되었습니다.

이 출판물의 어떤 부분도 1976년 미국 저작권법 제107조 또는 108조에 따라 허용되는 경우를 제외하고, 출판사의 사전 서면 허가 없이 또는 Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, 팩스 (978) 750-4470 또는 www.copyright.com에서 복사당 적절한 수수료를 지불하여 승인받지 않는 한, 전자적, 기계적, 복사, 녹음, 스캔 또는 기타 방식으로 복제, 검색 시스템에 저장 또는 전송될 수 없습니다. 출판사에 대한 허가 요청은 Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, 팩스 (201) 748-6008 또는 http://www.wiley.com/go/permission에서 온라인으로 문의해야 합니다.

책임의 한계/보증의 부인: 출판사와 저자는 이 책을 준비하는 데 최선을 다했지만, 이 책의 내용의 정확성이나 완전성에 대해 어떠한 진술이나 보증도 하지 않으며, 특정 목적에 대한 상품성 또는 적합성에 대한 묵시적 보증을 명시적으로 부인합니다. 판매 대리점 또는 서면 판매 자료를 통해 어떠한 보증도 생성되거나 확장될 수 없습니다. 여기에 포함된 조언과 전략은 귀하의 상황에 적합하지 않을 수 있습니다. 적절한 경우 전문가와 상담해야 합니다. 출판사 또는 저자는 이익 손실 또는 기타 상업적 손해에 대해 책임을 지지 않으며, 여기에는 특별, 부수적, 결과적 또는 기타 손해가 포함되지만 이에 국한되지 않습니다.

당사의 다른 제품 및 서비스에 대한 일반 정보 또는 기술 지원은 미국 내에서는 (800) 762-2974, 미국 외 지역에서는 (317) 572-3993 또는 팩스 (317) 572-4002로 고객 관리 부서에 문의하십시오.

Wiley는 또한 다양한 전자 형식으로 책을 출판합니다. 인쇄본으로 제공되는 일부 콘텐츠는 전자 형식으로 제공되지 않을 수 있습니다. Wiley 제품에 대한 자세한 내용은 당사 웹사이트 www.wiley.com을 방문하십시오.

# 미국 의회 도서관 자료 목록:

Cover, T. M., 1938-
정보 이론의 요소 / Thomas M. Cover, Joy A. Thomas 저. - 제2판. cm.
"A Wiley-Interscience publication."
참고 문헌 및 색인이 포함되어 있습니다.
ISBN-13 978-0-471-24195-9
ISBN-10 0-471-24195-4

1. 정보 이론. I. Thomas, Joy A. II. 제목.

Q360.C68 2005
$003^{\prime}.54-$ dc22
2005047799

미국에서 인쇄되었습니다.
<!-- Page 7 -->
# CONTENTS

목차 ..... V
두 번째 판 서문 ..... xv
첫 번째 판 서문 ..... xvii
두 번째 판에 대한 감사 ..... xxi
첫 번째 판에 대한 감사 ..... xxiii
1 서론 및 개요 ..... 1
1.1 책의 개요 ..... 5
2 엔트로피, 상대 엔트로피 및 상호 정보량 ..... 13
2.1 엔트로피 ..... 13
2.2 결합 엔트로피 및 조건부 엔트로피 ..... 16
2.3 상대 엔트로피 및 상호 정보량 ..... 19
2.4 엔트로피와 상호 정보량의 관계 ..... 20
2.5 엔트로피, 상대 엔트로피 및 상호 정보량에 대한 연쇄 법칙 ..... 22
2.6 젠센 부등식 및 그 결과 ..... 25
2.7 로그 합 부등식 및 그 응용 ..... 30
2.8 데이터 처리 부등식 ..... 34
2.9 충분 통계량 ..... 35
2.10 파노 부등식 ..... 37
요약 ..... 41
문제 ..... 43
역사적 고찰 ..... 54
<!-- Page 8 -->
3 점근적 균등 분할 속성 ..... 57
3.1 점근적 균등 분할 속성 정리 ..... 58
3.2 AEP의 결과: 데이터 압축 ..... 60
3.3 고확률 집합과 전형 집합 ..... 62
요약 ..... 64
문제 ..... 64
역사적 참고 사항 ..... 69
4 확률 과정의 엔트로피율 ..... 71
4.1 마르코프 연쇄 ..... 71
4.2 엔트로피율 ..... 74
4.3 예시: 가중 그래프에서의 무작위 행보의 엔트로피율 ..... 78
4.4 열역학 제2법칙 ..... 81
4.5 마르코프 연쇄의 함수 ..... 84
요약 ..... 87
문제 ..... 88
역사적 참고 사항 ..... 100
5 데이터 압축 ..... 103
5.1 코드 예시 ..... 103
5.2 Kraft 부등식 ..... 107
5.3 최적 코드 ..... 110
5.4 최적 코드 길이에 대한 경계 ..... 112
5.5 유일 복호 가능 코드에 대한 Kraft 부등식 ..... 115
5.6 Huffman 코드 ..... 118
5.7 Huffman 코드에 대한 몇 가지 논평 ..... 120
5.8 Huffman 코드의 최적성 ..... 123
5.9 Shannon-Fano-Elias 코딩 ..... 127
5.10 Shannon 코드의 경쟁적 최적성 ..... 130
5.11 공정한 동전으로부터 이산 분포 생성 ..... 134
요약 ..... 141
문제 ..... 142
역사적 참고 사항 ..... 157
<!-- Page 9 -->
6 도박과 데이터 압축 ..... 159
6.1 경마 ..... 159
6.2 도박과 부가 정보 ..... 164
6.3 종속적인 경마와 entropy rate ..... 166
6.4 영어의 entropy ..... 168
6.5 데이터 압축과 도박 ..... 171
6.6 영어 entropy의 도박 추정치 ..... 173
요약 ..... 175
문제 ..... 176
역사적 고찰 ..... 182
7 채널 용량 ..... 183
7.1 채널 용량의 예시 ..... 184
7.1.1 잡음 없는 이진 채널 ..... 184
7.1.2 겹치지 않는 출력을 가진 잡음 채널 ..... 185
7.1.3 잡음 타자기 ..... 186
7.1.4 이진 대칭 채널 ..... 187
7.1.5 이진 삭제 채널 ..... 188
7.2 대칭 채널 ..... 189
7.3 채널 용량의 속성 ..... 191
7.4 채널 코딩 정리 미리보기 ..... 191
7.5 정의 ..... 192
7.6 결합적으로 전형적인 시퀀스 ..... 195
7.7 채널 코딩 정리 ..... 199
7.8 제로-에러 코드 ..... 205
7.9 파노의 부등식과 코딩 정리의 역정리 ..... 206
7.10 채널 코딩 정리의 역정리에서의 등식 ..... 208
7.11 해밍 코드 ..... 210
7.12 피드백 용량 ..... 216
7.13 소스-채널 분리 정리 ..... 218
요약 ..... 222
문제 ..... 223
역사적 고찰 ..... 240
<!-- Page 10 -->
8 미분 엔트로피 ..... 243
8.1 정의 ..... 243
8.2 연속 확률 변수에 대한 AEP ..... 245
8.3 미분 엔트로피와 이산 엔트로피의 관계 ..... 247
8.4 결합 및 조건부 미분 엔트로피 ..... 249
8.5 상대 엔트로피 및 mutual information ..... 250
8.6 미분 엔트로피, 상대 엔트로피 및 mutual information의 속성 ..... 252
요약 ..... 256
문제 ..... 256
역사적 참고 사항 ..... 259
9 가우시안 채널 ..... 261
9.1 가우시안 채널: 정의 ..... 263
9.2 가우시안 채널 코딩 정리에 대한 역 ..... 268
9.3 대역 제한 채널 ..... 270
9.4 병렬 가우시안 채널 ..... 274
9.5 컬러 가우시안 노이즈가 있는 채널 ..... 277
9.6 피드백이 있는 가우시안 채널 ..... 280
요약 ..... 289
문제 ..... 290
역사적 참고 사항 ..... 299
10 속도 왜곡 이론 ..... 301
10.1 양자화 ..... 301
10.2 정의 ..... 303
10.3 속도 왜곡 함수의 계산 ..... 307
10.3.1 이진 소스 ..... 307
10.3.2 가우시안 소스 ..... 310
10.3.3 독립 가우시안 확률 변수의 동시 기술 ..... 312
10.4 속도 왜곡 정리에 대한 역 ..... 315
10.5 속도 왜곡 함수의 달성 가능성 ..... 318
10.6 강한 전형적 시퀀스 및 속도 왜곡 ..... 325
10.7 속도 왜곡 함수의 특성화 ..... 329
<!-- Page 11 -->
10.8 채널 용량 및 속도 왜곡 함수의 계산 ..... 332
요약 ..... 335
문제 ..... 336
역사적 참고 사항 ..... 345
11 정보 이론 및 통계 ..... 347
11.1 타입 방법 ..... 347
11.2 대수의 법칙 ..... 355
11.3 범용 소스 코딩 ..... 357
11.4 대규모 편차 이론 ..... 360
11.5 사노프 정리의 예시 ..... 364
11.6 조건부 극한 정리 ..... 366
11.7 가설 검정 ..... 375
11.8 체르노프-스타인 보조정리 ..... 380
11.9 체르노프 정보 ..... 384
11.10 피셔 정보 및 크라메르-라오 부등식 ..... 392
요약 ..... 397
문제 ..... 399
역사적 참고 사항 ..... 408
12 최대 엔트로피 ..... 409
12.1 최대 엔트로피 분포 ..... 409
12.2 예시 ..... 411
12.3 이상 최대 엔트로피 문제 ..... 413
12.4 스펙트럼 추정 ..... 415
12.5 가우시안 과정의 엔트로피율 ..... 416
12.6 버그의 최대 엔트로피 정리 ..... 417
요약 ..... 420
문제 ..... 421
역사적 참고 사항 ..... 425
13 범용 소스 코딩 ..... 427
13.1 범용 코드 및 채널 용량 ..... 428
13.2 이진 시퀀스에 대한 범용 코딩 ..... 433
13.3 산술 코딩 ..... 436
<!-- Page 12 -->
13.4 Lempel-Ziv 코딩 ..... 440
13.4.1 슬라이딩 윈도우 Lempel-Ziv 알고리즘 ..... 441
13.4.2 트리 구조 Lempel-Ziv 알고리즘 ..... 442
13.5 Lempel-Ziv 알고리즘의 최적성 ..... 443
13.5.1 슬라이딩 윈도우 Lempel-Ziv 알고리즘 ..... 443
13.5.2 트리 구조 Lempel-Ziv 압축의 최적성 ..... 448
요약 ..... 456
문제 ..... 457
역사적 고찰 ..... 461
14 Kolmogorov 복잡도 ..... 463
14.1 계산 모델 ..... 464
14.2 Kolmogorov 복잡도: 정의 및 예시 ..... 466
14.3 Kolmogorov 복잡도와 엔트로피 ..... 473
14.4 정수의 Kolmogorov 복잡도 ..... 475
14.5 알고리즘적으로 무작위적이고 비압축 가능한 시퀀스 ..... 476
14.6 보편 확률 ..... 480
14.7 Kolmogorov 복잡도 ..... 482
$14.8 \quad \Omega$ ..... 484
14.9 보편 도박 ..... 487
14.10 Occam의 면도날 ..... 488
14.11 Kolmogorov 복잡도와 보편 확률 ..... 490
14.12 Kolmogorov 충분 통계량 ..... 496
14.13 최소 설명 길이 원칙 ..... 500
요약 ..... 501
문제 ..... 503
역사적 고찰 ..... 507
15 네트워크 정보 이론 ..... 509
15.1 가우시안 다중 사용자 채널 ..... 513
<!-- Page 13 -->
15.1.1 단일 사용자 가우시안 채널 ..... 513
15.1.2 $m$ 사용자 가우시안 다중 접속 채널 ..... 514
15.1.3 가우시안 방송 채널 ..... 515
15.1.4 가우시안 릴레이 채널 ..... 516
15.1.5 가우시안 간섭 채널 ..... 518
15.1.6 가우시안 양방향 채널 ..... 519
15.2 공동으로 전형적인 시퀀스 ..... 520
15.3 다중 접속 채널 ..... 524
15.3.1 다중 접속 채널의 용량 영역 달성 가능성 ..... 530
15.3.2 다중 접속 채널의 용량 영역에 대한 논평 ..... 532
15.3.3 다중 접속 채널의 용량 영역의 볼록성 ..... 534
15.3.4 다중 접속 채널에 대한 역정리 ..... 538
15.3.5 $m$-사용자 다중 접속 채널 ..... 543
15.3.6 가우시안 다중 접속 채널 ..... 544
15.4 상관 소스의 인코딩 ..... 549
15.4.1 Slepian-Wolf 정리의 달성 가능성 ..... 551
15.4.2 Slepian-Wolf 정리에 대한 역정리 ..... 555
15.4.3 다중 소스에 대한 Slepian-Wolf 정리 ..... 556
15.4.4 Slepian-Wolf 코딩의 해석 ..... 557
15.5 Slepian-Wolf 인코딩과 다중 접속 채널 간의 쌍대성 ..... 558
15.6 방송 채널 ..... 560
15.6.1 방송 채널의 정의 ..... 563
15.6.2 저하된 방송 채널 ..... 564
15.6.3 저하된 방송 채널의 용량 영역 ..... 565
15.7 릴레이 채널 ..... 571
15.8 측면 정보가 있는 소스 코딩 ..... 575
15.9 측면 정보가 있는 속도 왜곡 ..... 580
<!-- Page 14 -->
15.10 일반 다중 단자 네트워크 ..... 587
요약 ..... 594
문제 ..... 596
역사적 참고 사항 ..... 609
16 정보 이론 및 포트폴리오 이론 ..... 613
16.1 주식 시장: 일부 정의 ..... 613
16.2 로그 최적 포트폴리오의 Kuhn-Tucker 특성화 ..... 617
16.3 로그 최적 포트폴리오의 점근적 최적성 ..... 619
16.4 부가 정보 및 성장률 ..... 621
16.5 정상 시장에서의 투자 ..... 623
16.6 로그 최적 포트폴리오의 경쟁적 최적성 ..... 627
16.7 범용 포트폴리오 ..... 629
16.7.1 유한 기간 범용 포트폴리오 ..... 631
16.7.2 기간 불변 범용 포트폴리오 ..... 638
16.8 Shannon-McMillan-Breiman 정리 (일반 AEP) ..... 644
요약 ..... 650
문제 ..... 652
역사적 참고 사항 ..... 655
17 정보 이론의 부등식 ..... 657
17.1 정보 이론의 기본 부등식 ..... 657
17.2 차분 엔트로피 ..... 660
17.3 엔트로피 및 상대 엔트로피의 경계 ..... 663
17.4 유형에 대한 부등식 ..... 665
17.5 엔트로피에 대한 조합론적 경계 ..... 666
17.6 부분 집합의 엔트로피율 ..... 667
17.7 엔트로피 및 Fisher 정보 ..... 671
17.8 엔트로피 거듭제곱 부등식 및 Brunn-Minkowski 부등식 ..... 674
17.9 행렬식에 대한 부등식 ..... 679
<!-- Page 15 -->
17.10 행렬식 비율에 대한 부등식 ..... 683
요약 ..... 686
문제 ..... 686
역사적 참고사항 ..... 687
참고문헌 ..... 689
기호 목록 ..... 723
색인 ..... 727
<!-- Page 16 -->
.
<!-- Page 17 -->
# 제2판 서문

초판 출간 이후 여러 해 동안, 우리는 책의 여러 측면을 개선하고 재배열하거나 확장하고 싶었지만, 재인쇄의 제약으로 인해 인쇄물 간에 그러한 변경을 할 수 없었습니다. 새 판에서는 이러한 변경 중 일부를 적용하고, 연습문제를 추가하며, 초판에서 생략했던 일부 주제를 논의할 기회를 얻게 되었습니다.

주요 변경 사항에는 책을 더 쉽게 가르칠 수 있도록 장의 재구성 및 200개 이상의 새로운 연습문제 추가가 포함됩니다. 범용 포트폴리오, 범용 소스 코딩, 가우시안 피드백 용량, 네트워크 정보 이론에 대한 자료를 추가했으며, 데이터 압축과 채널 용량의 이중성을 개발했습니다. 새로운 장이 추가되었고 많은 증명이 간소화되었습니다. 또한 참고 문헌과 역사적 메모를 업데이트했습니다.

이 책의 내용은 2학기 과정으로 가르칠 수 있습니다. 첫 학기에는 점근적 균등 분할 속성, 데이터 압축, 채널 용량을 포함하는 1장부터 9장까지 다룰 수 있으며, 가우시안 채널의 용량으로 마무리됩니다. 두 번째 학기에는 속도 왜곡, 유형 방법, 콜모고로프 복잡성, 네트워크 정보 이론, 범용 소스 코딩 및 포트폴리오 이론을 포함한 나머지 장을 다룰 수 있습니다. 한 학기만 가능하다면, 첫 학기에 속도 왜곡과 콜모고로프 복잡성 및 네트워크 정보 이론에 대한 각각 한 번의 강의를 추가할 것입니다. 웹사이트 http://www.elementsofinformationtheory.com 에는 추가 자료 및 선택된 연습문제의 해답에 대한 링크가 제공됩니다.

이 책의 초판 이후 여러 해 동안 정보 이론은 50주년을 맞이했으며(이 분야를 시작한 Shannon의 원 논문 발표 50주년), 정보 이론의 아이디어는 생물정보학, 웹 검색, 무선 통신, 비디오 압축을 포함한 과학 및 기술의 많은 문제에 적용되었습니다.
<!-- Page 18 -->
다른 분야들이 있습니다. 응용 분야는 무한하지만, 이 분야의 핵심적인 매력은 여전히 근본적인 수학의 우아함에 있습니다. 저희는 이 책이 수학, 물리학, 통계학, 공학의 교차점에서 가장 흥미로운 분야 중 하나라고 저희가 믿는 이유에 대한 통찰력을 제공하기를 바랍니다.

Tom Cover
Joy Thomas

Palo Alto, California
2006년 1월
<!-- Page 19 -->
# 초판 서문

이 책은 정보 이론에 대한 쉽고 접근하기 쉬운 책을 목표로 합니다. 아인슈타인이 말했듯이, "모든 것은 가능한 한 단순하게 만들어져야 하지만, 더 단순하게는 안 된다." 이 인용구의 출처는 확인하지 않았지만 (포춘 쿠키에서 처음 발견됨), 이러한 관점은 책 전체에서 우리의 개발을 이끌어갑니다. 몇 가지 핵심적인 아이디어와 기법들이 있으며, 이를 숙달하면 이 주제가 단순해 보이고 새로운 질문에 대한 훌륭한 직관을 제공합니다.

이 책은 10년 이상 동안 정보 이론에 대한 시니어 및 1학년 대학원 수준의 두 학기 과정에서 강의한 내용을 바탕으로 만들어졌으며, 통신 이론, 컴퓨터 과학 및 통계학 학생들을 위한 정보 이론 입문서로 의도되었습니다.

정보 이론에 내재된 단순성에 대해 두 가지를 언급할 수 있습니다. 첫째, 엔트로피와 상호 정보와 같은 특정 양은 근본적인 질문에 대한 답으로 나타납니다. 예를 들어, 엔트로피는 확률 변수의 최소 기술 복잡도이며, 상호 정보는 노이즈가 있는 상태에서의 통신 속도입니다. 또한, 지적하겠지만, 상호 정보는 부가 정보가 주어졌을 때 부의 두 배 증가율에 해당합니다. 둘째, 정보 이론적 질문에 대한 답은 자연스러운 대수적 구조를 가집니다. 예를 들어, 엔트로피에 대한 연쇄 법칙이 있으며, 엔트로피와 상호 정보는 서로 관련되어 있습니다. 따라서 데이터 압축 및 통신 문제에 대한 답은 광범위한 해석을 허용합니다. 우리는 문제 하나를 조사하고, 많은 대수학을 거친 후, 답을 조사했을 때 전체 문제가 분석이 아닌 답의 검토를 통해 조명된다는 느낌을 모두 알고 있습니다. 아마도 물리학에서 이러한 가장 뛰어난 예는 뉴턴의 법칙과 슈뢰딩거의 파동 방정식일 것입니다. 슈뢰딩거의 파동 방정식에 대한 놀라운 철학적 해석을 누가 예측할 수 있었을까요?

본문에서는 질문을 살펴보기 전에 종종 답의 속성을 조사합니다. 예를 들어, 2장에서는 엔트로피, 상대 엔트로피 및 상호 정보를 정의하고 관계와 몇 가지를 연구합니다.
<!-- Page 20 -->
이러한 해석들은 다양한 방식으로 답들이 어떻게 연결되는지를 보여줍니다. 그 과정에서 우리는 열역학 제2법칙의 의미에 대해 추측합니다. 엔트로피는 항상 증가하는가? 답은 예, 아니오입니다. 이것은 해당 분야의 전문가들에게는 만족스러울 수 있지만 초심자에게는 표준적인 것으로 간주되어 간과될 수 있는 결과입니다.

사실, 이것은 가르칠 때 자주 발생하는 한 가지 요점을 제기합니다. 아무도 모르는 새로운 증명이나 약간의 새로운 결과를 찾는 것은 즐겁습니다. 이러한 아이디어들을 수업에서 기존 자료와 함께 제시하면 "물론이지, 물론이지, 물론이지"라는 반응이 나옵니다. 하지만 자료를 가르치는 즐거움은 크게 향상됩니다. 따라서 우리는 이 교과서에서 여러 새로운 아이디어를 조사하는 데 큰 즐거움을 얻었습니다.

이 교재의 새로운 자료 중 일부 예로는 정보 이론과 도박의 관계에 대한 장, 마르코프 연쇄의 맥락에서 열역학 제2법칙의 보편성에 대한 연구, 채널 용량 정리에 대한 공동 전형성 증명, 허프만 코드의 경쟁적 최적성, 그리고 최대 엔트로피 스펙트럼 밀도 추정에 대한 버그 정리의 증명이 있습니다. 또한, 콜모고로프 복잡성에 대한 장은 다른 정보 이론 교재에는 이에 상응하는 내용이 없습니다. 우리는 또한 피셔 정보, 상호 정보, 중심 극한 정리, 그리고 브룬-민코프스키 및 엔트로피 거듭제곱 부등식을 연관시키는 데 즐거움을 느꼈습니다. 놀랍게도, 행렬식 부등식에 대한 많은 고전적인 결과들은 정보 이론적 부등식을 사용하여 가장 쉽게 증명될 수 있습니다.

정보 이론 분야는 Shannon의 원래 논문 이후로 상당히 성장했지만, 우리는 그 일관성을 강조하기 위해 노력했습니다. Shannon이 정보 이론을 개발할 때 통신 이론의 문제에 의해 동기 부여되었다는 것은 분명하지만, 우리는 정보 이론을 통신 이론과 통계에 응용되는 그 자체의 분야로 취급합니다. 우리는 통신 이론, 확률 이론, 통계학의 배경에서 정보 이론 분야로 끌렸는데, 이는 정보라는 무형의 개념을 포착하는 것이 명백히 불가능해 보였기 때문입니다.

책의 결과 대부분은 정리와 증명으로 제시되므로, 결과의 우아함이 스스로 말해줄 것이라고 기대합니다. 많은 경우, 우리는 문제에 대한 해결책의 속성을 문제 자체보다 먼저 설명합니다. 다시 말하지만, 그 속성들은 그 자체로 흥미로우며 이어지는 증명에 자연스러운 리듬을 제공합니다.

발표의 한 가지 혁신은 중간 설명 없이 긴 부등식 연쇄를 사용하고 바로 뒤에 설명을 붙이는 것입니다. 독자들이 이러한 증명 중 많은 부분을 접할 때쯤이면, 우리는 그들이 설명 없이도 대부분의 단계를 따라갈 수 있고 필요한 설명을 골라낼 수 있을 것이라고 기대합니다. 이러한 부등식 연쇄는
<!-- Page 21 -->
부등식은 독자가 중요한 몇 가지 정리를 증명하는 데 필요한 지식을 갖추고 있음을 확신할 수 있는 퀴즈 역할을 합니다. 이러한 증명의 자연스러운 흐름은 매우 설득력이 있어 기술 문서 작성의 기본 규칙 중 하나를 어기게 만들었습니다. 또한, 설명이 부족하면 아이디어의 논리적 필연성이 명확해지고 핵심 아이디어가 명료해집니다. 이 책의 끝에 독자께서 정보 이론의 우아함, 단순함, 자연스러움에 대한 저희의 감상을 공유하시기를 바랍니다.

이 책 전반에 걸쳐 저희는 1948년 Shannon의 원본 작업에서 기원을 찾을 수 있지만 1970년대 초에 공식적으로 개발된 약한 전형적 시퀀스(weakly typical sequences) 방법을 사용합니다. 여기서 핵심 아이디어는 점근적 등확률 속성(asymptotic equipartition property)으로, 이는 대략 "거의 모든 것이 거의 동일한 확률을 갖는다"로 의역될 수 있습니다.

제2장에는 엔트로피, 상대 엔트로피, 상호 정보의 기본적인 대수적 관계가 포함되어 있습니다. 점근적 등확률 속성(AEP)은 제3장에서 중심적인 중요성을 갖습니다. 이는 제4장과 제5장에서 확률 과정의 엔트로피율과 데이터 압축에 대해 논의하게 합니다. 제6장에서는 도박에 대한 여정을 떠나는데, 여기서 데이터 압축과 부의 성장률의 이중성이 개발됩니다.

정보 이론의 지적 기반으로서 Kolmogorov 복잡성의 센세이셔널한 성공은 제14장에서 탐구됩니다. 여기서 저희는 평균적으로 좋은 설명을 찾는 목표를 보편적으로 가장 짧은 설명을 찾는 목표로 대체합니다. 객체의 기술적 복잡성에 대한 보편적인 개념이 실제로 존재합니다. 여기서 또한 놀라운 숫자 $\Omega$가 조사됩니다. 이 숫자는 튜링 기계가 정지할 확률의 이진 확장으로, 수학의 많은 비밀을 드러냅니다.

채널 용량은 제7장에서 확립됩니다. 차분 엔트로피에 대한 필요한 자료는 제8장에서 개발되며, 이는 이전 용량 정리를 연속 잡음 채널로 확장하기 위한 기반을 마련합니다. 근본적인 가우시안 채널의 용량은 제9장에서 조사됩니다.

정보 이론과 통계학 간의 관계는 1950년대 초 Kullback에 의해 처음 연구되었고 이후 상대적으로 무시되었는데, 제11장에서 개발됩니다. 속도 왜곡 이론(rate distortion theory)은 잡음 없는 데이터 압축의 대응물보다 약간 더 많은 배경 지식이 필요하며, 이것이 텍스트에서 제10장으로 늦게 배치된 이유입니다.

잡음과 간섭이 있는 상태에서 동시에 달성 가능한 정보 흐름을 연구하는 네트워크 정보 이론(network information theory)의 거대한 주제는 제15장에서 개발됩니다. 네트워크 정보 이론에서는 많은 새로운 아이디어가 등장합니다. 주요 새로운 요소는 간섭과 피드백입니다. 제16장에서는 주식 시장을 다루는데, 이는
<!-- Page 22 -->
제 6장에서 고려된 도박 과정의 일반화이며, 정보 이론과 도박 간의 긴밀한 상관관계를 다시 한번 보여줍니다.

정보 이론의 부등식에 관한 제 17장에서는 책 전체에 흩어져 있는 흥미로운 부등식들을 다시 한번 살펴보고, 새로운 틀에 넣어본 다음, 무작위로 추출된 부분집합의 엔트로피율에 대한 몇 가지 흥미로운 새로운 부등식을 추가할 기회를 제공합니다. 집합 합의 부피에 대한 Brunn-Minkowski 부등식, 독립 확률 변수의 합의 유효 분산에 대한 엔트로피 파워 부등식, 그리고 Fisher 정보 부등식 간의 아름다운 관계가 여기서 명확하게 드러납니다.

이론을 일관된 수준으로 유지하기 위해 노력했습니다. 수학적 수준은 상당히 높으며, 아마도 학부 고학년 또는 대학원 1학년 수준으로, 최소한 한 학기 분량의 확률론 강의와 탄탄한 수학적 배경 지식을 갖추고 있어야 합니다. 그러나 측도론의 사용은 피할 수 있었습니다. 측도론은 제 16장의 에르고딕 과정에 대한 AEP 증명에서만 간략하게 등장합니다. 이는 정보 이론의 기본 원리가 그것들을 완전한 일반화로 이끌기 위해 필요한 기술과 직교한다는 우리의 믿음과 일치합니다.

필수적인 비타민은 제 2장, 3장, 4장, 5장, 7장, 8장, 9장, 11장, 10장, 그리고 15장에 담겨 있습니다. 이 장들의 부분집합은 다른 장들에 대한 필수적인 참조 없이 읽을 수 있으며, 이해의 좋은 핵심을 이룹니다. 저희의 의견으로는, 제 14장의 Kolmogorov 복잡성에 관한 장도 정보 이론에 대한 깊은 이해를 위해 필수적입니다. 도박에서 부등식에 이르기까지 나머지 부분은 이 일관되고 아름다운 주제가 밝히는 영역의 일부입니다.

모든 강의에는 첫 번째 강의가 있으며, 여기서 아이디어에 대한 미리 보기와 개요가 제시됩니다. 제 1장이 이 역할을 수행합니다.

Tom Cover
Joy Thomas

Palo Alto, California
1990년 6월
<!-- Page 23 -->
# 제2판에 대한 감사

초판이 출간된 이후, 저희는 많은 독자들로부터 피드백, 제안, 그리고 교정 사항을 받을 수 있었습니다. 저희의 노력을 도와주신 모든 분들께 감사드리는 것은 불가능하겠지만, 그중 일부를 나열하고자 합니다. 특히, 이 책을 바탕으로 강의를 하신 모든 교수님들과 해당 강의를 수강한 학생들에게 감사드립니다. 그들을 통해 저희는 같은 내용을 다른 관점에서 바라보는 법을 배울 수 있었습니다.

특히 Andrew Barron, Alon Orlitsky, T. S. Han, Raymond Yeung, Nam Phamdo, Franz Willems, 그리고 Marty Cohn에게 그들의 의견과 제안에 대해 감사드립니다. 수년에 걸쳐 스탠포드 대학의 학생들은 변화를 위한 아이디어와 영감을 제공했습니다. 여기에는 George Gemelos, Navid Hassanpour, Young-Han Kim, Charles Mathis, Styrmir Sigurjonsson, Jon Yard, Michael Baer, Mung Chiang, Suhas Diggavi, Elza Erkip, Paul Fahn, Garud Iyengar, David Julian, Yiannis Kontoyiannis, Amos Lapidoth, Erik Ordentlich, Sandeep Pombra, Jim Roche, Arak Sutivong, Joshua Sweetkind-Singer, 그리고 Assaf Zeevi가 포함됩니다. Denise Murphy는 제2판 준비 과정 동안 많은 지원과 도움을 제공했습니다.

Joy Thomas는 귀중한 의견과 제안을 제공한 IBM 및 Stratify의 동료들의 지원에 감사드립니다. 특히 Peter Franaszek, C. S. Chang, Randy Nelson, Ramesh Gopinath, Pandurang Nayak, John Lamping, Vineet Gupta, 그리고 Ramana Venkata에게 특별한 감사를 표합니다. 특히 Brandon Roy와의 수많은 토론 시간은 책의 일부 논증을 다듬는 데 도움이 되었습니다. 무엇보다도 Joy는 그의 아내 Priya의 지원과 격려 없이는 제2판이 불가능했을 것이라고 인정하고 싶습니다. 그녀는 모든 것을 가치 있게 만듭니다.

Tom Cover는 그의 학생들과 아내 Karen에게 감사드립니다.
<!-- Page 24 -->
.
<!-- Page 25 -->
# 첫 번째 판에 대한 감사의 글

이 책을 완성하는 데 도움을 주신 모든 분들께 감사드립니다. 특히 Aaron Wyner, Toby Berger, Masoud Salehi, Alon Orlitsky, Jim Mazo, Andrew Barron 님께서는 책의 여러 초안에 대해 상세한 의견을 주셨으며, 이는 최종 내용 선정에 큰 도움이 되었습니다. Bob Gallager 님께는 원고 초독과 출판을 격려해주신 점에 대해 감사드립니다. Aaron Wyner 님께서는 Lempel-Ziv algorithm의 수렴에 관한 Ziv와의 새로운 증명을 기증해주셨습니다. 또한 Normam Abramson, Ed van der Meulen, Jack Salz, Raymond Yeung 님께서 제안해주신 수정 사항에 대해서도 감사드립니다.

Amir Dembo, Paul Algoet, Hirosuke Yamamoto, Ben Kawabata, M. Shimizu, Yoichiro Watanabe 님을 포함한 몇몇 주요 방문 연구원 및 연구원들도 기여했습니다. John Gill 님께서 이 교재를 수업에 사용하시면서 조언을 해주신 점도 큰 도움이 되었습니다. Abbas El Gamal 님께서는 귀중한 기여를 하셨으며, 다중 사용자 정보 이론에 대한 연구 단행본을 집필할 계획을 세웠을 때 몇 년 전 이 책을 시작하는 데 도움을 주셨습니다. 또한 이 책이 집필되는 동안 정보 이론 분야의 박사 과정 학생들인 Laura Ekroot, Will Equitz, Don Kimber, Mitchell Trott, Andrew Nobel, Jim Roche, Erik Ordentlich, Elza Erkip, Vittorio Castelli 님께도 감사드립니다. Mitchell Oslick, Chien-Wen Tseng, Michael Morrell 님 또한 교재에 질문과 제안을 기여한 가장 활발한 학생들 중 일부였습니다. Marc Goldberg, Anil Kaul 님께서는 일부 그림 제작을 도와주셨습니다. 마지막으로, Kirsten Goodell, Kathy Adams 님께서 원고 준비의 일부 측면에서 지원과 도움을 주신 점에 대해 감사드립니다.

Joy Thomas 님께서는 또한 Peter Franaszek, Steve Lavenberg, Fred Jelinek, David Nahamoo, Lalit Bahl 님께 이 책의 최종 제작 단계 동안 격려와 지원을 해주신 점에 대해 감사드립니다.
<!-- Page 26 -->
.
<!-- Page 27 -->
# 서론 및 미리보기

정보 이론은 통신 이론의 두 가지 근본적인 질문에 답합니다: 궁극적인 데이터 압축은 무엇인가 (답: 엔트로피 $H$), 그리고 통신의 궁극적인 전송 속도는 무엇인가 (답: 채널 용량 $C$). 이러한 이유로 일부에서는 정보 이론을 통신 이론의 하위 집합으로 간주합니다. 우리는 이것이 훨씬 더 많은 것이라고 주장합니다. 실제로 통계 물리학(열역학), 컴퓨터 과학(콜모고로프 복잡성 또는 알고리즘 복잡성), 통계적 추론(오컴의 면도날: "가장 간단한 설명이 최고다"), 그리고 확률 및 통계(최적 가설 검정 및 추정을 위한 오차 지수)에 근본적인 기여를 합니다.

이 "첫 번째 강의" 장에서는 정보 이론과 그와 자연스럽게 관련된 아이디어들을 앞뒤로 살펴봅니다. 이 주제에 대한 완전한 정의와 연구는 2장에서 시작됩니다. 그림 1.1은 정보 이론과 다른 분야와의 관계를 보여줍니다. 그림에서 알 수 있듯이 정보 이론은 물리학(통계 역학), 수학(확률 이론), 전기 공학(통신 이론), 컴퓨터 과학(알고리즘 복잡성)과 교차합니다. 이제 교차하는 분야에 대해 더 자세히 설명하겠습니다.

전기 공학 (통신 이론). 1940년대 초반에는 오류 확률이 무시할 수 있을 정도로 작으면서 양의 속도로 정보를 보내는 것이 불가능하다고 여겨졌습니다. Shannon은 채널 용량 이하의 모든 통신 속도에 대해 오류 확률을 거의 0으로 만들 수 있음을 증명함으로써 통신 이론 커뮤니티를 놀라게 했습니다. 채널 용량은 채널의 잡음 특성으로부터 간단히 계산될 수 있습니다. Shannon은 또한 음악과 음성과 같은 랜덤 프로세스는 신호가 압축될 수 없는 환원 불가능한 복잡성을 가지고 있다고 주장했습니다. 그는 열역학에서 이 단어가 사용되는 것에 대한 존경심으로 이를 엔트로피라고 명명했으며, 만약 엔트로피가

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 28 -->

그림 1.1. 정보 이론과 다른 분야와의 관계.

그림 1.2. 정보 이론을 통신 이론의 극단점으로 표현.
소스가 채널의 용량보다 작으면 점근적으로 오류 없는 통신을 달성할 수 있습니다.

오늘날 정보 이론은 그림 1.2에 나타난 것처럼 가능한 모든 통신 방식의 집합에서 극단점을 나타냅니다. 데이터 압축 최소값 $I(X ; \hat{X})$는 통신 아이디어 집합의 한 극단에 위치합니다. 모든 데이터 압축 방식은 설명이 필요합니다.
<!-- Page 29 -->
이 최소치와 같거나 더 높은 비율을 갖습니다. 다른 극단에는 채널 용량이라고 알려진 데이터 전송 최대치인 $I(X ; Y)$가 있습니다. 따라서 모든 변조 방식과 데이터 압축 방식은 이러한 한계 사이에 존재합니다.

정보 이론은 또한 이러한 궁극적인 통신 한계를 달성하는 수단을 제시합니다. 그러나 이론적으로 최적인 이러한 통신 방식은 그 자체로 훌륭하지만 계산적으로 비현실적일 수 있습니다. 섀넌의 채널 용량 정리에 대한 증명에서 제안된 무작위 코딩 및 최근접 이웃 디코딩 규칙 대신 간단한 변조 및 복조 방식의 계산적 실현 가능성 때문에 우리는 이러한 방식을 사용합니다. 집적 회로 및 코드 설계의 발전은 섀넌의 이론이 제시한 이점의 일부를 활용할 수 있게 해주었습니다. 터보 코드의 출현으로 계산 실용성이 최종적으로 달성되었습니다. 정보 이론 아이디어의 응용에 대한 좋은 예는 CD 및 DVD의 오류 정정 코드 사용입니다.

정보 이론의 통신 측면에 대한 최근 연구는 네트워크 정보 이론, 즉 간섭 및 잡음이 있는 상태에서 여러 송신자에서 여러 수신자로의 동시 통신 속도에 대한 이론에 집중되어 있습니다. 송신자와 수신자 간의 속도에 대한 일부 절충점은 예상치 못한 것이며, 모두 특정 수학적 단순성을 가지고 있습니다. 그러나 통합된 이론은 아직 발견되지 않았습니다.

컴퓨터 과학 (콜모고로프 복잡성). 콜모고로프, 샤이닌, 솔로모노프는 데이터 문자열의 복잡성을 해당 문자열을 계산하는 가장 짧은 이진 컴퓨터 프로그램의 길이로 정의할 수 있다는 아이디어를 제시했습니다. 따라서 복잡성은 최소 설명 길이입니다. 이 복잡성 정의는 보편적, 즉 컴퓨터 독립적임이 밝혀졌으며 근본적인 중요성을 갖습니다. 따라서 콜모고로프 복잡성은 기술 복잡성 이론의 토대를 마련합니다. 만족스럽게도, 콜모고로프 복잡성 $K$는 시퀀스가 엔트로피 $H$를 갖는 분포에서 무작위로 추출된 경우 섀넌 엔트로피 $H$와 거의 같습니다. 따라서 정보 이론과 콜모고로프 복잡성 간의 연관성은 완벽합니다. 실제로 우리는 콜모고로프 복잡성을 섀넌 엔트로피보다 더 근본적인 것으로 간주합니다. 이는 궁극적인 데이터 압축이며 논리적으로 일관된 추론 절차로 이어집니다.

알고리즘 복잡성과 계산 복잡성 사이에는 만족스러운 상보적 관계가 있습니다. 계산 복잡성(시간 복잡성)과 콜모고로프 복잡성(프로그램 길이 또는 기술 복잡성)을 다음 두 축으로 생각할 수 있습니다.
<!-- Page 30 -->
프로그램 실행 시간과 프로그램 길이입니다. Kolmogorov 복잡성은 두 번째 축을 따라 최소화하는 데 초점을 맞추고, 계산 복잡성은 첫 번째 축을 따라 최소화하는 데 초점을 맞춥니다. 두 가지를 동시에 최소화하는 데는 거의 연구되지 않았습니다.

물리학 (열역학). 통계 역학은 엔트로피와 열역학 제2법칙의 탄생지입니다. 엔트로피는 항상 증가합니다. 다른 것들 중에서 제2법칙은 영구 기관에 대한 모든 주장을 기각할 수 있게 합니다. 우리는 제4장에서 제2법칙을 간략하게 논의합니다.

수학 (확률 이론 및 통계). 정보 이론의 기본 양인 엔트로피, 상대 엔트로피 및 상호 정보는 확률 분포의 함수로 정의됩니다. 차례로, 이들은 무작위 변수의 긴 시퀀스의 동작을 특성화하고 희귀 이벤트의 확률을 추정하고(대규모 편차 이론) 가설 검정에서 최적의 오류 지수를 찾는 것을 가능하게 합니다.

과학 철학 (오컴의 면도날). William of Occam은 "원인은 불필요하게 늘어나서는 안 된다"고 말했거나, 이를 의역하면 "가장 간단한 설명이 최고다"라고 말했습니다. Solomonoff와 Chaitin은 데이터를 설명하는 모든 프로그램의 가중 평균을 취하고 다음에 출력되는 것을 관찰하면 보편적으로 좋은 예측 절차를 얻을 수 있다고 설득력 있게 주장했습니다. 더욱이, 이 추론은 통계에서 다루지 않는 많은 문제에서 작동할 것입니다. 예를 들어, 이 절차는 결국 $\pi$의 후속 숫자를 예측할 것입니다. 이 절차가 앞면이 나올 확률이 0.7인 동전 던지기에 적용될 때도 추론될 것입니다. 주식 시장에 적용될 때, 이 절차는 본질적으로 주식 시장의 모든 "법칙"을 찾아 최적으로 외삽해야 합니다. 원칙적으로, 그러한 절차는 뉴턴의 물리학 법칙을 발견했을 것입니다. 물론, 기존 데이터를 생성하지 못하는 모든 컴퓨터 프로그램을 걸러내는 데 불가능할 정도로 오랜 시간이 걸리기 때문에 이러한 추론은 매우 비실용적입니다. 우리는 지금으로부터 백 년 후에 내일 일어날 일을 예측할 것입니다.

경제학 (투자). 정상적인 주식 시장에 반복적으로 투자하면 부가 기하급수적으로 성장합니다. 부의 성장률은 주식 시장의 엔트로피율의 쌍대입니다. 주식 시장에서의 최적 투자 이론과 정보 이론 사이의 유사성은 놀랍습니다. 우리는 이 쌍대를 탐구하기 위해 투자 이론을 개발합니다.

계산 대 통신. 더 작은 구성 요소로 더 큰 컴퓨터를 구축함에 따라 계산 한계와 통신 한계에 모두 직면하게 됩니다. 계산은 통신에 의해 제한되고 통신은 계산에 의해 제한됩니다. 이들은 서로 얽혀 있으며, 따라서
<!-- Page 31 -->
정보 이론을 통한 통신 이론의 모든 발전은 계산 이론에 직접적인 영향을 미쳐야 합니다.

# 1.1 책 미리보기

정보 이론에서 다루는 초기 문제는 데이터 압축 및 전송 영역에 있었습니다. 그 답은 엔트로피 및 상호 정보와 같은 양이며, 이는 통신 과정을 뒷받침하는 확률 분포의 함수입니다. 몇 가지 정의가 초기 논의에 도움이 될 것입니다. 이 정의들은 2장에서 반복합니다.

확률 질량 함수 $p(x)$를 갖는 확률 변수 $X$의 엔트로피는 다음과 같이 정의됩니다.

$$
H(X)=-\sum_{x} p(x) \log _{2} p(x)
$$

밑이 2인 로그를 사용합니다. 그러면 엔트로피는 비트 단위로 측정됩니다. 엔트로피는 확률 변수의 평균 불확실성에 대한 척도입니다. 이는 확률 변수를 설명하는 데 평균적으로 필요한 비트 수입니다.

예제 1.1.1 32개의 결과에 대해 균일 분포를 갖는 확률 변수를 고려하십시오. 결과를 식별하려면 32개의 서로 다른 값을 갖는 레이블이 필요합니다. 따라서 5비트 문자열이 레이블로 충분합니다.

이 확률 변수의 엔트로피는 다음과 같습니다.

$$
H(X)=-\sum_{i=1}^{32} p(i) \log p(i)=-\sum_{i=1}^{32} \frac{1}{32} \log \frac{1}{32}=\log 32=5 \text { 비트 }
$$

이는 $X$를 설명하는 데 필요한 비트 수와 일치합니다. 이 경우 모든 결과는 동일한 길이의 표현을 갖습니다.

이제 비균일 분포의 예를 고려해 보겠습니다.
예제 1.1.2 여덟 마리의 말이 참가하는 경마가 있다고 가정해 봅시다. 여덟 마리 말의 우승 확률이 $\left(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}\right)$라고 가정합니다. 경마의 엔트로피를 다음과 같이 계산할 수 있습니다.

$$
\begin{aligned}
H(X) & =-\frac{1}{2} \log \frac{1}{2}-\frac{1}{4} \log \frac{1}{4}-\frac{1}{8} \log \frac{1}{8}-\frac{1}{16} \log \frac{1}{16}-4 \frac{1}{64} \log \frac{1}{64} \\
& =2 \text { 비트. }
\end{aligned}
$$
<!-- Page 32 -->
메시지가 경주에서 우승한 말을 나타내기를 원한다고 가정해 봅시다. 한 가지 대안은 우승한 말의 인덱스를 보내는 것입니다. 이 설명은 8마리의 말 모두에 대해 3비트를 필요로 합니다. 그러나 승률은 균일하지 않습니다. 따라서 확률이 높은 말에는 더 짧은 설명을, 확률이 낮은 말에는 더 긴 설명을 사용하여 평균 설명 길이를 더 낮추는 것이 합리적입니다. 예를 들어, 8마리의 말을 나타내기 위해 다음과 같은 비트 문자열 세트를 사용할 수 있습니다: 0, $10, 110, 1110, 111100, 111101, 111110, 111111$. 이 경우 평균 설명 길이는 2비트이며, 균일 코드를 사용한 3비트와 대조됩니다. 이 경우 평균 설명 길이가 엔트로피와 같다는 점에 유의하십시오. 5장에서는 확률 변수에 대한 엔트로피가 확률 변수를 나타내는 데 필요한 평균 비트 수와 "20개의 질문" 게임에서 변수를 식별하는 데 필요한 평균 질문 수에 대한 하한선임을 보여줍니다. 또한 엔트로피 내에서 1비트 이내의 평균 길이를 갖는 표현을 구성하는 방법을 보여줍니다.

정보 이론의 엔트로피 개념은 통계 역학의 엔트로피 개념과 관련이 있습니다. $n$개의 독립적이고 동일하게 분포된(i.i.d.) 확률 변수 시퀀스를 추출하면 "일반적인" 시퀀스의 확률은 약 $2^{-n H(X)}$이고 그러한 일반적인 시퀀스는 약 $2^{n H(X)}$개임을 보여줄 것입니다. 이 속성(점근적 균등 분할 속성(Asymptotic Equipartition Property:AEP)으로 알려짐)은 정보 이론의 많은 증명의 기초입니다. 나중에 엔트로피가 자연스러운 답으로 나타나는 다른 문제(예: 확률 변수를 생성하는 데 필요한 공정한 동전 던지기 횟수)를 제시합니다.

확률 변수의 설명 복잡성 개념은 단일 문자열의 설명 복잡성을 정의하도록 확장될 수 있습니다. 이진 문자열의 Kolmogorov 복잡성은 해당 문자열을 출력하는 가장 짧은 컴퓨터 프로그램의 길이로 정의됩니다. 문자열이 실제로 무작위인 경우 Kolmogorov 복잡성은 엔트로피에 가깝다는 것이 밝혀질 것입니다. Kolmogorov 복잡성은 통계적 추론 및 모델링 문제를 고려하는 자연스러운 프레임워크이며 오컴의 면도날("가장 간단한 설명이 최고다")에 대한 더 명확한 이해로 이어집니다. 1장에서는 Kolmogorov 복잡성의 몇 가지 간단한 속성을 설명합니다.

엔트로피는 단일 확률 변수의 불확실성입니다. 다른 확률 변수에 대한 지식에 조건화된 확률 변수의 엔트로피인 조건부 엔트로피 $H(X \mid Y)$를 정의할 수 있습니다. 다른 확률 변수로 인한 불확실성 감소를 상호 정보라고 합니다. 두 확률 변수 $X$와 $Y$에 대해 이 감소는 상호 정보입니다.
<!-- Page 33 -->
정보

$$
I(X ; Y)=H(X)-H(X \mid Y)=\sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}
$$

상호 정보 $I(X ; Y)$는 두 확률 변수 간의 의존성을 측정하는 척도입니다. 이는 $X$와 $Y$에 대해 대칭이며 항상 음수가 아니고 $X$와 $Y$가 독립일 때만 0과 같습니다.

통신 채널은 출력이 입력에 확률적으로 의존하는 시스템입니다. 이는 입력이 주어졌을 때 출력의 조건부 분포를 결정하는 확률 전이 행렬 $p(y \mid x)$로 특징지어집니다. 입력 $X$와 출력 $Y$를 갖는 통신 채널에 대해, 용량 $C$를 다음과 같이 정의할 수 있습니다.

$$
C=\max _{p(x)} I(X ; Y)
$$

나중에 우리는 용량이 채널을 통해 정보를 전송하고 오류 확률이 거의 0에 가깝게 정보를 출력에서 복구할 수 있는 최대 속도임을 보여줄 것입니다. 몇 가지 예시를 통해 이를 설명하겠습니다.

예시 1.1.3 (잡음 없는 이진 채널) 이 채널의 경우, 이진 입력은 출력에서 정확하게 복제됩니다. 이 채널은 그림 1.3에 나와 있습니다. 여기서 전송된 비트는 오류 없이 수신됩니다. 따라서 각 전송에서 수신자에게 1비트를 안정적으로 보낼 수 있으며, 용량은 1비트입니다. 정보 용량 $C=$ $\max I(X ; Y)=1$ 비트도 계산할 수 있습니다.

예시 1.1.4 (잡음 있는 4개 기호 채널) 그림 1.4에 표시된 채널을 고려하십시오. 이 채널에서 각 입력 문자는 확률 $\frac{1}{2}$로 동일한 문자로 수신되거나 확률 $\frac{1}{2}$로 다음 문자로 수신됩니다. 네 개의 입력 기호를 모두 사용하면 출력만으로는 어떤 입력 기호가 전송되었는지 확실하게 알 수 없습니다. 반면에, 우리가 사용한다면

그림 1.3. 잡음 없는 이진 채널. $C=1$ 비트.
<!-- Page 34 -->

그림 1.4. 노이즈 채널.
입력 중 두 개만 (예: 1과 3) 사용하면 출력에서 어떤 입력 기호가 전송되었는지 즉시 알 수 있습니다. 그러면 이 채널은 예제 1.1.3의 노이즈 없는 채널처럼 작동하며, 이 채널을 통해 오류 없이 전송당 1비트를 보낼 수 있습니다. 이 경우 채널 용량 $C=\max I(X ; Y)$를 계산할 수 있으며, 이는 위 분석과 일치하는 전송당 1비트와 같습니다.

일반적으로 통신 채널은 이 예제의 단순한 구조를 갖지 않으므로, 오류 없이 정보를 전송하기 위해 입력의 부분집합을 항상 식별할 수는 없습니다. 그러나 전송 시퀀스를 고려하면 모든 채널이 이 예제와 유사하게 보이며, 그러면 정보 전송에 사용될 수 있는 입력 시퀀스(코드워드)의 부분집합을 식별할 수 있습니다. 이 코드워드는 각 코드워드와 관련된 가능한 출력 시퀀스의 집합이 거의 분리되도록 채널을 통해 정보를 전송하는 데 사용될 수 있습니다. 그러면 출력 시퀀스를 보고 오류 확률이 매우 낮은 입력 시퀀스를 식별할 수 있습니다.

예제 1.1.5 (이진 대칭 채널) 이것은 노이즈 통신 시스템의 기본 예제입니다. 채널은 그림 1.5에 나와 있습니다.

그림 1.5. 이진 대칭 채널.
<!-- Page 35 -->
채널은 이진 입력을 가지며, 출력은 $1-p$의 확률로 입력과 동일합니다. 반면에 $p$의 확률로 0은 1로 수신되고, 그 반대도 마찬가지입니다. 이 경우 채널의 용량은 전송당 $C=1+p \log p+(1-p) \log (1-p)$ 비트로 계산될 수 있습니다. 그러나 이 용량을 어떻게 달성할 수 있는지는 더 이상 명확하지 않습니다. 그러나 채널을 여러 번 사용하면 채널은 예제 1.1.4의 잡음이 있는 네 개의 기호 채널과 유사해지며, 오류 확률을 임의로 낮게 하여 전송당 $C$ 비트의 속도로 정보를 전송할 수 있습니다.

채널을 통한 정보 통신 속도의 궁극적인 한계는 채널 용량에 의해 주어집니다. 채널 코딩 정리는 긴 블록 길이를 가진 코드를 사용하여 이 한계를 달성할 수 있음을 보여줍니다. 실제 통신 시스템에서는 사용할 수 있는 코드의 복잡성에 제한이 있으므로 용량을 달성하지 못할 수도 있습니다.

상호 정보량은 상대 엔트로피 $D(p \| q)$라고 하는 더 일반적인 양의 특수한 경우로 나타납니다. 이는 두 확률 질량 함수 $p$와 $q$ 사이의 "거리"를 측정하는 것입니다. 다음과 같이 정의됩니다.

$$
D(p \| q)=\sum_{x} p(x) \log \frac{p(x)}{q(x)}
$$

상대 엔트로피는 실제 메트릭은 아니지만 메트릭의 일부 속성을 가지고 있습니다. 특히 항상 음수가 아니며 $p=q$인 경우에만 0입니다. 상대 엔트로피는 분포 $p$와 $q$ 사이의 가설 검정에서 오류 확률의 지수로 나타납니다. 상대 엔트로피는 확률 분포에 대한 기하학을 정의하는 데 사용될 수 있으며, 이를 통해 대규모 편차 이론의 많은 결과를 해석할 수 있습니다.

정보 이론과 주식 시장 투자 이론 사이에는 여러 가지 유사점이 있습니다. 주식 시장은 음수가 아닌 숫자로 구성된 랜덤 벡터 $\mathbf{X}$로 정의되며, 이 숫자는 하루의 끝에 주식 가격과 하루의 시작에 주식 가격의 비율과 같습니다. 분포 $F(\mathbf{x})$를 가진 주식 시장에 대해 두 배율 $W$를 다음과 같이 정의할 수 있습니다.

$$
W=\max _{\mathbf{b}: b_{i} \geq 0, \sum b_{i}=1} \int \log \mathbf{b}^{t} \mathbf{x} d F(\mathbf{x})
$$

두 배율은 부의 성장에 있어 최대 점근 지수입니다. 두 배율은 엔트로피의 속성과 유사한 여러 속성을 가지고 있습니다. 이러한 속성 중 일부는 16장에서 탐구합니다.
<!-- Page 36 -->
$H, I, C, D, K, W$와 같은 양들은 다음과 같은 영역에서 자연스럽게 발생합니다.

- 데이터 압축. 랜덤 변수의 엔트로피 $H$는 해당 랜덤 변수의 가장 짧은 설명의 평균 길이에 대한 하한입니다. 우리는 엔트로피에 1비트 이내의 평균 길이를 갖는 설명을 구성할 수 있습니다. 만약 우리가 소스를 완벽하게 복구하는 제약 조건을 완화한다면, 왜곡 $D$까지 소스를 설명하는 데 필요한 통신 속도는 얼마인지 질문할 수 있습니다. 그리고 채널 용량은 이 소스를 채널을 통해 전송하고 왜곡이 $D$ 이하로 재구성하는 것을 가능하게 하기에 충분한가요? 이것이 바로 속도 왜곡 이론의 주제입니다.

랜덤하지 않은 객체에 대한 가장 짧은 설명의 개념을 형식화하려고 할 때, 우리는 콜모고로프 복잡도 $K$의 정의에 도달합니다. 나중에 우리는 콜모고로프 복잡도가 보편적이며 가장 짧은 설명 이론에 대한 직관적인 요구 사항 중 많은 부분을 만족한다는 것을 보여줄 것입니다.

- 데이터 전송. 우리는 수신자가 작은 오류 확률로 메시지를 디코딩할 수 있도록 정보를 전송하는 문제를 고려합니다. 본질적으로 우리는 채널의 입력 심볼 시퀀스인 코드가 잡음이 있는 버전(채널 출력에서 사용 가능)이 구별될 수 있다는 의미에서 서로 멀리 떨어져 있기를 바랍니다. 이것은 고차원 공간에서의 구 충족과 동등합니다. 임의의 코드 집합에 대해 수신자가 오류를 범할 확률(즉, 어떤 코드가 전송되었는지에 대해 잘못된 결정을 내릴 확률)을 계산할 수 있습니다. 그러나 대부분의 경우 이 계산은 지루합니다.

랜덤하게 생성된 코드를 사용하여, Shannon은 채널 용량 $C$보다 낮은 속도로 임의로 낮은 오류 확률로 정보를 전송할 수 있음을 보여주었습니다. 랜덤하게 생성된 코드의 아이디어는 매우 특이합니다. 이는 매우 어려운 문제에 대한 간단한 분석의 기초를 제공합니다. 증명에서 핵심 아이디어 중 하나는 전형적인 시퀀스의 개념입니다. 채널 용량 $C$는 구별 가능한 입력 신호 수의 로그입니다.

- 네트워크 정보 이론. 앞에서 언급한 각 주제는 단일 소스 또는 단일 채널을 포함합니다. 여러 소스를 각각 압축한 다음 압축된 설명을 소스의 공동 재구성에 넣으려면 어떻게 해야 할까요? 이 문제는 Slepian-Wolf 정리에 의해 해결됩니다. 또는 여러 송신자가 공통 수신자에게 독립적으로 정보를 보내는 경우 어떻게 해야 할까요? 이 채널의 채널 용량은 얼마인가요? 이것은 Liao와 Ahlswede가 해결한 다중 접속 채널입니다. 또는 송신자가 하나 있고 여러 송신자가 있는 경우 어떻게 해야 할까요?
<!-- Page 37 -->
수신자에게 (아마도 다른) 정보를 동시에 전달하고자 할 때 어떻게 해야 합니까? 이것이 브로드캐스트 채널입니다. 마지막으로, 간섭과 잡음이 있는 환경에서 임의의 수의 송신자와 수신자가 있을 경우 어떻게 해야 합니까? 다양한 송신자로부터 수신자까지의 달성 가능한 속도의 용량 영역은 무엇입니까? 이것이 일반적인 네트워크 정보 이론 문제입니다. 앞서 언급한 모든 문제는 다중 사용자 또는 네트워크 정보 이론의 일반적인 영역에 속합니다. 네트워크에 대한 포괄적인 이론에 대한 희망은 현재 연구 기법을 넘어서는 것일 수 있지만, 모든 답이 상호 정보와 상대 엔트로피의 정교한 형태만을 포함할 것이라는 희망은 여전히 있습니다.

- 에르고딕 이론. 점근적 균등 분할 정리는 에르고딕 과정의 대부분의 표본 $n$-시퀀스는 확률이 약 $2^{-n H}$이고 그러한 전형적인 시퀀스는 약 $2^{n H}$개라고 말합니다.
- 가설 검정. 상대 엔트로피 $D$는 두 분포 간의 가설 검정에서 오류 확률의 지수로 나타납니다. 이는 분포 간의 거리 측정에 자연스러운 척도입니다.
- 통계 역학. 엔트로피 $H$는 통계 역학에서 물리 시스템의 불확실성 또는 무질서의 척도로 나타납니다. 대략적으로 말하면, 엔트로피는 물리 시스템이 구성될 수 있는 방법의 수의 로그입니다. 열역학 제2법칙은 닫힌 시스템의 엔트로피가 감소할 수 없다고 말합니다. 나중에 우리는 제2법칙에 대한 몇 가지 해석을 제공할 것입니다.
- 양자 역학. 여기서 폰 노이만 엔트로피 $S=\operatorname{tr}(\rho \ln \rho)=$ $\sum_{i} \lambda_{i} \log \lambda_{i}$는 고전적인 Shannon-Boltzmann 엔트로피 $H=-\sum_{i} p_{i} \log p_{i}$의 역할을 합니다. 그런 다음 데이터 압축 및 채널 용량의 양자 역학적 버전을 찾을 수 있습니다.
- 추론. 우리는 Kolmogorov 복잡성 $K$의 개념을 사용하여 데이터의 가장 짧은 설명을 찾고 이를 모델로 사용하여 다음에 올 것을 예측할 수 있습니다. 불확실성 또는 엔트로피를 최대화하는 모델은 추론에 대한 최대 엔트로피 접근 방식을 제공합니다.
- 도박 및 투자. 부의 성장률에서 최적의 지수는 두 배 증가율 $W$로 주어집니다. 균등한 배당률을 가진 경마의 경우, 두 배 증가율 $W$와 엔트로피 $H$의 합은 일정합니다. 부가 정보로 인한 두 배 증가율의 증가는 경마와 부가 정보 간의 상호 정보 $I$와 같습니다. 주식 시장 투자에도 유사한 결과가 적용됩니다.
- 확률 이론. 점근적 균등 분할 속성(AEP)은 대부분의 시퀀스가 표본 엔트로피가 $H$에 가까운 전형적인 시퀀스임을 보여줍니다. 따라서 주의는 이러한 약 $2^{n H}$개의 전형적인 시퀀스로 제한될 수 있습니다. 큰 편차 이론에서
<!-- Page 38 -->
집합의 확률은 $2^{-n D}$로 근사되며, 여기서 $D$는 집합 내 가장 가까운 요소와 실제 분포 간의 상대 엔트로피 거리입니다.

- 복잡도 이론. 콜모고로프 복잡도 $K$는 객체의 기술적 복잡도를 측정하는 척도입니다. 이는 계산에 필요한 시간 또는 공간을 측정하는 계산 복잡도와 관련이 있지만 다릅니다.

엔트로피 및 상대 엔트로피와 같은 정보 이론적 양은 통신 및 통계의 근본적인 질문에 대한 답변으로 계속해서 나타납니다. 이러한 질문을 연구하기 전에 답변의 몇 가지 속성을 연구할 것입니다. 2장에서는 엔트로피, 상대 엔트로피 및 상호 정보의 정의와 기본 속성으로 시작합니다.
<!-- Page 39 -->
# CHAPTER 2

## 엔트로피, 상대 엔트로피 및 상호 정보량

이 장에서는 이후 이론 전개를 위해 필요한 대부분의 기본 정의를 소개합니다. 나중에 유용하게 사용될 것이라는 믿음을 가지고, 이들의 관계와 해석을 탐구하는 것은 참을 수 없습니다. 엔트로피와 상호 정보량을 정의한 후, 연쇄 규칙, 상호 정보량의 비음수성, 데이터 처리 부등식을 확립하고, 충분 통계량과 파노 부등식을 검토하여 이러한 정의를 설명합니다.

정보라는 개념은 단일 정의로 완전히 포착하기에는 너무 광범위합니다. 그러나 임의의 확률 분포에 대해, 정보량의 척도로서 직관적인 개념과 일치하는 많은 속성을 가진 엔트로피라는 양을 정의합니다. 이 개념은 한 확률 변수가 다른 확률 변수에 대해 포함하는 정보량의 척도인 상호 정보량을 정의하도록 확장됩니다. 엔트로피는 확률 변수의 자기 정보량이 됩니다. 상호 정보량은 상대 엔트로피라고 불리는 더 일반적인 양의 특수한 경우로, 두 확률 분포 간의 거리 측정값입니다. 이 모든 양들은 밀접하게 관련되어 있으며, 이 장에서 몇 가지를 유도할 간단한 속성을 공유합니다.

이후 장에서는 통신, 통계, 복잡성 및 도박과 관련된 여러 질문에 대한 자연스러운 답변으로 이러한 양들이 어떻게 발생하는지 보여줄 것입니다. 그것이 이러한 정의의 가치를 궁극적으로 시험하는 것이 될 것입니다.

### 2.1 엔트로피

먼저 확률 변수의 불확실성 척도인 엔트로피의 개념을 소개합니다. $X$를 알파벳 $\mathcal{X}$와 확률 질량 함수 $p(x)=\operatorname{Pr}\{X=x\}, x \in \mathcal{X}$를 갖는 이산 확률 변수라고 합시다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 40 -->
$p(x)$ 대신 편의상 확률 질량 함수를 $p(x)$로 표기합니다. 따라서 $p(x)$와 $p(y)$는 두 개의 다른 확률 변수를 나타내며, 실제로는 각각 $p_{X}(x)$와 $p_{Y}(y)$인 서로 다른 확률 질량 함수입니다.

정의 이산 확률 변수 $X$의 엔트로피 $H(X)$는 다음과 같이 정의됩니다.

$$
H(X)=-\sum_{x \in \mathcal{X}} p(x) \log p(x)
$$

위의 양을 $H(p)$로 표기하기도 합니다. $\log$는 밑이 2인 로그이며, 엔트로피는 비트(bit) 단위로 표현됩니다. 예를 들어, 공정한 동전 던지기의 엔트로피는 1 비트입니다. $0 \log 0=0$이라는 관례를 사용하며, 이는 $x \rightarrow 0$일 때 $x \log x \rightarrow 0$이므로 연속성에 의해 쉽게 정당화됩니다. 확률이 0인 항을 더해도 엔트로피는 변하지 않습니다.

로그의 밑이 $b$인 경우, 엔트로피를 $H_{b}(X)$로 표기합니다. 로그의 밑이 $e$인 경우, 엔트로피는 네트(nat) 단위로 측정됩니다. 별도로 명시하지 않는 한, 모든 로그는 밑이 2인 로그를 사용하며, 따라서 모든 엔트로피는 비트 단위로 측정됩니다. 엔트로피는 $X$의 분포에 대한 함수임을 유의하십시오. 엔트로피는 확률 변수 $X$가 취하는 실제 값에 의존하는 것이 아니라 확률에만 의존합니다.

기댓값은 $E$로 표기합니다. 따라서 $X \sim p(x)$이면, 확률 변수 $g(X)$의 기댓값은 다음과 같이 표기합니다.

$$
E_{p} g(X)=\sum_{x \in \mathcal{X}} g(x) p(x)
$$

또는 확률 질량 함수가 문맥에서 이해되는 경우 $E g(X)$로 더 간단하게 표기합니다. 우리는 $g(X)=\log \frac{1}{p(X)}$일 때, $p(x)$ 하에서의 $g(X)$의 기묘하게 자기 참조적인 기댓값에 특별한 관심을 가질 것입니다.

비고 $X$의 엔트로피는 확률 질량 함수 $p(x)$에 따라 추출된 확률 변수 $\log \frac{1}{p(X)}$의 기댓값으로 해석될 수도 있습니다. 따라서,

$$
H(X)=E_{p} \log \frac{1}{p(X)}
$$

엔트로피의 이 정의는 열역학에서의 엔트로피 정의와 관련이 있으며, 일부 연결고리는 나중에 탐구됩니다. 확률 변수의 엔트로피가 만족해야 하는 특정 속성을 정의함으로써 엔트로피의 정의를 공리적으로 유도하는 것이 가능합니다. 이 접근 방식은 문제 2.46에 설명되어 있습니다. 우리는 공리적 접근 방식을 사용하지 않습니다.
<!-- Page 41 -->
엔트로피의 정의를 정당화하는 대신, "확률 변수의 가장 짧은 설명의 평균 길이는 얼마인가?"와 같은 여러 자연스러운 질문에 대한 답으로 엔트로피가 도출됨을 보여줍니다. 먼저, 정의의 즉각적인 결과 몇 가지를 도출합니다.

정리 2.1.1 $H(X) \geq 0$.
증명: $\quad 0 \leq p(x) \leq 1$ 이므로 $\log \frac{1}{p(x)} \geq 0$ 입니다.
정리 2.1.2 $H_{b}(X)=\left(\log _{b} a\right) H_{a}(X)$.
증명: $\log _{b} p=\log _{b} a \log _{a} p$.
엔트로피의 두 번째 속성은 정의에서 로그의 밑을 변경할 수 있게 합니다. 엔트로피는 적절한 인수를 곱하여 한 밑에서 다른 밑으로 변경될 수 있습니다.

예제 2.1.1 다음을 고려하십시오.

$$
X=\left\{\begin{array}{ll}
1 & \text { 확률 } p \\
0 & \text { 확률 } 1-p
\end{array}\right.
$$

그러면

$$
H(X)=-p \log p-(1-p) \log (1-p) \stackrel{\text { def }}{=} H(p)
$$

특히, $p=\frac{1}{2}$일 때 $H(X)=1$ bit 입니다. 함수 $H(p)$의 그래프는 그림 2.1에 나와 있습니다. 이 그림은 엔트로피의 몇 가지 기본 속성을 보여줍니다. 엔트로피는 분포의 오목 함수이며 $p=0$ 또는 1일 때 0이 됩니다. 이는 $p=0$ 또는 1일 때 변수가 무작위가 아니며 불확실성이 없기 때문에 합리적입니다. 마찬가지로, 불확실성은 $p=\frac{1}{2}$일 때 최대이며, 이는 엔트로피의 최대값과도 일치합니다.

예제 2.1.2 다음을 고려하십시오.

$$
X=\left\{\begin{array}{ll}
a & \text { 확률 } \frac{1}{2} \\
b & \text { 확률 } \frac{1}{4} \\
c & \text { 확률 } \frac{1}{8} \\
d & \text { 확률 } \frac{1}{8}
\end{array}\right.
$$

$X$의 엔트로피는 다음과 같습니다.

$$
H(X)=-\frac{1}{2} \log \frac{1}{2}-\frac{1}{4} \log \frac{1}{4}-\frac{1}{8} \log \frac{1}{8}-\frac{1}{8} \log \frac{1}{8}=\frac{7}{4} \text { bits. }
$$
<!-- Page 42 -->

그림 2.1. $H(p)$ 대 $p$.

최소한의 이진 질문으로 $X$의 값을 결정하고자 한다고 가정해 봅시다. 효율적인 첫 번째 질문은 "Is $X=a$ ?"입니다. 이 질문은 확률을 절반으로 나눕니다. 첫 번째 질문에 대한 답이 '아니오'라면, 두 번째 질문은 "Is $X=b$ ?"가 될 수 있습니다. 세 번째 질문은 "Is $X=c$ ?"가 될 수 있습니다. 결과적인 이진 질문의 기대값은 1.75입니다. 이것이 $X$의 값을 결정하는 데 필요한 최소 이진 질문 기대값임이 밝혀졌습니다. 5장에서는 $X$를 결정하는 데 필요한 최소 이진 질문 기대값이 $H(X)$와 $H(X)+1$ 사이에 있음을 보여줄 것입니다.

# 2.2 결합 엔트로피 및 조건부 엔트로피

2.1절에서 단일 확률 변수의 엔트로피를 정의했습니다. 이제 이 정의를 두 확률 변수의 쌍으로 확장합니다. 이 정의에는 새로운 것이 없습니다. 왜냐하면 $(X, Y)$는 단일 벡터 값 확률 변수로 간주될 수 있기 때문입니다.

정의 결합 분포 $p(x, y)$를 갖는 두 이산 확률 변수 $(X, Y)$의 결합 엔트로피 $H(X, Y)$는 다음과 같이 정의됩니다.

$$
H(X, Y)=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y)
$$

<!-- Page 43 -->
이는 다음과 같이 표현될 수도 있습니다.

$$
H(X, Y)=-E \log p(X, Y)
$$

또한, 조건부 확률 분포의 엔트로피에 대한 기댓값을 조건부 확률 변수에 대해 평균한 값으로, 두 확률 변수에 대한 조건부 엔트로피를 정의합니다.

정의: $(X, Y) \sim p(x, y)$일 때, 조건부 엔트로피 $H(Y \mid X)$는 다음과 같이 정의됩니다.

$$
\begin{aligned}
H(Y \mid X) & =\sum_{x \in \mathcal{X}} p(x) H(Y \mid X=x) \\
& =-\sum_{x \in \mathcal{X}} p(x) \sum_{y \in \mathcal{Y}} p(y \mid x) \log p(y \mid x) \\
& =-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(y \mid x) \\
& =-E \log p(Y \mid X)
\end{aligned}
$$

결합 엔트로피와 조건부 엔트로피 정의의 자연스러움은 두 확률 변수의 쌍에 대한 엔트로피가 한 변수의 엔트로피에 다른 변수의 조건부 엔트로피를 더한 것과 같다는 사실로 나타납니다. 이는 다음 정리에 의해 증명됩니다.

정리 2.2.1 (연쇄 법칙)

$$
H(X, Y)=H(X)+H(Y \mid X)
$$

# 증명

$$
\begin{aligned}
H(X, Y) & =-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y) \\
& =-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x) p(y \mid x) \\
& =-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x)-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(y \mid x) \\
& =-\sum_{x \in \mathcal{X}} p(x) \log p(x)-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(y \mid x) \\
& =H(X)+H(Y \mid X)
\end{aligned}
$$
<!-- Page 44 -->
동등하게, 다음과 같이 쓸 수 있습니다.

$$
\log p(X, Y)=\log p(X)+\log p(Y \mid X)
$$

양변에 기댓값을 취하면 정리를 얻을 수 있습니다.

# 따름정리

$$
H(X, Y \mid Z)=H(X \mid Z)+H(Y \mid X, Z)
$$

증명: 증명은 정리와 동일한 방식으로 진행됩니다.
예제 2.2.1 $(X, Y)$가 다음과 같은 결합 분포를 가진다고 가정합니다.

|  | 1 | 2 | 3 | 4 |
| :--: | :--: | :--: | :--: | :--: |
| 1 | $\frac{1}{8}$ | $\frac{1}{16}$ | $\frac{1}{32}$ | $\frac{1}{32}$ |
| 2 | $\frac{1}{16}$ | $\frac{1}{8}$ | $\frac{1}{32}$ | $\frac{1}{32}$ |
| 3 | $\frac{1}{16}$ | $\frac{1}{16}$ | $\frac{1}{16}$ | $\frac{1}{16}$ |
| 4 | $\frac{1}{4}$ | 0 | 0 | 0 |

$X$의 주변 분포는 $\left(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8}\right)$이고 $Y$의 주변 분포는 $\left(\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}\right)$이므로, $H(X)=\frac{7}{4}$ 비트이고 $H(Y)=2$ 비트입니다. 또한,

$$
\begin{aligned}
H(X \mid Y)= & \sum_{i=1}^{4} p(Y=i) H(X \mid Y=i) \\
= & \frac{1}{4} H\left(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8}\right)+\frac{1}{4} H\left(\frac{1}{4}, \frac{1}{2}, \frac{1}{8}, \frac{1}{8}\right) \\
& \quad+\frac{1}{4} H\left(\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}\right)+\frac{1}{4} H(1,0,0,0) \\
= & \frac{1}{4} \times \frac{7}{4}+\frac{1}{4} \times \frac{7}{4}+\frac{1}{4} \times 2+\frac{1}{4} \times 0 \\
= & \frac{11}{8} \text { 비트. }
\end{aligned}
$$

마찬가지로, $H(Y \mid X)=\frac{13}{8}$ 비트이고 $H(X, Y)=\frac{27}{8}$ 비트입니다.
비고 $H(Y \mid X) \neq H(X \mid Y)$임을 주목하십시오. 그러나 $H(X)-H(X \mid Y)=$ $H(Y)-H(Y \mid X)$이며, 이 속성은 나중에 활용됩니다.
<!-- Page 45 -->
# 2.3 상대 엔트로피와 상호 정보량

확률 변수의 엔트로피는 해당 확률 변수의 불확실성을 측정하는 척도이며, 확률 변수를 설명하는 데 평균적으로 필요한 정보의 양을 측정하는 척도입니다. 이 절에서는 두 가지 관련 개념인 상대 엔트로피와 상호 정보량을 소개합니다.

상대 엔트로피는 두 분포 간의 거리를 측정하는 척도입니다. 통계학에서 이는 가능도 비율의 로그에 대한 기댓값으로 나타납니다. 상대 엔트로피 $D(p \| q)$는 실제 분포가 $p$일 때 분포가 $q$라고 가정하는 비효율성을 측정합니다. 예를 들어, 확률 변수의 실제 분포 $p$를 알고 있다면 평균 설명 길이 $H(p)$를 갖는 코드를 구성할 수 있습니다. 대신 분포 $q$에 대한 코드를 사용하면 확률 변수를 설명하는 데 평균적으로 $H(p)+D(p \| q)$ 비트가 필요합니다.

정의 두 확률 질량 함수 $p(x)$와 $q(x)$ 사이의 상대 엔트로피 또는 Kullback-Leibler 거리(Kullback-Leibler distance)는 다음과 같이 정의됩니다.

$$
\begin{aligned}
D(p \| q) & =\sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} \\
& =E_{p} \log \frac{p(X)}{q(X)}
\end{aligned}
$$

위 정의에서 $0 \log \frac{0}{0}=0$이라는 관례와 $0 \log \frac{0}{q}=0$ 및 $p \log \frac{p}{0}=\infty$라는 관례(연속성 논증에 기반)를 사용합니다. 따라서 $p(x)>0$이고 $q(x)=0$인 기호 $x \in \mathcal{X}$가 존재하면 $D(p \| q)=\infty$입니다.

곧 상대 엔트로피가 항상 음수가 아니며 $p=q$일 때만 0이 된다는 것을 보여줄 것입니다. 그러나 대칭적이지 않고 삼각 부등식을 만족하지 않기 때문에 분포 간의 실제 거리는 아닙니다. 그럼에도 불구하고 상대 엔트로피를 분포 간의 "거리"로 생각하는 것이 유용할 때가 많습니다.

이제 상호 정보량(mutual information)을 소개합니다. 이는 한 확률 변수가 다른 확률 변수에 대해 포함하는 정보의 양을 측정하는 척도입니다. 이는 다른 변수를 앎으로써 한 확률 변수의 불확실성이 감소하는 정도입니다.

정의 두 확률 변수 $X$와 $Y$가 결합 확률 질량 함수 $p(x, y)$와 주변 확률 질량 함수 $p(x)$ 및 $p(y)$를 갖는다고 가정합니다. 상호 정보량 $I(X ; Y)$는 다음과 같은 상대 엔트로피입니다.
<!-- Page 46 -->
결합 분포와 곱 분포 $p(x) p(y)$ :

$$
\begin{aligned}
I(X ; Y) & =\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x) p(y)} \\
& =D(p(x, y) \| p(x) p(y)) \\
& =E_{p(x, y)} \log \frac{p(X, Y)}{p(X) p(Y)}
\end{aligned}
$$

8장에서는 이 정의를 연속 확률 변수로 일반화하고, (8.54)에서는 이산 및 연속 확률 변수의 혼합일 수 있는 일반 확률 변수로 확장합니다.

예제 2.3.1 $\mathcal{X}=\{0,1\}$이고 $\mathcal{X}$에 대한 두 분포 $p$와 $q$를 고려하십시오. $p(0)=1-r, p(1)=r$이고 $q(0)=1-s, q(1)=s$라고 하면,

$$
D(p \| q)=(1-r) \log \frac{1-r}{1-s}+r \log \frac{r}{s}
$$

그리고

$$
D(q \| p)=(1-s) \log \frac{1-s}{1-r}+s \log \frac{s}{r}
$$

$r=s$이면 $D(p \| q)=D(q \| p)=0$입니다. $r=\frac{1}{2}, s=\frac{1}{4}$이면 다음과 같이 계산할 수 있습니다.

$$
D(p \| q)=\frac{1}{2} \log \frac{\frac{1}{2}}{\frac{3}{4}}+\frac{1}{2} \log \frac{\frac{1}{2}}{\frac{1}{4}}=1-\frac{1}{2} \log 3=0.2075 \text { bit, }
$$

반면에

$$
D(q \| p)=\frac{3}{4} \log \frac{\frac{3}{4}}{\frac{1}{2}}+\frac{1}{4} \log \frac{\frac{1}{4}}{\frac{1}{2}}=\frac{3}{4} \log 3-1=0.1887 \text { bit. }
$$

일반적으로 $D(p \| q) \neq D(q \| p)$임을 유의하십시오.

# 2.4 엔트로피와 상호 정보량의 관계

상호 정보량 $I(X ; Y)$의 정의를 다음과 같이 다시 쓸 수 있습니다.

$$
I(X ; Y)=\sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}
$$
<!-- Page 47 -->
$$
\begin{aligned}
& =\sum_{x, y} p(x, y) \log \frac{p(x \mid y)}{p(x)} \\
& =-\sum_{x, y} p(x, y) \log p(x)+\sum_{x, y} p(x, y) \log p(x \mid y) \\
& =-\sum_{x} p(x) \log p(x)-\left(-\sum_{x, y} p(x, y) \log p(x \mid y)\right) \\
& =H(X)-H(X \mid Y)
\end{aligned}
$$

따라서, 상호 정보량 $I(X ; Y)$는 $Y$를 앎으로써 $X$의 불확실성이 감소하는 정도입니다.

대칭성에 의해 다음도 성립합니다.

$$
I(X ; Y)=H(Y)-H(Y \mid X)
$$

따라서, $X$는 $Y$에 대해 $Y$가 $X$에 대해 말하는 것만큼 많은 것을 말합니다.
2.2절에서 보였듯이 $H(X, Y)=H(X)+H(Y \mid X)$이므로, 다음을 얻습니다.

$$
I(X ; Y)=H(X)+H(Y)-H(X, Y)
$$

마지막으로 다음을 주목합니다.

$$
I(X ; X)=H(X)-H(X \mid X)=H(X)
$$

따라서, 확률 변수와 자기 자신의 상호 정보량은 해당 확률 변수의 엔트로피입니다. 이것이 엔트로피가 때때로 자기 정보량이라고 불리는 이유입니다.

이 결과들을 종합하면 다음 정리를 얻습니다.
정리 2.4.1 (상호 정보량과 엔트로피)

$$
\begin{aligned}
& I(X ; Y)=H(X)-H(X \mid Y) \\
& I(X ; Y)=H(Y)-H(Y \mid X) \\
& I(X ; Y)=H(X)+H(Y)-H(X, Y) \\
& I(X ; Y)=I(Y ; X) \\
& I(X ; X)=H(X)
\end{aligned}
$$
<!-- Page 48 -->

그림 2.2. 엔트로피와 상호 정보량 간의 관계.
$H(X), H(Y), H(X, Y), H(X \mid Y), H(Y \mid X)$, 그리고 $I(X ; Y)$ 간의 관계는 벤 다이어그램(그림 2.2)으로 표현됩니다. 상호 정보량 $I(X ; Y)$가 $X$의 정보와 $Y$의 정보의 교집합에 해당함을 주목하십시오.

예제 2.4.1 예제 2.2.1의 결합 분포에 대해, 상호 정보량 $I(X ; Y)=H(X)-H(X \mid Y)=H(Y)-$ $H(Y \mid X)=0.375$ bit를 계산하는 것은 쉽습니다.

# 2.5 엔트로피, 상대 엔트로피, 상호 정보량에 대한 연쇄 법칙

이제 확률 변수들의 모음의 엔트로피가 조건부 엔트로피들의 합임을 보이겠습니다.

정리 2.5.1 (엔트로피에 대한 연쇄 법칙) $X_{1}, X_{2}, \ldots, X_{n}$이 $p\left(x_{1}, x_{2}, \ldots, x_{n}\right)$에 따라 추출된다고 가정합니다. 그러면

$$
H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right)
$$

증명: 엔트로피에 대한 두 변수 확장 규칙을 반복적으로 적용하면 다음과 같습니다.

$$
\begin{aligned}
H\left(X_{1}, X_{2}\right) & =H\left(X_{1}\right)+H\left(X_{2} \mid X_{1}\right) \\
H\left(X_{1}, X_{2}, X_{3}\right) & =H\left(X_{1}\right)+H\left(X_{2}, X_{3} \mid X_{1}\right)
\end{aligned}
$$
<!-- Page 49 -->
$$
\begin{aligned}
& =H\left(X_{1}\right)+H\left(X_{2} \mid X_{1}\right)+H\left(X_{3} \mid X_{2}, X_{1}\right) \\
& \vdots \\
H\left(X_{1}, X_{2}, \ldots, X_{n}\right) & =H\left(X_{1}\right)+H\left(X_{2} \mid X_{1}\right)+\cdots+H\left(X_{n} \mid X_{n-1}, \ldots, X_{1}\right) \\
& =\sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right)
\end{aligned}
$$

대안 증명: $p\left(x_{1}, \ldots, x_{n}\right)=\prod_{i=1}^{n} p\left(x_{i} \mid x_{i-1}, \ldots, x_{1}\right)$로 표기하고 다음을 평가합니다.

$$
\begin{aligned}
& H\left(X_{1}, X_{2}, \ldots, X_{n}\right) \\
& \quad=-\sum_{x_{1}, x_{2}, \ldots, x_{n}} p\left(x_{1}, x_{2}, \ldots, x_{n}\right) \log p\left(x_{1}, x_{2}, \ldots, x_{n}\right) \\
& \quad=-\sum_{x_{1}, x_{2}, \ldots, x_{n}} p\left(x_{1}, x_{2}, \ldots, x_{n}\right) \log \prod_{i=1}^{n} p\left(x_{i} \mid x_{i-1}, \ldots, x_{1}\right) \\
& \quad=-\sum_{x_{1}, x_{2}, \ldots, x_{n}} \sum_{i=1}^{n} p\left(x_{1}, x_{2}, \ldots, x_{n}\right) \log p\left(x_{i} \mid x_{i-1}, \ldots, x_{1}\right) \\
& \quad=-\sum_{i=1}^{n} \sum_{x_{1}, x_{2}, \ldots, x_{n}} p\left(x_{1}, x_{2}, \ldots, x_{n}\right) \log p\left(x_{i} \mid x_{i-1}, \ldots, x_{1}\right) \\
& \quad=-\sum_{i=1}^{n} \sum_{x_{1}, x_{2}, \ldots, x_{i}} p\left(x_{1}, x_{2}, \ldots, x_{i}\right) \log p\left(x_{i} \mid x_{i-1}, \ldots, x_{1}\right) \\
& \quad=\sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right)
\end{aligned}
$$

이제 조건부 상호 정보량을 $Z$가 주어졌을 때 $Y$의 지식으로 인한 $X$의 불확실성 감소로 정의합니다.

정의 $Z$가 주어졌을 때 확률 변수 $X$와 $Y$의 조건부 상호 정보량은 다음과 같이 정의됩니다.

$$
\begin{aligned}
I(X ; Y \mid Z) & =H(X \mid Z)-H(X \mid Y, Z) \\
& =E_{p(x, y, z)} \log \frac{p(X, Y \mid Z)}{p(X \mid Z) p(Y \mid Z)}
\end{aligned}
$$

상호 정보량은 연쇄 법칙도 만족합니다.
<!-- Page 50 -->
정리 2.5.2 (정보에 대한 연쇄 법칙)

$$
I\left(X_{1}, X_{2}, \ldots, X_{n} ; Y\right)=\sum_{i=1}^{n} I\left(X_{i} ; Y \mid X_{i-1}, X_{i-2}, \ldots, X_{1}\right)
$$

# 증명

$$
\begin{aligned}
& I\left(X_{1}, X_{2}, \ldots, X_{n} ; Y\right) \\
& \quad=H\left(X_{1}, X_{2}, \ldots, X_{n}\right)-H\left(X_{1}, X_{2}, \ldots, X_{n} \mid Y\right) \\
& \quad=\sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right)-\sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}, Y\right) \\
& \quad=\sum_{i=1}^{n} I\left(X_{i} ; Y \mid X_{1}, X_{2}, \ldots, X_{i-1}\right)
\end{aligned}
$$

상대 entropy의 조건부 버전을 정의합니다.
정의 결합 확률 질량 함수 $p(x, y)$와 $q(x, y)$에 대해, 조건부 상대 entropy $D(p(y \mid x) \| q(y \mid x))$는 조건부 확률 질량 함수 $p(y \mid x)$와 $q(y \mid x)$ 사이의 상대 entropy를 확률 질량 함수 $p(x)$에 대해 평균한 것입니다. 더 정확하게는,

$$
\begin{aligned}
D(p(y \mid x) \| q(y \mid x)) & =\sum_{x} p(x) \sum_{y} p(y \mid x) \log \frac{p(y \mid x)}{q(y \mid x)} \\
& =E_{p(x, y)} \log \frac{p(Y \mid X)}{q(Y \mid X)}
\end{aligned}
$$

조건부 상대 entropy의 표기법은 조건 변수 $p(x)$의 분포를 언급하지 않기 때문에 명시적이지 않습니다. 그러나 일반적으로 문맥에서 이해됩니다.

두 결합 분포 간의 상대 entropy는 상대 entropy와 조건부 상대 entropy의 합으로 확장될 수 있습니다. 상대 entropy에 대한 연쇄 법칙은 제2법칙 thermodynamics를 증명하기 위해 섹션 4.4에서 사용됩니다.

정리 2.5.3 (상대 entropy에 대한 연쇄 법칙)

$$
D(p(x, y) \| q(x, y))=D(p(x) \| q(x))+D(p(y \mid x) \| q(y \mid x))
$$
<!-- Page 51 -->
# 증명

$$
\begin{aligned}
& D(p(x, y) \| q(x, y)) \\
& \quad=\sum_{x} \sum_{y} p(x, y) \log \frac{p(x, y)}{q(x, y)} \\
& \quad=\sum_{x} \sum_{y} p(x, y) \log \frac{p(x) p(y \mid x)}{q(x) q(y \mid x)} \\
& \quad=\sum_{x} \sum_{y} p(x, y) \log \frac{p(x)}{q(x)}+\sum_{x} \sum_{y} p(x, y) \log \frac{p(y \mid x)}{q(y \mid x)} \\
& \quad=D(p(x) \| q(x))+D(p(y \mid x) \| q(y \mid x))
\end{aligned}
$$

### 2.6 젠센 부등식과 그 결과

이 절에서는 앞에서 정의한 양들의 몇 가지 간단한 성질을 증명합니다. 볼록 함수(convex function)의 성질부터 시작하겠습니다.

정의 구간 $(a, b)$에서 함수 $f(x)$는 모든 $x_{1}, x_{2} \in(a, b)$와 $0 \leq \lambda \leq 1$에 대해 다음을 만족하면 볼록하다고 합니다.

$$
f\left(\lambda x_{1}+(1-\lambda) x_{2}\right) \leq \lambda f\left(x_{1}\right)+(1-\lambda) f\left(x_{2}\right)
$$

함수 $f$는 $\lambda=0$ 또는 $\lambda=1$일 때만 등호가 성립하면 엄격한 볼록 함수(strictly convex)라고 합니다.

정의 함수 $f$는 $-f$가 볼록 함수이면 오목 함수(concave)라고 합니다. 함수는 항상 모든 현(chord) 아래에 있으면 볼록 함수입니다. 함수는 항상 모든 현 위에 있으면 오목 함수입니다.

볼록 함수의 예로는 $x^{2},|x|, e^{x}, x \log x$ (단, $x \geq 0$) 등이 있습니다. 오목 함수의 예로는 $x \geq 0$일 때의 $\log x$와 $\sqrt{x}$가 있습니다. 그림 2.3은 볼록 함수와 오목 함수의 몇 가지 예를 보여줍니다. 선형 함수 $a x+b$는 볼록 함수이면서 동시에 오목 함수임을 유의하십시오. 볼록성은 엔트로피(entropy)와 상호 정보량(mutual information)과 같은 정보 이론적 양들의 많은 기본적인 성질의 기초가 됩니다. 이러한 성질 중 일부를 증명하기 전에 볼록 함수에 대한 몇 가지 간단한 결과를 도출합니다.

정리 2.6.1 구간에서 함수 $f$의 이계 도함수(second derivative)가 음이 아니면(양수이면), 그 함수는 해당 구간에서 볼록 함수(엄격한 볼록 함수)입니다.
<!-- Page 52 -->

그림 2.3. $(a)$ 볼록 함수와 $(b)$ 오목 함수의 예시.

증명: $x_{0}$ 주변의 함수에 대한 테일러 급수 전개를 사용합니다:

$$
f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{f^{\prime \prime}\left(x^{*}\right)}{2}\left(x-x_{0}\right)^{2}
$$

여기서 $x^{*}$는 $x_{0}$와 $x$ 사이에 있습니다. 가정에 의해 $f^{\prime \prime}\left(x^{*}\right) \geq 0$이므로, 마지막 항은 모든 $x$에 대해 음수가 아닙니다.

$x_{0}=\lambda x_{1}+(1-\lambda) x_{2}$로 놓고 $x=x_{1}$으로 취하면 다음과 같습니다.

$$
f\left(x_{1}\right) \geq f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left((1-\lambda)\left(x_{1}-x_{2}\right)\right)
$$

마찬가지로 $x=x_{2}$로 놓으면 다음과 같습니다.

$$
f\left(x_{2}\right) \geq f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(\lambda\left(x_{2}-x_{1}\right)\right)
$$

(2.74)에 $\lambda$를 곱하고 (2.75)에 $1-\lambda$를 곱한 후 더하면 (2.72)를 얻습니다. 엄격한 볼록성에 대한 증명도 같은 방식으로 진행됩니다.

정리 2.6.1을 통해 $x \geq 0$일 때 $x^{2}, e^{x}, x \log x$의 엄격한 볼록성과 $x \geq 0$일 때 $\log x, \sqrt{x}$의 엄격한 오목성을 즉시 확인할 수 있습니다.

$E$는 기댓값을 나타냅니다. 따라서 이산적인 경우 $E X=\sum_{x \in \mathcal{X}} p(x) x$이고 연속적인 경우 $E X=\int x f(x) d x$입니다.
<!-- Page 53 -->
다음 부등식은 수학에서 가장 널리 사용되는 부등식 중 하나이며 정보 이론의 많은 기본 결과의 기초가 됩니다.

정리 2.6.2 (젠센의 부등식) $f$가 볼록 함수이고 $X$가 확률 변수이면,

$$
E f(X) \geq f(E X)
$$

또한, $f$가 엄격한 볼록 함수이면, (2.76)에서의 등식은 $X=E X$가 확률 1로 성립함을 의미합니다 (즉, $X$는 상수입니다).

증명: 질량점의 개수에 대한 귀납법을 사용하여 이산 분포에 대해 이를 증명합니다. $f$가 엄격한 볼록 함수일 때 등식 조건에 대한 증명은 독자에게 맡깁니다.

두 개의 질량점을 갖는 분포의 경우, 부등식은 다음과 같습니다.

$$
p_{1} f\left(x_{1}\right)+p_{2} f\left(x_{2}\right) \geq f\left(p_{1} x_{1}+p_{2} x_{2}\right)
$$

이는 볼록 함수의 정의에서 직접적으로 도출됩니다. $k-1$개의 질량점을 갖는 분포에 대해 정리가 참이라고 가정합니다. 그러면 $i=1,2, \ldots, k-1$에 대해 $p_{i}^{\prime}=p_{i} /\left(1-p_{k}\right)$로 쓰면 다음과 같습니다.

$$
\begin{aligned}
\sum_{i=1}^{k} p_{i} f\left(x_{i}\right) & =p_{k} f\left(x_{k}\right)+\left(1-p_{k}\right) \sum_{i=1}^{k-1} p_{i}^{\prime} f\left(x_{i}\right) \\
& \geq p_{k} f\left(x_{k}\right)+\left(1-p_{k}\right) f\left(\sum_{i=1}^{k-1} p_{i}^{\prime} x_{i}\right) \\
& \geq f\left(p_{k} x_{k}+\left(1-p_{k}\right) \sum_{i=1}^{k-1} p_{i}^{\prime} x_{i}\right) \\
& =f\left(\sum_{i=1}^{k} p_{i} x_{i}\right)
\end{aligned}
$$

여기서 첫 번째 부등식은 귀납적 가설에서 비롯되고, 두 번째 부등식은 볼록 함수의 정의에서 비롯됩니다.

연속성 논증을 통해 연속 분포로 증명을 확장할 수 있습니다.

이제 이러한 결과를 사용하여 엔트로피와 상대 엔트로피의 일부 속성을 증명합니다. 다음 정리는 근본적인 중요성을 갖습니다.
<!-- Page 54 -->
정리 2.6.3 (정보 부등식) $p(x), q(x), x \in \mathcal{X}$를 두 확률 질량 함수라고 가정합니다. 그러면

$$
D(p \| q) \geq 0
$$

이며, 등호는 $p(x)=q(x)$일 때만 성립합니다.
증명: $p(x)$의 지지 집합을 $A=\{x: p(x)>0\}$라고 합시다. 그러면

$$
\begin{aligned}
-D(p \| q) & =-\sum_{x \in A} p(x) \log \frac{p(x)}{q(x)} \\
& =\sum_{x \in A} p(x) \log \frac{q(x)}{p(x)} \\
& \leq \log \sum_{x \in A} p(x) \frac{q(x)}{p(x)} \\
& =\log \sum_{x \in A} q(x) \\
& \leq \log \sum_{x \in \mathcal{X}} q(x) \\
& =\log 1 \\
& =0
\end{aligned}
$$

여기서 (2.85)는 젠센 부등식에서 유도됩니다. $\log t$는 $t$에 대해 엄격하게 오목한 함수이므로, (2.85)에서의 등호는 $q(x) / p(x)$가 모든 곳에서 상수일 때만 성립합니다 [즉, 모든 $x$에 대해 $q(x)=c p(x)$]. 따라서 $\sum_{x \in A} q(x)=$ $c \sum_{x \in A} p(x)=c$입니다. (2.87)에서의 등호는 $\sum_{x \in A} q(x)=\sum_{x \in \mathcal{X}}$ $q(x)=1$일 때만 성립하며, 이는 $c=1$을 의미합니다. 그러므로 $D(p \| q)=0$은 $p(x)=q(x)$일 때만 성립합니다.

따름정리 (상호 정보량의 비음수성) 임의의 두 확률 변수 $X, Y$에 대해,

$$
I(X ; Y) \geq 0
$$

이며, 등호는 $X$와 $Y$가 독립일 때만 성립합니다.
증명: $I(X ; Y)=D(p(x, y) \| p(x) p(y)) \geq 0$이며, 등호는 $p(x, y)=p(x) p(y)$ (즉, $X$와 $Y$가 독립)일 때만 성립합니다.
<!-- Page 55 -->
# 따름정리

$$
D(p(y \mid x) \| q(y \mid x)) \geq 0
$$

이는 $p(x)>0$인 모든 $y$와 $x$에 대해 $p(y \mid x)=q(y \mid x)$일 때 그리고 그럴 때만 등식이 성립합니다.

## 따름정리

$$
I(X ; Y \mid Z) \geq 0
$$

이는 $X$와 $Y$가 $Z$가 주어졌을 때 조건부로 독립일 때 그리고 그럴 때만 등식이 성립합니다.
이제 $\mathcal{X}$의 범위에 대한 균등 분포가 이 범위에 대한 최대 엔트로피 분포임을 보이겠습니다. 따라서 이 범위를 갖는 임의의 확률 변수는 $\log |\mathcal{X}|$보다 크지 않은 엔트로피를 갖게 됩니다.

정리 2.6.4 $H(X) \leq \log |\mathcal{X}|$, 여기서 $|\mathcal{X}|$는 $X$의 범위에 있는 원소의 수를 나타내며, $X$가 $\mathcal{X}$에 대한 균등 분포를 가질 때 그리고 그럴 때만 등식이 성립합니다.

증명: $u(x)=\frac{1}{|\mathcal{X}|}$를 $\mathcal{X}$에 대한 균등 확률 질량 함수로 놓고, $p(x)$를 $X$의 확률 질량 함수로 놓습니다. 그러면

$$
D(p \| u)=\sum p(x) \log \frac{p(x)}{u(x)}=\log |\mathcal{X}|-H(X)
$$

따라서 상대 엔트로피의 비음수성에 의해,

$$
0 \leq D(p \| u)=\log |\mathcal{X}|-H(X)
$$

정리 2.6.5 (조건화는 엔트로피를 감소시킨다)(정보는 해를 끼치지 않는다)

$$
H(X \mid Y) \leq H(X)
$$

이는 $X$와 $Y$가 독립일 때 그리고 그럴 때만 등식이 성립합니다.
증명: $\quad 0 \leq I(X ; Y)=H(X)-H(X \mid Y)$.
직관적으로, 이 정리는 다른 확률 변수 $Y$를 아는 것이 $X$의 불확실성을 감소시킬 뿐이라는 것을 말합니다. 이는 평균적으로만 참이라는 점에 유의하십시오. 구체적으로, $H(X \mid Y=y)$는 $H(X)$보다 크거나 작거나 같을 수 있지만, 평균적으로 $H(X \mid Y)=\sum_{y} p(y) H(X \mid Y=y) \leq$ $H(X)$입니다. 예를 들어, 법정 사건에서 특정 새로운 증거는 불확실성을 증가시킬 수 있지만, 평균적으로 증거는 불확실성을 감소시킵니다.
<!-- Page 56 -->
예제 2.6.1 $(X, Y)$의 결합 분포는 다음과 같습니다:

| $X$ |  |  |
| :--: | :--: | :--: |
| $Y$ | 1 | 2 |
| 1 | 0 | $\frac{3}{4}$ |
| 2 | $\frac{1}{8}$ | $\frac{1}{8}$ |

그러면 $H(X)=H\left(\frac{1}{8}, \frac{7}{8}\right)=0.544$ bit, $\quad H(X \mid Y=1)=0$ bits, 그리고 $H(X \mid Y=2)=1$ bit입니다. $H(X \mid Y)=\frac{3}{4} H(X \mid Y=1)+\frac{1}{4}$ $H(X \mid Y=2)=0.25$ bit를 계산합니다. 따라서 $Y=2$가 관찰되면 $X$의 불확실성이 증가하고 $Y=1$이 관찰되면 감소하지만, 평균적으로 불확실성은 감소합니다.

정리 2.6.6 (엔트로피에 대한 독립성 경계) $X_{1}, X_{2}, \ldots, X_{n}$이 $p\left(x_{1}, x_{2}, \ldots, x_{n}\right)$에 따라 추출된다고 가정합니다. 그러면

$$
H\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq \sum_{i=1}^{n} H\left(X_{i}\right)
$$

등호는 $X_{i}$가 독립일 때 그리고 그럴 때만 성립합니다.
증명: 엔트로피에 대한 연쇄 법칙에 의해,

$$
\begin{aligned}
H\left(X_{1}, X_{2}, \ldots, X_{n}\right) & =\sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right) \\
& \leq \sum_{i=1}^{n} H\left(X_{i}\right)
\end{aligned}
$$

여기서 부등식은 정리 2.6.5에서 직접적으로 도출됩니다. 모든 $i$에 대해 $X_{i}$가 $X_{i-1}, \ldots, X_{1}$과 독립일 때 (즉, $X_{i}$들이 독립일 때) 그리고 그럴 때만 등호가 성립합니다.

# 2.7 로그 합 부등식 및 그 응용

이제 로그의 오목성(concavity)의 간단한 결과인 로그 합 부등식을 증명할 것이며, 이는 엔트로피에 대한 일부 오목성 결과를 증명하는 데 사용될 것입니다.
<!-- Page 57 -->
정리 2.7.1 (로그 합 부등식) 음수가 아닌 수 $a_{1}, a_{2}, \ldots, a_{n}$ 및 $b_{1}, b_{2}, \ldots, b_{n}$에 대하여,

$$
\sum_{i=1}^{n} a_{i} \log \frac{a_{i}}{b_{i}} \geq\left(\sum_{i=1}^{n} a_{i}\right) \log \frac{\sum_{i=1}^{n} a_{i}}{\sum_{i=1}^{n} b_{i}}
$$

등호는 $\frac{a_{i}}{b_{i}}=$ 상수일 때 성립합니다.
$0 \log 0=0, a \log \frac{a}{0}=\infty$ (단, $a>0$) 및 $0 \log \frac{0}{0}=0$이라는 관례를 다시 사용합니다. 이는 연속성으로부터 쉽게 도출됩니다.

증명: 일반성을 잃지 않고 $a_{i}>0$ 및 $b_{i}>0$이라고 가정합니다. 함수 $f(t)=t \log t$는 $f^{\prime \prime}(t)=\frac{1}{t} \log e>0$ (모든 양수 $t$에 대하여)이므로 엄격하게 볼록합니다. 따라서 젠센 부등식에 의해, 우리는

$$
\sum \alpha_{i} f\left(t_{i}\right) \geq f\left(\sum \alpha_{i} t_{i}\right)
$$

($\alpha_{i} \geq 0, \sum_{i} \alpha_{i}=1$)에 대하여 다음을 얻습니다. $\alpha_{i}=\frac{b_{i}}{\sum_{j=1}^{n} b_{j}}$ 및 $t_{i}=\frac{a_{i}}{b_{i}}$로 설정하면,

$$
\sum \frac{a_{i}}{\sum b_{j}} \log \frac{a_{i}}{b_{i}} \geq \sum \frac{a_{i}}{\sum b_{j}} \log \sum \frac{a_{i}}{\sum b_{j}}
$$

이는 로그 합 부등식입니다.
이제 로그 합 부등식을 사용하여 다양한 볼록성 결과를 증명합니다. 먼저 $D(p \| q) \geq 0$이며 등호는 $p(x)=q(x)$일 때 성립한다는 정리 2.6.3을 다시 증명합니다. 로그 합 부등식에 의해,

$$
\begin{aligned}
D(p \| q) & =\sum p(x) \log \frac{p(x)}{q(x)} \\
& \geq\left(\sum p(x)\right) \log \sum p(x) / \sum q(x) \\
& =1 \log \frac{1}{1}=0
\end{aligned}
$$

등호는 $\frac{p(x)}{q(x)}=c$일 때 성립합니다. $p$와 $q$ 모두 확률 질량 함수이므로 $c=1$이며, 따라서 $D(p \| q)=0$은 모든 $x$에 대해 $p(x)=q(x)$일 때 성립합니다.
<!-- Page 58 -->
정리 2.7.2 (상대 엔트로피의 볼록성) $\quad D(p \| q)$는 쌍 $(p, q)$에 대해 볼록합니다. 즉, $\left(p_{1}, q_{1}\right)$와 $\left(p_{2}, q_{2}\right)$가 두 확률 질량 함수 쌍이라면, 모든 $0 \leq \lambda \leq 1$에 대해
$D\left(\lambda p_{1}+(1-\lambda) p_{2} \| \lambda q_{1}+(1-\lambda) q_{2}\right) \leq \lambda D\left(p_{1} \| q_{1}\right)+(1-\lambda) D\left(p_{2} \| q_{2}\right)$
입니다.
증명: (2.105)의 좌변 항에 log sum 부등식을 적용합니다:

$$
\begin{aligned}
& \left(\lambda p_{1}(x)+(1-\lambda) p_{2}(x)\right) \log \frac{\lambda p_{1}(x)+(1-\lambda) p_{2}(x)}{\lambda q_{1}(x)+(1-\lambda) q_{2}(x)} \\
& \quad \leq \lambda p_{1}(x) \log \frac{\lambda p_{1}(x)}{\lambda q_{1}(x)}+(1-\lambda) p_{2}(x) \log \frac{(1-\lambda) p_{2}(x)}{(1-\lambda) q_{2}(x)}
\end{aligned}
$$

이것을 모든 $x$에 대해 합하면 원하는 성질을 얻습니다.
정리 2.7.3 (엔트로피의 오목성) $\quad H(p)$는 $p$에 대한 오목 함수입니다.

# 증명

$$
H(p)=\log |\mathcal{X}|-D(p \| u)
$$

여기서 $u$는 $|\mathcal{X}|$개의 결과에 대한 균일 분포입니다. 그러면 $H$의 오목성은 $D$의 볼록성으로부터 직접적으로 도출됩니다.

대안 증명: $A$ 집합에서 값을 가지는 분포 $p_{1}$를 갖는 확률 변수 $X_{1}$을 정의합니다. 동일한 집합에서 분포 $p_{2}$를 갖는 다른 확률 변수 $X_{2}$를 정의합니다.

$$
\theta= \begin{cases}1 & \text { 확률 } \lambda \text{ 으로} \\ 2 & \text { 확률 } 1-\lambda \text{ 으로}\end{cases}
$$

라고 정의합니다.
$Z=X_{\theta}$라고 하면, $Z$의 분포는 $\lambda p_{1}+(1-\lambda) p_{2}$입니다. 이제 조건화는 엔트로피를 감소시키므로, 다음이 성립합니다:

$$
H(Z) \geq H(Z \mid \theta)
$$

또는 동등하게,

$$
H\left(\lambda p_{1}+(1-\lambda) p_{2}\right) \geq \lambda H\left(p_{1}\right)+(1-\lambda) H\left(p_{2}\right)
$$

이는 분포의 함수로서 엔트로피의 오목성을 증명합니다.
<!-- Page 59 -->
엔트로피의 오목성(concavity)의 한 결과는 동일한 엔트로피를 가진 두 기체를 혼합하면 더 높은 엔트로피를 가진 기체가 된다는 것입니다.

정리 2.7.4 $(X, Y) \sim p(x, y)=p(x) p(y \mid x)$라고 가정합니다. 상호 정보량 $I(X ; Y)$는 $p(y \mid x)$가 고정되었을 때 $p(x)$의 오목 함수이며, $p(x)$가 고정되었을 때 $p(y \mid x)$의 볼록 함수입니다.

증명: 첫 번째 부분을 증명하기 위해 상호 정보량을 확장합니다.

$$
I(X ; Y)=H(Y)-H(Y \mid X)=H(Y)-\sum_{x} p(x) H(Y \mid X=x)
$$

$p(y \mid x)$가 고정되면 $p(y)$는 $p(x)$의 선형 함수가 됩니다. 따라서 $p(y)$의 오목 함수인 $H(Y)$는 $p(x)$의 오목 함수가 됩니다. 두 번째 항은 $p(x)$의 선형 함수입니다. 따라서 차이는 $p(x)$의 오목 함수가 됩니다.

두 번째 부분을 증명하기 위해 $p(x)$를 고정하고 두 개의 다른 조건부 분포 $p_{1}(y \mid x)$와 $p_{2}(y \mid x)$를 고려합니다. 해당 결합 분포는 $p_{1}(x, y)=p(x) p_{1}(y \mid x)$와 $p_{2}(x, y)=p(x) p_{2}(y \mid x)$이며, 해당 주변 분포는 각각 $p(x), p_{1}(y)$ 및 $p(x), p_{2}(y)$입니다. 조건부 분포를 고려합니다.

$$
p_{\lambda}(y \mid x)=\lambda p_{1}(y \mid x)+(1-\lambda) p_{2}(y \mid x)
$$

이는 $0 \leq \lambda \leq 1$인 $p_{1}(y \mid x)$와 $p_{2}(y \mid x)$의 혼합입니다. 해당 결합 분포 또한 해당 결합 분포의 혼합이며,

$$
p_{\lambda}(x, y)=\lambda p_{1}(x, y)+(1-\lambda) p_{2}(x, y)
$$

그리고 $Y$의 분포 또한 혼합입니다.

$$
p_{\lambda}(y)=\lambda p_{1}(y)+(1-\lambda) p_{2}(y)
$$

따라서 $q_{\lambda}(x, y)=p(x) p_{\lambda}(y)$를 주변 분포의 곱으로 놓으면 다음과 같습니다.

$$
q_{\lambda}(x, y)=\lambda q_{1}(x, y)+(1-\lambda) q_{2}(x, y)
$$

상호 정보량은 결합 분포와 주변 분포의 곱 사이의 상대 엔트로피이므로,

$$
I(X ; Y)=D\left(p_{\lambda}(x, y) \| q_{\lambda}(x, y)\right)
$$

상대 엔트로피 $D(p \| q)$는 $(p, q)$의 볼록 함수이므로, 상호 정보량은 조건부 분포의 볼록 함수임을 알 수 있습니다.
<!-- Page 60 -->
# 2.8 데이터 처리 불평등

데이터 처리 불평등은 데이터에 대한 영리한 조작이 데이터로부터 내릴 수 있는 추론을 개선할 수 없음을 보여주는 데 사용될 수 있습니다.

정의 확률 변수 $X, Y, Z$가 순서대로 마르코프 연쇄를 형성한다고 가정합니다 ( $X \rightarrow Y \rightarrow Z$ 로 표기). 이는 $Z$의 조건부 분포가 $Y$에만 의존하며 $X$와 조건부 독립임을 의미합니다. 구체적으로, $X, Y, Z$가 마르코프 연쇄 $X \rightarrow Y \rightarrow Z$를 형성하는 경우, 결합 확률 질량 함수는 다음과 같이 작성될 수 있습니다.

$$
p(x, y, z)=p(x) p(y \mid x) p(z \mid y)
$$

몇 가지 간단한 결과는 다음과 같습니다.

- $X \rightarrow Y \rightarrow Z$ 는 $X$와 $Z$가 $Y$가 주어졌을 때 조건부 독립이라는 것과 동치입니다. 마르코프성은 조건부 독립을 함의합니다. 왜냐하면

$$
p(x, z \mid y)=\frac{p(x, y, z)}{p(y)}=\frac{p(x, y) p(z \mid y)}{p(y)}=p(x \mid y) p(z \mid y)
$$

이는 마르코프 연쇄의 특성화이며, 마르코프 장을 정의하기 위해 확장될 수 있습니다. 마르코프 장은 내부와 외부가 경계 값들이 주어졌을 때 독립적인 $n$차원 확률 과정입니다.

- $X \rightarrow Y \rightarrow Z$ 는 $Z \rightarrow Y \rightarrow X$ 를 함의합니다. 따라서 이 조건은 때때로 $X \leftrightarrow Y \leftrightarrow Z$ 로 작성됩니다.
- $Z=f(Y)$ 이면, $X \rightarrow Y \rightarrow Z$ 입니다.

이제 $Y$의 어떤 처리도, 결정적이든 무작위적이든, $X$에 대한 $Y$의 정보를 증가시킬 수 없음을 보여주는 중요하고 유용한 정리를 증명할 수 있습니다.

정리 2.8.1 (데이터 처리 불평등) $X \rightarrow Y \rightarrow Z$ 이면, $I(X ; Y) \geq I(X ; Z)$ 입니다.

증명: 연쇄 법칙에 따라, 상호 정보량을 두 가지 다른 방식으로 확장할 수 있습니다.

$$
\begin{aligned}
I(X ; Y, Z) & =I(X ; Z)+I(X ; Y \mid Z) \\
& =I(X ; Y)+I(X ; Z \mid Y)
\end{aligned}
$$
<!-- Page 61 -->
$X$와 $Z$는 $Y$가 주어졌을 때 조건부 독립이므로, $I(X ; Z \mid Y)=0$입니다. $I(X ; Y \mid Z) \geq 0$이므로,

$$
I(X ; Y) \geq I(X ; Z)
$$

등호는 $I(X ; Y \mid Z)=0$일 때, 즉 $X \rightarrow Z \rightarrow Y$가 Markov chain을 형성할 때 성립합니다. 마찬가지로 $I(Y ; Z) \geq I(X ; Z)$임을 증명할 수 있습니다.

결론적으로, 특히 $Z=g(Y)$이면 $I(X ; Y) \geq I(X ; g(Y))$입니다.
증명: $X \rightarrow Y \rightarrow g(Y)$는 Markov chain을 형성합니다.
따라서 데이터 $Y$의 함수는 $X$에 대한 정보를 증가시킬 수 없습니다.
결론적으로, 만약 $X \rightarrow Y \rightarrow Z$이면, $I(X ; Y \mid Z) \leq I(X ; Y)$입니다.
증명: (2.119)와 (2.120)에서 Markovity에 의해 $I(X ; Z \mid Y)=0$이고 $I(X ; Z) \geq 0$임을 주목합니다. 따라서,

$$
I(X ; Y \mid Z) \leq I(X ; Y)
$$

따라서 $X$와 $Y$의 종속성은 "하류" 랜덤 변수 $Z$를 관찰함으로써 감소하거나 (변하지 않거나) 유지됩니다. $X, Y, Z$가 Markov chain을 형성하지 않을 때 $I(X ; Y \mid Z)>I(X ; Y)$일 수도 있다는 점에 유의하십시오. 예를 들어, $X$와 $Y$가 독립적인 공정한 이진 랜덤 변수이고 $Z=X+Y$라고 가정합니다. 그러면 $I(X ; Y)=0$이지만, $I(X ; Y \mid Z)=$ $H(X \mid Z)-H(X \mid Y, Z)=H(X \mid Z)=P(Z=1) H(X \mid Z=1)=\frac{1}{2}$ 비트입니다.

# 2.9 충분 통계량

이 섹션은 중요한 통계적 아이디어를 명확히 하는 데 데이터 처리 부등식의 힘을 보여주는 부수적인 내용입니다. $\theta$로 인덱싱된 확률 질량 함수족 $\left\{f_{\theta}(x)\right\}$이 있고, $X$가 이족에서 추출된 표본이라고 가정합니다. $T(X)$를 표본 평균 또는 표본 분산과 같은 통계량(표본의 함수)이라고 가정합니다. 그러면 $\theta \rightarrow X \rightarrow T(X)$이고, 데이터 처리 부등식에 의해,

$$
I(\theta ; T(X)) \leq I(\theta ; X)
$$

$\theta$에 대한 모든 분포에 대해 성립합니다. 그러나 등호가 성립하면 정보 손실이 없습니다.

통계량 $T(X)$는 $X$에 대한 모든 정보를 포함하는 경우 $\theta$에 대해 충분하다고 합니다.
<!-- Page 62 -->
정의 $T(X)$라는 함수는 $\theta$에 대한 모든 분포에 대해 $X$가 $T(X)$가 주어졌을 때 $\theta$와 독립적이라면 (즉, $\theta \rightarrow T(X) \rightarrow X$가 마르코프 연쇄를 형성한다면) $\left\{f_{\theta}(x)\right\}$족에 대해 충분 통계량이라고 합니다.

이는 데이터 처리 부등식에서의 등식 조건과 동일합니다.

$$
I(\theta ; X)=I(\theta ; T(X))
$$

$\theta$에 대한 모든 분포에 대해 성립합니다. 따라서 충분 통계량은 mutual information을 보존하며 그 역도 성립합니다.

다음은 충분 통계량의 몇 가지 예시입니다.

1. $X_{1}, X_{2}, \ldots, X_{n}, X_{i} \in\{0,1\}$을 알 수 없는 모수 $\theta=\operatorname{Pr}\left(X_{i}=1\right)$를 가진 동전 던지기의 독립적이고 동일하게 분포된 (i.i.d.) 시퀀스라고 가정합니다. $n$이 주어졌을 때, 1의 개수는 $\theta$에 대한 충분 통계량입니다. 여기서 $T\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} X_{i}$입니다. 사실, $T$가 주어졌을 때, 해당 개수의 1을 가진 모든 시퀀스가 동일하게 발생하며 모수 $\theta$와 독립적임을 보일 수 있습니다. 구체적으로,

$$
\begin{aligned}
& \operatorname{Pr}\left\{\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\left(x_{1}, x_{2}, \ldots, x_{n}\right) \mid \sum_{i=1}^{n} X_{i}=k\right\} \\
& \quad=\left\{\begin{array}{cl}
\frac{1}{(2)} & \text { if } \sum x_{i}=k \\
0 & \text { otherwise. }
\end{array}\right.
\end{aligned}
$$

따라서 $\theta \rightarrow \sum X_{i} \rightarrow\left(X_{1}, X_{2}, \ldots, X_{n}\right)$는 마르코프 연쇄를 형성하며, $T$는 $\theta$에 대한 충분 통계량입니다.

다음 두 예시는 확률 질량 함수 대신 확률 밀도 함수를 포함하지만, 이론은 여전히 적용됩니다. 연속 확률 변수에 대한 entropy와 mutual information은 8장에서 정의합니다.
2. $X$가 평균이 $\theta$이고 분산이 1인 정규 분포를 따른다고 가정합니다. 즉,

$$
f_{\theta}(x)=\frac{1}{\sqrt{2 \pi}} e^{-(x-\theta)^{2} / 2}=\mathcal{N}(\theta, 1)
$$

이고 $X_{1}, X_{2}, \ldots, X_{n}$이 이 분포에 따라 독립적으로 추출된다면, $\theta$에 대한 충분 통계량은 표본 평균 $\bar{X}_{n}=\frac{1}{\theta} \sum_{i=1}^{n} X_{i}$입니다. $\bar{X}_{n}$과 $n$이 주어졌을 때 $X_{1}, X_{2}, \ldots, X_{n}$의 조건부 분포가 $\theta$에 의존하지 않음을 확인할 수 있습니다.
<!-- Page 63 -->
3. $f_{\theta}=\operatorname{Uniform}(\theta, \theta+1)$일 때, $\theta$에 대한 충분 통계량은 다음과 같습니다.

$$
\begin{aligned}
& T\left(X_{1}, X_{2}, \ldots, X_{n}\right) \\
& \quad=\left(\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}, \min \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}\right)
\end{aligned}
$$

이것에 대한 증명은 약간 더 복잡하지만, 다시 한번 통계량 $T$가 주어졌을 때 데이터의 분포가 모수와 독립적임을 보일 수 있습니다.

최소 충분 통계량은 다른 모든 충분 통계량의 함수인 충분 통계량입니다.

정의 통계량 $T(X)$는 $\left\{f_{\theta}(x)\right\}$에 대해 다른 모든 충분 통계량 $U$의 함수일 때 최소 충분 통계량입니다. 데이터 처리 부등식의 관점에서 볼 때, 이는 다음을 의미합니다.

$$
\theta \rightarrow T(X) \rightarrow U(X) \rightarrow X
$$

따라서 최소 충분 통계량은 표본에서 $\theta$에 대한 정보를 최대한 압축합니다. 다른 충분 통계량은 추가적인 관련 없는 정보를 포함할 수 있습니다. 예를 들어, 평균이 $\theta$인 정규 분포의 경우, 모든 홀수 표본의 평균과 모든 짝수 표본의 평균을 제공하는 함수 쌍은 충분 통계량이지만 최소 충분 통계량은 아닙니다. 앞선 예에서 충분 통계량은 최소이기도 합니다.

# 2.10 파노의 부등식

확률 변수 $Y$를 알고 있고 상관 관계가 있는 확률 변수 $X$의 값을 추측하고자 한다고 가정합니다. 파노의 부등식은 확률 변수 $X$를 추측하는 데 드는 오류 확률과 조건부 엔트로피 $H(X \mid Y)$를 관련시킵니다. 이는 7장에서 섀넌의 채널 용량 정리에 대한 역증명을 하는 데 중요할 것입니다. 문제 2.5에서 다른 확률 변수 $Y$가 주어졌을 때 확률 변수 $X$의 조건부 엔트로피는 $X$가 $Y$의 함수일 때만 0임을 알고 있습니다. 따라서 $H(X \mid Y)=0$일 때만 $Y$로부터 $X$를 오류 확률 0으로 추정할 수 있습니다.

이러한 논증을 확장하면, 조건부 엔트로피 $H(X \mid Y)$가 작을 때만 낮은 오류 확률로 $X$를 추정할 수 있을 것으로 예상할 수 있습니다. 파노의 부등식은 이 아이디어를 정량화합니다. 분포 $p(x)$를 갖는 확률 변수 $X$를 추정하고자 한다고 가정합니다. 우리는 조건부 분포 $p(y \mid x)$에 의해 $X$와 관련된 확률 변수 $Y$를 관찰합니다. $Y$로부터 우리는
<!-- Page 64 -->
함수 $g(Y)=\hat{X}$를 계산합니다. 여기서 $\hat{X}$는 $X$의 추정값이며 $\hat{\mathcal{X}}$의 값을 가집니다. 우리는 알파벳 $\hat{\mathcal{X}}$를 $\mathcal{X}$와 같도록 제한하지 않을 것이며, 함수 $g(Y)$가 확률적일 수도 있도록 허용할 것입니다. 우리는 $\hat{X} \neq X$일 확률을 제한하고자 합니다. 우리는 $X \rightarrow Y \rightarrow \hat{X}$가 마르코프 연쇄를 형성함을 관찰합니다. 오류 확률을 다음과 같이 정의합니다.

$$
P_{e}=\operatorname{Pr}\{\hat{X} \neq X\}
$$

정리 2.10.1 (파노의 부등식) 모든 추정량 $\hat{X}$에 대해 $X \rightarrow Y \rightarrow \hat{X}$이고 $P_{e}=\operatorname{Pr}(X \neq \hat{X})$일 때, 다음이 성립합니다.

$$
H\left(P_{e}\right)+P_{e} \log |\mathcal{X}| \geq H(X \mid \hat{X}) \geq H(X \mid Y)
$$

이 부등식은 다음과 같이 약화될 수 있습니다.

$$
1+P_{e} \log |\mathcal{X}| \geq H(X \mid Y)
$$

또는

$$
P_{e} \geq \frac{H(X \mid Y)-1}{\log |\mathcal{X}|}
$$

비고 (2.130)에서 $P_{e}=0$이면 직관적으로 예상되는 대로 $H(X \mid Y)=0$임을 알 수 있습니다.

증명: 먼저 $Y$의 역할을 무시하고 (2.130)의 첫 번째 부등식을 증명합니다. 그런 다음 데이터 처리 부등식을 사용하여 (2.130)의 두 번째 부등식으로 주어진 파노의 부등식의 보다 전통적인 형태를 증명할 것입니다. 오류 확률 변수를 다음과 같이 정의합니다.

$$
E= \begin{cases}1 & \text { if } \hat{X} \neq X \\ 0 & \text { if } \hat{X}=X\end{cases}
$$

그러면 엔트로피의 연쇄 법칙을 사용하여 $H(E, X \mid \hat{X})$를 두 가지 다른 방식으로 확장하면 다음과 같습니다.

$$
\begin{aligned}
H(E, X \mid \hat{X}) & =H(X \mid \hat{X})+\underbrace{H(E \mid X, \hat{X})}_{=0} \\
& =\underbrace{H(E \mid \hat{X})}_{\leq H\left(P_{e}\right)}+\underbrace{H(X \mid E, \hat{X})}_{\leq P_{e} \log |\mathcal{X}|}
\end{aligned}
$$

조건화는 엔트로피를 감소시키므로 $H(E \mid \hat{X}) \leq H(E)=H\left(P_{e}\right)$입니다. 이제 $E$가 $X$와 $\hat{X}$의 함수이므로, 조건부 엔트로피 $H(E \mid X, \hat{X})$는 다음과 같습니다.
<!-- Page 65 -->
0 입니다. 또한, $E$는 이진값 확률 변수이므로 $H(E) = H(P_e)$ 입니다. 남은 항인 $H(X \mid E, \hat{X})$는 다음과 같이 상한을 정할 수 있습니다.

$$
\begin{aligned}
H(X \mid E, \hat{X}) & =\operatorname{Pr}(E=0) H(X \mid \hat{X}, E=0)+\operatorname{Pr}(E=1) H(X \mid \hat{X}, E=1) \\
& \leq\left(1-P_{e}\right) 0+P_{e} \log |\mathcal{X}|
\end{aligned}
$$

$E=0$이 주어졌을 때 $X=\hat{X}$이고, $E=1$이 주어졌을 때 가능한 결과의 수의 로그로 조건부 엔트로피를 상한할 수 있기 때문입니다. 이 결과들을 종합하면 다음과 같습니다.

$$
H\left(P_{e}\right)+P_{e} \log |\mathcal{X}| \geq H(X \mid \hat{X})
$$

데이터 처리 부등식에 의해, $X \rightarrow Y \rightarrow \hat{X}$는 마르코프 연쇄이므로 $I(X ; \hat{X}) \leq I(X ; Y)$이고, 따라서 $H(X \mid \hat{X}) \geq H(X \mid Y)$ 입니다. 그러므로 다음과 같습니다.

$$
H\left(P_{e}\right)+P_{e} \log |\mathcal{X}| \geq H(X \mid \hat{X}) \geq H(X \mid Y)
$$

따름정리: 임의의 두 확률 변수 $X$와 $Y$에 대해 $p=\operatorname{Pr}(X \neq Y)$라고 하면,

$$
H(p)+p \log |\mathcal{X}| \geq H(X \mid Y)
$$

증명: 파노의 부등식에서 $\hat{X}=Y$로 둡니다.
임의의 두 확률 변수 $X$와 $Y$에 대해, 추정량 $g(Y)$가 집합 $\mathcal{X}$의 값을 취한다면, $\log |\mathcal{X}|$를 $\log (|\mathcal{X}|-1)$로 대체함으로써 부등식을 약간 강화할 수 있습니다.

따름정리: $P_{e}=\operatorname{Pr}(X \neq \hat{X})$이고 $\hat{X}: \mathcal{Y} \rightarrow \mathcal{X}$라고 하면,

$$
H\left(P_{e}\right)+P_{e} \log (|\mathcal{X}|-1) \geq H(X \mid Y)
$$

증명: 정리의 증명은 다음과 같이 변경되는 것을 제외하고는 동일하게 진행됩니다.

$$
\begin{aligned}
H(X \mid E, \hat{X}) & =\operatorname{Pr}(E=0) H(X \mid \hat{X}, E=0)+\operatorname{Pr}(E=1) H(X \mid \hat{X}, E=1) \\
& \leq\left(1-P_{e}\right) 0+P_{e} \log (|\mathcal{X}|-1)
\end{aligned}
$$

$E=0$이 주어졌을 때 $X=\hat{X}$이고, $E=1$이 주어졌을 때 가능한 $X$ 결과의 범위는 $|\mathcal{X}|-1$이므로, 조건부 엔트로피를 가능한 결과의 수의 로그인 $\log (|\mathcal{X}|-1)$로 상한할 수 있습니다. 이를 대입하면 더 강력한 부등식을 얻을 수 있습니다.
<!-- Page 66 -->
비고 $Y$에 대한 지식이 없다고 가정합니다. 따라서 $X$는 정보 없이 추측되어야 합니다. $X \in\{1,2, \ldots, m\}$이고 $p_{1} \geq p_{2} \geq$ $\cdots \geq p_{m}$이라고 가정합니다. 그러면 $X$의 최적 추측은 $\hat{X}=1$이며 결과적인 오류 확률은 $P_{e}=1-p_{1}$입니다. 파노의 부등식은 다음과 같습니다.

$$
H\left(P_{e}\right)+P_{e} \log (m-1) \geq H(X)
$$

확률 질량 함수

$$
\left(p_{1}, p_{2}, \ldots, p_{m}\right)=\left(1-P_{e}, \frac{P_{e}}{m-1}, \ldots, \frac{P_{e}}{m-1}\right)
$$

는 이 경계를 등호로 달성합니다. 따라서 파노의 부등식은 날카롭습니다.
김에, 오류 확률과 엔트로피를 연관시키는 새로운 부등식을 소개하겠습니다. $X$와 $X^{\prime}$를 엔트로피 $H(X)$를 갖는 두 개의 독립적이고 동일하게 분포된 확률 변수라고 가정합니다. $X=X^{\prime}$에서의 확률은 다음과 같이 주어집니다.

$$
\operatorname{Pr}\left(X=X^{\prime}\right)=\sum_{x} p^{2}(x)
$$

다음과 같은 부등식을 얻습니다.

정리 2.10.1 $X$와 $X^{\prime}$가 i.i.d.이고 엔트로피가 $H(X)$이면,

$$
\operatorname{Pr}\left(X=X^{\prime}\right) \geq 2^{-H(X)}
$$

등호는 $X$가 균등 분포를 가질 때 그리고 그럴 때만 성립합니다.
증명: $X \sim p(x)$라고 가정합니다. 젠센의 부등식에 의해 다음과 같습니다.

$$
2^{E \log p(X)} \leq E 2^{\log p(X)}
$$

이는 다음을 의미합니다.

$$
2^{-H(X)}=2^{\sum p(x) \log p(x)} \leq \sum p(x) 2^{\log p(x)}=\sum p^{2}(x)
$$

따름 정리 $X, X^{\prime}$가 독립이고 $X \sim p(x), X^{\prime} \sim r(x), x, x^{\prime} \in$ $\mathcal{X}$이면,

$$
\begin{aligned}
& \operatorname{Pr}\left(X=X^{\prime}\right) \geq 2^{-H(p)-D(p \| r)} \\
& \operatorname{Pr}\left(X=X^{\prime}\right) \geq 2^{-H(r)-D(r \| p)}
\end{aligned}
$$
<!-- Page 67 -->
증명: 다음과 같습니다.

$$
\begin{aligned}
2^{-H(p)-D(p \| r)} & =2^{\sum p(x) \log p(x)+\sum p(x) \log \frac{r(x)}{p(x)}} \\
& =2^{\sum p(x) \log r(x)} \\
& \leq \sum p(x) 2^{\log r(x)} \\
& =\sum p(x) r(x) \\
& =\operatorname{Pr}\left(X=X^{\prime}\right)
\end{aligned}
$$

여기서 부등식은 젠센 부등식과 함수 $f(y)=2^{y}$의 볼록성으로부터 유도됩니다.

다음의 간결한 요약은 조건을 생략합니다.

# 요약

정의 이산 확률 변수 $X$의 엔트로피 $H(X)$는 다음과 같이 정의됩니다.

$$
H(X)=-\sum_{x \in \mathcal{X}} p(x) \log p(x)
$$

## $\boldsymbol{H}$의 속성

1. $H(X) \geq 0$.
2. $H_{b}(X)=\left(\log _{b} a\right) H_{a}(X)$.
3. (조건화는 엔트로피를 감소시킨다) 임의의 두 확률 변수 $X$와 $Y$에 대해 다음이 성립합니다.

$$
H(X \mid Y) \leq H(X)
$$

등호는 $X$와 $Y$가 독립일 때 성립합니다.
4. $H\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq \sum_{i=1}^{n} H\left(X_{i}\right)$, 등호는 $X_{i}$가 독립일 때 성립합니다.
5. $H(X) \leq \log |\mathcal{X}|$, 등호는 $X$가 $\mathcal{X}$에 대해 균등하게 분포될 때 성립합니다.
6. $H(p)$는 $p$에 대해 오목합니다.
<!-- Page 68 -->
정의 확률 질량 함수 $p$와 확률 질량 함수 $q$에 대한 상대 엔트로피 $D(p \| q)$는 다음과 같이 정의됩니다.

$$
D(p \| q)=\sum_{x} p(x) \log \frac{p(x)}{q(x)}
$$

정의 두 확률 변수 $X$와 $Y$ 간의 mutual information은 다음과 같이 정의됩니다.

$$
I(X ; Y)=\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}
$$

# 대체 표현

$$
\begin{aligned}
H(X) & =E_{p} \log \frac{1}{p(X)} \\
H(X, Y) & =E_{p} \log \frac{1}{p(X, Y)} \\
H(X \mid Y) & =E_{p} \log \frac{1}{p(X \mid Y)} \\
I(X ; Y) & =E_{p} \log \frac{p(X, Y)}{p(X) p(Y)} \\
D(p \| q) & =E_{p} \log \frac{p(X)}{q(X)}
\end{aligned}
$$

## $D$와 $I$의 속성

1. $I(X ; Y)=H(X)-H(X \mid Y)=H(Y)-H(Y \mid X)=H(X)+$ $H(Y)-H(X, Y)$.
2. $D(p \| q) \geq 0$이며, 등호는 모든 $x \in \mathcal{X}$에 대해 $p(x)=q(x)$일 때만 성립합니다.
3. $I(X ; Y)=D(p(x, y) \| p(x) p(y)) \geq 0$이며, 등호는 $p(x, y)=p(x) p(y)$ (즉, $X$와 $Y$가 독립일 때)일 때만 성립합니다.
4. $|\mathcal{X}|=m$이고 $u$가 $\mathcal{X}$에 대한 균일 분포일 때, $D(p \|$ $u)=\log m-H(p)$입니다.
5. $D(p \| q)$는 쌍 $(p, q)$에 대해 볼록 함수입니다.

## 연쇄 법칙

엔트로피: $H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right)$.
Mutual information:

$$
I\left(X_{1}, X_{2}, \ldots, X_{n} ; Y\right)=\sum_{i=1}^{n} I\left(X_{i} ; Y \mid X_{1}, X_{2}, \ldots, X_{i-1}\right)
$$
<!-- Page 69 -->
상대 엔트로피:

$$
D(p(x, y) \| q(x, y))=D(p(x) \| q(x))+D(p(y \mid x) \| q(y \mid x))
$$

젠센 부등식. 만약 $f$가 볼록 함수라면, $E f(X) \geq f(E X)$입니다.
로그 합 부등식. $n$개의 양수 $a_{1}, a_{2}, \ldots, a_{n}$와 $b_{1}, b_{2}, \ldots, b_{n}$에 대해

$$
\sum_{i=1}^{n} a_{i} \log \frac{a_{i}}{b_{i}} \geq\left(\sum_{i=1}^{n} a_{i}\right) \log \frac{\sum_{i=1}^{n} a_{i}}{\sum_{i=1}^{n} b_{i}}
$$

등호는 $\frac{a_{i}}{b_{i}}=$ 상수일 때만 성립합니다.
데이터 처리 부등식. 만약 $X \rightarrow Y \rightarrow Z$가 마르코프 연쇄를 형성한다면, $I(X ; Y) \geq I(X ; Z)$입니다.

충분 통계량. $T(X)$는 $\left\{f_{\theta}(x)\right\}$에 대해 충분 통계량입니다. 이는 $\theta$에 대한 모든 분포에 대해 $I(\theta ; X)=I(\theta ; T(X))$일 때만 성립합니다.

파노 부등식. $P_{e}=\operatorname{Pr}\{\hat{X}(Y) \neq X\}$라고 할 때,

$$
H\left(P_{e}\right)+P_{e} \log |\lambda| \geq H(X \mid Y)
$$

부등식. 만약 $X$와 $X^{\prime}$가 독립이고 동일하게 분포되어 있다면,

$$
\operatorname{Pr}\left(X=X^{\prime}\right) \geq 2^{-H(X)}
$$

# 연습문제

2.1 동전 던지기. 공정한 동전을 첫 번째 앞면이 나올 때까지 던집니다. $X$를 필요한 던진 횟수로 나타냅니다.
(a) 비트 단위의 엔트로피 $H(X)$를 구하십시오. 다음 표현식이 유용할 수 있습니다:

$$
\sum_{n=0}^{\infty} r^{n}=\frac{1}{1-r}, \quad \sum_{n=0}^{\infty} n r^{n}=\frac{r}{(1-r)^{2}}
$$

(b) 이 분포에 따라 확률 변수 $X$가 추출됩니다. "효율적인" 예-아니오 질문 시퀀스를 찾으십시오. 형식은 다음과 같습니다.
<!-- Page 70 -->
"$X$가 집합 $S$에 포함됩니까?" $H(X)$와 $X$를 결정하는 데 필요한 질문의 기댓값과 비교하십시오.
2.2 함수의 엔트로피. $X$가 유한한 수의 값을 갖는 확률 변수라고 가정합니다. $H(X)$와 $H(Y)$의 (일반적인) 부등식 관계는 어떻게 됩니까? 만약
(a) $Y=2^{X}$ 이면?
(b) $Y=\cos X$ 이면?
2.3 최소 엔트로피. $\mathbf{p}$가 $n$차원 확률 벡터의 집합을 범위로 할 때 $H\left(p_{1}, \ldots, p_{n}\right)=H(\mathbf{p})$의 최소값은 무엇입니까? 이 최소값을 달성하는 모든 $\mathbf{p}$를 찾으십시오.
2.4 확률 변수의 함수의 엔트로피. 이산 확률 변수 $X$가 주어졌을 때, $X$의 함수 엔트로피가 $X$의 엔트로피보다 작거나 같음을 다음 단계를 정당화하여 보이십시오:

$$
\begin{aligned}
H(X, g(X)) & \stackrel{(\mathbf{a})}{=} H(X)+H(g(X) \mid X) \\
& \stackrel{(\mathbf{b})}{=} H(X) \\
H(X, g(X)) & \stackrel{(\mathbf{c})}{=} H(g(X))+H(X \mid g(X)) \\
& \stackrel{(\mathbf{d})}{=} H(g(X))
\end{aligned}
$$

따라서, $H(g(X)) \leq H(X)$입니다.
2.5 0 조건부 엔트로피. $H(Y \mid X)=0$이면 $Y$는 $X$의 함수임을 보이십시오 [즉, $p(x)>0$인 모든 $x$에 대해, $p(x, y)>0$인 $y$의 값은 하나만 존재합니다].
2.6 조건부 상호 정보 대 비조건부 상호 정보. 결합 확률 변수 $X, Y, Z$의 예를 들어 다음을 만족하는 경우를 제시하십시오.
(a) $I(X ; Y \mid Z)<I(X ; Y)$.
(b) $I(X ; Y \mid Z)>I(X ; Y)$.
2.7 동전 저울질. 위조 동전이 있을 수도 있고 없을 수도 있는 $n$개의 동전이 있다고 가정합니다. 위조 동전이 있다면, 다른 동전들보다 더 무겁거나 더 가벼울 수 있습니다. 동전들은 저울로 측정될 것입니다.
(a) 위조 동전(있다면)을 찾아 그것이 더 무거운지 또는 더 가벼운지를 정확하게 선언하기 위해 $k$번의 저울질로 찾을 수 있는 동전 $n$의 개수에 대한 상한을 찾으십시오.
<!-- Page 71 -->
(b) $k=3$번의 무게 측정으로 12개의 동전을 구별하는 전략은 무엇입니까?
2.8 복원 추출과 비복원 추출. 항아리에는 $r$개의 빨간색, $w$개의 흰색, $b$개의 검은색 공이 들어 있습니다. $k \geq 2$개의 공을 항아리에서 복원 추출하는 경우와 비복원 추출하는 경우 중 어느 쪽의 엔트로피가 더 높습니까? 설정하고 그 이유를 설명하십시오. (이것에는 어려운 방법과 비교적 쉬운 방법이 모두 있습니다.)
2.9 거리. 함수 $\rho(x, y)$는 모든 $x, y$에 대해 다음을 만족하면 거리입니다.

- $\rho(x, y) \geq 0$.
- $\rho(x, y)=\rho(y, x)$.
- $\rho(x, y)=0$은 $x=y$일 때 그리고 그럴 때만 성립합니다.
- $\rho(x, y)+\rho(y, z) \geq \rho(x, z)$.
(a) $\rho(X, Y)=H(X \mid Y)+H(Y \mid X)$가 위의 첫 번째, 두 번째, 네 번째 속성을 만족함을 보이십시오. $X=Y$라는 것은 $X$에서 $Y$로의 일대일 함수가 존재한다고 말할 때, 세 번째 속성도 만족하며 $\rho(X, Y)$는 거리입니다.
(b) $\rho(X, Y)$가 다음과 같이 표현될 수도 있음을 확인하십시오.

$$
\begin{aligned}
\rho(X, Y) & =H(X)+H(Y)-2 I(X ; Y) \\
& =H(X, Y)-I(X ; Y) \\
& =2 H(X, Y)-H(X)-H(Y)
\end{aligned}
$$

2.10 분리된 혼합의 엔트로피. $X_{1}$과 $X_{2}$를 각각의 알파벳 $\mathcal{X}_{1}=\{1,2, \ldots, m\}$과 $\mathcal{X}_{2}=\{m+1, \ldots, n\}$에 대해 확률 질량 함수 $p_{1}(\cdot)$와 $p_{2}(\cdot)$에 따라 추출된 이산 확률 변수라고 합시다.

$$
X=\left\{\begin{array}{ll}
X_{1} & \text { 확률 } \alpha \\
X_{2} & \text { 확률 } 1-\alpha
\end{array}\right.
$$

(a) $H\left(X_{1}\right), H\left(X_{2}\right)$, 그리고 $\alpha$로 $H(X)$를 구하십시오.
(b) $\alpha$에 대해 최적화하여 $2^{H(X)} \leq 2^{H\left(X_{1}\right)}+2^{H\left(X_{2}\right)}$임을 보이고, $2^{H(X)}$를 유효 알파벳 크기라는 개념을 사용하여 해석하십시오.
2.11 상관 관계의 척도. $X_{1}$과 $X_{2}$가 동일하게 분포하지만 반드시 독립적이지는 않다고 합시다.

$$
\rho=1-\frac{H\left(X_{2} \mid X_{1}\right)}{H\left(X_{1}\right)}
$$
<!-- Page 72 -->
(a) $\rho=\frac{I\left(X_{1} ; X_{2}\right)}{H\left(X_{1}\right)}$ 임을 보이십시오.
(b) $0 \leq \rho \leq 1$ 임을 보이십시오.
(c) $\rho=0$ 일 때는 언제입니까?
(d) $\rho=1$ 일 때는 언제입니까?
2.12 결합 엔트로피의 예. $p(x, y)$가 다음과 같이 주어졌다고 가정합니다.

다음 값을 구하십시오:
(a) $H(X), H(Y)$.
(b) $H(X \mid Y), H(Y \mid X)$.
(c) $H(X, Y)$.
(d) $H(Y)-H(Y \mid X)$.
(e) $I(X ; Y)$.
(f) (a)부터 (e)까지의 값에 대한 Venn 다이어그램을 그리십시오.
2.13 부등식. $x>0$일 때 $\ln x \geq 1-\frac{1}{x}$ 임을 보이십시오.
2.14 합의 엔트로피. $X$와 $Y$가 각각 $x_{1}, x_{2}, \ldots, x_{r}$ 및 $y_{1}, y_{2}, \ldots, y_{s}$의 값을 갖는 확률 변수라고 가정합니다. $Z=X+Y$라고 가정합니다.
(a) $H(Z \mid X)=H(Y \mid X)$ 임을 보이십시오. $X, Y$가 독립이면 $H(Y) \leq H(Z)$이고 $H(X) \leq H(Z)$임을 논증하십시오. 따라서 독립 확률 변수의 합은 불확실성을 더합니다.
(b) $H(X)>H(Z)$이고 $H(Y)>H(Z)$인 (필연적으로 종속적인) 확률 변수의 예를 드십시오.
(c) $H(Z)=H(X)+H(Y)$는 어떤 조건에서 성립합니까?
2.15 데이터 처리. $X_{1} \rightarrow X_{2} \rightarrow X_{3} \rightarrow \cdots \rightarrow X_{n}$이 순서대로 마르코프 연쇄를 형성한다고 가정합니다. 즉,
$$
p\left(x_{1}, x_{2}, \ldots, x_{n}\right)=p\left(x_{1}\right) p\left(x_{2} \mid x_{1}\right) \cdots p\left(x_{n} \mid x_{n-1}\right)
$$
$I\left(X_{1} ; X_{2}, \ldots, X_{n}\right)$을 가장 간단한 형태로 줄이십시오.
2.16 병목 현상. (비정상) 마르코프 연쇄가 $n$개의 상태 중 하나에서 시작하여 $k<n$개의 상태로 좁아졌다가 다시 $m>k$개의 상태로 퍼진다고 가정합니다. 즉, $X_{1} \rightarrow X_{2} \rightarrow X_{3}$입니다.
<!-- Page 73 -->
$p\left(x_{1}, x_{2}, x_{3}\right)=p\left(x_{1}\right) p\left(x_{2} \mid x_{1}\right) p\left(x_{3} \mid x_{2}\right)$, 모든 $x_{1} \in\{1,2, \ldots, n\}$, $x_{2} \in\{1,2, \ldots, k\}, x_{3} \in\{1,2, \ldots, m\}$에 대하여.
(a) $I\left(X_{1} ; X_{3}\right) \leq \log k$를 증명함으로써 $X_{1}$와 $X_{3}$의 종속성이 병목 현상에 의해 제한됨을 보이십시오.
(b) $k=1$일 때 $I\left(X_{1} ; X_{3}\right)$를 평가하고, 어떠한 종속성도 그러한 병목 현상을 견딜 수 없다고 결론지으십시오.
2.17 순수한 무작위성과 편향된 동전. $X_{1}, X_{2}, \ldots, X_{n}$을 편향된 동전의 독립적인 던지기 결과라고 가정합니다. 따라서 $\operatorname{Pr}\left\{X_{i}=\right.$ $1\}=p, \operatorname{Pr}\left\{X_{i}=0\right\}=1-p$이며, 여기서 $p$는 알려지지 않았습니다. 우리는 $X_{1}, X_{2}, \ldots, X_{n}$으로부터 공정한 동전 던지기 결과인 $Z_{1}, Z_{2}, \ldots, Z_{K}$의 시퀀스를 얻고자 합니다. 이를 위해, $\mathcal{X}^{n} \rightarrow\{0,1\}^{*}$ (여기서 $\{0,1\}^{*}=\{\Lambda, 0,1,00,01, \ldots\}$는 모든 유한 길이의 이진 시퀀스의 집합입니다)인 매핑 $f\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\left(Z_{1}, Z_{2}, \ldots, Z_{K}\right)$를 정의합니다. 여기서 $Z_{i} \sim$ Bernoulli $\left(\frac{1}{2}\right)$이고, $K$는 $\left(X_{1}, \ldots, X_{n}\right)$에 의존할 수 있습니다. 시퀀스 $Z_{1}, Z_{2}, \ldots$가 공정한 동전 던지기 결과처럼 보이게 하기 위해, 편향된 동전 던지기 결과에서 공정한 던지기 결과로의 맵 $f$는 주어진 길이 $k$에 대해 모든 $2^{k}$ 시퀀스 $\left(Z_{1}, Z_{2}, \ldots, Z_{k}\right)$가 동일한 확률(0일 수도 있음)을 갖는 속성을 가져야 합니다. 예를 들어, $n=2$일 때, 맵 $f(01)=0, f(10)=1, f(00)=f(11)=\Lambda$ (널 문자열)은 $\operatorname{Pr}\left\{Z_{1}=1 \mid K=1\right\}=\operatorname{Pr}\left\{Z_{1}=\right.$ $0 \mid K=1\}=\frac{1}{2}$의 속성을 갖습니다. 다음 부등식에 대한 이유를 제시하십시오:

$$
\begin{aligned}
n H(p) & \stackrel{(\mathrm{a})}{=} H\left(X_{1}, \ldots, X_{n}\right) \\
& \stackrel{(\mathrm{b})}{\geq} H\left(Z_{1}, \ldots, Z_{K}, K\right) \\
& \stackrel{(\mathrm{c})}{=} H(K)+H\left(Z_{1}, \ldots, Z_{K} \mid K\right) \\
& \stackrel{(\mathrm{d})}{=} H(K)+E(K) \\
& \geq E K
\end{aligned}
$$

따라서, 평균적으로 $\left(X_{1}, \ldots, X_{n}\right)$으로부터 $n H(p)$보다 더 많은 공정한 동전 던지기 결과를 얻을 수 없습니다. 길이 4인 시퀀스에 대한 좋은 맵 $f$를 제시하십시오.
2.18 월드 시리즈. 월드 시리즈는 한 팀이 4승을 먼저 거두면 종료되는 7전 4선승제 경기입니다. 팀 A와 B 간의 월드 시리즈 결과를 나타내는 확률 변수 $X$를 정의하며, 가능한 값은 AAAA, BABABAB, BBBAAAA입니다. $Y$는 4에서 7까지의 범위를 갖는 경기 수를 나타냅니다. A와 B가 동등한 실력을 가지고 있으며
<!-- Page 74 -->
게임들이 독립적이라고 가정할 때, $H(X), H(Y), H(Y \mid X)$, 그리고 $H(X \mid Y)$를 계산하십시오.
2.19 무한 엔트로피. 이 문제는 이산 확률 변수의 엔트로피가 무한할 수 있음을 보여줍니다. $A=\sum_{n=2}^{\infty}\left(n \log ^{2} n\right)^{-1}$라고 합시다. [무한 합을 $\left.\left(x \log ^{2} x\right)^{-1}\right)$의 적분으로 경계함으로써 $A$가 유한함을 보이는 것은 쉽습니다.] $n=2,3, \ldots$에 대해 $\operatorname{Pr}(X=n)=\left(A n \log ^{2} n\right)^{-1}$로 정의된 정수값 확률 변수 $X$가 $H(X)=+\infty$를 가짐을 보이십시오.
2.20 런 길이 코딩. $X_{1}, X_{2}, \ldots, X_{n}$을 (종속적일 수 있는) 이진 확률 변수라고 합시다. 이 시퀀스의 런 길이 $\mathbf{R}=\left(R_{1}, R_{2}, \ldots\right)$를 계산한다고 가정합니다 (발생하는 순서대로). 예를 들어, 시퀀스 $\mathbf{X}=0001100100$은 런 길이 $\mathbf{R}=(3,2,2,1,2)$를 생성합니다. $H\left(X_{1}, X_{2}, \ldots, X_{n}\right), H(\mathbf{R})$, 그리고 $H\left(X_{n}, \mathbf{R}\right)$을 비교하십시오. 모든 등식과 부등식을 보이고 모든 차이를 경계하십시오.
2.21 확률에 대한 마르코프 부등식. $p(x)$가 확률 질량 함수라고 합시다. 모든 $d \geq 0$에 대해 다음을 증명하십시오.

$$
\operatorname{Pr}\{p(X) \leq d\} \log \frac{1}{d} \leq H(X)
$$

2.22 아이디어의 논리적 순서. 아이디어는 필요에 따라 개발되었고 필요한 경우 일반화되었습니다. 다음 아이디어를 가장 강력한 것부터 순서대로 재정렬하고, 그 뒤에 함의를 따르게 하십시오:
(a) $I\left(X_{1}, \ldots, X_{n} ; Y\right)$에 대한 연쇄 법칙, $D\left(p\left(x_{1}, \ldots\right.\right.$, $\left.\left.x_{n}\right) \| q\left(x_{1}, x_{2}, \ldots, x_{n}\right)\right)$에 대한 연쇄 법칙, 그리고 $H\left(X_{1}, X_{2}, \ldots, X_{n}\right)$에 대한 연쇄 법칙.
(b) $D(f \| g) \geq 0$, 젠센 부등식, $I(X ; Y) \geq 0$.
2.23 조건부 상호 정보량. $n$개의 이진 확률 변수 $X_{1}, X_{2}, \ldots, X_{n}$의 시퀀스를 고려하십시오. 1의 개수가 짝수인 각 시퀀스는 확률 $2^{-(n-1)}$을 가지며, 1의 개수가 홀수인 각 시퀀스는 확률 0을 가집니다. 상호 정보량을 찾으십시오.

$$
I\left(X_{1} ; X_{2}\right), \quad I\left(X_{2} ; X_{3} \mid X_{1}\right), \ldots, I\left(X_{n-1} ; X_{n} \mid X_{1}, \ldots, X_{n-2}\right)
$$

2.24 평균 엔트로피. $H(p)=-p \log _{2} p-(1-p) \log _{2}(1-p)$를 이진 엔트로피 함수라고 합시다.
(a) $\log _{2} 3 \approx 1.584$라는 사실을 사용하여 $H\left(\frac{1}{4}\right)$를 평가하십시오. (힌트: 네 개의 동일하게 가능성 있는 결과가 있고 그중 하나가 다른 것들보다 더 흥미로운 실험을 고려하는 것이 유용할 수 있습니다.)
<!-- Page 75 -->
(b) 확률 $p$가 $0 \leq p \leq 1$ 범위에서 균일하게 선택될 때 평균 엔트로피 $H(p)$를 계산하십시오.
(c) (선택 사항) $\left(p_{1}, p_{2}, p_{3}\right)$가 균일하게 분포된 확률 벡터일 때 평균 엔트로피 $H\left(p_{1}, p_{2}, p_{3}\right)$를 계산하십시오. 차원 $n$에 대해 일반화하십시오.
2.25 벤 다이어그램. 세 개의 랜덤 변수에 대한 상호 정보의 개념은 실제로 존재하지 않습니다. 한 가지 시도는 다음과 같습니다. 벤 다이어그램을 사용하여 세 개의 랜덤 변수 $X, Y, Z$에 공통인 상호 정보는 다음과 같이 정의될 수 있습니다.

$$
I(X ; Y ; Z)=I(X ; Y)-I(X ; Y \mid Z)
$$

이 양은 앞선 비대칭 정의에도 불구하고 $X, Y, Z$에 대해 대칭입니다. 불행히도 $I(X ; Y ; Z)$는 반드시 음수가 아닌 것은 아닙니다. $I(X ; Y ; Z)<0$인 $X, Y, Z$를 찾고 다음 두 항등식을 증명하십시오.
(a) $I(X ; Y ; Z)=H(X, Y, Z)-H(X)-H(Y)-H(Z)+$ $I(X ; Y)+I(Y ; Z)+I(Z ; X)$.
(b) $I(X ; Y ; Z)=H(X, Y, Z)-H(X, Y)-H(Y, Z)-$ $H(Z, X)+H(X)+H(Y)+H(Z)$.
첫 번째 항등식은 엔트로피와 상호 정보에 대한 벤 다이어그램 비유를 사용하여 이해할 수 있습니다. 두 번째 항등식은 첫 번째 항등식에서 쉽게 도출됩니다.
2.26 상대 엔트로피의 비음수성에 대한 또 다른 증명. 결과 $D(p \| q) \geq 0$의 근본적인 성격을 고려하여 또 다른 증명을 제시하겠습니다.
(a) $0<x<\infty$에 대해 $\ln x \leq x-1$임을 보이십시오.
(b) 다음 단계들을 정당화하십시오:

$$
\begin{aligned}
-D(p \| q) & =\sum_{x} p(x) \ln \frac{q(x)}{p(x)} \\
& \leq \sum_{x} p(x)\left(\frac{q(x)}{p(x)}-1\right) \\
& \leq 0
\end{aligned}
$$

(c) 등호 조건은 무엇입니까?
2.27 엔트로피에 대한 그룹화 규칙. $m$개의 원소에 대한 확률 분포를 $\mathbf{p}=\left(p_{1}, p_{2}, \ldots, p_{m}\right)$라고 합시다 (즉, $p_{i} \geq 0$이고 $\sum_{i=1}^{m} p_{i}=1$ ).
<!-- Page 76 -->
새로운 분포 $\mathbf{q}$를 $m-1$개의 원소에 대해 $q_{1}=p_{1}, q_{2}=p_{2}$, $\ldots, q_{m-2}=p_{m-2}$, 그리고 $q_{m-1}=p_{m-1}+p_{m}$으로 정의합니다 [즉, 분포 $\mathbf{q}$는 $\{1,2, \ldots, m-2\}$에 대해 $\mathbf{p}$와 동일하며, $\mathbf{q}$의 마지막 원소의 확률은 $\mathbf{p}$의 마지막 두 확률의 합입니다]. 다음을 보이십시오.

$$
H(\mathbf{p})=H(\mathbf{q})+\left(p_{m-1}+p_{m}\right) H\left(\frac{p_{m-1}}{p_{m-1}+p_{m}}, \frac{p_{m}}{p_{m-1}+p_{m}}\right)
$$

2.28 혼합은 엔트로피를 증가시킵니다. 확률 분포 $\left(p_{1}, \ldots, p_{i}, \ldots, p_{j}, \ldots, p_{m}\right)$의 엔트로피가 분포 $\left(p_{1}, \ldots, \frac{p_{i}+p_{j}}{2}, \ldots, \frac{p_{i}+p_{j}}{2}\right.$, $\left.\ldots, p_{m}\right)$의 엔트로피보다 작음을 보이십시오. 일반적으로 분포를 더 균일하게 만드는 모든 확률 이전은 엔트로피를 증가시킴을 보이십시오.
2.29 부등식. $X, Y$, 그리고 $Z$가 결합 확률 변수라고 가정합니다. 다음 부등식을 증명하고 등호가 성립하는 조건을 찾으십시오.
(a) $H(X, Y \mid Z) \geq H(X \mid Z)$.
(b) $I(X, Y ; Z) \geq I(X ; Z)$.
(c) $H(X, Y, Z)-H(X, Y) \leq H(X, Z)-H(X)$.
(d) $I(X ; Z \mid Y) \geq I(Z ; Y \mid X)-I(Z ; Y)+I(X ; Z)$.
2.30 최대 엔트로피. 음이 아닌 정수 값을 갖는 확률 변수 $X$의 엔트로피 $H(X)$를 제약 조건 하에서 최대화하는 확률 질량 함수 $p(x)$를 찾으십시오.

$$
E X=\sum_{n=0}^{\infty} n p(n)=A
$$

고정된 값 $A>0$에 대해. 이 최대 $H(X)$를 평가하십시오.
2.31 조건부 엔트로피. 어떤 조건에서 $H(X \mid g(Y))=$ $H(X \mid Y)$가 성립합니까?
2.32 파노. $(X, Y)$에 대한 다음 결합 분포가 주어졌습니다.

| $X$ | $a$ | $b$ | $c$ |
| :--: | :--: | :--: | :--: |
| 1 | $\frac{1}{6}$ | $\frac{1}{12}$ | $\frac{1}{12}$ |
| 2 | $\frac{1}{12}$ | $\frac{1}{6}$ | $\frac{1}{12}$ |
| 3 | $\frac{1}{12}$ | $\frac{1}{12}$ | $\frac{1}{6}$ |
<!-- Page 77 -->
$\hat{X}(Y)$를 $X$에 대한 추정량( $Y$에 기반한)이라고 하고 $P_{e}=$ $\operatorname{Pr}\{\hat{X}(Y) \neq X\}$라고 합시다.
(a) 최소 오류 확률 추정량 $\hat{X}(Y)$와 관련된 $P_{e}$를 찾으십시오.
(b) 이 문제에 대해 Fano의 부등식을 평가하고 비교하십시오.
2.33 Fano의 부등식. $\operatorname{Pr}(X=i)=p_{i}, i=1,2, \ldots, m$이고 $p_{1} \geq p_{2} \geq p_{3} \geq \cdots \geq p_{m}$이라고 합시다. $X$의 최소 오류 확률 예측자는 $\hat{X}=1$이며, 결과적인 오류 확률은 $P_{e}=$ $1-p_{1}$입니다. $1-p_{1}=P_{e}$라는 제약 조건 하에서 $H(\mathbf{p})$를 최대화하여 $H$에 대한 $P_{e}$의 경계를 찾으십시오. 이것은 조건이 없는 경우의 Fano의 부등식입니다.
2.34 초기 조건의 엔트로피. 모든 마르코프 연쇄에 대해 $H\left(X_{0} \mid X_{n}\right)$이 $n$에 따라 비감소함을 증명하십시오.
2.35 상대 엔트로피는 대칭이 아닙니다.

확률 변수 $X$가 세 가지 가능한 결과 $\{a, b, c\}$를 갖는다고 합시다. 이 확률 변수에 대한 두 가지 분포를 고려하십시오:

| 기호 | $p(x)$ | $q(x)$ |
| :--: | :--: | :--: |
| $a$ | $\frac{1}{2}$ | $\frac{1}{3}$ |
| $b$ | $\frac{1}{4}$ | $\frac{1}{3}$ |
| $c$ | $\frac{1}{4}$ | $\frac{1}{3}$ |

$H(p), H(q), D(p \| q), D(q \| p)$를 계산하십시오. 이 경우 $D(p \| q) \neq D(q \| p)$임을 확인하십시오.
2.36 대칭 상대 엔트로피. 문제 2.35에서 보듯이 일반적으로 $D(p \| q) \neq D(q \| p)$이지만, 등식이 성립하는 분포가 있을 수 있습니다. $D(p \| q)=D(q \| p)$가 되는 이진 알파벳에 대한 두 분포 $p$와 $q$의 예시를 ( $p=q$라는 자명한 경우를 제외하고) 제시하십시오.
2.37 상대 엔트로피. $X, Y, Z$를 결합 확률 질량 함수 $p(x, y, z)$를 갖는 세 개의 확률 변수라고 합시다. 결합 분포와 주변 분포의 곱 사이의 상대 엔트로피는 다음과 같습니다.

$$
D(p(x, y, z) \| p(x) p(y) p(z))=E\left[\log \frac{p(x, y, z)}{p(x) p(y) p(z)}\right]
$$

이를 엔트로피로 확장하십시오. 이 양은 언제 0이 됩니까?
<!-- Page 78 -->
2.38 질문의 가치. $X \sim p(x), x=1,2, \ldots, m$이라고 가정합니다. 집합 $S \subseteq\{1,2, \ldots, m\}$이 주어졌습니다. $X \in S$인지 묻고 다음과 같은 답변을 받습니다.

$$
Y= \begin{cases}1 & \text { if } X \in S \\ 0 & \text { if } X \notin S\end{cases}
$$

$\operatorname{Pr}\{X \in S\}=\alpha$라고 가정합니다. 불확실성의 감소량 $H(X)-H(X \mid Y)$를 구하십시오.
주어진 $\alpha$에 대해 어떤 집합 $S$도 다른 집합과 마찬가지로 좋다는 것이 분명합니다.
2.39 엔트로피와 쌍별 독립. $X, Y, Z$는 세 개의 이진 베르누이 $\left(\frac{1}{2}\right)$ 확률 변수이며 쌍별 독립입니다. 즉, $I(X ; Y)=I(X ; Z)=I(Y ; Z)=0$입니다.
(a) 이 제약 조건 하에서 $H(X, Y, Z)$의 최소값은 얼마입니까?
(b) 이 최소값을 달성하는 예를 제시하십시오.
2.40 이산 엔트로피. $X$와 $Y$는 두 개의 독립적인 정수값 확률 변수입니다. $X$는 $\{1,2, \ldots, 8\}$ 위에서 균등하게 분포되고, $\operatorname{Pr}\{Y=k\}=2^{-k}, k=1,2,3, \ldots$입니다.
(a) $H(X)$를 구하십시오.
(b) $H(Y)$를 구하십시오.
(c) $H(X+Y, X-Y)$를 구하십시오.
2.41 무작위 질문. $X \sim p(x)$인 무작위 객체를 식별하고자 합니다. $r(q)$에 따라 무작위로 질문 $Q \sim r(q)$를 합니다. 이는 결정론적 답변 $A=A(x, q) \in\left\{a_{1}, a_{2}, \ldots\right\}$를 생성합니다. $X$와 $Q$가 독립이라고 가정합니다. 그러면 $I(X ; Q, A)$는 질문-답변 $(Q, A)$에 의해 제거된 $X$의 불확실성입니다.
(a) $I(X ; Q, A)=H(A \mid Q)$임을 보이십시오. 해석하십시오.
(b) 이제 두 개의 i.i.d. 질문 $Q_{1}, Q_{2}, \sim r(q)$를 하여 답변 $A_{1}$과 $A_{2}$를 얻는다고 가정합니다. 두 질문은 단일 질문의 두 배보다 가치가 적다는 것을 $I\left(X ; Q_{1}, A_{1}, Q_{2}, A_{2}\right) \leq 2 I\left(X ; Q_{1}, A_{1}\right)$를 보여줌으로써 증명하십시오.
2.42 부등식. 다음 부등식 중 어느 것이 일반적으로 $\geq,=, \leq$입니까? 각 항목에 $\geq,=$, 또는 $\leq$로 표시하십시오.
(a) $H(5 X)$ 대 $H(X)$
(b) $I(g(X) ; Y)$ 대 $I(X ; Y)$
(c) $H\left(X_{0} \mid X_{-1}\right)$ 대 $H\left(X_{0} \mid X_{-1}, X_{1}\right)$
(d) $H(X, Y) /(H(X)+H(Y))$ 대 1
<!-- Page 79 -->
2.43 앞면과 뒷면의 상호 정보량
(a) 공정한 동전 던지기를 고려하십시오. 동전의 윗면과 아랫면 사이의 상호 정보량은 얼마입니까?
(b) 여섯 면짜리 공정한 주사위를 굴립니다. 윗면과 가장 가까운 면(가장 앞에 보이는 면) 사이의 상호 정보량은 얼마입니까?
2.44 순수한 무작위성. 세 면짜리 동전을 사용하여 공정한 동전 던지기를 생성하려고 합니다. 동전 $X$가 확률 질량 함수를 갖는다고 가정합니다.

$$
X=\left\{\begin{array}{ll}
A, & p_{A} \\
B, & p_{B} \\
C, & p_{C}
\end{array}\right.
$$

여기서 $p_{A}, p_{B}, p_{C}$는 알려지지 않았습니다.
(a) 두 번의 독립적인 던지기 $X_{1}, X_{2}$를 사용하여 베르누이 $\left(\frac{1}{2}\right)$ 확률 변수 $Z$를 생성하려면 어떻게 해야 합니까(가능하다면)?
(b) 생성된 공정한 비트의 최대 기대값은 얼마입니까?
2.45 유한 엔트로피. 이산 확률 변수 $X \in$ $\{1,2, \ldots\}$에 대해 $E \log X<\infty$이면 $H(X)<\infty$임을 보이십시오.
2.46 엔트로피의 공리적 정의 (어려움). 정보 측정에 대한 특정 공리를 가정하면 엔트로피와 같은 로그 측정값을 사용해야 합니다. Shannon은 이를 엔트로피의 초기 정의를 정당화하는 데 사용했습니다. 이 책에서는 엔트로피의 사용을 정당화하기 위해 공리적 유도보다는 엔트로피의 다른 속성에 더 의존합니다. 다음 문제는 이 섹션의 다른 문제보다 상당히 어렵습니다.

대칭 함수 $H_{m}\left(p_{1}, p_{2}, \ldots, p_{m}\right)$의 시퀀스가 다음 속성을 만족한다고 가정합니다.

- 정규화: $H_{2}\left(\frac{1}{2}, \frac{1}{2}\right)=1$,
- 연속성: $H_{2}(p, 1-p)$는 $p$의 연속 함수입니다.
- 그룹화: $H_{m}\left(p_{1}, p_{2}, \ldots, p_{m}\right)=H_{m-1}\left(p_{1}+p_{2}, p_{3}, \ldots, p_{m}\right)+$ $\left(p_{1}+p_{2}\right) H_{2}\left(\frac{p_{1}}{p_{1}+p_{2}}, \frac{p_{2}}{p_{1}+p_{2}}\right)$,

$H_{m}$이 다음과 같은 형태여야 함을 증명하십시오.

$$
H_{m}\left(p_{1}, p_{2}, \ldots, p_{m}\right)=-\sum_{i=1}^{m} p_{i} \log p_{i}, \quad m=2,3, \ldots
$$
<!-- Page 80 -->
엔트로피의 동일한 정의를 도출하는 다양한 다른 공리적 공식이 존재합니다. 예를 들어, Csiszár와 Körner의 서적 [149]을 참조하십시오.
2.47 잘못 정렬된 파일의 엔트로피. 순서 $1,2, \ldots, n$으로 정렬된 $n$장의 카드 덱이 제공됩니다. 카드를 무작위로 하나 제거한 다음 무작위로 다시 넣습니다. 결과 덱의 엔트로피는 얼마입니까?
2.48 시퀀스 길이. 시퀀스의 길이는 시퀀스의 내용에 대해 얼마나 많은 정보를 제공합니까? 베르누이 $\left(\frac{1}{2}\right)$ 프로세스 $\left\{X_{i}\right\}$를 고려한다고 가정합니다. 첫 번째 1이 나타날 때 프로세스를 중지합니다. 이 중지 시간을 $N$으로 지정합니다. 따라서 $X^{N}$은 모든 유한 길이 이진 시퀀스 집합 $\{0,1\}^{*}=$ $\{0,1,00,01,10,11,000, \ldots\}$의 요소입니다.
(a) $I\left(N ; X^{N}\right)$을 찾으십시오.
(b) $H\left(X^{N} \mid N\right)$을 찾으십시오.
(c) $H\left(X^{N}\right)$을 찾으십시오.

이제 다른 중지 시간을 고려해 보겠습니다. 이 부분에서는 다시 $X_{i} \sim \operatorname{Bernoulli}\left(\frac{1}{2}\right)$라고 가정하지만, 확률 $\frac{1}{3}$으로 시간 $N=6$에서 중지하고 확률 $\frac{2}{3}$으로 시간 $N=12$에서 중지합니다. 이 중지 시간은 시퀀스 $X_{1} X_{2} \cdots X_{12}$와 독립적이라고 가정합니다.
(d) $I\left(N ; X^{N}\right)$을 찾으십시오.
(e) $H\left(X^{N} \mid N\right)$을 찾으십시오.
(f) $H\left(X^{N}\right)$을 찾으십시오.

# 역사적 고찰

엔트로피 개념은 열역학에서 도입되었으며, 여기서 열역학 제2법칙을 진술하는 데 사용되었습니다. 이후 통계 역학은 열역학적 엔트로피와 시스템의 거시 상태에 대한 미시 상태 수의 로그 사이에 연결을 제공했습니다. 이 작업은 $S=k \ln W$ 방정식을 그의 묘비명으로 새긴 Boltzmann의 업적의 정점입니다 [361].

1930년대에 Hartley는 통신을 위한 정보의 로그 측정값을 도입했습니다. 그의 측정값은 본질적으로 알파벳 크기의 로그였습니다. Shannon [472]은 이 장에서 정의된 엔트로피와 상호 정보를 처음으로 정의했습니다. 상대 엔트로피는 Kullback과 Leibler [339]에 의해 처음 정의되었습니다. 이는 Kullback-Leibler 거리, 교차 엔트로피, 정보 발산, 판별 정보 등 다양한 이름으로 알려져 있으며 Csiszár [138]와 Amari [22]에 의해 자세히 연구되었습니다.
<!-- Page 81 -->
이러한 양들의 단순한 속성 중 다수는 Shannon에 의해 개발되었습니다. Fano의 부등식은 Fano [201]에서 증명되었습니다. 충분 통계량의 개념은 Fisher [209]에 의해 정의되었으며, 최소 충분 통계량의 개념은 Lehmann과 Scheffé [350]에 의해 도입되었습니다. mutual information과 충분성의 관계는 Kullback [335]에 기인합니다. 정보 이론과 thermodynamics의 관계는 Brillouin [77]과 Jaynes [294]에 의해 광범위하게 논의되었습니다.

정보의 물리학은 통계 역학, 양자 역학, 정보 이론에서 파생된 방대한 새로운 탐구 주제입니다. 핵심 질문은 정보가 물리적으로 어떻게 표현되는가입니다. 양자 channel capacity (물리 시스템의 구별 가능한 준비 수의 로그)와 양자 데이터 압축 [299]은 von Neumann entropy를 포함하는 좋은 해답을 가진 잘 정의된 문제입니다. 양자 정보의 새로운 요소는 양자 얽힘의 존재와 관찰된 물리적 사건의 주변 분포가 어떤 결합 분포 (local realism 없음)와도 일치하지 않는다는 결과 (Bell의 부등식에서 나타남)에서 발생합니다. Nielsen과 Chuang [395]의 기본 텍스트는 양자 정보 이론과 이 책의 많은 결과에 대한 양자 대응을 개발합니다. 또한 Bennett [47]과 Bennett 및 Landauer [48]의 연구를 포함하여 계산에 대한 근본적인 물리적 한계가 있는지 여부를 결정하려는 시도도 있었습니다.
<!-- Page 82 -->
.
<!-- Page 83 -->
# CHAPTER 3

## 점근적 균등 분할 속성 (ASYMPTOTIC EQUIPARTITION PROPERTY)

정보 이론에서 대수의 법칙에 해당하는 것이 점근적 균등 분할 속성(AEP)입니다. 이는 약한 대수의 법칙의 직접적인 결과입니다. 대수의 법칙은 독립적이고 동일하게 분포된(i.i.d.) 확률 변수에 대해, $n$이 클 때 $\frac{1}{n} \sum_{i=1}^{n} X_{i}$는 기댓값 $E X$에 가깝다고 말합니다. AEP는 $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. 확률 변수이고 $p\left(X_{1}, X_{2}, \ldots, X_{n}\right)$이 시퀀스 $X_{1}, X_{2}, \ldots, X_{n}$을 관찰할 확률일 때, $\frac{1}{n} \log \frac{1}{p\left(X_{1}, X_{2}, \ldots, X_{n}\right)}$이 entropy $H$에 가깝다고 말합니다. 따라서 관찰된 시퀀스에 할당된 확률 $p\left(X_{1}, X_{2}, \ldots, X_{n}\right)$은 $2^{-n H}$에 가까울 것입니다.

이를 통해 우리는 모든 시퀀스의 집합을 두 개의 집합으로 나눌 수 있습니다. 하나는 표본 entropy가 실제 entropy에 가까운 전형적인 집합(typical set)이고, 다른 하나는 나머지 시퀀스를 포함하는 비전형적인 집합(nontypical set)입니다. 우리의 관심은 대부분 전형적인 시퀀스에 집중될 것입니다. 전형적인 시퀀스에 대해 증명된 모든 속성은 높은 확률로 참이 될 것이며, 큰 표본의 평균 행동을 결정할 것입니다.

먼저 예시를 들어보겠습니다. 확률 변수 $X \in\{0,1\}$이 $p(1)=p$와 $p(0)=q$로 정의된 확률 질량 함수를 가진다고 가정합니다. 만약 $X_{1}, X_{2}, \ldots, X_{n}$이 $p(x)$에 따라 i.i.d.라면, 시퀀스 $x_{1}, x_{2}, \ldots, x_{n}$의 확률은 $\prod_{i=1}^{n} p\left(x_{i}\right)$입니다. 예를 들어, 시퀀스 $(1,0,1,1,0,1)$의 확률은 $p^{\sum X_{i}} q^{n-\sum X_{i}}=p^{4} q^{2}$입니다. 명백히, 길이가 $n$인 모든 $2^{n}$개의 시퀀스가 동일한 확률을 갖는 것은 아닙니다.

하지만 우리가 실제로 관찰하는 시퀀스의 확률을 예측할 수 있을지도 모릅니다. 우리는 $X_{1}, X_{2}, \ldots$이 i.i.d. $\sim p(x)$일 때, 결과 $X_{1}, X_{2}, \ldots, X_{n}$의 확률 $p\left(X_{1}, X_{2}, \ldots, X_{n}\right)$을 묻습니다. 이는 자기 참조적이지만 그럼에도 불구하고 잘 정의됩니다. 명백히, 우리는 동일한 분포에 따라 그려진 사건의 확률을 묻고 있습니다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (c) 2006 John Wiley \& Sons, Inc.
<!-- Page 84 -->
확률 분포입니다. 여기서 $p\left(X_{1}, X_{2}, \ldots, X_{n}\right)$이 높은 확률로 $2^{-n H}$에 가까워진다는 것을 알 수 있습니다.

이를 "거의 모든 사건은 거의 동일하게 놀랍다"고 요약합니다. 이는 다음과 같이 말하는 한 방법입니다.

$$
\operatorname{Pr}\left\{\left(X_{1}, X_{2}, \ldots, X_{n}\right): p\left(X_{1}, X_{2}, \ldots, X_{n}\right)=2^{-n(H \pm \epsilon)}\right\} \approx 1
$$

만약 $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim p(x)$라면 말입니다.
방금 주어진 예시에서 $p\left(X_{1}, X_{2}, \ldots, X_{n}\right)=p^{\sum X_{i}} q^{n-\sum X_{i}}$인 경우, 우리는 단순히 시퀀스에서 1의 개수가 $n p$에 가깝다는 것(높은 확률로)을 말하고 있으며, 이러한 모든 시퀀스는 (대략적으로) 동일한 확률 $2^{-n H(p)}$을 갖습니다. 우리는 다음과 같이 정의되는 확률 수렴의 개념을 사용합니다.

정의 (확률 변수의 수렴). 확률 변수 시퀀스 $X_{1}, X_{2}, \ldots$가 주어졌을 때, 시퀀스 $X_{1}, X_{2}, \ldots$가 확률 변수 $X$로 수렴한다고 말합니다.

1. 확률적으로 (in probability) 만약 모든 $\epsilon>0$에 대해 $\operatorname{Pr}\left\{\left|X_{n}-X\right|>\epsilon\right\} \rightarrow 0$
2. 평균 제곱으로 (in mean square) 만약 $E\left(X_{n}-X\right)^{2} \rightarrow 0$
3. 확률 1로 (almost surely) 만약 $\operatorname{Pr}\left\{\lim _{n \rightarrow \infty} X_{n}=\right.$ $X\}=1$

# 3.1 점근적 균등 분할 속성 정리

점근적 균등 분할 속성은 다음 정리로 형식화됩니다.

정리 3.1.1 (AEP) 만약 $X_{1}, X_{2}, \ldots$가 i.i.d. $\sim p(x)$라면,

$$
-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right) \rightarrow H(X) \quad \text { 확률적으로. }
$$

증명: 독립 확률 변수의 함수도 독립 확률 변수입니다. 따라서 $X_{i}$가 i.i.d.이므로 $\log p\left(X_{i}\right)$도 i.i.d.입니다. 그러므로 대수의 약한 법칙에 의해,

$$
\begin{aligned}
-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right) & =-\frac{1}{n} \sum_{i} \log p\left(X_{i}\right) \\
& \rightarrow-E \log p(X) \quad \text { 확률적으로 } \\
& =H(X)
\end{aligned}
$$

이는 정리를 증명합니다.
<!-- Page 85 -->
정의 $p(x)$에 대한 전형 집합 $A_{\epsilon}^{(n)}$은 다음 속성을 갖는 시퀀스 $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in \mathcal{X}^{n}$의 집합입니다.

$$
2^{-n(H(X)+\epsilon)} \leq p\left(x_{1}, x_{2}, \ldots, x_{n}\right) \leq 2^{-n(H(X)-\epsilon)}
$$

AEP의 결과로, 집합 $A_{\epsilon}^{(n)}$이 다음 속성을 갖는다는 것을 보일 수 있습니다.

# 정리 3.1.2

1. 만약 $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in A_{\epsilon}^{(n)}$이면, $H(X)-\epsilon \leq-\frac{1}{n} \log p\left(x_{1}, x_{2}, \ldots\right.$, $\left.x_{n}\right) \leq H(X)+\epsilon$.
2. $n$이 충분히 크면 $\operatorname{Pr}\left\{A_{\epsilon}^{(n)}\right\}>1-\epsilon$.
3. $\left|A_{\epsilon}^{(n)}\right| \leq 2^{n(H(X)+\epsilon)}$, 여기서 $|A|$는 집합 $A$의 원소 수를 나타냅니다.
4. $n$이 충분히 크면 $\left|A_{\epsilon}^{(n)}\right| \geq(1-\epsilon) 2^{n(H(X)-\epsilon)}$.

따라서 전형 집합은 확률이 거의 1이고, 전형 집합의 모든 원소는 거의 균등 확률이며, 전형 집합의 원소 수는 거의 $2^{n H}$입니다.

증명: 속성 (1)의 증명은 $A_{\epsilon}^{(n)}$의 정의로부터 즉시 얻어집니다. 두 번째 속성은 Theorem 3.1.1에서 직접적으로 따르며, $n \rightarrow \infty$일 때 $\left(X_{1}, X_{2}, \ldots, X_{n}\right) \in A_{\epsilon}^{(n)}$ 사건의 확률이 1로 수렴하기 때문입니다. 따라서 임의의 $\delta>0$에 대해, 모든 $n \geq n_{0}$에 대해 다음과 같은 $n_{0}$가 존재합니다.

$$
\operatorname{Pr}\left\{\left|-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right)-H(X)\right|<\epsilon\right\}>1-\delta
$$

$\delta=\epsilon$으로 설정하면 정리의 두 번째 부분을 얻습니다. $\delta=\epsilon$으로 설정하면 나중에 표기법을 편리하게 단순화할 수 있습니다.

속성 (3)을 증명하기 위해 다음과 같이 작성합니다.

$$
\begin{aligned}
1 & =\sum_{\mathbf{x} \in \mathcal{X}^{n}} p(\mathbf{x}) \\
& \geq \sum_{\mathbf{x} \in A_{\epsilon}^{(n)}} p(\mathbf{x}) \\
& \geq \sum_{\mathbf{x} \in A_{\epsilon}^{(n)}} 2^{-n(H(X)+\epsilon)} \\
& =2^{-n(H(X)+\epsilon)}\left|A_{\epsilon}^{(n)}\right|
\end{aligned}
$$
<!-- Page 86 -->
두 번째 부등식은 (3.6)으로부터 도출됩니다. 따라서

$$
\left|A_{\epsilon}^{(n)}\right| \leq 2^{n(H(X)+\epsilon)}
$$

마지막으로, 충분히 큰 $n$에 대해 $\operatorname{Pr}\left\{A_{\epsilon}^{(n)}\right\}>1-\epsilon$이므로,

$$
\begin{aligned}
1-\epsilon & <\operatorname{Pr}\left\{A_{\epsilon}^{(n)}\right\} \\
& \leq \sum_{\mathbf{x} \in A_{\epsilon}^{(n)}} 2^{-n(H(X)-\epsilon)} \\
& =2^{-n(H(X)-\epsilon)}\left|A_{\epsilon}^{(n)}\right|
\end{aligned}
$$

여기서 두 번째 부등식은 (3.6)으로부터 도출됩니다. 따라서,

$$
\left|A_{\epsilon}^{(n)}\right| \geq(1-\epsilon) 2^{n(H(X)-\epsilon)}
$$

이는 $A_{\epsilon}^{(n)}$의 속성에 대한 증명을 완료합니다.

# 3.2 AEP의 결과: 데이터 압축

$X_{1}, X_{2}, \ldots, X_{n}$을 확률 질량 함수 $p(x)$에서 추출된 독립적이고 동일하게 분포된 확률 변수라고 가정합니다. 이러한 확률 변수 시퀀스에 대한 짧은 설명을 찾고자 합니다. $\mathcal{X}^{n}$의 모든 시퀀스를 두 개의 집합으로 나눕니다: 그림 3.1에 표시된 대로 전형 집합 $A_{\epsilon}^{(n)}$과 그 여집합입니다.

그림 3.1. 전형 집합과 소스 코딩.
<!-- Page 87 -->

그림 3.2. 일반적인 집합을 사용한 소스 코드.

각 집합의 모든 요소를 어떤 순서(예: 사전식 순서)에 따라 정렬합니다. 그러면 $A_{\epsilon}^{(n)}$의 각 시퀀스를 해당 집합에서의 시퀀스 인덱스를 제공하여 표현할 수 있습니다. $A_{\epsilon}^{(n)}$에는 $\leq 2^{n(H+\epsilon)}$개의 시퀀스가 있으므로, 인덱싱에는 $n(H+\epsilon)+1$ 비트 이상이 필요하지 않습니다. [ $n(H+\epsilon)$이 정수가 아닐 수 있으므로 추가 비트가 필요할 수 있습니다.] 이 모든 시퀀스 앞에 0을 접두사로 붙이면 $A_{\epsilon}^{(n)}$의 각 시퀀스를 표현하는 데 총 $\leq n(H+\epsilon)+2$ 비트가 소요됩니다(그림 3.2 참조). 마찬가지로, $n \log |\mathcal{X}|+1$ 비트 이하를 사용하여 $A_{\epsilon}^{(n)}$에 속하지 않는 각 시퀀스를 인덱싱할 수 있습니다. 이러한 인덱스 앞에 1을 접두사로 붙이면 $\mathcal{X}^{n}$의 모든 시퀀스에 대한 코드가 생성됩니다.

위의 코딩 방식의 다음 특징에 주목하십시오.

- 코드는 일대일이며 쉽게 디코딩할 수 있습니다. 초기 비트는 뒤따르는 코드워드의 길이를 나타내는 플래그 비트 역할을 합니다.
- 비정상 집합 $A_{\epsilon}^{(n)^{c}}$의 요소 수가 $\mathcal{X}^{n}$의 요소 수보다 적다는 사실을 고려하지 않고, 비정상 집합 $A_{\epsilon}^{(n)^{c}}$를 무차별 대입 방식으로 열거했습니다. 놀랍게도 이는 효율적인 설명을 제공하기에 충분합니다.
- 일반적인 시퀀스는 길이가 $\approx n H$인 짧은 설명을 가집니다.

시퀀스 $x_{1}, x_{2}, \ldots, x_{n}$을 나타내기 위해 $x^{n}$ 표기법을 사용합니다. $x^{n}$에 해당하는 코드워드의 길이를 $l\left(x^{n}\right)$이라고 합시다. $n$이 충분히 커서 $\operatorname{Pr}\left\{A_{\epsilon}^{(n)}\right\} \geq 1-\epsilon$이면, 코드워드의 기대 길이는 다음과 같습니다.

$$
E\left(l\left(X^{n}\right)\right)=\sum_{x^{n}} p\left(x^{n}\right) l\left(x^{n}\right)
$$
<!-- Page 88 -->
$$
\begin{aligned}
= & \sum_{x^{n} \in A_{\epsilon}^{(n)}} p\left(x^{n}\right) l\left(x^{n}\right)+\sum_{x^{n} \in A_{\epsilon}^{(n)^{c}}} p\left(x^{n}\right) l\left(x^{n}\right) \\
\leq & \sum_{x^{n} \in A_{\epsilon}^{(n)}} p\left(x^{n}\right)(n(H+\epsilon)+2) \\
& +\sum_{x^{n} \in A_{\epsilon}^{(n)^{c}}} p\left(x^{n}\right)(n \log |\mathcal{X}|+2) \\
= & \operatorname{Pr}\left\{A_{\epsilon}^{(n)}\right\}(n(H+\epsilon)+2)+\operatorname{Pr}\left\{A_{\epsilon}^{(n)^{c}}\right\}(n \log |\mathcal{X}|+2) \\
\leq & n(H+\epsilon)+\epsilon n(\log |\mathcal{X}|)+2 \\
= & n\left(H+\epsilon^{\prime}\right)
\end{aligned}
$$

여기서 $\epsilon^{\prime}=\epsilon+\epsilon \log |\mathcal{X}|+\frac{2}{n}$는 적절한 $\epsilon$의 선택과 그에 따른 적절한 $n$의 선택으로 임의로 작게 만들 수 있습니다. 따라서 다음 정리를 증명했습니다.

정리 3.2.1 $X^{n}$이 i.i.d. $\sim p(x)$라고 가정합니다. $\epsilon>0$이라고 가정합니다. 그러면 길이가 $n$인 시퀀스 $x^{n}$을 이진 문자열로 매핑하는 코드가 존재하며, 이 매핑은 일대일(따라서 역변환 가능)이고

$$
E\left[\frac{1}{n} l\left(X^{n}\right)\right] \leq H(X)+\epsilon
$$

$n$이 충분히 클 때 성립합니다.
따라서 시퀀스 $X^{n}$을 평균적으로 $n H(X)$ 비트로 표현할 수 있습니다.

# 3.3 높은 확률 집합과 일반적인 집합

$A_{\epsilon}^{(n)}$의 정의로부터 $A_{\epsilon}^{(n)}$이 확률의 대부분을 포함하는 상당히 작은 집합이라는 것을 알 수 있습니다. 그러나 정의만으로는 이것이 그러한 집합 중 가장 작은 집합인지 명확하지 않습니다. 일반적인 집합이 지수에서 일차적으로 가장 작은 집합과 본질적으로 동일한 수의 요소를 가지고 있음을 증명할 것입니다.

정의 각 $n=1,2, \ldots$에 대해 $B_{\delta}^{(n)} \subset \mathcal{X}^{n}$을 다음을 만족하는 가장 작은 집합으로 정의합니다.

$$
\operatorname{Pr}\left\{B_{\delta}^{(n)}\right\} \geq 1-\delta
$$
<!-- Page 89 -->
$B_{\delta}^{(n)}$는 $A_{\epsilon}^{(n)}$와 상당한 교집합을 가져야 하며, 따라서 거의 같은 수의 원소를 가져야 한다고 주장합니다. 문제 3.3.11에서 다음 정리에 대한 증명을 개략적으로 설명합니다.

정리 3.3.1 $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim p(x)$라고 가정합니다. $\delta<\frac{1}{2}$이고 임의의 $\delta^{\prime}>0$에 대해, $\operatorname{Pr}\left\{B_{\delta}^{(n)}\right\}>1-\delta$이면,

$$
\frac{1}{n} \log \left|B_{\delta}^{(n)}\right|>H-\delta^{\prime} \quad n \text{이 충분히 클 때}
$$

따라서 $B_{\delta}^{(n)}$는 지수의 일차 근사치로 적어도 $2^{n H}$개의 원소를 가져야 합니다. 그러나 $A_{\epsilon}^{(n)}$는 $2^{n(H \pm \epsilon)}$개의 원소를 가집니다. 그러므로 $A_{\epsilon}^{(n)}$는 고확률 집합 중 가장 작은 집합과 크기가 거의 같습니다.

이제 지수의 일차 근사치에서의 동등성을 표현하기 위해 새로운 표기법을 정의하겠습니다.

정의 표기법 $a_{n} \doteq b_{n}$은 다음을 의미합니다.

$$
\lim _{n \rightarrow \infty} \frac{1}{n} \log \frac{a_{n}}{b_{n}}=0
$$

따라서 $a_{n} \doteq b_{n}$은 $a_{n}$과 $b_{n}$이 지수의 일차 근사치에서 같다는 것을 의미합니다.

이제 위의 결과를 다시 기술할 수 있습니다. 만약 $\delta_{n} \rightarrow 0$이고 $\epsilon_{n} \rightarrow 0$이면,

$$
\left|B_{\delta_{n}}^{(n)}\right| \doteq\left|A_{\epsilon_{n}}^{(n)}\right| \doteq 2^{n H}
$$

$A_{\epsilon}^{(n)}$와 $B_{\delta}^{(n)}$의 차이를 설명하기 위해 매개변수 $p=0.9$인 베르누이 수열 $X_{1}, X_{2}, \ldots, X_{n}$을 고려해 보겠습니다. [베르누이 $(\theta)$ 확률 변수는 확률 $\theta$로 값 1을 취하는 이진 확률 변수입니다.] 이 경우의 일반적인 수열은 1의 비율이 0.9에 가까운 수열입니다. 그러나 이것은 가장 가능성 있는 단일 수열, 즉 모든 1로 이루어진 수열을 포함하지 않습니다. $B_{\delta}^{(n)}$ 집합은 가장 가능성 있는 모든 수열을 포함하므로 모든 1로 이루어진 수열을 포함합니다. 정리 3.3.1은 $A_{\epsilon}^{(n)}$와 $B_{\delta}^{(n)}$가 모두 약 90%의 1을 가지는 수열을 포함해야 하며, 두 집합의 크기는 거의 같다는 것을 의미합니다.
<!-- Page 90 -->
# 요약

AEP. "거의 모든 사건은 거의 동일하게 놀랍습니다." 구체적으로, $X_{1}, X_{2}, \ldots$가 i.i.d. $\sim p(x)$이면,

$$
-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right) \rightarrow H(X) \text { 확률로. }
$$

정의. 전형적인 집합 $A_{\epsilon}^{(n)}$은 다음을 만족하는 시퀀스 $x_{1}, x_{2}, \ldots, x_{n}$의 집합입니다.

$$
2^{-n(H(X)+\epsilon)} \leq p\left(x_{1}, x_{2}, \ldots, x_{n}\right) \leq 2^{-n(H(X)-\epsilon)}
$$

## 전형적인 집합의 속성

1. 만약 $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in A_{\epsilon}^{(n)}$이면, $p\left(x_{1}, x_{2}, \ldots, x_{n}\right)=2^{-n(H \pm \epsilon)}$입니다.
2. $\operatorname{Pr}\left\{A_{\epsilon}^{(n)}\right\}>1-\epsilon$는 $n$이 충분히 클 때 성립합니다.
3. $\left|A_{\epsilon}^{(n)}\right| \leq 2^{n(H(X)+\epsilon)}$이며, 여기서 $|A|$는 집합 $A$의 원소 수를 나타냅니다.

정의. $a_{n} \doteq b_{n}$은 $n \rightarrow \infty$일 때 $\frac{1}{n} \log \frac{a_{n}}{b_{n}} \rightarrow 0$임을 의미합니다.
가장 작은 확률적 집합. $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim p(x)$이고, $\delta<\frac{1}{2}$에 대해, $\operatorname{Pr}\left\{B_{\delta}^{(n)}\right\} \geq 1-\delta$를 만족하는 가장 작은 집합 $B_{\delta}^{(n)} \subset \mathcal{X}^{n}$이라고 하면,

$$
\left|B_{\delta}^{(n)}\right| \doteq 2^{n H}
$$

## 문제

3.1 마르코프 부등식과 체비셰프 부등식
(a) (마르코프 부등식) 임의의 음이 아닌 확률 변수 $X$와 임의의 $t>0$에 대해 다음을 보여주십시오.

$$
\operatorname{Pr}\{X \geq t\} \leq \frac{E X}{t}
$$

이 부등식이 등호로 달성되는 확률 변수를 제시하십시오.
(b) (체비셰프 부등식) 평균이 $\mu$이고 분산이 $\sigma^{2}$인 확률 변수 $Y$에 대해, $X=(Y-\mu)^{2}$으로 두면 다음을 보여주십시오.
<!-- Page 91 -->
임의의 $\epsilon>0$에 대하여,

$$
\operatorname{Pr}\{|Y-\mu|>\epsilon\} \leq \frac{\sigma^{2}}{\epsilon^{2}}
$$

(c) (대수의 약한 법칙) $Z_{1}, Z_{2}, \ldots, Z_{n}$을 평균 $\mu$와 분산 $\sigma^{2}$을 갖는 i.i.d. 확률 변수열이라고 하자. 표본 평균을 $\bar{Z}_{n}=\frac{1}{n} \sum_{i=1}^{n} Z_{i}$라고 하자. 다음을 증명하시오.

$$
\operatorname{Pr}\left\{\left|\bar{Z}_{n}-\mu\right|>\epsilon\right\} \leq \frac{\sigma^{2}}{n \epsilon^{2}}
$$

따라서, $n \rightarrow \infty$일 때 $\operatorname{Pr}\left\{\left|\bar{Z}_{n}-\mu\right|>\epsilon\right\} \rightarrow 0$이다. 이것은 대수의 약한 법칙으로 알려져 있다.
3.2 AEP와 상호 정보량. $\left(X_{i}, Y_{i}\right)$를 i.i.d. $\sim p(x, y)$라고 하자. $X$와 $Y$가 독립이라는 가설과 $X$와 $Y$가 종속이라는 가설의 로그 우도비(log likelihood ratio)를 형성한다. 다음의 극한은 무엇인가?

$$
\frac{1}{n} \log \frac{p\left(X^{n}\right) p\left(Y^{n}\right)}{p\left(X^{n}, Y^{n}\right)} ?
$$

3.3 케이크 조각.

케이크를 대략 반으로 자르고, 매번 가장 큰 조각을 선택하며 나머지 조각은 버린다. 무작위 절단이 조각의 비율을 다음과 같이 생성한다고 가정한다.

$$
P= \begin{cases}\left(\frac{2}{3}, \frac{1}{3}\right) & \text { 확률 } \frac{3}{4} \\ \left(\frac{2}{5}, \frac{3}{5}\right) & \text { 확률 } \frac{1}{4}\end{cases}
$$

따라서, 예를 들어, 첫 번째 절단(가장 큰 조각 선택)은 $\frac{3}{5}$ 크기의 조각을 생성할 수 있다. 이 조각에서 절단하고 선택하는 것은 2번째 시점에 크기를 $\left(\frac{3}{5}\right)\left(\frac{2}{3}\right)$로 줄일 수 있다. $n$번의 절단 후 케이크 조각의 크기는 대략적으로 지수에서 1차적으로 얼마나 큰가?
3.4 AEP. $X_{i}$를 iid $\sim p(x), x \in\{1,2, \ldots, m\}$라고 하자. $\mu=E X$이고 $H=-\sum p(x) \log p(x)$라고 하자. $A^{n}=\left\{x^{n} \in \mathcal{X}^{m}:\right\}-\frac{1}{n} \log p\left(x^{n}\right)-$ $H \mid \leq \epsilon\}$라고 하자. $B^{n}=\left\{x^{n} \in \mathcal{X}^{m}:\left|\frac{1}{n} \sum_{i=1}^{n} X_{i}-\mu\right| \leq \epsilon\right\}$라고 하자.
(a) $\operatorname{Pr}\left\{X^{n} \in A^{n}\right\} \longrightarrow 1$인가?
(b) $\operatorname{Pr}\left\{X^{n} \in A^{n} \cap B^{n}\right\} \longrightarrow 1$인가?
<!-- Page 92 -->
(c) 모든 $n$에 대해 $\left|A^{n} \cap B^{n}\right| \leq 2^{n(H+\epsilon)}$임을 보이십시오.
(d) 충분히 큰 $n$에 대해 $\left|A^{n} \cap B^{n}\right| \geq\left(\frac{1}{2}\right) 2^{n(H-\epsilon)}$임을 보이십시오.
3.5 확률로 정의된 집합. 엔트로피 $H(X)$를 갖는 이산 확률 변수의 i.i.d. 시퀀스 $X_{1}, X_{2}, \ldots$를 고려하십시오. 다음을 정의합니다.

$$
C_{n}(t)=\left\{x^{n} \in \mathcal{X}^{n}: p\left(x^{n}\right) \geq 2^{-n t}\right\}
$$

이는 확률이 $\geq 2^{-n t}$인 $n$-시퀀스의 부분집합을 나타냅니다.
(a) $\left|C_{n}(t)\right| \leq 2^{n t}$임을 보이십시오.
(b) $P\left(\left\{X^{n} \in C_{n}(t)\right\}\right) \rightarrow 1$이 되는 $t$의 값은 무엇입니까?
3.6 AEP 유사 극한. 확률 질량 함수 $p(x)$에 따라 i.i.d.로 추출된 $X_{1}, X_{2}, \ldots$를 고려하십시오. 다음의 극한값을 찾으십시오.

$$
\lim _{n \rightarrow \infty}\left(p\left(X_{1}, X_{2}, \ldots, X_{n}\right)\right)^{\frac{1}{n}}
$$

3.7 AEP 및 소스 코딩. 통계적으로 독립적인 이진 숫자 시퀀스를 생성하는 이산 무기억 소스가 있으며, $p(1)=0.005$이고 $p(0)=0.995$입니다. 숫자는 100개씩 묶어서 처리되며, 1이 세 개 이하인 모든 시퀀스에 대해 이진 코드가 제공됩니다.
(a) 모든 코드가 동일한 길이를 갖는다고 가정할 때, 1이 세 개 이하인 모든 시퀀스에 대해 코드를 제공하는 데 필요한 최소 길이를 찾으십시오.
(b) 코드가 할당되지 않은 소스 시퀀스를 관찰할 확률을 계산하십시오.
(c) Chebyshev의 부등식을 사용하여 코드가 할당되지 않은 소스 시퀀스를 관찰할 확률의 상한을 구하십시오. 이 상한을 (b)에서 계산한 실제 확률과 비교하십시오.

# 3.8 곱.

다음과 같이 정의된 확률 변수 $X$를 고려하십시오.

$$
X=\left\{\begin{array}{ll}
1, & \text { 확률 } \frac{1}{2} \\
2, & \text { 확률 } \frac{1}{4} \\
3, & \text { 확률 } \frac{1}{4}
\end{array}\right.
$$

$X_{1}, X_{2}, \ldots$가 이 분포에 따라 i.i.d.로 추출된다고 가정합니다. 다음 곱의 극한 거동을 찾으십시오.

$$
\left(X_{1} X_{2} \cdots X_{n}\right)^{\frac{1}{n}}
$$
<!-- Page 93 -->
3.9 AEP. $X_{1}, X_{2}, \ldots$를 확률 질량 함수 $p(x), x \in\{1,2, \ldots, m\}$에 따라 추출된 독립적이고 동일하게 분포된(i.i.d.) 확률 변수라고 합시다. 따라서 $p\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\prod_{i=1}^{n} p\left(x_{i}\right)$입니다. 우리는 $-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right) \rightarrow H(X)$가 확률적으로 수렴한다는 것을 알고 있습니다. 여기서 $q$는 $\{1,2, \ldots, m\}$ 상의 또 다른 확률 질량 함수입니다.
(a) $X_{1}, X_{2}, \ldots$가 i.i.d. $\sim p(x)$일 때 $\lim -\frac{1}{n} \log q\left(X_{1}, X_{2}, \ldots, X_{n}\right)$를 평가하십시오.
(b) 이제 $X_{1}, X_{2}, \ldots$가 i.i.d. $\sim p(x)$일 때 $\log$ 가능도 비율 $\frac{1}{n} \log \frac{q\left(X_{1}, \ldots, X_{n}\right)}{p\left(X_{1}, \ldots, X_{n}\right)}$의 극한을 평가하십시오. 따라서 $p$가 참일 때 $q$를 지지하는 오즈비(odds)는 지수적으로 작습니다.
3.10 랜덤 박스 크기.

변의 길이가 $X_{1}, X_{2}, X_{3}, \ldots, X_{n}$인 $n$차원 직사각형 박스를 구성하려고 합니다. 부피는 $V_{n}=\prod_{i=1}^{n} X_{i}$입니다. 랜덤 박스와 동일한 부피를 갖는 $n$-큐브의 변 길이 $l$은 $l=V_{n}^{1 / n}$입니다. $X_{1}, X_{2}, \ldots$를 단위 구간 $[0,1]$ 상의 i.i.d. 균등 확률 변수라고 합시다. $\lim _{n \rightarrow \infty} V_{n}^{1 / n}$을 찾고 $\left(E V_{n}\right)^{\frac{1}{n}}$과 비교하십시오. 명확하게, 기대 변 길이는 박스의 부피라는 개념을 포착하지 못합니다. 산술 평균보다는 기하 평균이 곱의 행동을 특징짓습니다.
3.11 정리 3.3.1의 증명. 이 문제는 가장 작은 "확률적인" 집합의 크기가 약 $2^{n H}$임을 보여줍니다. $X_{1}, X_{2}, \ldots, X_{n}$을 i.i.d. $\sim p(x)$라고 합시다. $\operatorname{Pr}\left(B_{\delta}^{(n)}\right)>1-\delta$인 $B_{\delta}^{(n)} \subset \mathcal{X}^{n}$를 정의합시다. $\epsilon<\frac{1}{2}$를 고정합니다.
(a) $\operatorname{Pr}(A)>1-\epsilon_{1}$이고 $\operatorname{Pr}(B)>1-\epsilon_{2}$인 임의의 두 집합 $A, B$에 대해 $\operatorname{Pr}(A \cap B)>1-\epsilon_{1}-\epsilon_{2}$임을 보이십시오. 따라서 $\operatorname{Pr}\left(A_{\epsilon}^{(n)} \cap B_{\delta}^{(n)}\right) \geq 1-\epsilon-\delta$입니다.
(b) 부등식 연쇄의 단계를 정당화하십시오.

$$
\begin{aligned}
1-\epsilon-\delta & \leq \operatorname{Pr}\left(A_{\epsilon}^{(n)} \cap B_{\delta}^{(n)}\right) \\
& =\sum_{A_{\epsilon}^{(n)} \cap B_{\delta}^{(n)}} p\left(x^{n}\right) \\
& \leq \sum_{A_{\epsilon}^{(n)} \cap B_{\delta}^{(n)}} 2^{-n(H-\epsilon)} \\
& =\left|A_{\epsilon}^{(n)} \cap B_{\delta}^{(n)}\right| 2^{-n(H-\epsilon)} \\
& \leq\left|B_{\delta}^{(n)}\right| 2^{-n(H-\epsilon)}
\end{aligned}
$$

(c) 정리의 증명을 완료하십시오.
<!-- Page 94 -->
3.12 경험적 분포의 단조 수렴.

$\hat{p}_{n}$을 $X_{1}, X_{2}, \ldots, X_{n}$ i.i.d. $\sim p(x), x \in \mathcal{X}$에 해당하는 경험적 확률 질량 함수라고 합시다. 구체적으로,

$$
\hat{p}_{n}(x)=\frac{1}{n} \sum_{i=1}^{n} I\left(X_{i}=x\right)
$$

는 첫 $n$개의 표본에서 $X_{i}=x$인 경우의 비율이며, 여기서 $I$는 지시 함수입니다.
(a) $\mathcal{X}$가 이진(binary)일 때 다음을 보이십시오.

$$
E D\left(\hat{p}_{2 n} \| p\right) \leq E D\left(\hat{p}_{n} \| p\right)
$$

따라서 경험적 분포에서 실제 분포까지의 기대 상대 엔트로피 "거리"는 표본 크기에 따라 감소합니다. (힌트: $\hat{p}_{2 n}=\frac{1}{2} \hat{p}_{n}+\frac{1}{2} \hat{p}_{n}^{\prime}$로 쓰고 $D$의 볼록성을 사용하십시오.)
(b) 임의의 이산(discrete) $\mathcal{X}$에 대해 다음을 보이십시오.

$$
E D\left(\hat{p}_{n} \| p\right) \leq E D\left(\hat{p}_{n-1} \| p\right)
$$

(힌트: $\hat{p}_{n}$을 각 표본이 차례로 삭제된 $n$개의 경험적 질량 함수의 평균으로 쓰십시오.)
3.13 전형 집합(typical set)의 계산. 전형 집합 $A_{\epsilon}^{(n)}$과 높은 확률을 갖는 가장 작은 집합 $B_{\delta}^{(n)}$의 개념을 명확히 하기 위해 간단한 예제에 대한 집합을 계산할 것입니다. 확률이 0.6인 i.i.d. 이진 랜덤 변수 $X_{1}, X_{2}, \ldots, X_{n}$의 시퀀스를 고려하십시오 (따라서 $X_{i}=0$일 확률은 0.4입니다).
(a) $H(X)$를 계산하십시오.
(b) $n=25$이고 $\epsilon=0.1$일 때, 어떤 시퀀스가 전형 집합 $A_{\epsilon}^{(n)}$에 속합니까? 전형 집합의 확률은 얼마입니까? 전형 집합의 원소 수는 몇 개입니까? (이는 $k$개의 1을 갖는 시퀀스에 대한 확률 표( $0 \leq k \leq 25$)의 계산을 포함하며, 전형 집합에 속하는 시퀀스를 찾습니다.)
(c) 확률이 0.9인 가장 작은 집합의 원소 수는 몇 개입니까?
(d) (b)와 (c)의 집합의 교집합에 있는 원소 수는 몇 개입니까? 이 교집합의 확률은 얼마입니까?
<!-- Page 95 -->
| $k$ | $\binom{n}{k}$ | $\binom{n}{k} p^{k}(1-p)^{n-k}$ | $-\frac{1}{n} \log p\left(x^{n}\right)$ |
| --: | --: | :--: | :--: |
| 0 | 1 | 0.000000 | 1.321928 |
| 1 | 25 | 0.000000 | 1.298530 |
| 2 | 300 | 0.000000 | 1.275131 |
| 3 | 2300 | 0.000001 | 1.251733 |
| 4 | 12650 | 0.000007 | 1.228334 |
| 5 | 53130 | 0.000054 | 1.204936 |
| 6 | 177100 | 0.000227 | 1.181537 |
| 7 | 480700 | 0.001205 | 1.158139 |
| 8 | 1081575 | 0.003121 | 1.134740 |
| 9 | 2042975 | 0.013169 | 1.111342 |
| 10 | 3268760 | 0.021222 | 1.087943 |
| 11 | 4457400 | 0.077801 | 1.064545 |
| 12 | 5200300 | 0.075967 | 1.041146 |
| 13 | 5200300 | 0.267718 | 1.017748 |
| 14 | 4457400 | 0.146507 | 0.994349 |
| 15 | 3268760 | 0.575383 | 0.970951 |
| 16 | 2042975 | 0.151086 | 0.947552 |
| 17 | 1081575 | 0.846448 | 0.924154 |
| 18 | 480700 | 0.079986 | 0.900755 |
| 19 | 177100 | 0.970638 | 0.877357 |
| 20 | 53130 | 0.019891 | 0.853958 |
| 21 | 12650 | 0.997633 | 0.830560 |
| 22 | 2300 | 0.001937 | 0.807161 |
| 23 | 300 | 0.999950 | 0.783763 |
| 24 | 25 | 0.000047 | 0.760364 |
| 25 | 1 | 0.000003 | 0.736966 |

# 역사적 고찰

점근적 균등 분할 속성(AEP)은 1948년 Shannon의 원 논문 [472]에서 처음으로 언급되었으며, 여기서 그는 i.i.d. 과정에 대한 결과를 증명하고 정상 에르고딕 과정에 대한 결과를 명시했습니다. McMillan [384]과 Breiman [74]은 에르고딕 유한 알파벳 소스에 대한 AEP를 증명했습니다. 이 결과는 이제 AEP 또는 Shannon-McMillan-Breiman 정리라고 불립니다. Chung [101]은 이 정리를 가산 알파벳의 경우로 확장했으며, Moy [392], Perez [417], Kieffer [312]는 $\left\{X_{i}\right\}$가 연속 값이고 에르고딕일 때 $\mathcal{L}_{1}$ 수렴을 증명했습니다. Barron [34]과 Orey [402]는 실수 값 에르고딕 과정에 대한 거의 확실한 수렴을 증명했습니다. 간단한 샌드위치 논증(Algoet and Cover [20])은 일반 AEP를 증명하기 위해 16.8절에서 사용될 것입니다.
<!-- Page 96 -->
.
<!-- Page 97 -->
# 제 4 장

## 확률 과정의 엔트로피율

3 장의 점근적 등분산성 속성은 $n$개의 독립적이고 동일하게 분포된 확률 변수를 기술하는 데 평균적으로 $n H(X)$ 비트가 충분함을 확립합니다. 그러나 확률 변수가 종속적이라면 어떻게 될까요? 특히, 확률 변수가 정상 과정(stationary process)을 형성한다면 어떻게 될까요? i.i.d. 경우와 마찬가지로, 엔트로피 $H\left(X_{1}, X_{2}, \ldots, X_{n}\right)$가 $n$에 대해 (점근적으로) 선형적으로 증가하며, 그 비율을 $H(\mathcal{X})$라고 부를 것이며, 이를 과정의 엔트로피율이라고 명명할 것입니다. $H(\mathcal{X})$를 최상의 달성 가능한 데이터 압축으로 해석하는 것은 5 장의 분석을 기다려야 합니다.

### 4.1 마르코프 연쇄 (MARKOV CHAINS)

확률 과정 $\left\{X_{i}\right\}$는 인덱싱된 확률 변수 시퀀스입니다. 일반적으로 확률 변수 간에는 임의의 종속성이 존재할 수 있습니다. 이 과정은 결합 확률 질량 함수 $\operatorname{Pr}\left\{\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\left(x_{1}, x_{2}, \ldots, x_{n}\right)\right\}=p\left(x_{1}, x_{2}, \ldots, x_{n}\right),\left(x_{1}, x_{2}, \ldots\right.$, $\left.x_{n}\right) \in \mathcal{X}^{n}$ for $n=1,2, \ldots$에 의해 특징지어집니다.

정의 확률 과정은 시간 인덱스의 이동에 대해 시퀀스의 임의의 부분 집합의 결합 분포가 불변이면 정상(stationary)이라고 합니다. 즉,

$$
\begin{aligned}
& \operatorname{Pr}\left\{X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{n}=x_{n}\right\} \\
& \quad=\operatorname{Pr}\left\{X_{1+l}=x_{1}, X_{2+l}=x_{2}, \ldots, X_{n+l}=x_{n}\right\}
\end{aligned}
$$

모든 $n$과 모든 이동 $l$, 그리고 모든 $x_{1}, x_{2}, \ldots, x_{n} \in \mathcal{X}$에 대해 성립합니다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (c) 2006 John Wiley \& Sons, Inc.
<!-- Page 98 -->
의존성을 갖는 확률 과정의 간단한 예는 각 확률 변수가 바로 이전의 변수에만 의존하고 다른 모든 이전 확률 변수로부터 조건부로 독립적인 경우입니다. 이러한 과정을 마르코프 과정이라고 합니다.

정의 이산 확률 과정 $X_{1}, X_{2}, \ldots$는 $n=1,2, \ldots$에 대해 다음을 만족하면 마르코프 연쇄 또는 마르코프 과정이라고 합니다.

$$
\begin{aligned}
\operatorname{Pr}\left(X_{n+1}\right. & \left.=x_{n+1} \mid X_{n}=x_{n}, X_{n-1}=x_{n-1}, \ldots, X_{1}=x_{1}\right) \\
& =\operatorname{Pr}\left(X_{n+1}=x_{n+1} \mid X_{n}=x_{n}\right)
\end{aligned}
$$

모든 $x_{1}, x_{2}, \ldots, x_{n}, x_{n+1} \in \mathcal{X}$에 대해.
이 경우, 확률 변수들의 결합 확률 질량 함수는 다음과 같이 쓸 수 있습니다.

$$
p\left(x_{1}, x_{2}, \ldots, x_{n}\right)=p\left(x_{1}\right) p\left(x_{2} \mid x_{1}\right) p\left(x_{3} \mid x_{2}\right) \cdots p\left(x_{n} \mid x_{n-1}\right)
$$

정의 마르코프 연쇄는 조건부 확률 $p\left(x_{n+1} \mid x_{n}\right)$가 $n$에 의존하지 않을 때, 즉 $n=1,2, \ldots$에 대해 다음을 만족할 때 시간 불변이라고 합니다.

$$
\operatorname{Pr}\left\{X_{n+1}=b \mid X_{n}=a\right\}=\operatorname{Pr}\left\{X_{2}=b \mid X_{1}=a\right\} \quad \text { 모든 } a, b \in \mathcal{X} \text{에 대해}
$$

별도로 명시하지 않는 한 마르코프 연쇄는 시간 불변이라고 가정합니다.

$\left\{X_{i}\right\}$가 마르코프 연쇄이면, $X_{n}$은 시간 $n$에서의 상태라고 불립니다. 시간 불변 마르코프 연쇄는 초기 상태와 확률 전이 행렬 $P=\left[P_{i j}\right], i, j \in\{1,2, \ldots, m\}$로 특징지어지며, 여기서 $P_{i j}=\operatorname{Pr}\left\{X_{n+1}=\right.$ $\left.j \mid X_{n}=i\right\}$입니다.

마르코프 연쇄의 어떤 상태에서든 유한한 단계 내에 다른 어떤 상태로든 양의 확률로 이동할 수 있다면, 그 마르코프 연쇄는 기약이라고 합니다. 어떤 상태에서 자기 자신으로 가는 여러 경로 길이의 최대 공약수가 1이면, 그 마르코프 연쇄는 비주기라고 합니다.

시간 $n$에서의 확률 변수의 확률 질량 함수가 $p\left(x_{n}\right)$이면, 시간 $n+1$에서의 확률 질량 함수는 다음과 같습니다.

$$
p\left(x_{n+1}\right)=\sum_{x_{n}} p\left(x_{n}\right) P_{x_{n} x_{n+1}}
$$

시간 $n+1$에서의 분포가 시간 $n$에서의 분포와 동일한 상태에 대한 분포를 정상 분포라고 합니다.
<!-- Page 99 -->
정상 분포는 만약 마르코프 연쇄의 초기 상태가 정상 분포에 따라 추출된다면, 마르코프 연쇄가 정상 과정을 형성하기 때문에 그렇게 불립니다.

만약 유한 상태 마르코프 연쇄가 기약적이고 비주기적이라면, 정상 분포는 유일하며, 임의의 시작 분포로부터 $X_{n}$의 분포는 $n \rightarrow \infty$일 때 정상 분포로 수렴합니다.

예제 4.1.1 확률 전이 행렬이 다음과 같은 두 상태 마르코프 연쇄를 고려하십시오.

$$
P=\left[\begin{array}{cc}
1-\alpha & \alpha \\
\beta & 1-\beta
\end{array}\right]
$$

그림 4.1과 같습니다.
정상 분포를 벡터 $\mu$로 나타내고, 그 성분은 각각 상태 1과 2의 정상 확률이라고 합시다. 그러면 정상 확률은 방정식 $\mu P=\mu$를 풀거나, 더 간단하게는 확률 균형을 통해 찾을 수 있습니다. 정상 분포의 경우, 상태 전이 그래프의 임의의 컷셋을 가로지르는 순 확률 흐름은 0입니다. 이를 그림 4.1에 적용하면 다음과 같습니다.

$$
\mu_{1} \alpha=\mu_{2} \beta
$$

$\mu_{1}+\mu_{2}=1$이므로, 정상 분포는 다음과 같습니다.

$$
\mu_{1}=\frac{\beta}{\alpha+\beta}, \quad \mu_{2}=\frac{\alpha}{\alpha+\beta}
$$

만약 마르코프 연쇄의 초기 상태가 정상 분포에 따라 추출된다면, 결과 과정은 정상적일 것입니다. 엔트로피는

그림 4.1. 두 상태 마르코프 연쇄.
<!-- Page 100 -->
시간 $n$에서의 상태 $X_{n}$은 다음과 같습니다.

$$
H\left(X_{n}\right)=H\left(\frac{\beta}{\alpha+\beta}, \frac{\alpha}{\alpha+\beta}\right)
$$

그러나 이것은 $H\left(X_{1}, X_{2}, \ldots\right.$, $\left.X_{n}\right)$에 대한 엔트로피 성장률이 아닙니다. $X_{i}$들 간의 종속성은 꾸준한 영향을 미칠 것입니다.

# 4.2 엔트로피율

$n$개의 확률 변수 시퀀스가 주어졌을 때, 자연스러운 질문은 다음과 같습니다. 시퀀스의 엔트로피는 $n$에 따라 어떻게 증가하는가? 우리는 이 성장률을 다음과 같이 엔트로피율로 정의합니다.

정의 확률 과정 $\left\{X_{i}\right\}$의 엔트로피는 다음과 같이 정의됩니다.

$$
H(\mathcal{X})=\lim _{n \rightarrow \infty} \frac{1}{n} H\left(X_{1}, X_{2}, \ldots, X_{n}\right)
$$

이때 극한값이 존재합니다.
이제 몇 가지 간단한 확률 과정 예시와 해당 엔트로피율을 고려해 보겠습니다.

1. 타자기.

$m$개의 동일하게 확률적인 출력 문자를 가진 타자기의 경우를 고려해 보십시오. 타자기는 길이가 $n$인 $m^{n}$개의 시퀀스를 생성할 수 있으며, 이들 모두 동일하게 확률적입니다. 따라서 $H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\log m^{n}$이고 엔트로피율은 기호당 $H(\mathcal{X})=\log m$ 비트입니다.
2. $X_{1}, X_{2}, \ldots$는 i.i.d. 확률 변수입니다. 그러면

$$
H(\mathcal{X})=\lim \frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n}=\lim \frac{n H\left(X_{1}\right)}{n}=H\left(X_{1}\right)
$$

이는 기호당 엔트로피율로 예상되는 바입니다.
3. 독립적이지만 동일하게 분포되지 않은 확률 변수의 시퀀스. 이 경우,

$$
H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} H\left(X_{i}\right)
$$

그러나 $H\left(X_{i}\right)$들은 모두 같지 않습니다. 우리는 $\frac{1}{n} \sum H\left(X_{i}\right)$의 극한이 존재하지 않도록 $X_{1}, X_{2}, \ldots$에 대한 분포 시퀀스를 선택할 수 있습니다. 이러한 시퀀스의 예는 랜덤 이진 시퀀스입니다.
<!-- Page 101 -->
여기서 $p_{i}=P\left(X_{i}=1\right)$는 상수가 아니라 $i$의 함수이며, (4.10)의 극한이 존재하지 않도록 신중하게 선택됩니다. 예를 들어,

$$
p_{i}= \begin{cases}0.5 & \text { if } 2 k<\log \log i \leq 2 k+1 \\ 0 & \text { if } 2 k+1<\log \log i \leq 2 k+2\end{cases}
$$

$k=0,1,2, \ldots$에 대해
그러면 $H\left(X_{i}\right)=1$인 임의로 긴 구간이 있고, 그 뒤를 이어 $H\left(X_{i}\right)=0$인 지수적으로 더 긴 구간이 있습니다. 따라서 $H\left(X_{i}\right)$의 이동 평균은 0과 1 사이를 진동하며 극한을 갖지 않을 것입니다. 따라서 이 과정에 대해 $H(\mathcal{X})$는 정의되지 않습니다.

엔트로피율에 대한 관련 양을 다음과 같이 정의할 수도 있습니다.

$$
H^{\prime}(\mathcal{X})=\lim _{n \rightarrow \infty} H\left(X_{n} \mid X_{n-1}, X_{n-2}, \ldots, X_{1}\right)
$$

극한이 존재할 때.
두 양 $H(\mathcal{X})$와 $H^{\prime}(\mathcal{X})$는 엔트로피율의 두 가지 다른 개념에 해당합니다. 첫 번째는 $n$개의 확률 변수의 기호당 엔트로피이고, 두 번째는 과거가 주어졌을 때 마지막 확률 변수의 조건부 엔트로피입니다. 이제 정상 과정의 경우 두 극한이 존재하고 같다는 중요한 결과를 증명합니다.

정리 4.2.1 정상 확률 과정의 경우 (4.10)과 (4.14)의 극한이 존재하고 같습니다.

$$
H(\mathcal{X})=H^{\prime}(\mathcal{X})
$$

먼저 $\lim H\left(X_{n} \mid X_{n-1}, \ldots, X_{1}\right)$이 존재함을 증명합니다.
정리 4.2.2 정상 확률 과정의 경우 $H\left(X_{n} \mid X_{n-1}, \ldots\right.$, $\left.X_{1}\right)$은 $n$에 대해 비증가하며 극한 $H^{\prime}(\mathcal{X})$를 갖습니다.

# 증명

$$
\begin{aligned}
H\left(X_{n+1} \mid X_{1}, X_{2}, \ldots, X_{n}\right) & \leq H\left(X_{n+1} \mid X_{n}, \ldots, X_{2}\right) \\
& =H\left(X_{n} \mid X_{n-1}, \ldots, X_{1}\right)
\end{aligned}
$$

여기서 부등식은 조건화가 엔트로피를 감소시킨다는 사실에서 비롯되고, 등식은 과정의 정상성에서 비롯됩니다. $H\left(X_{n} \mid X_{n-1}, \ldots, X_{1}\right)$은 음수가 아닌 숫자의 감소하는 수열이므로 극한 $H^{\prime}(\mathcal{X})$를 갖습니다.
<!-- Page 102 -->
이제 해석학에서 사용하는 다음의 간단한 결과를 사용합니다.
정리 4.2.3 (체사로 평균) $a_{n} \rightarrow a$이고 $b_{n}=\frac{1}{n} \sum_{i=1}^{n} a_{i}$이면, $b_{n} \rightarrow a$입니다.

증명: (비공식 개요). 수열 $\left\{a_{k}\right\}$의 대부분의 항은 결국 $a$에 가까워지므로, 처음 $n$개 항의 평균인 $b_{n}$ 또한 결국 $a$에 가까워집니다.

정식 증명: $\epsilon>0$이라고 합시다. $a_{n} \rightarrow a$이므로, 모든 $n \geq N(\epsilon)$에 대해 $\left|a_{n}-a\right| \leq \epsilon$을 만족하는 수 $N(\epsilon)$이 존재합니다. 따라서,

$$
\begin{aligned}
\left|b_{n}-a\right| & =\left|\frac{1}{n} \sum_{i=1}^{n}\left(a_{i}-a\right)\right| \\
& \leq \frac{1}{n} \sum_{i=1}^{n}\left|\left(a_{i}-a\right)\right| \\
& \leq \frac{1}{n} \sum_{i=1}^{N(\epsilon)}\left|a_{i}-a\right|+\frac{n-N(\epsilon)}{n} \epsilon \\
& \leq \frac{1}{n} \sum_{i=1}^{N(\epsilon)}\left|a_{i}-a\right|+\epsilon
\end{aligned}
$$

모든 $n \geq N(\epsilon)$에 대해 성립합니다. 첫 번째 항은 $n \rightarrow \infty$일 때 0으로 가므로, $n$을 충분히 크게 잡으면 $\left|b_{n}-a\right| \leq 2 \epsilon$으로 만들 수 있습니다. 따라서, $n \rightarrow \infty$일 때 $b_{n} \rightarrow a$입니다.

정리 4.2.1의 증명: 연쇄 법칙에 의해,

$$
\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n}=\frac{1}{n} \sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right)
$$

즉, entropy rate는 조건부 entropy의 시간 평균입니다. 그러나 조건부 entropy는 극한값 $H^{\prime}$으로 수렴한다는 것을 알고 있습니다. 따라서, 정리 4.2.3에 의해, 이들의 누적 평균은 항들의 극한값 $H^{\prime}$과 같은 극한값을 가집니다. 그러므로, 정리 4.2.2에 의해,

$$
\begin{aligned}
H(\mathcal{X}) & =\lim \frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n}=\lim H\left(X_{n} \mid X_{n-1}, \ldots, X_{1}\right) \\
& =H^{\prime}(\mathcal{X})
\end{aligned}
$$
<!-- Page 103 -->
확률 과정의 엔트로피율의 중요성은 정상 순환 과정에 대한 AEP에서 비롯됩니다. 일반 AEP는 섹션 16.8에서 증명하며, 여기서 모든 정상 순환 과정에 대해 다음이 성립함을 보여줍니다.

$$
-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right) \rightarrow H(\mathcal{X})
$$

확률 1로. 이를 사용하여 챕터 3의 정리를 일반 정상 순환 과정으로 쉽게 확장할 수 있습니다. 챕터 3의 i.i.d. 사례에서와 같은 방식으로 일반적인 집합을 정의할 수 있습니다. 동일한 논증을 통해 일반적인 집합은 1에 가까운 확률을 가지며, 길이가 $n$인 일반 순서열은 약 $2^{n H(\mathcal{X})}$개이고 각 순서열의 확률은 약 $2^{-n H(\mathcal{X})}$임을 보여줄 수 있습니다. 따라서 길이가 $n$인 일반 순서열을 약 $n H(\mathcal{X})$ 비트로 표현할 수 있습니다. 이는 정상 순환 과정의 평균 설명 길이로서 엔트로피율의 중요성을 보여줍니다.

엔트로피율은 모든 정상 과정에 대해 잘 정의됩니다. 엔트로피율은 마르코프 연쇄의 경우 특히 계산하기 쉽습니다.

마르코프 연쇄. 정상 마르코프 연쇄의 경우 엔트로피율은 다음과 같이 주어집니다.

$$
\begin{aligned}
H(\mathcal{X}) & =H^{\prime}(\mathcal{X})=\lim H\left(X_{n} \mid X_{n-1}, \ldots, X_{1}\right)=\lim H\left(X_{n} \mid X_{n-1}\right) \\
& =H\left(X_{2} \mid X_{1}\right)
\end{aligned}
$$

여기서 조건부 엔트로피는 주어진 정상 분포를 사용하여 계산됩니다. 정상 분포 $\mu$는 다음 방정식의 해임을 기억하십시오.

$$
\mu_{j}=\sum_{i} \mu_{i} P_{i j} \quad \text { 모든 } j에 대해
$$

다음 정리에서 조건부 엔트로피를 명시적으로 표현합니다.
정리 4.2.4 $\left\{X_{i}\right\}$가 정상 분포 $\mu$와 전이 행렬 $P$를 갖는 정상 마르코프 연쇄라고 가정합니다. $X_{1} \sim \mu$라고 가정합니다. 그러면 엔트로피율은 다음과 같습니다.

$$
H(\mathcal{X})=-\sum_{i j} \mu_{i} P_{i j} \log P_{i j}
$$

증명: $\quad H(\mathcal{X})=H\left(X_{2} \mid X_{1}\right)=\sum_{i} \mu_{i}\left(\sum_{j}-P_{i j} \log P_{i j}\right)$.
<!-- Page 104 -->
예제 4.2.1 (이중 상태 마르코프 연쇄) 그림 4.1의 이중 상태 마르코프 연쇄의 엔트로피율은 다음과 같습니다.

$$
H(\mathcal{X})=H\left(X_{2} \mid X_{1}\right)=\frac{\beta}{\alpha+\beta} H(\alpha)+\frac{\alpha}{\alpha+\beta} H(\beta)
$$

비고: 마르코프 연쇄가 기약적이고 비주기적이면, 상태에 대한 고유한 정상 분포를 가지며, 모든 초기 분포는 $n \rightarrow \infty$일 때 정상 분포로 수렴합니다. 이 경우, 초기 분포가 정상 분포가 아니더라도 장기적인 행동을 기준으로 정의되는 엔트로피율은 (4.25) 및 (4.27)에 정의된 $H(\mathcal{X})$입니다.

# 4.3 예제: 가중치 그래프에서의 무작위 보행 엔트로피율

확률 과정의 예로, 연결된 그래프(그림 4.2)에서의 무작위 보행을 고려해 보겠습니다. 노드가 $\{1,2, \ldots, m\}$으로 레이블링된 $m$개의 노드를 가진 그래프를 고려하고, 노드 $i$와 노드 $j$를 연결하는 간선에 가중치 $W_{i j} \geq 0$을 부여합니다. (그래프는 무방향이라고 가정하므로 $W_{i j}=W_{j i}$입니다. 노드 $i$와 $j$를 연결하는 간선이 없으면 $W_{i j}=0$으로 설정합니다.)

입자가 이 그래프에서 노드에서 노드로 무작위로 이동합니다. 무작위 보행 $\left\{X_{n}\right\}, X_{n} \in\{1,2, \ldots, m\}$은 그래프의 꼭짓점 시퀀스입니다. $X_{n}=i$가 주어졌을 때, 다음 꼭짓점 $j$는 노드 $i$에 연결된 노드 중에서 $i$와 $j$를 연결하는 간선의 가중치에 비례하는 확률로 선택됩니다. 따라서 $P_{i j}=W_{i j} / \sum_{k} W_{i k}$입니다.

그림 4.2. 그래프에서의 무작위 보행.
<!-- Page 105 -->
이 경우, 정상 분포는 놀랍도록 간단한 형태를 가지며, 이를 추측하고 검증할 것입니다. 이 마르코프 연쇄의 정상 분포는 노드 $i$에서 나오는 모든 엣지의 총 가중치에 비례하는 확률을 노드 $i$에 할당합니다.

$$
W_{i}=\sum_{j} W_{i j}
$$

를 노드 $i$에서 나오는 엣지의 총 가중치라고 하고,

$$
W=\sum_{i, j: j>i} W_{i j}
$$

를 모든 엣지 가중치의 합이라고 합시다. 그러면 $\sum_{i} W_{i}=2 W$입니다.
이제 정상 분포가 다음과 같다고 추측합니다.

$$
\mu_{i}=\frac{W_{i}}{2 W}
$$

$\mu P=\mu$를 확인하여 이것이 정상 분포임을 검증합니다. 여기서

$$
\begin{aligned}
\sum_{i} \mu_{i} P_{i j} & =\sum_{i} \frac{W_{i}}{2 W} \frac{W_{i j}}{W_{i}} \\
& =\sum_{i} \frac{1}{2 W} W_{i j} \\
& =\frac{W_{j}}{2 W} \\
& =\mu_{j}
\end{aligned}
$$

따라서 상태 $i$의 정상 확률은 노드 $i$에서 나오는 엣지의 가중치에 비례합니다. 이 정상 분포는 국소성이라는 흥미로운 속성을 가집니다. 이는 총 가중치와 노드에 연결된 엣지의 가중치에만 의존하므로, 총 가중치를 일정하게 유지하면서 그래프의 다른 부분의 가중치가 변경되어도 변하지 않습니다. 이제 엔트로피율을 다음과 같이 계산할 수 있습니다.

$$
\begin{aligned}
H(\mathcal{X}) & =H\left(X_{2} \mid X_{1}\right) \\
& =-\sum_{i} \mu_{i} \sum_{j} P_{i j} \log P_{i j}
\end{aligned}
$$
<!-- Page 106 -->
$$
\begin{aligned}
& =-\sum_{i} \frac{W_{i}}{2 W} \sum_{j} \frac{W_{i j}}{W_{i}} \log \frac{W_{i j}}{W_{i}} \\
& =-\sum_{i} \sum_{j} \frac{W_{i j}}{2 W} \log \frac{W_{i j}}{W_{i}} \\
& =-\sum_{i} \sum_{j} \frac{W_{i j}}{2 W} \log \frac{W_{i j}}{2 W}+\sum_{i} \sum_{j} \frac{W_{i j}}{2 W} \log \frac{W_{i}}{2 W} \\
& =H\left(\ldots, \frac{W_{i j}}{2 W}, \ldots\right)-H\left(\ldots, \frac{W_{i}}{2 W}, \ldots\right)
\end{aligned}
$$

모든 간선에 동일한 가중치가 부여된 경우, 정상 분포는 노드 $i$에 $E_{i} / 2 E$의 가중치를 부여합니다. 여기서 $E_{i}$는 노드 $i$에서 나오는 간선의 수이고 $E$는 그래프의 총 간선 수입니다. 이 경우, 랜덤 워크의 엔트로피율은 다음과 같습니다.

$$
H(\mathcal{X})=\log (2 E)-H\left(\frac{E_{1}}{2 E}, \frac{E_{2}}{2 E}, \ldots, \frac{E_{m}}{2 E}\right)
$$

엔트로피율에 대한 이 답은 거의 오해의 소지가 있을 정도로 매우 간단합니다. 명백히, 엔트로피율, 즉 평균 전이 엔트로피는 정상 분포의 엔트로피와 총 간선 수에만 의존합니다.

예제 4.3.1 (체스판에서의 랜덤 워크) 킹이 $8 \times 8$ 체스판에서 무작위로 움직인다고 가정합니다. 킹은 내부에서는 여덟 개의 이동, 가장자리에서는 다섯 개의 이동, 모서리에서는 세 개의 이동이 가능합니다. 이를 이용하고 앞선 결과들을 적용하면, 정상 확률은 각각 $\frac{8}{420}$, $\frac{5}{420}$, $\frac{3}{420}$이며, 엔트로피율은 $0.92 \log 8$입니다. 0.92라는 계수는 가장자리 효과 때문이며, 무한 체스판에서는 엔트로피율이 $\log 8$이 될 것입니다.

마찬가지로, 룩 (룩은 항상 14개의 가능한 이동을 가지므로 $\log 14$ 비트), 비숍, 퀸의 엔트로피율을 찾을 수 있습니다. 퀸은 룩과 비숍의 이동을 결합합니다. 퀸은 이 둘의 조합보다 더 많은 자유도를 가질까요, 아니면 더 적은 자유도를 가질까요?

참고 정상 랜덤 워크는 그래프 상에서 시간 가역적임을 쉽게 알 수 있습니다. 즉, 어떤 상태 시퀀스의 확률은
<!-- Page 107 -->
동일하게 앞뒤로:

$$
\begin{aligned}
& \operatorname{Pr}\left(X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{n}=x_{n}\right) \\
& \quad=\operatorname{Pr}\left(X_{n}=x_{1}, X_{n-1}=x_{2}, \ldots, X_{1}=x_{n}\right)
\end{aligned}
$$

놀랍게도 역도 성립합니다. 즉, 모든 시간 역행렬 마르코프 연쇄는 무방향 가중 그래프에서의 무작위 보행으로 표현될 수 있습니다.

# 4.4 열역학 제2법칙

물리학의 기본 법칙 중 하나인 열역학 제2법칙은 고립계의 엔트로피는 비감소한다고 명시합니다. 이제 제2법칙과 이 장에서 이전에 정의한 엔트로피 함수 간의 관계를 탐구합니다.

통계 열역학에서 엔트로피는 종종 시스템 내의 미시 상태 수의 로그로 정의됩니다. 모든 상태가 동등하게 가능성이 있다면 이는 엔트로피 개념과 정확히 일치합니다. 그러나 엔트로피는 왜 증가하는가?

고립계를 시스템을 지배하는 물리 법칙을 따르는 전이 확률을 가진 마르코프 연쇄로 모델링합니다. 이 가정에는 시스템의 전체 상태라는 개념과 현재 상태를 알면 시스템의 미래가 과거와 독립적이라는 사실이 내포되어 있습니다. 이러한 시스템에서 제2법칙에 대한 네 가지 다른 해석을 찾을 수 있습니다. 엔트로피가 항상 증가하지 않는다는 사실에 충격을 받을 수 있습니다. 그러나 상대 엔트로피는 항상 감소합니다.

1. 상대 엔트로피 $D\left(\mu_{n} \| \mu_{n}^{\prime}\right)$는 $n$에 따라 감소합니다. $\mu_{n}$과 $\mu_{n}^{\prime}$를 시간 $n$에서의 마르코프 연쇄의 상태 공간에 대한 두 확률 분포라고 하고, $\mu_{n+1}$과 $\mu_{n+1}^{\prime}$를 시간 $n+1$에서의 해당 분포라고 합시다. 해당 결합 질량 함수를 $p$와 $q$로 표시합니다. 따라서 $p\left(x_{n}, x_{n+1}\right)=p\left(x_{n}\right) r\left(x_{n+1} \mid x_{n}\right)$이고 $q\left(x_{n}, x_{n+1}\right)=q\left(x_{n}\right) r\left(x_{n+1} \mid x_{n}\right)$이며, 여기서 $r(\cdot \mid \cdot)$는 마르코프 연쇄의 확률 전이 함수입니다. 그러면 상대 엔트로피에 대한 연쇄 법칙에 따라 두 가지 확장이 있습니다.

$$
\begin{aligned}
D\left(p\left(x_{n}, x_{n+1}\right) \| q\left(x_{n}, x_{n+1}\right)\right)= & D\left(p\left(x_{n}\right) \| q\left(x_{n}\right)\right) \\
& +D\left(p\left(x_{n+1} \mid x_{n}\right) \| q\left(x_{n+1} \mid x_{n}\right)\right)
\end{aligned}
$$
<!-- Page 108 -->
$$
\begin{aligned}
= & D\left(p\left(x_{n+1}\right) \| q\left(x_{n+1}\right)\right) \\
& +D\left(p\left(x_{n} \mid x_{n+1}\right) \| q\left(x_{n} \mid x_{n+1}\right)\right)
\end{aligned}
$$

$p$와 $q$ 모두 Markov chain에서 파생되었으므로, 조건부 확률 질량 함수 $p\left(x_{n+1} \mid x_{n}\right)$와 $q\left(x_{n+1} \mid x_{n}\right)$는 모두 $r\left(x_{n+1} \mid x_{n}\right)$와 같으며, 따라서 $D\left(p\left(x_{n+1} \mid x_{n}\right) \| q\left(x_{n+1} \mid x_{n}\right)\right)=0$입니다. 이제 $D\left(p\left(x_{n} \mid x_{n+1}\right) \| q\left(x_{n} \mid x_{n+1}\right)\right)$의 비음수성(Theorem 2.6.3의 Corollary)을 사용하면 다음과 같습니다.

$$
D\left(p\left(x_{n}\right) \| q\left(x_{n}\right)\right) \geq D\left(p\left(x_{n+1}\right) \| q\left(x_{n+1}\right)\right)
$$

또는

$$
D\left(\mu_{n} \| \mu_{n}^{\prime}\right) \geq D\left(\mu_{n+1} \| \mu_{n+1}^{\prime}\right)
$$

결과적으로, 확률 질량 함수의 거리는 모든 Markov chain에 대해 시간 $n$에 따라 감소합니다.
앞선 부등식에 대한 한 가지 해석의 예는 캐나다와 영국에서 부의 재분배를 위한 조세 시스템이 동일하다고 가정하는 것입니다. 그러면 $\mu_{n}$와 $\mu_{n}^{\prime}$가 두 국가의 사람들 간의 부의 분포를 나타낸다면, 이 부등식은 두 분포 간의 상대 entropy 거리가 시간이 지남에 따라 감소함을 보여줍니다. 캐나다와 영국의 부 분포는 더욱 유사해집니다.
2. 시간 $n$에서의 분포 $\mu_{n}$와 정상 분포 $\mu$ 간의 상대 entropy $D\left(\mu_{n} \| \mu\right)$는 $n$에 따라 감소합니다. (4.45)에서 $\mu_{n}^{\prime}$는 시간 $n$에서의 임의의 분포입니다. 만약 $\mu_{n}^{\prime}$를 임의의 정상 분포 $\mu$로 설정하면, 다음 시간에서의 분포 $\mu_{n+1}^{\prime}$도 $\mu$와 같습니다. 따라서,

$$
D\left(\mu_{n} \| \mu\right) \geq D\left(\mu_{n+1} \| \mu\right)
$$

이는 모든 상태 분포가 시간이 지남에 따라 각 정상 분포에 점점 더 가까워짐을 의미합니다. 수열 $D\left(\mu_{n} \| \mu\right)$는 단조 비증가 비음수 수열이며 따라서 극한값을 가져야 합니다. 정상 분포가 유일하다면 극한값은 0이지만, 이를 증명하는 것은 더 어렵습니다.
3. 정상 분포가 균등할 경우 entropy는 증가합니다. 일반적으로 상대 entropy가 감소한다는 사실이 entropy가 증가함을 의미하지는 않습니다. 비균등한 정상 분포를 갖는 임의의 Markov chain에 의해 간단한 반례가 제공됩니다. 만약 시작한다면
<!-- Page 109 -->
이 균일 분포로부터 시작하는 마르코프 연쇄는 이미 최대 엔트로피 분포이므로, 분포는 더 낮은 엔트로피를 갖는 정상 분포로 수렴하게 됩니다. 여기서 엔트로피는 시간에 따라 감소합니다.
그러나 정상 분포가 균일 분포인 경우, 상대 엔트로피는 다음과 같이 표현할 수 있습니다.

$$
D\left(\mu_{n}| | \mu\right)=\log |\mathcal{X}|-H\left(\mu_{n}\right)=\log |\mathcal{X}|-H\left(X_{n}\right)
$$

이 경우 상대 엔트로피의 단조 감소는 엔트로피의 단조 증가를 의미합니다. 이는 모든 미시 상태가 동일하게 가능성이 있는 통계적 열역학과 가장 밀접하게 연관된 설명입니다. 이제 균일 정상 분포를 갖는 프로세스를 특징짓겠습니다.

정의 확률 전이 행렬 $\left[P_{i j}\right], P_{i j}=\operatorname{Pr}\left\{X_{n+1}=\right.$ $j \mid X_{n}=i\}$는 다음과 같을 때 이중 확률이라고 합니다.

$$
\sum_{i} P_{i j}=1, \quad j=1,2, \ldots
$$

그리고

$$
\sum_{j} P_{i j}=1, \quad i=1,2, \ldots
$$

비고 균일 분포는 확률 전이 행렬이 이중 확률일 때 $P$의 정상 분포입니다 (문제 4.1 참조).
4. 조건부 엔트로피 $H\left(X_{n} \mid X_{1}\right)$는 정상 마르코프 프로세스의 경우 $n$에 따라 증가합니다. 마르코프 프로세스가 정상이라면 $H\left(X_{n}\right)$는 상수입니다. 따라서 엔트로피는 비증가합니다. 그러나 $H\left(X_{n} \mid X_{1}\right)$가 $n$에 따라 증가함을 증명할 것입니다. 따라서 미래의 조건부 불확실성이 증가합니다. 이 결과에 대한 두 가지 대안 증명을 제시합니다. 첫째, 엔트로피의 속성을 사용합니다.

$$
\begin{aligned}
H\left(X_{n} \mid X_{1}\right) & \geq H\left(X_{n} \mid X_{1}, X_{2}\right) \quad \text { (조건화는 엔트로피를 감소시킴) } \\
& =H\left(X_{n} \mid X_{2}\right) \quad \text { (마르코프성으로 인해) } \\
& =H\left(X_{n-1} \mid X_{1}\right) \quad \text { (정상성으로 인해) }
\end{aligned}
$$
<!-- Page 110 -->
대안적으로, 마르코프 연쇄 $X_{1} \rightarrow X_{n-1} \rightarrow X_{n}$에 대한 데이터 처리 부등식(data-processing inequality)을 적용하면 다음과 같습니다.

$$
I\left(X_{1} ; X_{n-1}\right) \geq I\left(X_{1} ; X_{n}\right)
$$

상호 정보량(mutual information)을 엔트로피(entropy)로 확장하면 다음과 같습니다.

$$
H\left(X_{n-1}\right)-H\left(X_{n-1} \mid X_{1}\right) \geq H\left(X_{n}\right)-H\left(X_{n} \mid X_{1}\right)
$$

정상성(stationarity)에 의해 $H\left(X_{n-1}\right)=H\left(X_{n}\right)$이므로, 다음과 같습니다.

$$
H\left(X_{n-1} \mid X_{1}\right) \leq H\left(X_{n} \mid X_{1}\right)
$$

이러한 기법들은 임의의 마르코프 연쇄에 대해 $H\left(X_{0} \mid X_{n}\right)$이 에 대해 증가함을 보이는 데에도 사용될 수 있습니다.
5. 셔플(shuffle)은 엔트로피를 증가시킵니다. 만약 $T$가 카드 덱의 셔플(순열)이고 $X$가 덱 내 카드의 초기 (무작위) 위치이며, 셔플 $T$의 선택이 $X$와 독립적이라면,

$$
H(T X) \geq H(X)
$$

여기서 $T X$는 초기 순열 $X$에 대한 셔플 $T$에 의해 유도된 덱의 순열입니다. 문제 4.3에서 증명 개요를 설명합니다.

# 4.5 마르코프 연쇄의 함수

잘못된 방법으로 접근하면 매우 어려울 수 있는 예시입니다. 이는 지금까지 개발된 기법들의 강력함을 보여줍니다. $X_{1}, X_{2}, \ldots, X_{n}, \ldots$를 정상 마르코프 연쇄라고 하고, $Y_{i}=\phi\left(X_{i}\right)$를 마르코프 연쇄의 해당 상태에 대한 함수인 각 항으로 구성된 프로세스라고 합시다. 엔트로피율 $H(\mathcal{Y})$는 얼마입니까? 이러한 마르코프 연쇄의 함수는 실제 상황에서 자주 발생합니다. 많은 경우, 시스템 상태에 대한 부분적인 정보만 가지고 있습니다. 만약 $Y_{1}, Y_{2}, \ldots, Y_{n}$도 마르코프 연쇄를 형성한다면 문제가 크게 단순화될 것이지만, 많은 경우 그렇지 않습니다. 마르코프 연쇄가 정상적이므로, $Y_{1}, Y_{2}, \ldots, Y_{n}$도 정상적이며 엔트로피율은 잘 정의됩니다. 그러나 $H(\mathcal{Y})$를 계산하고 싶다면, 각 $n$에 대해 $H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}\right)$를 계산하고 극한값을 찾을 수 있습니다. 수렴이 임의로 느릴 수 있으므로, 극한값에 얼마나 가까운지 결코 알 수 없습니다. ( $n$과 $n+1$에서의 값 사이의 차이를 볼 수 없습니다. 왜냐하면 이 차이는 우리가 극한값에서 멀리 떨어져 있을 때에도 작을 수 있기 때문입니다. 예를 들어, $\left.\sum \frac{1}{n}.\right)$를 고려하십시오.)
<!-- Page 111 -->
계산상으로는 상한과 하한이 위아래에서 극한값으로 수렴하는 것이 유용할 것입니다. 상한과 하한의 차이가 작아지면 계산을 중단할 수 있으며, 그러면 극한값에 대한 좋은 추정치를 얻게 될 것입니다.

이미 $H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}\right)$가 위에서부터 $H(\mathcal{Y})$로 단조적으로 수렴한다는 것을 알고 있습니다. 하한에 대해서는 $H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}, X_{1}\right)$를 사용할 것입니다. 이는 $X_{1}$이 $Y_{n}$에 대해 $Y_{1}, Y_{0}, Y_{-1}, \ldots$만큼의 정보를 포함한다는 아이디어에 기반한 깔끔한 기법입니다.

# 보조정리 4.5.1

$$
H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{2}, X_{1}\right) \leq H(\mathcal{Y})
$$

증명: $k=1,2, \ldots$에 대해 다음과 같습니다.

$$
\begin{aligned}
H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{2}, X_{1}\right) & \stackrel{(a)}{=} H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{2}, Y_{1}, X_{1}\right) \\
& \stackrel{(b)}{=} H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}, X_{1}, X_{0}, X_{-1}, \ldots, X_{-k}\right) \\
& \stackrel{(c)}{=} H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}, X_{1}, X_{0}, X_{-1}, \ldots\right. \\
& \left.X_{-k}, Y_{0}, \ldots, Y_{-k}\right) \\
& \stackrel{(d)}{\leq} H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}, Y_{0}, \ldots, Y_{-k}\right) \\
& \stackrel{(e)}{=} H\left(Y_{n+k+1} \mid Y_{n+k}, \ldots, Y_{1}\right)
\end{aligned}
$$

여기서 (a)는 $Y_{1}$이 $X_{1}$의 함수라는 사실에서 비롯되며, (b)는 $X$의 마르코프성에서 비롯되며, (c)는 $Y_{i}$가 $X_{i}$의 함수라는 사실에서 비롯되며, (d)는 조건화가 엔트로피를 감소시킨다는 사실에서 비롯되며, (e)는 정상성(stationarity)에 의해 비롯됩니다. 이 부등식은 모든 $k$에 대해 참이므로 극한에서도 참입니다. 따라서,

$$
\begin{aligned}
H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}, X_{1}\right) & \leq \lim _{k} H\left(Y_{n+k+1} \mid Y_{n+k}, \ldots, Y_{1}\right) \\
& =H(\mathcal{Y})
\end{aligned}
$$

다음 보조정리는 상한과 하한 사이의 간격이 줄어드는 것을 보여줍니다.

## 보조정리 4.5.2

$$
H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}\right)-H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}, X_{1}\right) \rightarrow 0
$$
<!-- Page 112 -->
증명: 구간 길이는 다음과 같이 다시 쓸 수 있습니다.

$$
\begin{aligned}
& H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}\right)-H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}, X_{1}\right) \\
& \quad=I\left(X_{1} ; Y_{n} \mid Y_{n-1}, \ldots, Y_{1}\right)
\end{aligned}
$$

mutual information의 속성에 의해,

$$
I\left(X_{1} ; Y_{1}, Y_{2}, \ldots, Y_{n}\right) \leq H\left(X_{1}\right)
$$

그리고 $I\left(X_{1} ; Y_{1}, Y_{2}, \ldots, Y_{n}\right)$는 $n$에 따라 증가합니다. 따라서, $\lim I\left(X_{1} ; Y_{1}, Y_{2}, \ldots\right.$, $Y_{n}$ )는 존재하며

$$
\lim _{n \rightarrow \infty} I\left(X_{1} ; Y_{1}, Y_{2}, \ldots, Y_{n}\right) \leq H\left(X_{1}\right)
$$

chain rule에 의해,

$$
\begin{aligned}
H(X) & \geq \lim _{n \rightarrow \infty} I\left(X_{1} ; Y_{1}, Y_{2}, \ldots, Y_{n}\right) \\
& =\lim _{n \rightarrow \infty} \sum_{i=1}^{n} I\left(X_{1} ; Y_{i} \mid Y_{i-1}, \ldots, Y_{1}\right) \\
& =\sum_{i=1}^{\infty} I\left(X_{1} ; Y_{i} \mid Y_{i-1}, \ldots, Y_{1}\right)
\end{aligned}
$$

이 무한 합이 유한하고 항들이 음수가 아니므로, 항들은 0으로 수렴해야 합니다. 즉,

$$
\lim I\left(X_{1} ; Y_{n} \mid Y_{n-1}, \ldots, Y_{1}\right)=0
$$

이는 lemma를 증명합니다.
Lemma 4.5.1과 4.5.2를 결합하면 다음과 같은 theorem을 얻습니다.
Theorem 4.5.1 만약 $X_{1}, X_{2}, \ldots, X_{n}$이 stationary Markov chain을 형성하고, $Y_{i}=\phi\left(X_{i}\right)$이면,

$$
H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}, X_{1}\right) \leq H(\mathcal{Y}) \leq H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}\right)
$$

그리고

$$
\lim H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}, X_{1}\right)=H(\mathcal{Y})=\lim H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}\right)
$$

일반적으로, $Y_{i}$가 $X_{i}$의 stochastic function (deterministic function과 반대)인 경우도 고려할 수 있습니다. Markov를 고려하십시오.
<!-- Page 113 -->
프로세스 $X_{1}, X_{2}, \ldots, X_{n}$을 정의하고, 새로운 프로세스 $Y_{1}, Y_{2}, \ldots, Y_{n}$을 정의합니다. 여기서 각 $Y_{i}$는 $p\left(y_{i} \mid x_{i}\right)$에 따라 추출되며, 다른 모든 $X_{j}, j \neq i$와 조건부 독립입니다. 즉,

$$
p\left(x^{n}, y^{n}\right)=p\left(x_{1}\right) \prod_{i=1}^{n-1} p\left(x_{i+1} \mid x_{i}\right) \prod_{i=1}^{n} p\left(y_{i} \mid x_{i}\right)
$$

이러한 프로세스를 은닉 마르코프 모델(HMM)이라고 하며, 음성 인식, 필기 인식 등에 광범위하게 사용됩니다. 마르코프 연쇄의 함수에 대해 위에서 사용된 것과 동일한 논증이 은닉 마르코프 모델에도 적용되며, 은닉 마르코프 모델의 엔트로피율을 기저 마르코프 상태에 조건화함으로써 하한을 설정할 수 있습니다. 논증의 세부 사항은 독자에게 맡깁니다.

# 요약

엔트로피율. 확률 프로세스에 대한 엔트로피율의 두 가지 정의는 다음과 같습니다.

$$
\begin{aligned}
H(\mathcal{X}) & =\lim _{n \rightarrow \infty} \frac{1}{n} H\left(X_{1}, X_{2}, \ldots, X_{n}\right) \\
H^{\prime}(\mathcal{X}) & =\lim _{n \rightarrow \infty} H\left(X_{n} \mid X_{n-1}, X_{n-2}, \ldots, X_{1}\right)
\end{aligned}
$$

정상 확률 프로세스의 경우,

$$
H(\mathcal{X})=H^{\prime}(\mathcal{X})
$$

## 정상 마르코프 연쇄의 엔트로피율

$$
H(\mathcal{X})=-\sum_{i j} \mu_{i} P_{i j} \log P_{i j}
$$

열역학 제2법칙. 마르코프 연쇄의 경우:

1. 상대 엔트로피 $D\left(\mu_{n} \| \mu_{n}^{\prime}\right)$는 시간에 따라 감소합니다.
2. 분포와 정상 분포 간의 상대 엔트로피 $D\left(\mu_{n} \| \mu\right)$는 시간에 따라 감소합니다.
3. 정상 분포가 균일한 경우 엔트로피 $H\left(X_{n}\right)$는 증가합니다.
<!-- Page 114 -->
4. 조건부 엔트로피 $H\left(X_{n} \mid X_{1}\right)$는 정상 마르코프 연쇄에 대해 시간이 지남에 따라 증가합니다.
5. 초기 조건 $X_{0}$의 조건부 엔트로피 $H\left(X_{0} \mid X_{n}\right)$는 모든 마르코프 연쇄에 대해 증가합니다.

마르코프 연쇄의 함수. 만약 $X_{1}, X_{2}, \ldots, X_{n}$이 정상 마르코프 연쇄를 형성하고 $Y_{i}=\phi\left(X_{i}\right)$이면,

$$
H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}, X_{1}\right) \leq H(\mathcal{Y}) \leq H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}\right)
$$

그리고

$$
\lim _{n \rightarrow \infty} H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}, X_{1}\right)=H(\mathcal{Y})=\lim _{n \rightarrow \infty} H\left(Y_{n} \mid Y_{n-1}, \ldots, Y_{1}\right)
$$

# 문제

4.1 이중 확률 행렬. $n \times n$ 행렬 $P=\left[P_{i j}\right]$는 $P_{i j} \geq 0$이고 모든 $i$에 대해 $\sum_{j} P_{i j}=1$이며 모든 $j$에 대해 $\sum_{i} P_{i j}=1$이면 이중 확률 행렬이라고 합니다. $n \times n$ 행렬 $P$는 이중 확률 행렬이고 각 행과 각 열에 정확히 하나의 $P_{i j}=1$이 있으면 순열 행렬이라고 합니다. 모든 이중 확률 행렬은 순열 행렬의 볼록 조합으로 작성될 수 있음을 보일 수 있습니다.
(a) 확률 벡터 $\mathbf{a}^{t}=\left(a_{1}, a_{2}, \ldots, a_{n}\right), a_{i} \geq 0, \sum a_{i}=1$을 가정합니다. $P$가 이중 확률 행렬일 때 $\mathbf{b}=\mathbf{a} P$라고 가정합니다. $\mathbf{b}$가 확률 벡터이고 $H\left(b_{1}, b_{2}, \ldots, b_{n}\right) \geq H\left(a_{1}, a_{2}, \ldots, a_{n}\right)$임을 보이십시오. 따라서 확률적 혼합은 엔트로피를 증가시킵니다.
(b) 이중 확률 행렬 $P$의 정상 분포 $\mu$는 균등 분포임을 보이십시오.
(c) 반대로, 균등 분포가 마르코프 전이 행렬 $P$의 정상 분포이면 $P$는 이중 확률 행렬임을 증명하십시오.
4.2 시간의 화살. $\left\{X_{i}\right\}_{i=-\infty}^{\infty}$가 정상 확률 과정이라고 가정합니다. 다음을 증명하십시오.

$$
H\left(X_{0} \mid X_{-1}, X_{-2}, \ldots, X_{-n}\right)=H\left(X_{0} \mid X_{1}, X_{2}, \ldots, X_{n}\right)
$$
<!-- Page 115 -->
다시 말해, 현재는 과거가 주어졌을 때의 조건부 엔트로피가 미래가 주어졌을 때의 조건부 엔트로피와 같습니다. 이는 미래로의 흐름이 과거로의 흐름과 상당히 다르게 보이는 정상 확률 과정을 쉽게 만들어낼 수 있음에도 불구하고 사실입니다. 즉, 과정의 샘플 함수를 보고 시간의 방향을 결정할 수 있습니다. 그럼에도 불구하고, 현재 상태가 주어지면 미래의 다음 심볼에 대한 조건부 불확실성은 과거의 이전 심볼에 대한 조건부 불확실성과 같습니다.
4.3 셔플은 엔트로피를 증가시킵니다. 셔플 $T$에 대한 모든 분포와 카드 위치 $X$에 대한 모든 분포에 대해 다음이 성립함을 논증하십시오.

$$
\begin{aligned}
H(T X) & \geq H(T X \mid T) \\
& =H\left(T^{-1} T X \mid T\right) \\
& =H(X \mid T) \\
& =H(X)
\end{aligned}
$$

만약 $X$와 $T$가 독립이라면.
4.4 열역학 제2법칙. $X_{1}, X_{2}, X_{3}, \ldots$를 정상 1차 Markov 연쇄라고 합시다. 4.4절에서는 $n=2,3, \ldots$에 대해 $H\left(X_{n} \mid X_{1}\right) \geq H\left(X_{n-1} \mid X_{1}\right)$임을 보였습니다. 따라서 미래에 대한 조건부 불확실성은 시간이 지남에 따라 증가합니다. 이는 비조건부 불확실성 $H\left(X_{n}\right)$이 일정하게 유지됨에도 불구하고 사실입니다. 그러나 $H\left(X_{n} \mid X_{1}=x_{1}\right)$이 모든 $x_{1}$에 대해 반드시 $n$과 함께 증가하지는 않는다는 것을 예시를 통해 보이십시오.
4.5 랜덤 트리의 엔트로피. $n$개의 노드를 가진 랜덤 트리를 생성하는 다음 방법을 고려하십시오. 먼저 루트 노드를 확장합니다.

그런 다음 무작위로 두 개의 말단 노드 중 하나를 확장합니다.

시간 $k$에서, 균등 분포에 따라 $k-1$개의 말단 노드 중 하나를 선택하고 확장합니다. $n$개의 말단 노드가 될 때까지 계속합니다.
<!-- Page 116 -->
생성되었습니다. 따라서 다섯 개의 노드를 가진 트리를 생성하는 시퀀스는 다음과 같을 수 있습니다.

놀랍게도, 다음과 같은 방법으로 랜덤 트리를 생성하면 $n$개의 터미널 노드를 가진 트리에 대해 동일한 확률 분포를 얻게 됩니다. 먼저 $\{1,2, \ldots, n-1\}$에서 균일하게 분포된 정수 $N_{1}$을 선택합니다. 그러면 다음과 같은 그림을 얻게 됩니다.

그런 다음 $\{1,2, \ldots, N_{1}-1\}$에서 균일하게 분포된 정수 $N_{2}$를 선택하고, 독립적으로 $\left\{1,2, \ldots,\left(n-N_{1}\right)-1\right\}$에서 균일하게 분포된 정수 $N_{3}$를 선택합니다. 이제 그림은 다음과 같습니다.

더 이상 분할할 수 없을 때까지 이 과정을 계속합니다. (이 두 트리 생성 방식의 동등성은 예를 들어 Polya의 항아리 모델에서 비롯됩니다.)
이제 $T_{n}$을 위에서 설명한 대로 생성된 랜덤 $n$-노드 트리라고 합시다. 이러한 트리에 대한 확률 분포는 설명하기 어려워 보이지만, 재귀적인 형태로 이 분포의 entropy를 찾을 수 있습니다.
먼저 몇 가지 예를 들어보겠습니다. $n=2$의 경우, 트리는 하나뿐입니다. 따라서 $H\left(T_{2}\right)=0$입니다. $n=3$의 경우, 두 개의 동일하게 확률적인 트리가 있습니다.

<!-- Page 117 -->
따라서 $H\left(T_{3}\right)=\log 2$입니다. $n=4$의 경우, 다섯 개의 가능한 트리가 있으며 확률은 $\frac{1}{3}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6}$입니다.
이제 점화식을 살펴보겠습니다. $N_{1}\left(T_{n}\right)$을 트리 오른쪽 절반에 있는 $T_{n}$의 말단 노드 수라고 합시다. 다음 단계들을 각각 정당화하십시오:

$$
\begin{aligned}
H\left(T_{n}\right) & \stackrel{(\mathbf{a})}{=} H\left(N_{1}, T_{n}\right) \\
& \stackrel{(\mathbf{b})}{=} H\left(N_{1}\right)+H\left(T_{n} \mid N_{1}\right) \\
& \stackrel{(\mathbf{c})}{=} \log (n-1)+H\left(T_{n} \mid N_{1}\right) \\
& \stackrel{(\mathbf{d})}{=} \log (n-1)+\frac{1}{n-1} \sum_{k=1}^{n-1}\left(H\left(T_{k}\right)+H\left(T_{n-k}\right)\right) \\
& \stackrel{(\mathbf{e})}{=} \log (n-1)+\frac{2}{n-1} \sum_{k=1}^{n-1} H\left(T_{k}\right) \\
& =\log (n-1)+\frac{2}{n-1} \sum_{k=1}^{n-1} H_{k}
\end{aligned}
$$

(f) 이를 사용하여 다음을 보이십시오:

$$
(n-1) H_{n}=n H_{n-1}+(n-1) \log (n-1)-(n-2) \log (n-2)
$$

또는

$$
\frac{H_{n}}{n}=\frac{H_{n-1}}{n-1}+c_{n}
$$

적절하게 정의된 $c_{n}$에 대해. $\sum c_{n}=c<\infty$이므로, $\frac{1}{n} H\left(T_{n}\right)$이 상수로 수렴함을 증명했습니다. 따라서, 랜덤 트리 $T_{n}$을 설명하는 데 필요한 비트의 기대값은 $n$에 선형적으로 증가합니다.
4.6 단위당 엔트로피의 단조성. 정상 확률 과정 $X_{1}, X_{2}, \ldots, X_{n}$에 대해 다음을 보이십시오:
(a)

$$
\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n} \leq \frac{H\left(X_{1}, X_{2}, \ldots, X_{n-1}\right)}{n-1}
$$

(b)

$$
\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n} \geq H\left(X_{n} \mid X_{n-1}, \ldots, X_{1}\right)
$$
<!-- Page 118 -->
4.7 마르코프 연쇄의 엔트로피율
(a) 전이 행렬이 다음과 같은 두 상태 마르코프 연쇄의 엔트로피율을 구하십시오.

$$
P=\left[\begin{array}{cc}
1-p_{01} & p_{01} \\
p_{10} & 1-p_{10}
\end{array}\right]
$$

(b) $p_{01}, p_{10}$의 어떤 값이 엔트로피율을 최대화합니까?
(c) 전이 행렬이 다음과 같은 두 상태 마르코프 연쇄의 엔트로피율을 구하십시오.

$$
P=\left[\begin{array}{cc}
1-p & p \\
1 & 0
\end{array}\right]
$$

(d) (c) 부분의 마르코프 연쇄의 엔트로피율의 최대값을 구하십시오. 0 상태가 1 상태보다 더 많은 정보를 생성할 수 있으므로, $p$의 최대화하는 값이 $\frac{1}{2}$보다 작을 것으로 예상합니다.
(e) (c) 부분의 마르코프 연쇄에 대해 허용 가능한 길이 $t$의 상태 시퀀스 수를 $N(t)$라고 할 때, $N(t)$를 구하고 다음을 계산하십시오.

$$
H_{0}=\lim _{t \rightarrow \infty} \frac{1}{t} \log N(t)
$$

[힌트: $N(t)$를 $N(t-1)$과 $N(t-2)$로 표현하는 선형 점화식을 찾으십시오. $H_{0}$가 마르코프 연쇄의 엔트로피율에 대한 상한인 이유는 무엇입니까? $H_{0}$를 (d) 부분에서 찾은 최대 엔트로피와 비교하십시오.]
4.8 최대 엔트로피 과정. 이산 메모리 소스는 알파벳 $\{1,2\}$를 가지며, 여기서 심볼 1의 지속 시간은 1이고 심볼 2의 지속 시간은 2입니다. 1과 2의 확률은 각각 $p_{1}$과 $p_{2}$입니다. 단위 시간당 소스 엔트로피 $H(\mathcal{X})=\frac{H(\mathcal{X})}{E T}$를 최대화하는 $p_{1}$의 값을 찾으십시오. $H(\mathcal{X})$의 최대값은 얼마입니까?
4.9 초기 조건. 마르코프 연쇄에 대해 다음을 보이십시오.

$$
H\left(X_{0} \mid X_{n}\right) \geq H\left(X_{0} \mid X_{n-1}\right)
$$

따라서 미래 $X_{n}$이 전개됨에 따라 초기 조건 $X_{0}$를 복구하기가 더 어려워집니다.
4.10 쌍별 독립. $X_{1}, X_{2}, \ldots, X_{n-1}$을 $\{0,1\}$에서 값을 가지는 i.i.d. 확률 변수라고 하고, $\operatorname{Pr}\left\{X_{i}=1\right\}=\frac{1}{2}$라고 합시다. $\sum_{i=1}^{n-1} X_{i}$가 홀수이면 $X_{n}=1$이고 그렇지 않으면 $X_{n}=0$이라고 합시다. $n \geq 3$이라고 합시다.
<!-- Page 119 -->
(a) $i \neq j$인 경우 $X_{i}$와 $X_{j}$가 독립임을 보이십시오.
(b) $i \neq j$일 때 $H\left(X_{i}, X_{j}\right)$를 구하십시오.
(c) $H\left(X_{1}, X_{2}, \ldots, X_{n}\right)$를 구하십시오. 이것이 $n H\left(X_{1}\right)$와 같습니까?
4.11 정상 과정. $\ldots, X_{-1}, X_{0}, X_{1}, \ldots$가 정상(반드시 Markov는 아님) 확률 과정이라고 가정합니다. 다음 진술 중 어느 것이 참입니까? 증명하거나 반례를 제시하십시오.
(a) $H\left(X_{n} \mid X_{0}\right)=H\left(X_{-n} \mid X_{0}\right)$.
(b) $H\left(X_{n} \mid X_{0}\right) \geq H\left(X_{n-1} \mid X_{0}\right)$.
(c) $H\left(X_{n} \mid X_{1}, X_{2}, \ldots, X_{n-1}, X_{n+1}\right)$는 $n$에 대해 비증가합니다.
(d) $H\left(X_{n} \mid X_{1}, \ldots, X_{n-1}, X_{n+1}, \ldots, X_{2 n}\right)$는 $n$에 대해 비증가합니다.
4.12 뼈를 찾는 개의 엔트로피율. 개가 정수 위를 걸어 다니며, 각 단계에서 확률 $p=0.1$로 방향을 바꿀 수 있습니다. $X_{0}=0$이라고 가정합니다. 첫 번째 단계는 양수 또는 음수일 가능성이 같습니다. 일반적인 경로는 다음과 같을 수 있습니다.

$$
\left(X_{0}, X_{1}, \ldots\right)=(0,-1,-2,-3,-4,-3,-2,-1,0,1, \ldots)
$$

(a) $H\left(X_{1}, X_{2}, \ldots, X_{n}\right)$를 구하십시오.
(b) 개의 엔트로피율을 구하십시오.
(c) 개가 방향을 바꾸기 전에 이동하는 예상 단계 수를 구하십시오.
4.13 과거는 미래에 대해 거의 말해주지 않습니다. 정상 확률 과정 $X_{1}, X_{2}, \ldots, X_{n}, \ldots$에 대해 다음을 보이십시오.

$$
\lim _{n \rightarrow \infty} \frac{1}{2 n} I\left(X_{1}, X_{2}, \ldots, X_{n} ; X_{n+1}, X_{n+2}, \ldots, X_{2 n}\right)=0
$$

따라서 정상 과정의 인접한 $n$-블록 간의 의존성은 $n$에 대해 선형적으로 증가하지 않습니다.
4.14 확률 과정의 함수
(a) 정상 확률 과정 $X_{1}, X_{2}, \ldots, X_{n}$을 고려하고, 어떤 함수 $\phi$에 대해

$$
Y_{i}=\phi\left(X_{i}\right), \quad i=1,2, \ldots
$$

로 정의된 $Y_{1}, Y_{2}, \ldots, Y_{n}$을 가정합니다. 다음을 증명하십시오.

$$
H(\mathcal{Y}) \leq H(\mathcal{X})
$$
<!-- Page 120 -->
(b) 엔트로피율 $H(\mathcal{Z})$와 $H(\mathcal{X})$ 사이의 관계는 무엇입니까? 단,

$$
Z_{i}=\psi\left(X_{i}, X_{i+1}\right), \quad i=1,2, \ldots
$$

어떤 함수 $\psi$에 대해 성립한다고 가정합니다.

4.15 엔트로피율. 이산 정상 확률 과정 $\left\{X_{i}\right\}$의 엔트로피율을 $H(\mathcal{X})$라고 할 때, 다음이 성립함을 보이십시오.

$$
\frac{1}{n} H\left(X_{n}, \ldots, X_{1} \mid X_{0}, X_{-1}, \ldots, X_{-k}\right) \rightarrow H(\mathcal{X})
$$

$k=1,2, \ldots$에 대해.

4.16 제약이 있는 시퀀스의 엔트로피율. 자기 기록에서 비트를 기록하고 읽는 메커니즘은 기록될 수 있는 비트 시퀀스에 제약을 부과합니다. 예를 들어, 적절한 동기화를 보장하기 위해 두 개의 1 사이에 0의 연속 길이를 제한해야 하는 경우가 많습니다. 또한 심볼 간 간섭을 줄이기 위해 두 개의 1 사이에 최소한 하나의 0이 있어야 할 수도 있습니다. 우리는 이러한 제약의 간단한 예를 고려합니다. 시퀀스에서 두 개의 1 사이에 최소한 하나의 0과 최대 두 개의 0이 있어야 한다고 가정합니다. 따라서 101001 및 0101001과 같은 시퀀스는 유효하지만 0110010 및 0000101은 유효하지 않습니다. 길이 $n$의 유효한 시퀀스 수를 계산하고자 합니다.
(a) 제약이 있는 시퀀스의 집합이 다음 상태 다이어그램의 허용된 경로 집합과 동일함을 보이십시오.

(b) $X_{i}(n)$을 상태 $i$에서 끝나는 길이 $n$의 유효한 경로 수라고 합시다. $\mathbf{X}(n)=\left[X_{1}(n) X_{2}(n) X_{3}(n)\right]^{t}$가 다음을 만족한다고 논증하십시오.
<!-- Page 121 -->
재귀를 따릅니다:

$$
\left[\begin{array}{c}
X_{1}(n) \\
X_{2}(n) \\
X_{3}(n)
\end{array}\right]=\left[\begin{array}{lll}
0 & 1 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0
\end{array}\right]\left[\begin{array}{c}
X_{1}(n-1) \\
X_{2}(n-1) \\
X_{3}(n-1)
\end{array}\right]
$$

초기 조건은 $\mathbf{X}(1)=\left[\begin{array}{lll}1 & 1 & 0\end{array}\right]^{t}$입니다.
(c) 다음을 가정합니다.

$$
A=\left[\begin{array}{lll}
0 & 1 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0
\end{array}\right]
$$

그러면 귀납법에 의해 다음을 얻습니다.

$$
\mathbf{X}(n)=A \mathbf{X}(n-1)=A^{2} \mathbf{X}(n-2)=\cdots=A^{n-1} \mathbf{X}(1)
$$

$A$의 고유값 분해를 사용하여 고유값이 서로 다른 경우 $A=U^{-1} \Lambda U$로 쓸 수 있으며, 여기서 $\Lambda$는 고유값의 대각 행렬입니다. 그러면 $A^{n-1}=U^{-1} \Lambda^{n-1} U$입니다. 다음을 쓸 수 있음을 보이십시오.

$$
\mathbf{X}(n)=\lambda_{1}^{n-1} \mathbf{Y}_{1}+\lambda_{2}^{n-1} \mathbf{Y}_{2}+\lambda_{3}^{n-1} \mathbf{Y}_{3}
$$

여기서 $\mathbf{Y}_{1}, \mathbf{Y}_{2}, \mathbf{Y}_{3}$는 $n$에 의존하지 않습니다. $n$이 클 때 이 합은 가장 큰 항에 의해 지배됩니다. 따라서 $i=$ $1,2,3$에 대해 다음이 성립한다고 주장합니다.

$$
\frac{1}{n} \log X_{i}(n) \rightarrow \log \lambda
$$

여기서 $\lambda$는 가장 큰 (양수) 고유값입니다. 따라서 길이가 $n$인 시퀀스의 수는 $n$이 클 때 $\lambda^{n}$으로 증가합니다. 위의 행렬 $A$에 대해 $\lambda$를 계산하십시오. (고유값이 서로 같지 않은 경우도 유사하게 처리할 수 있습니다.)
(d) 이제 다른 접근 방식을 취합니다. 부분 (a)의 상태 다이어그램을 가지지만 임의의 전이 확률을 갖는 마르코프 연쇄를 고려합니다. 따라서 이 마르코프 연쇄의 확률 전이 행렬은 다음과 같습니다.

$$
P=\left[\begin{array}{ccc}
0 & 1 & 0 \\
\alpha & 0 & 1-\alpha \\
1 & 0 & 0
\end{array}\right]
$$

이 마르코프 연쇄의 정상 분포가 다음과 같음을 보이십시오.

$$
\mu=\left[\frac{1}{3-\alpha}, \frac{1}{3-\alpha}, \frac{1-\alpha}{3-\alpha}\right]
$$
<!-- Page 122 -->
(e) $\alpha$의 선택에 대한 마르코프 연쇄의 엔트로피율을 최대화하십시오. 연쇄의 최대 엔트로피율은 얼마입니까?
(f) 부분 (e)의 최대 엔트로피율을 부분 (c)의 $\log \lambda$와 비교하십시오. 두 답변이 같은 이유는 무엇입니까?
4.17 재현 시간은 분포에 민감하지 않습니다. $X_{0}, X_{1}, X_{2}, \ldots$를 i.i.d. $\sim p(x), x \in \mathcal{X}=\{1,2, \ldots, m\}$로 추출하고, $N$을 $X_{0}$의 다음 발생까지의 대기 시간이라고 합시다. 따라서 $N=\min _{n}\left\{X_{n}=\right.$ $\left.X_{0}\right\}$입니다.
(a) $E N=m$임을 보이십시오.
(b) $E \log N \leq H(X)$임을 보이십시오.
(c) (선택 사항) $\left\{X_{i}\right\}$가 정상적이고 에르고딕인 경우 부분 (a)를 증명하십시오.
4.18 정상적이지만 에르고딕이 아닌 과정. 한 통에는 두 개의 편향된 동전이 있습니다. 하나는 앞면 확률이 $p$이고 다른 하나는 앞면 확률이 $1-p$입니다. 이 동전 중 하나를 무작위로 (즉, 확률 $\frac{1}{2}$로) 선택한 다음 $n$번 던집니다. $X$는 선택된 동전의 식별자를 나타내고, $Y_{1}$과 $Y_{2}$는 첫 두 번의 던지기 결과를 나타냅니다.
(a) $I\left(Y_{1} ; Y_{2} \mid X\right)$를 계산하십시오.
(b) $I\left(X ; Y_{1}, Y_{2}\right)$를 계산하십시오.
(c) $Y$ 과정 (동전 던지기 시퀀스)의 엔트로피율을 $\mathcal{H}(\mathcal{Y})$라고 할 때, $\mathcal{H}(\mathcal{Y})$를 계산하십시오. [힌트: $\lim \frac{1}{n} H\left(X, Y_{1}, Y_{2}, \ldots, Y_{n}\right)$와 관련시키십시오.
$p \rightarrow \frac{1}{2}$일 때의 동작을 고려하여 답을 확인할 수 있습니다.
4.19 그래프에서의 랜덤 워크. 다음 그래프에서의 랜덤 워크를 고려하십시오:

(a) 정상 분포를 계산하십시오.
<!-- Page 123 -->
(b) 엔트로피율은 무엇입니까?
(c) 과정이 정상 상태라고 가정할 때 상호 정보량 $I\left(X_{n+1} ; X_{n}\right)$을 찾으십시오.
4.20 체스판에서의 무작위 행보. $3 \times 3$ 체스판에서의 킹의 무작위 행보와 관련된 마르코프 연쇄의 엔트로피율을 찾으십시오.

| 1 | 2 | 3 |
| :--: | :--: | :--: |
| 4 | 5 | 6 |
| 7 | 8 | 9 |

룩, 비숍, 퀸의 엔트로피율은 어떻습니까? 비숍에는 두 가지 종류가 있습니다.
4.21 최대 엔트로피 그래프. 네 개의 간선을 가진 연결 그래프에서의 무작위 행보를 고려하십시오.
(a) 어떤 그래프가 가장 높은 엔트로피율을 가집니까?
(b) 어떤 그래프가 가장 낮은 엔트로피율을 가집니까?
4.22 3차원 미로. 새가 $3 \times 3 \times 3$ 정육면체 미로에서 길을 잃었습니다. 새는 방과 방 사이를 이동하며, 각 벽을 통해 인접한 방으로 동일한 확률로 이동합니다. 예를 들어, 모서리 방에는 세 개의 출구가 있습니다.
(a) 정상 분포는 무엇입니까?
(b) 이 무작위 행보의 엔트로피율은 무엇입니까?
4.23 엔트로피율. $\left\{X_{i}\right\}$가 엔트로피율 $H(\mathcal{X})$를 갖는 정상 확률 과정이라고 가정합니다.
(a) $H(\mathcal{X}) \leq H\left(X_{1}\right)$임을 논증하십시오.
(b) 등호가 성립하는 조건은 무엇입니까?
4.24 엔트로피율. $\left\{X_{i}\right\}$가 정상 과정이라고 가정합니다. $Y_{i}=\left(X_{i}\right.$, $\left.X_{i+1}\right)$이라고 가정합니다. $Z_{i}=\left(X_{2 i}, X_{2 i+1}\right)$이라고 가정합니다. $V_{i}=X_{2 i}$라고 가정합니다. 과정 $\left\{X_{i}\right\},\left\{Y_{i}\right\}$, $\left\{Z_{i}\right\}$, 및 $\left\{V_{i}\right\}$의 엔트로피율 $H(\mathcal{X}), H(\mathcal{Y}), H(\mathcal{Z})$, 및 $H(\mathcal{V})$을 고려하십시오. 아래 나열된 각 쌍 사이의 부등식 관계( $\leq, =$ 또는 $\geq$)는 무엇입니까?
(a) $H(\mathcal{X}) \geqq H(\mathcal{Y})$.
(b) $H(\mathcal{X}) \geqq H(\mathcal{Z})$.
(c) $H(\mathcal{X}) \geqq H(\mathcal{V})$.
(d) $H(\mathcal{Z}) \geqq H(\mathcal{X})$.
4.25 단조성
(a) $I\left(X ; Y_{1}, Y_{2}, \ldots, Y_{n}\right)$이 $n$에 대해 비감소함을 보이십시오.
<!-- Page 124 -->
(b) 상호 정보량이 모든 $n$에 대해 상수인 조건은 무엇입니까?
4.26 마르코프 연쇄의 전이. $\left\{X_{i}\right\}$가 전이 행렬 $P$와 정상 분포 $\mu$를 갖는 기약 마르코프 연쇄를 형성한다고 가정합니다. 전이만 추적하여 연관된 "엣지 프로세스" $\left\{Y_{i}\right\}$를 형성합니다. 따라서 새로운 프로세스 $\left\{Y_{i}\right\}$는 $\mathcal{X} \times \mathcal{X}$에서 값을 가지며, $Y_{i}=\left(X_{i-1}, X_{i}\right)$입니다. 예를 들어,

$$
X^{n}=3,2,8,5,7, \ldots
$$

는 다음과 같이 됩니다.

$$
Y^{n}=(\emptyset, 3),(3,2),(2,8),(8,5),(5,7), \ldots
$$

엣지 프로세스 $\left\{Y_{i}\right\}$의 엔트로피율을 찾으십시오.
4.27 엔트로피율. $\left\{X_{i}\right\}$가 다음을 따르는 $\{0,1\}$ 값 확률 프로세스라고 가정합니다.

$$
X_{k+1}=X_{k} \oplus X_{k-1} \oplus Z_{k+1}
$$

여기서 $\left\{Z_{i}\right\}$는 Bernoulli $(p)$이고 $\oplus$는 $\bmod 2$ 덧셈을 나타냅니다. 엔트로피율 $H(\mathcal{X})$는 무엇입니까?
4.28 프로세스의 혼합. 두 확률 프로세스 중 하나를 관찰하지만 어느 것인지는 알지 못한다고 가정합니다. 엔트로피율은 무엇입니까? 구체적으로, $X_{11}, X_{12}, X_{13}, \ldots$는 매개변수 $p_{1}$을 갖는 Bernoulli 프로세스이고, $X_{21}, X_{22}, X_{23}, \ldots$는 Bernoulli $\left(p_{2}\right)$입니다. 다음을 정의합니다.

$$
\theta= \begin{cases}1 & \text { 확률 } \frac{1}{2} \text{으로} \\ 2 & \text { 확률 } \frac{1}{2} \text{으로}\end{cases}
$$

그리고 $Y_{i}=X_{\theta i}, i=1,2, \ldots$를 관찰되는 확률 프로세스로 정의합니다. 따라서 $Y$는 프로세스 $\left\{X_{1 i}\right\}$ 또는 $\left\{X_{2 i}\right\}$를 관찰합니다. 결국 $Y$는 어느 것인지 알게 될 것입니다.
(a) $\left\{Y_{i}\right\}$는 정상 프로세스입니까?
(b) $\left\{Y_{i}\right\}$는 i.i.d. 프로세스입니까?
(c) $\left\{Y_{i}\right\}$의 엔트로피율 $H$는 무엇입니까?
(d) 다음이 성립합니까?

$$
-\frac{1}{n} \log p\left(Y_{1}, Y_{2}, \ldots Y_{n}\right) \longrightarrow H ?
$$

(e) 평균 기호당 설명 길이 $\frac{1}{n} E L_{n} \longrightarrow H$를 달성하는 코드가 존재합니까?
<!-- Page 125 -->
이제 $\theta_{i}$를 $\operatorname{Bern}\left(\frac{1}{2}\right)$라고 합시다. 다음을 관찰합니다.

$$
Z_{i}=X_{\theta_{i} i}, \quad i=1,2, \ldots
$$

따라서 $\theta$는 첫 번째 부분에서와 같이 모든 시간에 대해 고정된 것이 아니라, 매번 i.i.d.로 선택됩니다. 프로세스 $\left\{Z_{i}\right\}$에 대해 (a), (b), (c), (d), (e)에 답하고, 답에 $\left(a^{\prime}\right),\left(b^{\prime}\right),\left(c^{\prime}\right),\left(d^{\prime}\right),\left(e^{\prime}\right)$라고 표시하십시오.
4.29 대기 시간. 공정한 동전의 연속적인 던지기에서 첫 번째 앞면이 나올 때까지의 대기 시간을 $X$라고 합시다. 예를 들어, $\operatorname{Pr}\{X=3\}=$ $\left(\frac{1}{2}\right)^{3}$입니다. $n$번째 앞면이 나올 때까지의 대기 시간을 $S_{n}$이라고 합시다. 따라서,

$$
\begin{aligned}
S_{0} & =0 \\
S_{n+1} & =S_{n}+X_{n+1}
\end{aligned}
$$

여기서 $X_{1}, X_{2}, X_{3}, \ldots$는 위 분포에 따라 i.i.d.입니다.
(a) 프로세스 $\left\{S_{n}\right\}$은 정상적입니까?
(b) $H\left(S_{1}, S_{2}, \ldots, S_{n}\right)$을 계산하십시오.
(c) 프로세스 $\left\{S_{n}\right\}$은 엔트로피율을 가집니까? 그렇다면, 그것은 무엇입니까? 그렇지 않다면, 왜 그렇습니까?
(d) $S_{n}$과 동일한 분포를 갖는 랜덤 변수를 생성하는 데 필요한 공정한 동전 던지기의 기대 횟수는 얼마입니까?
4.30 마르코프 연쇄 전이

$$
P=\left[P_{i j}\right]=\left[\begin{array}{cccc}
\frac{1}{2} & \frac{1}{4} & \frac{1}{4} \\
\frac{1}{4} & \frac{1}{2} & \frac{1}{4} \\
\frac{1}{4} & \frac{1}{4} & \frac{1}{2}
\end{array}\right]
$$

$X_{1}$이 상태 $\{0,1,2\}$에 대해 균등하게 분포한다고 합시다. $\left\{X_{i}\right\}_{1}^{\infty}$가 전이 행렬 $P$를 갖는 마르코프 연쇄라고 합시다. 즉, $P\left(X_{n+1}=\right.$ $j \mid X_{n}=i)=P_{i j}, i, j \in\{0,1,2\}$입니다.
(a) $\left\{X_{n}\right\}$은 정상적입니까?
(b) $\lim _{n \rightarrow \infty} \frac{1}{n} H\left(X_{1}, \ldots, X_{n}\right)$을 찾으십시오.

이제 유도된 프로세스 $Z_{1}, Z_{2}, \ldots, Z_{n}$을 고려합니다. 여기서

$$
\begin{aligned}
Z_{1} & =X_{1} \\
Z_{i} & =X_{i}-X_{i-1} \quad(\bmod 3), \quad i=2, \ldots, n
\end{aligned}
$$

따라서 $Z^{n}$은 상태가 아닌 전이를 인코딩합니다.
(c) $H\left(Z_{1}, Z_{2}, \ldots, Z_{n}\right)$을 찾으십시오.
(d) $n \geq 2$에 대해 $H\left(Z_{n}\right)$과 $H\left(X_{n}\right)$을 찾으십시오.
<!-- Page 126 -->
(e) $n \geq 2$에 대해 $H\left(Z_{n} \mid Z_{n-1}\right)$을 찾으십시오.
(f) $n \geq 2$에 대해 $Z_{n-1}$과 $Z_{n}$은 독립입니까?
4.31 마르코프 연쇄. $\left\{X_{i}\right\} \sim$ 베르누이 $(p)$라고 가정합니다. 다음을 만족하는 연관된 마르코프 연쇄 $\left\{Y_{i}\right\}_{i=1}^{n}$를 고려하십시오.
$Y_{i}=$ (현재 연속된 1의 개수). 예를 들어, $X^{n}=101110 \ldots$이면 $Y^{n}=101230 \ldots$입니다.
(a) $X^{n}$의 엔트로피율을 찾으십시오.
(b) $Y^{n}$의 엔트로피율을 찾으십시오.
4.32 시간 대칭. $\left\{X_{n}\right\}$이 정상 마르코프 과정이라고 가정합니다. $\left(X_{0}, X_{1}\right)$에 조건을 부여하고 과거와 미래를 살펴봅니다. 어떤 인덱스 $k$에 대해 다음이 성립합니까?

$$
H\left(X_{-n} \mid X_{0}, X_{1}\right)=H\left(X_{k} \mid X_{0}, X_{1}\right) ?
$$

논증을 제시하십시오.
4.33 연쇄 부등식. $X_{1} \rightarrow X_{2} \rightarrow X_{3} \rightarrow X_{4}$가 마르코프 연쇄를 형성한다고 가정합니다. 다음을 증명하십시오.

$$
I\left(X_{1} ; X_{3}\right)+I\left(X_{2} ; X_{4}\right) \leq I\left(X_{1} ; X_{4}\right)+I\left(X_{2} ; X_{3}\right)
$$

4.34 방송 채널. $X \rightarrow Y \rightarrow(Z, W)$가 마르코프 연쇄를 형성한다고 가정합니다 [즉, 모든 $x, y, z, w$에 대해 $p(x, y, z, w)=p(x) p(y \mid x) p(z, w \mid y)$]. 다음을 증명하십시오.

$$
I(X ; Z)+I(X ; W) \leq I(X ; Y)+I(Z ; W)
$$

4.35 제2법칙의 오목성. $\left\{X_{n}\right\}_{-\infty}^{\infty}$이 정상 마르코프 과정이라고 가정합니다. $H\left(X_{n} \mid X_{0}\right)$이 $n$에 대해 오목함을 증명하십시오. 구체적으로 다음을 증명하십시오.

$$
\begin{aligned}
H\left(X_{n} \mid X_{0}\right) & -H\left(X_{n-1} \mid X_{0}\right)-\left(H\left(X_{n-1} \mid X_{0}\right)-H\left(X_{n-2} \mid X_{0}\right)\right) \\
& =-I\left(X_{1} ; X_{n-1} \mid X_{0}, X_{n}\right) \leq 0
\end{aligned}
$$

따라서 두 번째 차이가 음수이므로 $H\left(X_{n} \mid X_{0}\right)$이 $n$의 오목 함수임을 입증합니다.

# 역사적 참고사항

확률 과정의 엔트로피율은 Shannon [472]에 의해 도입되었으며, 그는 또한 엔트로피율과 해당 과정으로 생성된 가능한 시퀀스의 수 사이의 일부 관계를 탐구했습니다. Shannon 이후, 기본

<!-- Page 127 -->
정보 이론의 정리를 일반적인 확률 과정에 적용합니다. 일반적인 정상 확률 과정에 대한 AEP는 16장에서 증명됩니다.

은닉 마르코프 모델은 음성 인식 [432]과 같은 여러 응용 분야에 사용됩니다. 제약이 있는 시퀀스의 엔트로피율 계산은 Shannon [472]에 의해 도입되었습니다. 이러한 시퀀스는 자기 및 광 채널 [288] 코딩에 사용됩니다.
<!-- Page 128 -->
.
<!-- Page 129 -->
# 데이터 압축

이제 정보 압축의 근본적인 한계를 확립함으로써 엔트로피의 정의에 내용을 담겠습니다. 데이터 압축은 데이터 소스의 가장 빈번한 결과에 짧은 설명을 할당하고, 필연적으로 덜 빈번한 결과에는 더 긴 설명을 할당함으로써 달성될 수 있습니다. 예를 들어, 모스 부호에서 가장 빈번한 기호는 단일 점으로 표현됩니다. 이 장에서는 확률 변수의 가장 짧은 평균 설명 길이를 찾습니다.

먼저 즉석 코드의 개념을 정의한 다음, 지수화된 코드어 길이 할당이 확률 질량 함수처럼 보여야 한다고 주장하는 중요한 Kraft 부등식을 증명합니다. 그런 다음 기본 미적분학은 기대 설명 길이가 엔트로피, 즉 첫 번째 주요 결과보다 크거나 같아야 함을 보여줍니다. 그런 다음 Shannon의 간단한 구성은 반복적인 설명에 대해 기대 설명 길이가 점근적으로 이 경계에 도달할 수 있음을 보여줍니다. 이는 엔트로피를 효율적인 설명 길이의 자연스러운 척도로 확립합니다. 최소 기대 설명 길이 할당을 찾는 유명한 Huffman 코딩 절차가 제공됩니다. 마지막으로 Huffman 코드가 경쟁적으로 최적이며 엔트로피 $H$를 갖는 확률 변수의 샘플을 생성하는 데 약 $H$번의 공정한 동전 던지기가 필요함을 보여줍니다. 따라서 엔트로피는 데이터 압축 한계일 뿐만 아니라 난수 생성에 필요한 비트 수이며, $H$를 달성하는 코드는 여러 관점에서 최적임이 밝혀졌습니다.

### 5.1 코드의 예

정의 확률 변수 $X$에 대한 소스 코드 $C$는 $X$의 범위인 $\mathcal{X}$에서 $D$-진 알파벳의 유한 길이 문자열 집합인 $\mathcal{D}^{*}$로의 매핑입니다. $C(x)$는 $x$에 해당하는 코드어를 나타내고 $l(x)$는 $C(x)$의 길이를 나타냅니다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 130 -->
예를 들어, $C(\operatorname{red})=00, C(\text { blue })=11$은 알파벳 $\mathcal{D}=\{0,1\}$을 갖는 $\mathcal{X}=\{$ red, blue $\}$에 대한 소스 코드입니다.

정의 확률 질량 함수 $p(x)$를 갖는 랜덤 변수 $X$에 대한 소스 코드 $C(x)$의 기대 길이 $L(C)$는 다음과 같이 주어집니다.

$$
L(C)=\sum_{x \in \mathcal{X}} p(x) l(x)
$$

여기서 $l(x)$는 $x$와 연관된 코드워드의 길이입니다.
일반성을 잃지 않고, $D$-진 알파벳이 $\mathcal{D}=\{0,1, \ldots, D-1\}$이라고 가정할 수 있습니다.

코드의 몇 가지 예시는 다음과 같습니다.
예제 5.1.1 다음 분포 및 코드워드 할당을 갖는 랜덤 변수 $X$를 고려하십시오.

$$
\begin{array}{ll}
\operatorname{Pr}(X=1)=\frac{1}{2}, & \text { codeword } C(1)=0 \\
\operatorname{Pr}(X=2)=\frac{1}{4}, & \text { codeword } C(2)=10 \\
\operatorname{Pr}(X=3)=\frac{1}{8}, & \text { codeword } C(3)=110 \\
\operatorname{Pr}(X=4)=\frac{1}{8}, & \text { codeword } C(4)=111
\end{array}
$$

$X$의 entropy $H(X)$는 1.75 비트이며, 이 코드의 기대 길이 $L(C)=$ $E l(X)$도 1.75 비트입니다. 여기서 우리는 entropy와 동일한 평균 길이를 갖는 코드를 가지고 있습니다. 비트의 모든 시퀀스는 $X$의 심볼 시퀀스로 고유하게 디코딩될 수 있음을 주목합니다. 예를 들어, 비트 문자열 0110111100110은 134213으로 디코딩됩니다.

예제 5.1.2 랜덤 변수에 대한 또 다른 간단한 코드 예시를 고려하십시오.

$$
\begin{array}{ll}
\operatorname{Pr}(X=1)=\frac{1}{3}, & \text { codeword } C(1)=0 \\
\operatorname{Pr}(X=2)=\frac{1}{3}, & \text { codeword } C(2)=10 \\
\operatorname{Pr}(X=3)=\frac{1}{3}, & \text { codeword } C(3)=11
\end{array}
$$

예제 5.1.1과 마찬가지로 이 코드는 고유하게 디코딩 가능합니다. 그러나 이 경우 entropy는 $\log 3=1.58$ 비트이고 인코딩의 평균 길이는 1.66 비트입니다. 여기서 $\operatorname{El}(X)>H(X)$입니다.

예제 5.1.3 (모스 부호) 모스 부호는 네 개의 심볼: 점,
<!-- Page 131 -->
점, 글자 간격, 단어 간격으로 구성됩니다. 짧은 시퀀스는 빈번한 문자를 나타내고(예: 점 하나는 E를 나타냄), 긴 시퀀스는 드문 문자를 나타냅니다(예: Q는 "점, 점, 점, 점"으로 표현됨). 이것은 네 개의 기호로 알파벳을 표현하는 최적의 방법이 아닙니다. 사실, 많은 가능한 코드워드가 활용되지 않는데, 이는 문자의 코드워드가 글자 간격 외에는 공백을 포함하지 않으며, 어떤 공백도 다른 공백 뒤에 올 수 없기 때문입니다. 이러한 제약 조건 하에서 구성할 수 있는 시퀀스의 수를 계산하는 것은 흥미로운 문제입니다. 이 문제는 Shannon이 1948년 원 논문에서 해결했습니다. 이 문제는 긴 0 문자열이 금지되는 자기 기록 코딩과도 관련이 있습니다[5], [370].

이제 코드에 대해 점점 더 엄격한 조건을 정의합니다. $x^{n}$을 $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$으로 나타냅니다.

정의 코드는 비특이적(nonsingular)이라고 하는데, 이는 $X$의 범위에 있는 모든 요소가 $\mathcal{D}^{*}$의 다른 문자열로 매핑될 때를 말합니다. 즉,

$$
x \neq x^{\prime} \Rightarrow C(x) \neq C\left(x^{\prime}\right)
$$

비특이성은 단일 $X$ 값의 모호하지 않은 설명을 위해 충분합니다. 그러나 일반적으로 값의 시퀀스를 보내고자 합니다. 이러한 경우, 두 코드워드 사이에 특수 기호("쉼표")를 추가하여 복호화 가능성을 보장할 수 있습니다. 그러나 이는 특수 기호의 비효율적인 사용입니다. 자체 구획화(self-punctuating) 또는 즉석(instantaneous) 코드의 아이디어를 개발하여 더 나은 방법을 사용할 수 있습니다. $X$의 기호 시퀀스를 보내야 할 필요성에 의해 동기 부여되어, 코드의 확장을 다음과 같이 정의합니다.

정의 코드 $C$의 확장 $C^{*}$는 $\mathcal{X}$의 유한 길이 문자열에서 $\mathcal{D}$의 유한 길이 문자열로의 매핑으로 정의되며, 다음과 같이 정의됩니다.

$$
C\left(x_{1} x_{2} \cdots x_{n}\right)=C\left(x_{1}\right) C\left(x_{2}\right) \cdots C\left(x_{n}\right)
$$

여기서 $C\left(x_{1}\right) C\left(x_{2}\right) \cdots C\left(x_{n}\right)$는 해당 코드워드의 연결을 나타냅니다.

예제 5.1.4 $C\left(x_{1}\right)=00$이고 $C\left(x_{2}\right)=11$이면, $C\left(x_{1} x_{2}\right)=0011$입니다.
정의 코드는 고유하게 복호화 가능(uniquely decodable)하다고 하는데, 이는 코드의 확장이 비특이적일 때를 말합니다.

다른 말로 하면, 고유하게 복호화 가능한 코드의 인코딩된 문자열은 그것을 생성하는 유일한 가능한 소스 문자열을 가집니다. 그러나 해당하는 소스 문자열의 첫 번째 기호를 결정하기 위해 전체 문자열을 봐야 할 수도 있습니다.
<!-- Page 132 -->
정의 코드(code)는 어떤 코드어도 다른 코드어의 접두사(prefix)가 되지 않을 때, 접두사 코드(prefix code) 또는 즉석 코드(instantaneous code)라고 불립니다.

즉석 코드는 미래의 코드어를 참조하지 않고도 디코딩될 수 있습니다. 왜냐하면 코드어의 끝이 즉시 인식 가능하기 때문입니다. 따라서 즉석 코드의 경우, 해당 코드어에 대응하는 코드어의 끝에 도달하는 즉시 기호 $x_{i}$를 디코딩할 수 있습니다. 나중에 오는 코드어를 볼 때까지 기다릴 필요가 없습니다. 즉석 코드는 자체적으로 구분되는 코드(self-punctuating code)입니다. 코드 기호의 시퀀스를 내려다보면서 나중 기호를 보지 않고도 코드어를 분리하기 위해 쉼표를 추가할 수 있습니다. 예를 들어, 예제 5.1.1의 코드로 생성된 이진 문자열 01011111010은 $0,10,111,110,10$으로 파싱됩니다.

이러한 정의들의 중첩은 그림 5.1에 나타나 있습니다. 다양한 종류의 코드 간의 차이점을 설명하기 위해 표 5.1에서 $x \in \mathcal{X}$에 대한 코드어 할당 $C(x)$의 예시를 고려해 보겠습니다. 비단일 코드(nonsingular code)의 경우, 코드 문자열 010은 세 가지 가능한 소스 시퀀스(2 또는 14 또는 31)를 가집니다. 따라서 코드는 고유하게 디코딩될 수 없습니다. 고유하게 디코딩 가능한 코드(uniquely decodable code)는 접두사 자유(prefix-free)가 아니므로 즉석 코드가 아닙니다. 고유하게 디코딩 가능하다는 것을 보기 위해, 임의의 코드 문자열을 가져와 처음부터 시작합니다. 처음 두 비트가 00 또는 10이면 즉시 디코딩될 수 있습니다. 만약

그림 5.1. 코드의 종류.
<!-- Page 133 -->
TABLE 5.1 Classes of Codes

|  |  | Nonsingular, But Not | Uniquely Decodable, |  |
| :-- | :--: | :--: | :--: | :--: |
| $X$ | Singular | Uniquely Decodable | But Not Instantaneous | Instantaneous |
| 1 | 0 | 0 | 10 | 0 |
| 2 | 0 | 010 | 00 | 10 |
| 3 | 0 | 01 | 11 | 110 |
| 4 | 0 | 10 | 110 | 111 |

처음 두 비트가 11이면, 다음 비트를 살펴보아야 합니다. 다음 비트가 1이면, 첫 번째 소스 심볼은 3입니다. 11 바로 뒤에 오는 0의 문자열 길이가 홀수이면, 첫 번째 코드는 110이어야 하고 첫 번째 소스 심볼은 4여야 합니다. 0의 문자열 길이가 짝수이면, 첫 번째 소스 심볼은 3입니다. 이 논증을 반복함으로써 이 코드가 고유하게 복호화 가능함을 알 수 있습니다. Sardinas와 Patterson [455]는 코드를 체계적으로 제거하면서 가능한 코드의 접미사 집합을 형성하는 고유 복호화 가능성에 대한 유한 테스트를 고안했습니다. 이 테스트는 문제 5.5.27에서 더 자세히 설명되어 있습니다. Table 5.1의 마지막 코드가 즉시 복호화 가능한 코드라는 사실은 명백한데, 어떤 코드도 다른 코드의 접두사가 아니기 때문입니다.

# 5.2 KRAFT 부등식

주어진 소스를 설명하기 위해 최소 기대 길이를 갖는 즉시 복호화 가능한 코드를 구성하고자 합니다. 모든 소스 심볼에 짧은 코드를 할당하면서도 접두사 없는 상태를 유지할 수 없다는 것은 분명합니다. 즉시 복호화 가능한 코드에 대해 가능한 코드 길이 집합은 다음 부등식에 의해 제한됩니다.

정리 5.2.1 (Kraft 부등식) 크기 $D$의 알파벳에 대한 모든 즉시 복호화 가능한 코드(접두사 코드)에 대해, 코드 길이 $l_{1}, l_{2}, \ldots, l_{m}$는 다음 부등식을 만족해야 합니다.

$$
\sum_{i} D^{-l_{i}} \leq 1
$$

반대로, 이 부등식을 만족하는 코드 길이 집합이 주어지면, 이 코드 길이를 갖는 즉시 복호화 가능한 코드가 존재합니다.

증명: 각 노드가 $D$개의 자식을 갖는 $D$-진 트리를 고려하십시오. 트리의 가지는 코드의 심볼을 나타냅니다. 예를 들어, 루트 노드에서 발생하는 $D$개의 가지는 코드의 첫 번째 심볼의 $D$가지 가능한 값을 나타냅니다. 그러면 각 코드는 다음과 같이 표현됩니다.
<!-- Page 134 -->

그림 5.2. Kraft 부등식을 위한 코드 트리.
트리의 리프(leaf)에 의해 결정됩니다. 루트에서부터의 경로는 부호어의 심볼을 추적합니다. 이러한 트리의 이진 예시는 그림 5.2에 나와 있습니다. 부호어에 대한 접두사 조건은 어떤 부호어도 트리 상의 다른 부호어의 조상이 되지 않음을 의미합니다. 따라서 각 부호어는 가능한 부호어로서 자신의 후손들을 제거합니다.

$l_{\max }$를 부호어 집합에서 가장 긴 부호어의 길이로 가정합니다. 레벨 $l_{\max }$에 있는 트리의 모든 노드를 고려합니다. 이들 중 일부는 부호어이고, 일부는 부호어의 후손이며, 일부는 둘 다 아닙니다. 레벨 $l_{i}$에 있는 부호어는 레벨 $l_{\max }$에서 $D^{l_{\max }-l_{i}}$개의 후손을 가집니다. 이들 후손 집합은 각각 서로 분리되어야 합니다. 또한, 이들 집합의 총 노드 수는 $D^{l_{\max }}$보다 작거나 같아야 합니다. 따라서 모든 부호어에 대해 합산하면 다음과 같습니다.

$$
\sum D^{l_{\max }-l_{i}} \leq D^{l_{\max }}
$$

또는

$$
\sum D^{-l_{i}} \leq 1
$$

이는 Kraft 부등식입니다.
반대로, Kraft 부등식을 만족하는 임의의 부호어 길이 집합 $l_{1}, l_{2}, \ldots, l_{m}$이 주어지면, 그림과 같은 트리를 항상 구성할 수 있습니다.
<!-- Page 135 -->
그림 5.2. 깊이 $l_{1}$의 첫 번째 노드(사전식 순서)를 부호어 1로 레이블링하고 그 자손들을 트리에서 제거합니다. 그런 다음, 남은 첫 번째 깊이 $l_{2}$의 노드를 부호어 2로 레이블링하는 식으로 진행합니다. 이와 같이 진행하여 지정된 $l_{1}, l_{2}, \ldots, l_{m}$을 갖는 접두사 코드를 구성합니다.

이제 무한 접두사 코드도 Kraft 부등식을 만족함을 보이겠습니다.

정리 5.2.2 (확장 Kraft 부등식) 접두사 코드를 형성하는 가산 무한 집합의 모든 부호어에 대해, 부호어 길이는 확장 Kraft 부등식을 만족합니다.

$$
\sum_{i=1}^{\infty} D^{-l_{i}} \leq 1
$$

반대로, 확장 Kraft 부등식을 만족하는 임의의 $l_{1}, l_{2}, \ldots$가 주어지면, 이 부호어 길이를 갖는 접두사 코드를 구성할 수 있습니다.

증명: $D$-진수 알파벳을 $\{0,1, \ldots, D-1\}$이라고 합시다. $i$번째 부호어 $y_{1} y_{2} \cdots y_{l_{i}}$를 고려하십시오. $0 . y_{1} y_{2} \cdots y_{l_{i}}$를 $D$-진수 전개로 주어진 실수라고 합시다.

$$
0 . y_{1} y_{2} \cdots y_{l_{i}}=\sum_{j=1}^{l_{i}} y_{j} D^{-j}
$$

이 부호어는 다음 구간에 해당합니다.

$$
\left[0 . y_{1} y_{2} \cdots y_{l_{i}}, 0 . y_{1} y_{2} \cdots y_{l_{i}}+\frac{1}{D^{l_{i}}}\right)
$$

이는 $D$-진수 전개가 $0 . y_{1} y_{2} \cdots y_{l_{i}}$로 시작하는 모든 실수의 집합입니다. 이는 단위 구간 $[0,1]$의 부분 구간입니다. 접두사 조건에 의해, 이 구간들은 서로소입니다. 따라서 그 길이의 합은 1보다 작거나 같아야 합니다. 이는 다음을 증명합니다.

$$
\sum_{i=1}^{\infty} D^{-l_{i}} \leq 1
$$

유한한 경우와 마찬가지로, 증명을 역으로 하여 Kraft 부등식을 만족하는 주어진 $l_{1}, l_{2}, \ldots$에 대한 코드를 구성할 수 있습니다. 먼저, $l_{1} \leq l_{2} \leq \ldots$가 되도록 인덱스를 재정렬합니다. 그런 다음 단순히 구간을 할당합니다.
<!-- Page 136 -->
단위 구간의 하한에서 순서대로 정렬합니다. 예를 들어, $l_{1}=1, l_{2}=2, \ldots$인 이진 코드를 구성하고자 한다면, 해당 심볼에 구간 $\left[0, \frac{1}{2}\right),\left[\frac{1}{2}, \frac{1}{4}\right), \ldots$를 할당하고, 해당 코드워드는 각각 0, 10, 입니다.

5.5절에서는 고유 복호 가능한 코드의 코드워드 길이 또한 Kraft 부등식을 만족함을 보입니다. 이를 수행하기 전에, 가장 짧은 즉시 복호 가능한 코드를 찾는 문제를 고려합니다.

# 5.3 최적 코드

5.2절에서는 접두사 조건을 만족하는 모든 코드워드 집합이 Kraft 부등식을 만족해야 하며, Kraft 부등식이 지정된 코드워드 길이 집합을 갖는 코드워드 집합의 존재에 대한 충분 조건임을 증명했습니다. 이제 최소 기대 길이를 갖는 접두사 코드를 찾는 문제를 고려합니다. 5.2절의 결과에 따르면, 이는 Kraft 부등식을 만족하고 다른 모든 접두사 코드의 기대 길이보다 작은 기대 길이 $L=\sum p_{i} l_{i}$를 갖는 길이 집합 $l_{1}, l_{2}, \ldots, l_{m}$을 찾는 것과 동등합니다. 이는 표준 최적화 문제입니다. 다음을 최소화합니다.

$$
L=\sum p_{i} l_{i}
$$

다음 조건을 만족하는 모든 정수 $l_{1}, l_{2}, \ldots, l_{m}$에 대해

$$
\sum D^{-l_{i}} \leq 1
$$

미적분을 이용한 간단한 분석은 최소화하는 $l_{i}^{*}$의 형태를 제시합니다. $l_{i}$에 대한 정수 제약 조건을 무시하고 제약 조건의 등식을 가정합니다. 따라서 라그랑주 승수법을 사용하여 제약 조건이 있는 최소화를 다음의 최소화로 작성할 수 있습니다.

$$
J=\sum p_{i} l_{i}+\lambda\left(\sum D^{-l_{i}}\right)
$$

$l_{i}$에 대해 미분하면 다음을 얻습니다.

$$
\frac{\partial J}{\partial l_{i}}=p_{i}-\lambda D^{-l_{i}} \log _{e} D
$$

미분을 0으로 설정하면 다음을 얻습니다.

$$
D^{-l_{i}}=\frac{p_{i}}{\lambda \log _{e} D}
$$
<!-- Page 137 -->
이것을 $\lambda$를 찾기 위한 제약 조건에 대입하면, $\lambda=1 / \log _{e} D$를 찾게 되며, 따라서

$$
p_{i}=D^{-l_{i}}
$$

최적의 코드 길이를 얻게 됩니다.

$$
l_{i}^{*}=-\log _{D} p_{i}
$$

이 정수가 아닌 복호화 길이 선택은 기대 복호화 길이를 제공합니다.

$$
L^{*}=\sum p_{i} l_{i}^{*}=-\sum p_{i} \log _{D} p_{i}=H_{D}(X)
$$

하지만 $l_{i}$는 정수여야 하므로, (5.19)와 같이 복호화 길이를 항상 설정할 수는 없습니다. 대신, 최적 집합에 "가까운" 복호화 길이 집합 $l_{i}$를 선택해야 합니다. 미적분을 통해 $l_{i}^{*}=-\log _{D} p_{i}$가 전역 최소값임을 입증하는 대신, 다음 정리의 증명에서 최적성을 직접 검증합니다.

정리 5.3.1 확률 변수 $X$에 대한 모든 즉석 $D$-진 코드의 기대 길이 $L$은 엔트로피 $H_{D}(X)$보다 크거나 같습니다. 즉,

$$
L \geq H_{D}(X)
$$

등호는 $D^{-l_{i}}=p_{i}$일 때만 성립합니다.
증명: 기대 길이와 엔트로피의 차이를 다음과 같이 쓸 수 있습니다.

$$
\begin{aligned}
L-H_{D}(X) & =\sum p_{i} l_{i}-\sum p_{i} \log _{D} \frac{1}{p_{i}} \\
& =-\sum p_{i} \log _{D} D^{-l_{i}}+\sum p_{i} \log _{D} p_{i}
\end{aligned}
$$

$r_{i}=D^{-l_{i}} / \sum_{j} D^{-l_{j}}$ 및 $c=\sum D^{-l_{i}}$라고 하면,

$$
\begin{aligned}
L-H & =\sum p_{i} \log _{D} \frac{p_{i}}{r_{i}}-\log _{D} c \\
& =D(\mathbf{p} \| \mathbf{r})+\log _{D} \frac{1}{c} \\
& \geq 0
\end{aligned}
$$
<!-- Page 138 -->
상대 엔트로피의 비음성과 (Kraft 부등식) $c \leq 1$이라는 사실에 의해 증명됩니다. 따라서 $L \geq H이며$, 등호는 $p_{i}=D^{-l_{i}}$일 때 (즉, $-\log _{D} p_{i}$가 모든 $i$에 대해 정수일 때) 성립합니다.

정의 확률 분포는 각 확률이 $D^{-n}$ 형태일 때 $D$-adic이라고 불립니다. 따라서, 정리에서 등호는 $X$의 분포가 $D$-adic일 때만 성립합니다.

앞선 증명은 최적 코드를 찾는 절차도 제시합니다. $X$의 분포에 가장 가까운 (상대 엔트로피 기준) $D$-adic 분포를 찾으십시오. 이 분포는 코드워드 길이 집합을 제공합니다. Kraft 부등식 증명에서와 같이 첫 번째 사용 가능한 노드를 선택하여 코드를 구성하십시오. 그러면 $X$에 대한 최적 코드를 얻게 됩니다.

그러나 이 절차는 가장 가까운 $D$-adic 분포를 찾는 것이 명확하지 않기 때문에 쉽지 않습니다. 다음 섹션에서는 좋은 차선책 절차 (Shannon-Fano 코딩)를 제시합니다. 섹션 5.6에서는 최적 코드를 실제로 찾는 간단한 절차 (Huffman 코딩)를 설명합니다.

# 5.4 최적 코드 길이에 대한 경계

이제 우리는 기대 설명 길이 $L$이 하한보다 1 비트 이내인 코드를 보여줍니다. 즉,

$$
H(X) \leq L<H(X)+1
$$

섹션 5.3의 설정을 다시 불러옵니다. 우리는 $l_{1}, l_{2}, \ldots, l_{m}$이 정수이고 $\sum D^{-l_{i}} \leq 1$이라는 제약 조건 하에서 $L=\sum p_{i} l_{i}$를 최소화하고자 합니다. 우리는 최적 코드워드 길이를 상대 엔트로피에서 $X$의 분포에 가장 가까운 $D$-adic 확률 분포를 찾음으로써 얻을 수 있음을 증명했습니다. 즉, $D$-adic $\mathbf{r}\left(r_{i}=D^{-l_{i}} / \sum_{j} D^{-l_{j}}\right)$를 최소화합니다.

$$
L-H_{D}=D(\mathbf{p} \| \mathbf{r})-\log \left(\sum D^{-l_{i}}\right) \geq 0
$$

단어 길이 $l_{i}=\log _{D} \frac{1}{p_{i}}$의 선택은 $L=H$를 제공합니다. $\log _{D} \frac{1}{p_{i}}$는 정수가 아닐 수 있으므로, 정수 단어 길이 할당을 위해 올림합니다.

$$
l_{i}=\left\lceil\log _{D} \frac{1}{p_{i}}\right\rceil
$$
<!-- Page 139 -->
여기서 $\lceil x\rceil$는 $x$보다 크거나 같은 가장 작은 정수입니다. 이 길이들은 Kraft 부등식을 만족시키는데, 왜냐하면

$$
\sum D^{-\lceil\log \frac{1}{p_{i}}\rceil} \leq \sum D^{-\log \frac{1}{p_{i}}}=\sum p_{i}=1
$$

이러한 코드워드 길이 선택은 다음을 만족시킵니다.

$$
\log _{D} \frac{1}{p_{i}} \leq l_{i}<\log _{D} \frac{1}{p_{i}}+1
$$

$p_{i}$를 곱하고 $i$에 대해 합하면 다음을 얻습니다.

$$
H_{D}(X) \leq L<H_{D}(X)+1
$$

최적 코드는 이 코드보다 더 좋을 수밖에 없으므로, 다음과 같은 정리를 얻습니다.

정리 5.4.1 $l_{1}^{*}, l_{2}^{*}, \ldots, l_{m}^{*}$를 소스 분포 $\mathbf{p}$와 D진 알파벳에 대한 최적 코드워드 길이로 하고, $L^{*}$를 최적 코드의 관련 기대 길이로 합니다($L^{*}=\sum p_{i} l_{i}^{*}$). 그러면

$$
H_{D}(X) \leq L^{*}<H_{D}(X)+1
$$

증명: $l_{i}=\left\lceil\log _{D} \frac{1}{p_{i}}\right\rceil$로 둡니다. 그러면 $l_{i}$는 Kraft 부등식을 만족시키고 (5.32)로부터 다음을 얻습니다.

$$
H_{D}(X) \leq L=\sum p_{i} l_{i}<H_{D}(X)+1
$$

하지만 최적 코드의 기대 길이인 $L^{*}$는 $L=\sum p_{i} l_{i}$보다 작고, 정리 5.3.1로부터 $L^{*} \geq H_{D}$이므로, 정리가 성립합니다.

정리 5.4.1에서 $\log \frac{1}{p_{i}}$가 항상 정수가 아니라는 사실 때문에 최대 1비트의 오버헤드가 존재합니다. 이 오버헤드를 여러 심볼에 분산시켜 줄일 수 있습니다. 이를 염두에 두고, $n$개의 심볼 시퀀스를 전송하는 시스템을 고려해 봅시다. 심볼들은 $p(x)$에 따라 i.i.d.로 추출된다고 가정합니다. 이 $n$개의 심볼을 알파벳 $\mathcal{X}^{n}$의 슈퍼심볼로 간주할 수 있습니다.

입력 심볼당 기대 코드워드 길이인 $L_{n}$을 정의합니다. 즉, 이진 코드워드에 연관된 길이 $l\left(x_{1}, x_{2}, \ldots, x_{n}\right)$이 있다면,

<!-- Page 140 -->
$\left(x_{1}, x_{2}, \ldots, x_{n}\right)$으로 표현되며 (이 섹션의 나머지 부분에서는 단순성을 위해 $D=2$라고 가정합니다),

$$
L_{n}=\frac{1}{n} \sum p\left(x_{1}, x_{2}, \ldots, x_{n}\right) l\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\frac{1}{n} E l\left(X_{1}, X_{2}, \ldots, X_{n}\right)
$$

이제 위에서 유도된 경계를 코드에 적용할 수 있습니다.

$$
H\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq E l\left(X_{1}, X_{2}, \ldots, X_{n}\right)<H\left(X_{1}, X_{2}, \ldots, X_{n}\right)+1
$$

$X_{1}, X_{2}, \ldots, X_{n}$은 i.i.d.이므로, $H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum H\left(X_{i}\right)=$ $n H(X)$입니다. (5.36)을 $n$으로 나누면 다음과 같이 얻습니다.

$$
H(X) \leq L_{n}<H(X)+\frac{1}{n}
$$

따라서, 큰 블록 길이를 사용함으로써 기호당 평균 코드 길이를 entropy에 임의로 가깝게 달성할 수 있습니다.

반드시 i.i.d.가 아닌 확률 과정에서 나온 기호 시퀀스에 대해서도 동일한 논증을 사용할 수 있습니다. 이 경우에도 다음과 같은 경계를 가집니다.

$$
H\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq E l\left(X_{1}, X_{2}, \ldots, X_{n}\right)<H\left(X_{1}, X_{2}, \ldots, X_{n}\right)+1
$$

다시 $n$으로 나누고 기호당 평균 설명 길이를 $L_{n}$으로 정의하면 다음과 같이 얻습니다.

$$
\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n} \leq L_{n}<\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n}+\frac{1}{n}
$$

확률 과정이 정상적이라면, $H\left(X_{1}, X_{2}, \ldots, X_{n}\right) / n \rightarrow$ $H(\mathcal{X})$이고, 평균 설명 길이는 $n \rightarrow \infty$일 때 entropy rate로 수렴합니다. 따라서 다음과 같은 정리를 얻습니다.

정리 5.4.2 기호당 최소 평균 코드 길이는 다음을 만족합니다.

$$
\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n} \leq L_{n}^{*}<\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n}+\frac{1}{n}
$$

또한, $X_{1}, X_{2}, \ldots, X_{n}$이 정상 확률 과정이라면,

$$
L_{n}^{*} \rightarrow H(\mathcal{X})
$$

여기서 $H(\mathcal{X})$는 과정의 entropy rate입니다.
<!-- Page 141 -->
이 정리는 엔트로피율의 정의에 대한 또 다른 정당성을 제공합니다. 즉, 해당 프로세스를 설명하는 데 필요한 기호당 예상 비트 수입니다.

마지막으로, 코드가 잘못된 분포에 대해 설계된 경우 예상 설명 길이가 어떻게 되는지 질문합니다. 예를 들어, 잘못된 분포는 알 수 없는 실제 분포에 대해 우리가 할 수 있는 최선의 추정일 수 있습니다. 확률 질량 함수 $q(x)$에 대해 설계된 Shannon 코드 할당 $l(x)=\left\lceil\log \frac{1}{q(x)}\right\rceil$을 고려합니다. 실제 확률 질량 함수가 $p(x)$라고 가정합니다. 따라서 예상 길이 $L \approx H(p)=-\sum p(x) \log p(x)$를 달성하지 못할 것입니다. 이제 잘못된 분포로 인한 예상 설명 길이의 증가는 상대 엔트로피 $D(p \| q)$임을 보여줍니다. 따라서 $D(p \| q)$는 잘못된 정보로 인한 설명 복잡성의 증가로 구체적인 해석을 갖습니다.

정리 5.4.3 (잘못된 코드) 코드 할당 $l(x)=\left\lceil\log \frac{1}{q(x)}\right\rceil$이 $p(x)$ 하에서 갖는 예상 길이는 다음과 같습니다.

$$
H(p)+D(p \| q) \leq E_{p} l(X)<H(p)+D(p \| q)+1
$$

증명: 예상 코드 길이는 다음과 같습니다.

$$
\begin{aligned}
E l(X) & =\sum_{x} p(x)\left\lceil\log \frac{1}{q(x)}\right\rceil \\
& <\sum_{x} p(x)\left(\log \frac{1}{q(x)}+1\right) \\
& =\sum_{x} p(x) \log \frac{p(x)}{q(x)} \frac{1}{p(x)}+1 \\
& =\sum_{x} p(x) \log \frac{p(x)}{q(x)}+\sum_{x} p(x) \log \frac{1}{p(x)}+1 \\
& =D(p \| q)+H(p)+1
\end{aligned}
$$

하한은 유사하게 유도될 수 있습니다.
따라서 실제 분포가 $p(x)$일 때 분포가 $q(x)$라고 믿는 것은 평균 설명 길이에서 $D(p \| q)$의 페널티를 발생시킵니다.

# 5.5 고유 복호화 가능한 코드에 대한 KRAFT 부등식

모든 즉석 코드는 Kraft 부등식을 만족해야 함을 증명했습니다. 고유 복호화 가능한 코드의 클래스는

<!-- Page 142 -->
즉석 코드는 고유하게 디코딩 가능한 코드의 집합이 즉석 코드보다 더 많은 가능성을 제공하지 않는다는 것을 증명합니다. 이제 다음 정리에 대한 Karush의 우아한 증명을 제시합니다.

정리 5.5.1 (McMillan) 모든 고유하게 디코딩 가능한 D-진 코드는 Kraft 부등식을 만족해야 합니다.

$$
\sum D^{-l_{i}} \leq 1
$$

반대로, 이 부등식을 만족하는 코드 길이 집합이 주어지면 이 코드 길이로 고유하게 디코딩 가능한 코드를 구성할 수 있습니다.

증명: 코드의 $k$번째 확장인 $C^{k}$를 고려합니다 (즉, 주어진 고유하게 디코딩 가능한 코드 $C$를 $k$번 반복하여 연결하여 형성된 코드). 고유 디코딩의 정의에 따라 코드의 $k$번째 확장은 비특이적입니다. 길이가 $n$인 $D$-진 문자열은 $D^{n}$개뿐이므로, 고유 디코딩은 코드의 $k$번째 확장에서 길이 $n$인 코드 시퀀스의 수가 $D^{n}$개를 초과할 수 없음을 의미합니다. 이제 이 관찰을 사용하여 Kraft 부등식을 증명합니다.

기호 $x \in \mathcal{X}$의 코드 길이들을 $l(x)$로 표시합니다. 확장 코드의 경우 코드 시퀀스의 길이는 다음과 같습니다.

$$
l\left(x_{1}, x_{2}, \ldots, x_{k}\right)=\sum_{i=1}^{k} l\left(x_{i}\right)
$$

증명하고자 하는 부등식은 다음과 같습니다.

$$
\sum_{x \in \mathcal{X}} D^{-l(x)} \leq 1
$$

핵심은 이 양의 $k$제곱을 고려하는 것입니다. 따라서,

$$
\begin{aligned}
\left(\sum_{x \in \mathcal{X}} D^{-l(x)}\right)^{k} & =\sum_{x_{1} \in \mathcal{X}} \sum_{x_{2} \in \mathcal{X}} \cdots \sum_{x_{k} \in \mathcal{X}} D^{-l\left(x_{1}\right)} D^{-l\left(x_{2}\right)} \cdots D^{-l\left(x_{k}\right)} \\
& =\sum_{x_{1}, x_{2}, \ldots, x_{k} \in \mathcal{X}^{k}} D^{-l\left(x_{1}\right)} D^{-l\left(x_{2}\right)} \cdots D^{-l\left(x_{k}\right)} \\
& =\sum_{x^{k} \in \mathcal{X}^{k}} D^{-l\left(x^{k}\right)}
\end{aligned}
$$
<!-- Page 143 -->
(5.49)에 의해, 이제 단어 길이에 따라 항들을 모아 다음과 같이 얻습니다.

$$
\sum_{x^{k} \in \mathcal{X}^{k}} D^{-l\left(x^{k}\right)}=\sum_{m=1}^{k l_{\max }} a(m) D^{-m}
$$

여기서 $l_{\max }$는 최대 코드워드 길이이고 $a(m)$은 길이가 $m$인 코드워드로 매핑되는 소스 시퀀스 $x^{k}$의 수입니다. 그러나 코드는 고유하게 복호 가능하므로, 각 코드 $m$-시퀀스로 매핑되는 시퀀스는 최대 하나이며, $D^{m}$개의 코드 $m$-시퀀스가 존재합니다. 따라서 $a(m) \leq D^{m}$이며, 다음과 같이 됩니다.

$$
\begin{aligned}
\left(\sum_{x \in \mathcal{X}} D^{-l(x)}\right)^{k} & =\sum_{m=1}^{k l_{\max }} a(m) D^{-m} \\
& \leq \sum_{m=1}^{k l_{\max }} D^{m} D^{-m} \\
& =k l_{\max }
\end{aligned}
$$

따라서

$$
\sum_{j} D^{-l_{j}} \leq\left(k l_{\max }\right)^{1 / k}
$$

이 부등식은 모든 $k$에 대해 참이므로, $k \rightarrow \infty$로 갈 때의 극한에서도 참입니다. $\left(k l_{\max }\right)^{1 / k} \rightarrow 1$이므로, 다음과 같이 됩니다.

$$
\sum_{j} D^{-l_{j}} \leq 1
$$

이는 Kraft 부등식입니다.
반대로, Kraft 부등식을 만족하는 임의의 $l_{1}, l_{2}, \ldots, l_{m}$ 집합이 주어지면, 섹션 5.2에서 증명된 것처럼 즉시 코드(instantaneous code)를 구성할 수 있습니다. 모든 즉시 코드는 고유하게 복호 가능하므로, 고유하게 복호 가능한 코드도 구성한 것입니다.

따름정리: 무한한 소스 알파벳 $\mathcal{X}$에 대한 고유하게 복호 가능한 코드는 Kraft 부등식도 만족합니다.

증명: 이전 증명에서 $|\mathcal{X}|$가 무한할 때 문제가 발생하는 지점은 (5.58)입니다. 왜냐하면 무한한 코드의 경우 $l_{\max }$가 무한하기 때문입니다. 그러나 다음과 같은 점이 있습니다.
<!-- Page 144 -->
간단한 증명 수정입니다. 고유 복호화 가능한 코드의 모든 부분집합은 또한 고유 복호화 가능합니다. 따라서, 무한한 코드어 집합의 모든 유한 부분집합은 Kraft 부등식을 만족합니다. 그러므로,

$$
\sum_{i=1}^{\infty} D^{-l_{i}}=\lim _{N \rightarrow \infty} \sum_{i=1}^{N} D^{-l_{i}} \leq 1
$$

Kraft 부등식을 만족하는 단어 길이 집합 $l_{1}, l_{2}, \ldots$가 주어졌을 때, 섹션 5.4에서와 같이 즉시 복호화 가능한 코드를 구성할 수 있습니다. 즉시 복호화 가능한 코드는 고유 복호화 가능하므로, 무한한 수의 코드어를 가진 고유 복호화 가능한 코드를 구성했습니다. 따라서 McMillan 정리는 무한 알파벳에도 적용됩니다.

이 정리는 매우 놀라운 결과를 함의합니다. 즉, 고유 복호화 가능한 코드의 클래스는 접두사 코드의 클래스보다 코드어 길이 집합에 대해 더 이상 선택지를 제공하지 않는다는 것입니다. 달성 가능한 코드어 길이 집합은 고유 복호화 가능한 코드와 즉시 복호화 가능한 코드에 대해 동일합니다. 따라서, 최적 코드어 길이에 대해 도출된 경계는 허용되는 코드의 클래스를 모든 고유 복호화 가능한 코드의 클래스로 확장하더라도 계속 유효합니다.

# 5.6 허프만 코드

주어진 분포에 대한 최적의 (가장 짧은 기대 길이) 접두사 코드는 Huffman [283]이 발견한 간단한 알고리즘으로 구성할 수 있습니다. 동일한 알파벳에 대한 다른 어떤 코드도 알고리즘으로 구성된 코드보다 낮은 기대 길이를 가질 수 없음을 증명할 것입니다. 공식적인 증명을 제시하기 전에, 몇 가지 예시를 통해 Huffman 코드를 소개하겠습니다.

예시 5.6.1 확률이 각각 0.25, 0.25, 0.2, 0.15, 0.15인 집합 $\mathcal{X}=\{1,2,3,4,5\}$에서 값을 갖는 확률 변수 $X$를 고려해 봅시다. $X$에 대한 최적 이진 코드는 기호 4와 5에 가장 긴 코드어가 할당될 것으로 예상됩니다. 이 두 길이는 같아야 합니다. 그렇지 않으면 더 긴 코드어에서 비트를 삭제해도 여전히 접두사 코드이면서 더 짧은 기대 길이를 가질 수 있습니다. 일반적으로, 가장 긴 두 코드어가 마지막 비트만 다른 코드를 구성할 수 있습니다. 이 코드의 경우, 기호 4와 5를 확률 할당 0.30을 가진 단일 소스 기호로 결합할 수 있습니다. 이와 같이 가장 가능성이 낮은 두 기호를 하나로 결합하여 최종적으로 하나의 기호만 남을 때까지 진행한 후, 기호에 코드어를 할당하면 다음과 같은 표를 얻게 됩니다:
<!-- Page 145 -->
| 부호어 <br> 길이 | 부호어 | $X$ | 확률 |
| :-- | :-- | :-- | :-- |
| 2 | 01 | 1 | $0.25-0.3-0.45-0.55-1$ |
| 2 | 10 | 2 | $0.25-0.25-0.3-0.45-$ |
| 2 | 11 | 3 | $0.2-0.25-0.25$ |
| 3 | 000 | 4 | $0.15-0.2$ |
| 3 | 001 | 5 | $0.15^{\prime}$ |

이 코드는 평균 길이 2.3 비트입니다.
예제 5.6.2 동일한 확률 변수에 대한 3진 코드를 고려하십시오. 이제 가장 확률이 낮은 세 개의 기호를 하나의 슈퍼 기호로 결합하고 다음 표를 얻습니다.

| 부호어 | $X$ | 확률 |
| :-- | :-- | :-- |
| 1 | 1 | $0.25-0.5-1$ |
| 2 | 2 | $0.25-0.25-$ |
| 00 | 3 | $0.2-0.25-$ |
| 01 | 4 | $0.15^{\prime}$ |
| 02 | 5 | $0.15^{\prime}$ |

이 코드는 평균 길이 1.5개의 3진 숫자입니다.
예제 5.6.3 $D \geq 3$인 경우, $D$개씩 결합할 수 있는 충분한 기호가 없을 수 있습니다. 이러한 경우, 기호 집합의 끝에 더미 기호를 추가합니다. 더미 기호는 확률이 0이며 트리를 채우기 위해 삽입됩니다. 각 축소 단계에서 기호 수가 $D-1$만큼 감소하므로, 총 기호 수가 $1+k(D-1)$ 형태가 되기를 원합니다. 여기서 $k$는 병합 횟수입니다. 따라서 총 기호 수가 이 형태가 되도록 충분한 더미 기호를 추가합니다. 예를 들어:

| 부호어 | $X$ | 확률 |
| :-- | :--: | :-- |
| 1 | 1 | $0.25-0.25-1.0$ |
| 2 | 2 | $0.25-0.25-0.25-1.0$ |
| 01 | 3 | $0.2-0.2-0.25-0.25$ |
| 02 | 4 | $0.1-0.1-0.1$ |
| 000 | 5 | $0.1^{\prime}$ |
| 001 | 6 | $0.1^{\prime}$ |
| 002 | 더미 |  |

이 코드는 평균 길이 1.7개의 3진 숫자입니다.
<!-- Page 146 -->
Section 5.8에 허프만 코딩의 최적성에 대한 증명이 제시되어 있습니다.

# 5.7 허프만 코드에 대한 몇 가지 논평

1. 소스 코딩과 20개의 질문의 동등성. 이제 코딩과 "20개의 질문" 게임의 동등성을 보이기 위해 잠시 벗어납니다. 객체의 집합에서 객체를 결정하기 위한 가장 효율적인 예-아니오 질문 시퀀스를 찾고자 한다고 가정해 봅시다. 객체에 대한 확률 분포를 알고 있다고 가정할 때, 가장 효율적인 질문 시퀀스를 찾을 수 있습니까? (객체를 결정하기 위해서는 질문 시퀀스에 대한 응답이 가능한 객체 집합에서 객체를 고유하게 식별하도록 해야 합니다. 마지막 질문이 "예" 답변을 갖는 것은 필수는 아닙니다.)

먼저 질문 시퀀스가 객체에 대한 코드와 동등함을 보입니다. 모든 질문은 이전 질문에 대한 답변에만 의존합니다. 질문 시퀀스가 객체를 고유하게 결정하므로 각 객체는 다른 질문 시퀀스를 가지며, 예-아니오 답변을 0과 1로 표현하면 객체 집합에 대한 이진 코드를 갖게 됩니다. 이 코드의 평균 길이는 질문 방식에 대한 평균 질문 수입니다.

또한, 객체 집합에 대한 이진 코드로부터 해당 코드에 대응하는 질문 시퀀스를 찾을 수 있으며, 평균 질문 수는 코드의 기대 코딩어 길이에 해당합니다. 이 방식의 첫 번째 질문은 다음과 같습니다: 객체의 코딩어에서 첫 번째 비트가 1과 같습니까?

허프만 코드는 랜덤 변수에 대한 최적의 소스 코드이므로, 최적의 질문 시퀀스는 허프만 코드에 의해 결정되는 것입니다. 예제 5.6.1에서 최적의 첫 번째 질문은 다음과 같습니다: $X$가 2 또는 3과 같습니까? 이 질문에 대한 답변은 허프만 코드의 첫 번째 비트를 결정합니다. 첫 번째 질문에 대한 답변이 "예"라고 가정하면, 다음 질문은 "X가 3과 같습니까?"가 되어야 하며, 이는 허프만 코드의 두 번째 비트를 결정합니다. 그러나 두 번째 질문을 하기 위해 첫 번째 질문에 대한 답변을 기다릴 필요는 없습니다. 두 번째 질문으로 "X가 1 또는 3과 같습니까?"라고 물어볼 수 있으며, 이는 첫 번째 질문과 독립적으로 허프만 코드의 두 번째 비트를 결정합니다.

이 최적 방식에서의 기대 질문 수 $E Q$는 다음을 만족합니다.

$$
H(X) \leq E Q<H(X)+1
$$
<!-- Page 147 -->
2. 가중치 부여된 코드워드에 대한 허프만 코딩. $\sum p_{i} l_{i}$를 최소화하는 허프만 알고리즘은 $\sum p_{i}$의 값에 관계없이 임의의 $p_{i} \geq 0$ 집합에 적용될 수 있습니다. 이 경우 허프만 코드는 평균 코드 길이를 최소화하는 대신 가중치 부여된 코드 길이의 합 $\sum w_{i} l_{i}$를 최소화합니다.

예제 5.7.1 동일한 알고리즘을 사용하여 가중치 부여된 최소화를 수행합니다.

| $X$ | 코드워드 | 가중치 |
| :-- | :--: | :-- |
| 1 | 00 |  |
| 2 | 01 | 5 |
| 3 | 10 | 5 |
| 4 | 11 | 4 |

이 경우 코드는 코드워드 길이의 가중치 부여된 합을 최소화하며, 최소 가중치 부여된 합은 36입니다.
3. 허프만 코딩 및 "슬라이스" 질문 (알파벳 코드). 우리는 소스 코딩과 20개의 질문 게임의 동등성을 설명했습니다. 최적의 질문 순서는 확률 변수에 대한 최적의 소스 코드에 해당합니다. 그러나 허프만 코드는 임의의 집합 $A \subseteq\{1,2, \ldots, m\}$에 대해 "Is $X \in A$ ?" 형태의 임의의 질문을 합니다.

이제 제한된 질문 집합을 사용하여 "20개의 질문" 게임을 고려합니다. 구체적으로, $\mathcal{X}=$ $\{1,2, \ldots, m\}$의 요소들이 $p_{1} \geq p_{2} \geq \cdots \geq p_{m}$으로 정렬되어 있다고 가정하고, 허용되는 유일한 질문은 임의의 $a$에 대해 "Is $X>a$ ?" 형태라고 가정합니다. 허프만 알고리즘에 의해 구성된 허프만 코드는 슬라이스 ( $\{x: x<a\}$ 형태의 집합)에 해당하지 않을 수 있습니다. 허프만 코드에서 파생된 코드워드 길이 ($l_{1} \leq l_{2} \leq \cdots \leq l_{m}$, 보조 정리 5.8.1에 따름)를 가져와서 해당 레벨에서 첫 번째 사용 가능한 노드를 취하여 심볼을 코드 트리에 할당하면 또 다른 최적의 코드를 구성할 것입니다. 그러나 허프만 코드 자체와 달리 이 코드는 슬라이스 코드입니다. 각 질문 (코드의 각 비트)은 트리를 $\{x: x>a\}$와 $\{x: x<a\}$ 형태의 집합으로 분할하기 때문입니다.

이를 예제로 설명합니다.
예제 5.7.2 5.6절의 첫 번째 예제를 고려합니다. 허프만 코딩 절차에 의해 구성된 코드는

<!-- Page 148 -->
slice 코드입니다. 허프만 절차에서 얻은 코드어 길이, 즉 $\{2,2,2,3,3\}$을 사용하고, 심볼을 트리의 첫 번째 사용 가능한 노드에 할당하면 이 확률 변수에 대한 다음 코드를 얻게 됩니다.

$$
1 \rightarrow 00, \quad 2 \rightarrow 01, \quad 3 \rightarrow 10, \quad 4 \rightarrow 110, \quad 5 \rightarrow 111
$$

이 코드가 슬라이스 코드이며, 코드어가 알파벳순으로 정렬되어 있기 때문에 알파벳 코드라고 알려져 있음을 확인할 수 있습니다.
4. 허프만 코드와 섀넌 코드. $\left\lceil\log \frac{1}{p_{i}}\right\rceil$의 코드어 길이(이를 섀넌 코딩이라고 함)를 사용하는 것은 특정 심볼에 대한 최적 코드보다 훨씬 나쁠 수 있습니다. 예를 들어, 확률이 0.9999인 심볼 하나와 확률이 0.0001인 심볼 하나를 고려해 봅시다. 그러면 $\left\lceil\log \frac{1}{p_{i}}\right\rceil$의 코드어 길이를 사용하면 각각 1비트와 14비트의 코드어 길이가 됩니다. 최적 코드어 길이는 명백히 두 심볼 모두에 대해 1비트입니다. 따라서, 빈도가 낮은 심볼의 코드어는 섀넌 코드에서 최적 코드보다 훨씬 깁니다.

최적 코드의 코드어 길이는 항상 $\left\lceil\log \frac{1}{p_{i}}\right\rceil$보다 작습니까? 다음 예는 이것이 항상 참은 아님을 보여줍니다.

예제 5.7.3 분포 $\left(\frac{1}{3}, \frac{1}{3}, \frac{1}{4}, \frac{1}{12}\right)$를 갖는 확률 변수 $X$를 고려하십시오. 허프만 코딩 절차는 코드어 길이 $(2,2,2,2)$ 또는 $(1,2,3,3)$을 생성합니다 (병합된 확률을 어디에 두는지에 따라 달라지며, 독자가 확인할 수 있습니다 (문제 5.5.12)). 이 두 코드는 동일한 기대 코드어 길이를 달성합니다. 두 번째 코드에서 세 번째 심볼의 길이는 3이며, 이는 $\left\lceil\log \frac{1}{p_{3}}\right\rceil$보다 큽니다. 따라서 섀넌 코드의 코드어 길이는 최적 (허프만) 코드의 해당 심볼의 코드어 길이보다 짧을 수 있습니다. 이 예는 또한 최적 코드의 코드어 길이 집합이 고유하지 않다는 사실을 보여줍니다 (동일한 기대값을 갖는 여러 길이 집합이 있을 수 있습니다).

섀넌 코드 또는 허프만 코드 중 어느 것이 개별 심볼에 대해 더 짧을 수 있지만, 허프만 코드가 평균적으로 더 짧습니다. 또한 섀넌 코드와 허프만 코드는 기대 코드 길이에서 1비트 미만으로 차이가 납니다 (둘 다 $H$와 $H+1$ 사이에 있으므로).
<!-- Page 149 -->
5. 파노 코드. 파노는 슬라이스 코드의 아이디어와 유사한, 소스 코드를 구성하기 위한 차선의 절차를 제안했습니다. 그의 방법에서는 먼저 확률을 내림차순으로 정렬합니다. 그런 다음 $\left|\sum_{i=1}^{k} p_{i}-\sum_{i=k+1}^{m} p_{i}\right|$가 최소가 되는 $k$를 선택합니다. 이 지점은 소스 심볼을 거의 동일한 확률을 가진 두 개의 집합으로 나눕니다. 상위 집합의 첫 번째 비트에 0을 할당하고 하위 집합에 1을 할당합니다. 각 하위 집합에 대해 이 과정을 반복합니다. 이 재귀적 절차를 통해 각 소스 심볼에 대한 코드를 얻습니다. 이 방식은 일반적으로 최적이 아니지만, $L(C) \leq H(X)+2$를 달성합니다. (참고 [282].)

# 5.8 허프만 코드의 최적성

귀납법을 사용하여 이진 허프만 코드가 최적임을 증명합니다. 최적 코드는 여러 개 존재한다는 점을 기억하는 것이 중요합니다. 모든 비트를 반전시키거나 길이가 같은 두 개의 코드워드를 교환하면 다른 최적 코드를 얻을 수 있습니다. 허프만 절차는 이러한 최적 코드 중 하나를 구성합니다. 허프만 코드의 최적성을 증명하기 위해 먼저 특정 최적 코드의 몇 가지 속성을 증명합니다.

일반성을 잃지 않고, 확률 질량이 $p_{1} \geq p_{2} \geq \cdots \geq p_{m}$으로 정렬되었다고 가정합니다. 코드가 최적이라는 것은 $\sum p_{i} l_{i}$가 최소라는 것을 기억하십시오.

정리 5.8.1 모든 분포에 대해 다음 속성을 만족하는 최적의 즉석 코드(최소 기대 길이)가 존재합니다:

1. 길이는 확률에 반비례하여 정렬됩니다 (즉, $p_{j}>$ $p_{k}$이면 $\left.l_{j} \leq l_{k}\right)$.
2. 가장 긴 두 코드워드는 길이가 같습니다.
3. 가장 긴 두 코드워드 중 두 개는 마지막 비트만 다르고 가장 확률이 낮은 두 심볼에 해당합니다.

증명: 증명은 그림 5.3에 표시된 것처럼 교환, 트리밍 및 재배열을 포함합니다. 최적 코드 $C_{m}$을 고려합니다:

- 만약 $p_{j}>p_{k}$이면, $l_{j} \leq l_{k}$입니다. 여기서 코드워드를 교환합니다.

코드워드 $j$와 $k$가 $C_{m}$에서 교환된 $C_{m}^{\prime}$을 고려합니다. 그러면

$$
\begin{aligned}
L\left(C_{m}^{\prime}\right)-L\left(C_{m}\right) & =\sum p_{i} l_{i}^{\prime}-\sum p_{i} l_{i} \\
& =p_{j} l_{k}+p_{k} l_{j}-p_{j} l_{j}-p_{k} l_{k} \\
& =\left(p_{j}-p_{k}\right)\left(l_{k}-l_{j}\right)
\end{aligned}
$$
<!-- Page 150 -->

그림 5.3. 최적 코드의 속성. $p_{1} \geq p_{2} \geq \cdots \geq p_{m}$이라고 가정합니다. 가능한 즉석 코드는 (a)에 주어져 있습니다. 형제가 없는 가지를 다듬음으로써 코드를 (b)로 개선합니다. 이제 그림 (c)와 같이 트리를 재배열하여 단어 길이를 위에서 아래로 증가하는 길이로 정렬합니다. 마지막으로, 그림 (d)와 같이 확률 할당을 교환하여 트리의 예상 깊이를 개선합니다. 모든 최적 코드는 (d)와 같이 정규 형태로 재배열하고 교환할 수 있으며, 여기서 $l_{1} \leq l_{2} \leq \cdots \leq l_{m}$이고 $l_{m-1}=l_{m}$이며, 마지막 두 단어는 마지막 비트만 다릅니다.

그러나 $p_{j}-p_{k}>0$이고 $C_{m}$이 최적이므로 $L\left(C_{m}^{\prime}\right)-L\left(C_{m}\right) \geq 0$입니다. 따라서 $l_{k} \geq l_{j}$여야 합니다. 그러므로 $C_{m}$ 자체는 속성 1을 만족합니다.

- 가장 긴 두 단어는 길이가 같습니다. 여기서 단어를 다듬습니다. 가장 긴 두 단어의 길이가 같지 않으면, 더 긴 단어의 마지막 비트를 삭제하여 접두사 속성을 유지하고 더 낮은 예상 단어 길이를 달성할 수 있습니다. 따라서 가장 긴 두 단어는 길이가 같아야 합니다. 속성 1에 따라 가장 긴 단어는 가장 확률이 낮은 소스 기호에 속해야 합니다.
- 가장 긴 두 단어는 마지막 비트만 다르고 가장 확률이 낮은 두 기호에 해당합니다. 모든 최적 코드가 이 속성을 만족하는 것은 아니지만, 재배열을 통해 만족하는 최적 코드를 찾을 수 있습니다. 형제가 없는 최대 길이 단어가 있다면, 마지막 비트를 삭제해도 접두사 속성을 만족할 수 있습니다. 이는 평균 단어 길이를 줄이고 최적성에 위배됩니다.
<!-- Page 151 -->
코드의 일부입니다. 따라서 최적 코드의 모든 최대 길이 코드워드는 형제를 갖습니다. 이제 가장 긴 코드워드를 교환하여 확률이 가장 낮은 두 개의 소스 기호를 트리에서 두 형제와 연관시킬 수 있습니다. 이는 기대 길이에 영향을 주지 않습니다. $\sum p_{i} l_{i}$. 따라서 확률이 가장 낮은 두 소스 기호에 대한 코드워드는 최대 길이를 가지며 마지막 비트를 제외하고는 동일합니다.

요약하면, $p_{1} \geq p_{2} \geq \cdots \geq p_{m}$이면 $l_{1} \leq l_{2} \leq \cdots \leq l_{m-1}=l_{m}$이고 마지막 비트만 다른 $C\left(x_{m-1}\right)$와 $C\left(x_{m}\right)$ 코드워드를 갖는 최적 코드가 존재함을 보여주었습니다.

따라서 우리는 보조 정리의 속성을 만족하는 최적 코드가 존재함을 보여주었습니다. 이러한 코드를 표준 코드라고 부릅니다. 크기 $m$의 알파벳에 대한 모든 확률 질량 함수, $\mathbf{p}=$ $\left(p_{1}, p_{2}, \ldots, p_{m}\right)$에 대해 $p_{1} \geq p_{2} \geq \cdots \geq p_{m}$이면, 크기 $m-1$의 알파벳에 대한 Huffman 축소 $\mathbf{p}^{\prime}=\left(p_{1}, p_{2}, \ldots, p_{m-2}, p_{m-1}+p_{m}\right)$를 정의합니다 (그림 5.4). $C_{m-1}^{*}\left(\mathbf{p}^{\prime}\right)$를 $\mathbf{p}^{\prime}$에 대한 최적 코드로, $C_{m}^{*}(\mathbf{p})$를 $\mathbf{p}$에 대한 표준 최적 코드로 정의합니다.

최적성에 대한 증명은 두 가지 구성에서 비롯됩니다. 첫째, $\mathbf{p}^{\prime}$에 대한 최적 코드를 확장하여 $\mathbf{p}$에 대한 코드를 구성하고, 그런 다음

그림 5.4. Huffman 코딩을 위한 귀납적 단계. $p_{1} \geq p_{2} \geq \cdots \geq p_{5}$라고 가정합니다. (a)에는 표준 최적 코드가 나와 있습니다. 가장 낮은 두 확률을 결합하면 (b)의 코드를 얻습니다. 확률을 감소하는 순서로 재정렬하면 $m-1$개의 기호에 대한 (c)의 표준 코드를 얻습니다.
<!-- Page 152 -->
$\mathbf{p}$에 대한 최적의 표준 코드를 $\mathbf{p}^{\prime}$에 대한 Huffman 축약 코드를 구성하기 위해 압축합니다. 두 코드의 평균 코드어 길이를 비교하면 $\mathbf{p}^{\prime}$에 대한 최적 코드를 확장하여 $\mathbf{p}$에 대한 최적 코드를 얻을 수 있음을 알 수 있습니다.

$\mathbf{p}^{\prime}$에 대한 최적 코드로부터 $m$개의 요소에 대한 확장 코드를 다음과 같이 구성합니다: 가중치 $p_{m-1}+p_{m}$에 해당하는 $C_{m-1}^{*}(\mathbf{p}^{\prime})$의 코드어를 가져와서 심볼 $m-1$에 대한 코드어를 형성하기 위해 0을 추가하고 심볼 $m$에 대한 코드어를 형성하기 위해 1을 추가하여 확장합니다. 코드 구성은 다음과 같이 설명됩니다:

|  | $C_{m-1}^{*}\left(\mathbf{p}^{\prime}\right)$ |  | $C_{m}(\mathbf{p})$ |  |
| :--: | :--: | :--: | :--: | :--: |
| $p_{1}$ | $w_{1}^{\prime}$ | $l_{1}^{\prime}$ | $w_{1}=w_{1}^{\prime}$ | $l_{1}=l_{1}^{\prime}$ |
| $p_{2}$ | $w_{2}^{\prime}$ | $l_{2}^{\prime}$ | $w_{2}=w_{2}^{\prime}$ | $l_{2}=l_{2}^{\prime}$ |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| $p_{m-2}$ | $w_{m-2}^{\prime}$ | $l_{m-2}^{\prime}$ | $w_{m-2}=w_{m-2}^{\prime}$ | $l_{m-2}=l_{m-2}^{\prime}$ |
| $p_{m-1}+p_{m}$ | $w_{m-1}^{\prime}$ | $l_{m-1}^{\prime}$ | $w_{m-1}=w_{m-1}^{\prime} 0$ | $l_{m-1}=l_{m-1}^{\prime}+1$ |
|  |  |  | $w_{m}=w_{m-1}^{\prime} 1$ | $l_{m}=l_{m-1}^{\prime}+1$ |

평균 길이 $\sum_{i} p_{i}^{\prime} l_{i}^{\prime}$의 계산은 다음과 같이 보여줍니다:

$$
L(\mathbf{p})=L^{*}\left(\mathbf{p}^{\prime}\right)+p_{m-1}+p_{m}
$$

마찬가지로 $\mathbf{p}$에 대한 표준 코드로부터, 확률 $p_{m-1}$ 및 $p_{m}$을 갖는 두 개의 가장 낮은 확률 심볼 $m-1$ 및 $m$의 코드어를 병합하여 $\mathbf{p}^{\prime}$에 대한 코드를 구성합니다. 이들은 표준 코드의 속성에 의해 형제입니다. $\mathbf{p}^{\prime}$에 대한 새 코드는 평균 길이가 다음과 같습니다:

$$
\begin{aligned}
L\left(\mathbf{p}^{\prime}\right) & =\sum_{i=1}^{m-2} p_{i} l_{i}+p_{m-1}\left(l_{m-1}-1\right)+p_{m}\left(l_{m}-1\right) \\
& =\sum_{i=1}^{m} p_{i} l_{i}-p_{m-1}-p_{m} \\
& =L^{*}(\mathbf{p})-p_{m-1}-p_{m}
\end{aligned}
$$

(5.66)과 (5.69)를 더하면 다음과 같은 결과를 얻습니다:

$$
L\left(\mathbf{p}^{\prime}\right)+L(\mathbf{p})=L^{*}\left(\mathbf{p}^{\prime}\right)+L^{*}(\mathbf{p})
$$

또는

$$
\left(L\left(\mathbf{p}^{\prime}\right)-L^{*}\left(\mathbf{p}^{\prime}\right)\right)+\left(L(\mathbf{p})-L^{*}(\mathbf{p})\right)=0
$$
<!-- Page 153 -->
(5.71)의 두 항을 살펴보십시오. $L^{*}\left(\mathbf{p}^{\prime}\right)$이 $\mathbf{p}^{\prime}$에 대한 최적 길이이므로, $L\left(\mathbf{p}^{\prime}\right)-L^{*}\left(\mathbf{p}^{\prime}\right) \geq 0$이라고 가정합니다. 마찬가지로, $\mathbf{p}^{\prime}$에 대한 최적 코드의 확장 길이는 $\mathbf{p}$에 대한 최적 코드보다 크거나 같은 평균 길이를 가져야 합니다 [즉, $L(\mathbf{p})-L^{*}(\mathbf{p}) \geq 0$ ]. 그러나 두 개의 음수가 아닌 항의 합은 둘 다 0일 때만 0이 될 수 있으며, 이는 $L(\mathbf{p})=L^{*}(\mathbf{p})$임을 의미합니다 (즉, $\mathbf{p}^{\prime}$에 대한 최적 코드의 확장이 $\mathbf{p}$에 대해 최적입니다).

결과적으로, $m-1$개의 심볼을 가진 $\mathbf{p}^{\prime}$에 대한 최적 코드로 시작하여 $p_{m-1}+p_{m}$에 해당하는 코드워드를 확장하여 $m$개의 심볼에 대한 코드를 구성하면, 새로운 코드도 최적입니다. 두 개의 요소에 대한 코드로 시작하면, 이 경우 최적 코드는 명백하며, 귀납법을 통해 이 결과를 확장하여 다음 정리를 증명할 수 있습니다.

정리 5.8.1 허프만 코딩은 최적입니다. 즉, $C^{*}$가 허프만 코드이고 $C^{\prime}$이 다른 고유 복호 가능한 코드이면 $L\left(C^{*}\right) \leq L\left(C^{\prime}\right)$입니다.

이 정리를 이진 알파벳에 대해 증명했지만, 이 증명은 $D$-항 알파벳에 대한 허프만 코딩 알고리즘의 최적성을 확립하는 데에도 확장될 수 있습니다. 덧붙여, 허프만 코딩은 각 단계에서 가장 가능성이 낮은 두 개의 심볼을 병합하는 "탐욕스러운" 알고리즘임을 언급해야 합니다. 위의 증명은 이러한 지역적 최적이 최종 코드의 전역적 최적성을 보장함을 보여줍니다.

# 5.9 섀넌-파노-엘리아스 코딩

5.4절에서 코드워드 길이 $l(x)=\left\lceil\log \frac{1}{p(x)}\right\rceil$가 크래프트 부등식을 만족하며 따라서 고유 복호 가능한 코드 구성에 사용될 수 있음을 보였습니다. 이 절에서는 누적 분포 함수를 사용하여 코드워드를 할당하는 간단한 구성 절차를 설명합니다.

일반성을 잃지 않고 $\mathcal{X}=\{1,2, \ldots, m\}$이라고 가정할 수 있습니다. 모든 $x$에 대해 $p(x)>0$이라고 가정합니다. 누적 분포 함수 $F(x)$는 다음과 같이 정의됩니다.

$$
F(x)=\sum_{a \leq x} p(a)
$$

이 함수는 그림 5.5에 나와 있습니다. 수정된 누적 분포 함수를 고려하십시오.

$$
\bar{F}(x)=\sum_{a<x} p(a)+\frac{1}{2} p(x)
$$
<!-- Page 154 -->

그림 5.5. 누적 분포 함수와 Shannon-Fano-Elias 코딩.
여기서 $\bar{F}(x)$는 $x$보다 작은 모든 심볼의 확률 합에 심볼 $x$의 확률의 절반을 더한 값을 나타냅니다. 확률 변수가 이산적이므로 누적 분포 함수는 $p(x)$ 크기의 계단으로 구성됩니다. 함수 $\bar{F}(x)$의 값은 $x$에 해당하는 계단의 중간점입니다.

모든 확률이 양수이므로, $a \neq b$이면 $\bar{F}(a) \neq \bar{F}(b)$이며, 따라서 $\bar{F}(x)$를 알면 $x$를 결정할 수 있습니다. 누적 분포 함수의 그래프를 보고 해당 $x$를 찾기만 하면 됩니다. 따라서 $\bar{F}(x)$의 값은 $x$의 코드로 사용될 수 있습니다.

하지만 일반적으로 $\bar{F}(x)$는 무한한 수의 비트로만 표현될 수 있는 실수입니다. 따라서 $\bar{F}(x)$의 정확한 값을 $x$의 코드로 사용하는 것은 효율적이지 않습니다. 근사값을 사용한다면 필요한 정확도는 얼마입니까?

$\bar{F}(x)$를 $l(x)$ 비트로 절단한다고 가정합니다 ( $\left\lfloor\bar{F}(x)\right\rfloor_{l(x)}$ 로 표시). 따라서 $\bar{F}(x)$의 첫 $l(x)$ 비트를 $x$의 코드로 사용합니다. 반올림의 정의에 따라 다음이 성립합니다.

$$
\bar{F}(x)-\left\lfloor\bar{F}(x)\right\rfloor_{l(x)}<\frac{1}{2^{l(x)}}
$$

만약 $l(x)=\left\lceil\log \frac{1}{p(x)}\right\rceil+1$ 이면,

$$
\frac{1}{2^{l(x)}}<\frac{p(x)}{2}=\bar{F}(x)-F(x-1)
$$

따라서 $\left\lfloor\bar{F}(x)\right\rfloor_{l(x)}$ 는 $x$에 해당하는 계단 내에 있습니다. 따라서 $l(x)$ 비트는 $x$를 설명하기에 충분합니다.
<!-- Page 155 -->
부호어는 해당 기호를 식별해야 할 뿐만 아니라 부호어 집합이 접두사 없는(prefix-free) 집합이어야 합니다. 코드가 접두사 없는지 확인하기 위해 각 부호어 $z_{1} z_{2} \cdots z_{l}$를 점이 아니라 구간 $\left[0 . z_{1} z_{2} \cdots z_{l}, 0 . z_{1} z_{2} \cdots z_{l}+\frac{1}{2^{l}}\right)$으로 간주합니다. 코드가 접두사 없는 것은 부호어에 해당하는 구간들이 서로 겹치지 않을 때에만 해당합니다.

이제 위에서 제시된 코드가 접두사 없는지 확인하겠습니다. 임의의 부호어에 해당하는 구간의 길이는 $2^{-l(x)}$이며, 이는 (5.75)에 의해 $x$에 해당하는 계단의 높이의 절반보다 작습니다. 구간의 하한은 계단의 하반부에 있습니다. 따라서 구간의 상한은 계단의 상단 아래에 위치하며, 임의의 부호어에 해당하는 구간은 누적 분포 함수의 해당 기호에 해당하는 계단 내부에 완전히 포함됩니다. 그러므로 서로 다른 부호어에 해당하는 구간들은 겹치지 않으며, 코드는 접두사 없는 코드가 됩니다. 이 절차는 기호들이 확률에 따라 순서화될 필요가 없다는 점에 유의하십시오. 순서화된 확률을 사용하는 또 다른 절차는 문제 5.5.28에 설명되어 있습니다.

$x$를 표현하기 위해 $l(x)=\left\lceil\log \frac{1}{p(x)}\right\rceil+1$ 비트를 사용하므로, 이 코드의 기대 길이는 다음과 같습니다.

$$
L=\sum_{x} p(x) l(x)=\sum_{x} p(x)\left(\left\lceil\log \frac{1}{p(x)}\right\rceil+1\right)<H(X)+2
$$

따라서 이 코딩 방식은 평균 부호어 길이가 엔트로피보다 2 비트 이내로 달성됩니다.

예제 5.9.1 모든 확률이 이진수(dyadic)인 경우를 먼저 고려해 보겠습니다. 다음 표에서 코드를 구성합니다.

| $x$ | $p(x)$ | $F(x)$ | $\bar{F}(x)$ | $\bar{F}(x)$ 이진수 | $l(x)=\left\lceil\log \frac{1}{p(x)}\right\rceil+1$ | 부호어 |
| :-- | :-- | :-- | :-- | :--: | :--: | :--: |
| 1 | 0.25 | 0.25 | 0.125 | 0.001 | 3 | 001 |
| 2 | 0.5 | 0.75 | 0.5 | 0.10 | 2 | 10 |
| 3 | 0.125 | 0.875 | 0.8125 | 0.1101 | 4 | 1101 |
| 4 | 0.125 | 1.0 | 0.9375 | 0.1111 | 4 | 1111 |

이 경우, 평균 부호어 길이는 2.75 비트이고 엔트로피는 1.75 비트입니다. 이 경우의 Huffman 코드는 엔트로피 한계에 도달합니다. 부호어를 보면, 예를 들어 마지막 두 부호어의 마지막 비트를 생략할 수 있다는 명백한 비효율성이 있습니다. 하지만 모든 부호어에서 마지막 비트를 제거하면 코드는 더 이상 접두사 없는 코드가 되지 않습니다.
<!-- Page 156 -->
예제 5.9.2 Shannon-Fano-Elias 코드를 구성하는 또 다른 예제를 제시합니다. 이 경우, 분포가 이진이 아니므로 $F(x)$의 이진 표현은 무한한 수의 비트를 가질 수 있습니다. $0.01010101 \ldots$을 $0 . \overline{01}$로 표기합니다. 다음 표에서 코드를 구성합니다:

| $x$ | $p(x)$ | $F(x)$ | $\bar{F}(x)$ | $\bar{F}(x)$ 이진수 | $l(x)=\left\lceil\log \frac{1}{p(x)}\right\rceil+1$ | 코드워드 |
| :-- | :-- | :-- | :-- | :--: | :--: | :--: |
| 1 | 0.25 | 0.25 | 0.125 | 0.001 | 3 | 001 |
| 2 | 0.25 | 0.5 | 0.375 | 0.011 | 3 | 011 |
| 3 | 0.2 | 0.7 | 0.6 | $0.1 \overline{0011}$ | 4 | 1001 |
| 4 | 0.15 | 0.85 | 0.775 | $0.110 \overline{0011}$ | 4 | 1100 |
| 5 | 0.15 | 1.0 | 0.925 | $0.111 \overline{0110}$ | 4 | 1110 |

위 코드는 이 소스(예제 5.6.1)에 대한 Huffman 코드보다 평균적으로 1.2 비트 더 깁니다.

Shannon-Fano-Elias 코딩 절차는 확률 변수 시퀀스에도 적용될 수 있습니다. 핵심 아이디어는 시퀀스의 누적 분포 함수를 적절한 정확도로 표현하여 시퀀스의 코드로 사용하는 것입니다. 길이 $n$인 블록에 이 방법을 직접 적용하려면 길이 $n$인 모든 시퀀스에 대한 확률 및 누적 분포 함수를 계산해야 하는데, 이는 블록 길이가 증가함에 따라 지수적으로 증가하는 계산입니다. 그러나 간단한 트릭을 통해 블록의 각 기호를 볼 때마다 확률과 누적 분포 함수를 순차적으로 계산할 수 있으므로 계산이 블록 길이에 선형적으로만 증가하도록 보장합니다. Shannon-Fano-Elias 코딩을 직접 적용하려면 블록 크기에 따라 정밀도가 증가하는 산술 연산도 필요하며, 이는 긴 블록을 다룰 때 실용적이지 않습니다. 13장에서는 확률 변수 시퀀스에 대한 Shannon-Fano-Elias 방법의 확장으로, 시퀀스 길이에 선형적인 복잡도를 가진 고정 정밀도 산술 연산을 사용하여 인코딩하는 산술 코딩을 설명합니다. 이 방법은 JPEG 및 FAX 압축 표준과 같이 많은 실용적인 압축 방식의 기초가 됩니다.

# 5.10 Shannon 코드의 경쟁적 최적성

Huffman 코딩이 최소 기대 길이를 갖는다는 점에서 최적임을 보여주었습니다. 하지만 특정 시퀀스에서의 성능은 어떻습니까? 예를 들어, 모든 시퀀스에 대해 항상 다른 모든 코드보다 더 낫습니까? 명백히 그렇지 않습니다. 왜냐하면 짧은

<!-- Page 157 -->
codewords를 드문드문한 소스 심볼에 할당합니다. 이러한 코드는 해당 소스 심볼에 대해 Huffman 코드보다 더 나을 것입니다.

경쟁적 최적성의 질문을 형식화하기 위해 다음 두 사람 제로섬 게임을 고려합니다. 두 사람은 확률 분포를 받고 해당 분포에 대한 즉석 코드를 설계하도록 요청받습니다. 그런 다음 이 분포에서 소스 심볼이 추출되고, 플레이어 A의 코드가 플레이어 B의 코드보다 짧거나 길이에 따라 플레이어 A에 대한 보상이 1 또는 -1이 됩니다. 동점인 경우 보상은 0입니다.

Huffman 코드 길이를 다루는 것은 어렵습니다. 왜냐하면 코드 길이에는 명시적인 표현이 없기 때문입니다. 대신, 코드 길이 $l(x)=\left\lceil\log \frac{1}{p(x)}\right\rceil$를 갖는 Shannon 코드를 고려합니다. 이 경우 다음 정리가 있습니다.

정리 5.10.1 Shannon 코드와 관련된 코드 길이 $l(x)$와 다른 고유 복호 가능한 코드와 관련된 코드 길이 $l^{\prime}(x)$가 있다고 가정합니다. 그러면

$$
\operatorname{Pr}\left(l(X) \geq l^{\prime}(X)+c\right) \leq \frac{1}{2^{c-1}}
$$

예를 들어, $l^{\prime}(X)$가 $l(X)$보다 5비트 이상 짧을 확률은 $\frac{1}{16}$보다 작습니다.

# 증명

$$
\begin{aligned}
\operatorname{Pr}\left(l(X) \geq l^{\prime}(X)+c\right) & =\operatorname{Pr}\left(\left\lceil\log \frac{1}{p(X)}\right\rceil \geq l^{\prime}(X)+c\right) \\
& \leq \operatorname{Pr}\left(\log \frac{1}{p(X)} \geq l^{\prime}(X)+c-1\right) \\
& =\operatorname{Pr}\left(p(X) \leq 2^{-l^{\prime}(X)-c+1}\right) \\
& =\sum_{x: p(x) \leq 2^{-l^{\prime}(x)-c+1}} p(x) \\
& \leq \sum_{x: p(x) \leq 2^{-l^{\prime}(x)-c+1}} 2^{-l^{\prime}(x)-(c-1)} \\
& \leq \sum_{x} 2^{-l^{\prime}(x)} 2^{-(c-1)} \\
& \leq 2^{-(c-1)}
\end{aligned}
$$

Kraft 부등식에 의해 $\sum 2^{-l^{\prime}(x)} \leq 1$이므로.
<!-- Page 158 -->
따라서 대부분의 경우 다른 어떤 코드도 Shannon 코드보다 훨씬 더 나은 성능을 낼 수 없습니다. 이제 이 결과를 강화하겠습니다. 게임 이론적 설정에서는 $l(x)<l^{\prime}(x)$인 경우가 $l(x)>l^{\prime}(x)$인 경우보다 더 자주 발생하도록 보장하고 싶을 것입니다. $l(x) \leq l^{\prime}(x)+1$이 확률 $\geq \frac{1}{2}$로 성립한다는 사실이 이를 보장하지는 않습니다. 이제 이 더 엄격한 기준 하에서도 Shannon 코딩이 최적임을 보이겠습니다. 확률 질량 함수 $p(x)$가 모든 $x$에 대해 $\log \frac{1}{p(x)}$가 정수일 때 dyadic이라고 가정합니다.

정리 5.10.2 dyadic 확률 질량 함수 $p(x)$에 대해, 소스에 대한 이진 Shannon 코드의 단어 길이 $l(x)=\log \frac{1}{p(x)}$와 소스에 대한 다른 고유 복호 가능한 이진 코드의 길이 $l^{\prime}(x)$가 주어졌을 때, 다음이 성립합니다.

$$
\operatorname{Pr}\left(l(X)<l^{\prime}(X)\right) \geq \operatorname{Pr}\left(l(X)>l^{\prime}(X)\right)
$$

등호는 $l^{\prime}(x)=l(x)$인 경우에만 성립합니다. 따라서 코드 길이 할당 $l(x)=\log \frac{1}{p(x)}$는 고유하게 경쟁적으로 최적입니다.
증명: 함수 $\operatorname{sgn}(t)$를 다음과 같이 정의합니다.

$$
\operatorname{sgn}(t)= \begin{cases}1 & \text { if } t>0 \\ 0 & \text { if } t=0 \\ -1 & \text { if } t<0\end{cases}
$$

그러면 그림 5.6에서 다음과 같이 쉽게 알 수 있습니다.

$$
\operatorname{sgn}(t) \leq 2^{t}-1 \quad \text { for } t=0, \pm 1, \pm 2, \ldots
$$

그림 5.6. Sgn 함수와 경계.
<!-- Page 159 -->
참고로, 이 부등식은 모든 $t$에 대해 만족되지는 않지만, 모든 정수 $t$ 값에 대해서는 만족됩니다. 이제 다음과 같이 쓸 수 있습니다.

$$
\begin{aligned}
\operatorname{Pr}\left(l^{\prime}(X)<l(X)\right)-\operatorname{Pr}\left(l^{\prime}(X)>l(X)\right) & =\sum_{x: l^{\prime}(x)<l(x)} p(x)-\sum_{x: l^{\prime}(x)>l(x)} p(x) \\
& =\sum_{x} p(x) \operatorname{sgn}\left(l(x)-l^{\prime}(x)\right) \\
& =E \operatorname{sgn}\left(l(X)-l^{\prime}(X)\right) \\
& \stackrel{(\mathrm{a})}{\leq} \sum_{x} p(x)\left(2^{l(x)-l^{\prime}(x)}-1\right) \\
& =\sum_{x} 2^{-l(x)}\left(2^{l(x)-l^{\prime}(x)}-1\right) \\
& =\sum_{x} 2^{-l^{\prime}(x)}-\sum_{x} 2^{-l(x)} \\
& =\sum_{x} 2^{-l^{\prime}(x)}-1 \\
& \stackrel{(\mathrm{b})}{\leq} 1-1 \\
& =0
\end{aligned}
$$

여기서 (a)는 $\operatorname{sgn}(x)$에 대한 경계로부터 나오고, (b)는 $l^{\prime}(x)$가 Kraft 부등식을 만족한다는 사실로부터 나옵니다.

위의 연쇄에서 등호가 성립하는 경우는 (a)와 (b)에서 등호가 성립하는 경우뿐입니다. $\operatorname{sgn}(t)$에 대한 경계에서 등호가 성립하는 경우는 $t$가 0 또는 1일 때입니다 [즉, $l(x)=l^{\prime}(x)$ 또는 $l(x)=l^{\prime}(x)+1$]. (b)에서의 등호는 $l^{\prime}(x)$가 Kraft 부등식을 등호와 함께 만족한다는 것을 의미합니다. 이 두 사실을 결합하면 모든 $x$에 대해 $l^{\prime}(x)=l(x)$임을 알 수 있습니다.

따름정리 비이진 확률 질량 함수에 대해,

$$
E \operatorname{sgn}\left(l(X)-l^{\prime}(X)-1\right) \leq 0
$$

여기서 $l(x)=\left\lceil\log \frac{1}{p(x)}\right\rceil$이고 $l^{\prime}(x)$는 해당 소스에 대한 임의의 다른 코드입니다.
<!-- Page 160 -->
증명: 앞선 증명과 같은 맥락입니다.
따라서 우리는 Shannon 코딩 $l(x)=\left\lceil\log \frac{1}{p(x)}\right\rceil$이 다양한 기준 하에서 최적임을 보여주었습니다. 이는 보수 함수에 대해 강건합니다. 특히, 이항 분포의 경우 $E\left(l-l^{\prime}\right) \leq 0, E \operatorname{sgn}\left(l-l^{\prime}\right) \leq 0$이며, 부등식 (5.87)을 사용하면 $f(t) \leq 2^{t}-1, t=0, \pm 1, \pm 2, \ldots$를 만족하는 임의의 함수 $f$에 대해 $E f\left(l-l^{\prime}\right) \leq 0$입니다.

# 5.11 공정한 동전을 이용한 이산 분포 생성

이 장의 앞부분에서는 확률 변수를 비트 시퀀스로 표현하여 표현의 기대 길이를 최소화하는 문제를 고려했습니다. 인코딩된 시퀀스는 본질적으로 압축 불가능하며 따라서 기호당 1 비트에 가까운 엔트로피율을 갖는다고 주장할 수 있습니다 (문제 5.5.29). 따라서 인코딩된 시퀀스의 비트는 본질적으로 공정한 동전 던지기입니다.

이 섹션에서는 소스 코딩 논의에서 약간 벗어나 이중 질문을 고려합니다. 지정된 확률 질량 함수 $\mathbf{p}$에 따라 추출된 확률 변수 $X$를 생성하는 데 공정한 동전 던지기가 몇 번이나 필요합니까? 먼저 간단한 예시를 고려합니다.

예시 5.11.1 공정한 동전 던지기 시퀀스 (공정한 비트)가 주어졌다고 가정하고, 확률 분포를 갖는 확률 변수 $X$를 생성하고자 합니다.

$$
X=\left\{\begin{array}{ll}
a & \text { 확률 } \frac{1}{2} \\
b & \text { 확률 } \frac{1}{4} \\
c & \text { 확률 } \frac{1}{4}
\end{array}\right.
$$

답을 추측하기 쉽습니다. 첫 번째 비트가 0이면 $X=a$로 둡니다. 첫 두 비트가 10이면 $X=b$로 둡니다. 11을 보면 $X=c$로 둡니다. $X$가 원하는 분포를 갖는다는 것은 명확합니다.

이 경우 확률 변수 생성에 필요한 공정한 비트의 평균 개수를 $\frac{1}{2}(1)+\frac{1}{4}(2)+\frac{1}{4}(2)=1.5$ 비트로 계산합니다. 이것은 또한 분포의 엔트로피입니다. 이것이 특이한 일입니까? 아닙니다. 이 섹션의 결과가 이를 나타냅니다.

이제 일반적인 문제를 다음과 같이 공식화할 수 있습니다. 공정한 동전 던지기 시퀀스 $Z_{1}, Z_{2}, \ldots$가 주어졌고, 확률 질량 함수를 갖는 이산 확률 변수 $X \in \mathcal{X}=\{1,2, \ldots, m\}$를 생성하고자 합니다.
<!-- Page 161 -->

그림 5.7. 분포 $\left(\frac{1}{2}, \frac{1}{4}, \frac{1}{4}\right)$ 생성을 위한 트리.
$\mathbf{p}=\left(p_{1}, p_{2}, \ldots, p_{m}\right)$. 확률 변수 $T$는 알고리즘에서 사용되는 동전 던지기 횟수를 나타냅니다.

공정한 동전 던지기로 생성된 비트 시퀀스에 의해 주어지는 경로를 따라 나뭇가지 끝에 출력 기호 $X$가 표시됩니다. 예를 들어, 분포 $\left(\frac{1}{2}, \frac{1}{4}, \frac{1}{4}\right)$에 대한 트리는 그림 5.7에 나와 있습니다.

알고리즘을 나타내는 트리는 특정 속성을 만족해야 합니다.

1. 트리는 완전해야 합니다 (즉, 모든 노드는 잎이거나 트리 내에 두 개의 자손을 가집니다). 일부 예시에서 볼 수 있듯이 트리는 무한할 수 있습니다.
2. 깊이 $k$에 있는 잎의 확률은 $2^{-k}$입니다. 많은 잎이 동일한 출력 기호로 표시될 수 있습니다 - 이 모든 잎의 총 확률은 출력 기호의 원하는 확률과 같아야 합니다.
3. $X$를 생성하는 데 필요한 공정한 비트의 기대값 $E T$는 이 트리의 기대 깊이와 같습니다.

동일한 출력 분포를 생성하는 많은 가능한 알고리즘이 있습니다. 예를 들어, 매핑 $00 \rightarrow a, 01 \rightarrow b, 10 \rightarrow c, 11 \rightarrow a$도 분포 $\left(\frac{1}{2}, \frac{1}{4}, \frac{1}{4}\right)$를 생성합니다. 그러나 이 알고리즘은 각 샘플을 생성하는 데 두 개의 공정한 비트를 사용하므로 이전에 제공된 매핑보다 효율적이지 않습니다. 이는 질문을 제기합니다: 주어진 분포를 생성하는 가장 효율적인 알고리즘은 무엇이며, 이는 분포의 entropy와 어떻게 관련됩니까?

우리는 출력 샘플에서 생성하는 것만큼의 무작위성이 공정한 비트에도 필요할 것으로 예상합니다. entropy는 무작위성의 척도이며 각 공정한 비트는 1 비트의 entropy를 가지므로, 사용되는 공정한 비트의 수는 출력의 entropy보다 작지 않을 것으로 예상합니다. 이는 다음 정리에서 증명됩니다. 정리의 증명에서 트리에 대한 간단한 보조 정리가 필요합니다. 완전한 트리의 잎 집합을 $\mathcal{Y}$라고 합시다. 잎에 대한 분포를 고려하되, 확률이

<!-- Page 162 -->
나무의 깊이 $k$에 있는 잎의 확률은 $2^{-k}$입니다. 이 분포를 갖는 확률 변수 $Y$를 정의합니다. 그러면 다음과 같은 보조정리를 얻습니다.

보조정리 5.11.1 임의의 완전 이진 트리에 대해, 깊이 $k$에 있는 잎의 확률이 $2^{-k}$가 되도록 하는 잎에 대한 확률 분포를 고려합니다. 그러면 트리의 기대 깊이는 이 분포의 엔트로피와 같습니다.

증명: 트리의 기대 깊이는 다음과 같습니다.

$$
E T=\sum_{y \in \mathcal{Y}} k(y) 2^{-k(y)}
$$

그리고 $Y$ 분포의 엔트로피는 다음과 같습니다.

$$
\begin{aligned}
H(Y)= & -\sum_{y \in \mathcal{Y}} \frac{1}{2^{k(y)}} \log \frac{1}{2^{k(y)}} \\
& =\sum_{y \in \mathcal{Y}} k(y) 2^{-k(y)}
\end{aligned}
$$

여기서 $k(y)$는 잎 $y$의 깊이를 나타냅니다. 따라서 다음과 같습니다.

$$
H(Y)=E T
$$

정리 5.11.1 $X$를 생성하는 임의의 알고리즘에 대해, 사용된 공정한 비트의 기대값은 엔트로피 $H(X)$보다 크거나 같습니다. 즉,

$$
E T \geq H(X)
$$

증명: $X$를 공정한 비트로부터 생성하는 임의의 알고리즘은 완전 이진 트리로 표현될 수 있습니다. 이 트리의 모든 잎에 고유한 기호 $y \in \mathcal{Y}=\{1,2, \ldots\}$를 레이블링합니다. 트리가 무한하면 알파벳 $\mathcal{Y}$도 무한합니다.

이제 트리 잎에서 정의된 확률 변수 $Y$를 고려합니다. 여기서 깊이 $k$에 있는 임의의 잎 $y$에 대해 $Y=y$일 확률은 $2^{-k}$입니다. 보조정리 5.11.1에 의해, 이 트리의 기대 깊이는 $Y$의 엔트로피와 같습니다.

$$
E T=H(Y)
$$

이제 확률 변수 $X$는 $Y$의 함수이며 (하나 이상의 잎이 출력 기호에 매핑됨), 따라서 문제 2.4의 결과에 따라 다음과 같은 관계가 성립합니다.

$$
H(X) \leq H(Y)
$$
<!-- Page 163 -->
따라서, 임의의 확률 변수 $X$를 생성하는 알고리즘에 대해 다음이 성립합니다.

$$
H(X) \leq E T
$$

동일한 논증으로 이분 분포에 대한 최적성 문제를 해결할 수 있습니다.

정리 5.11.2. 확률 변수 $X$가 이분 분포를 따른다고 가정합니다. 공정한 동전 던지기로 $X$를 생성하는 최적 알고리즘은 정확히 엔트로피와 같은 기댓값의 동전 던지기 횟수를 요구합니다:

$$
E T=H(X)
$$

증명: 정리 5.11.1은 $X$를 생성하기 위해 최소 $H(X)$ 비트가 필요함을 보여줍니다. 구성적인 부분에 대해서는, 확률 변수를 생성하기 위한 트리로 $X$에 대한 허프만 코드 트리를 사용합니다. 이분 분포의 경우, 허프만 코드는 섀넌 코드와 동일하며 엔트로피 경계를 달성합니다. 임의의 $x \in \mathcal{X}$에 대해, $x$에 해당하는 리프의 깊이는 해당 코드워드의 길이이며, 이는 $\log \frac{1}{p(x)}$입니다. 따라서 이 코드 트리를 사용하여 $X$를 생성할 때, 리프는 $2^{-\log \frac{1}{p(x)}}=p(x)$의 확률을 갖게 됩니다. 동전 던지기의 기댓값은 트리의 기댓값 깊이이며, 이는 엔트로피와 같습니다 (분포가 이분이기 때문입니다). 따라서 이분 분포의 경우, 최적 생성 알고리즘은 다음을 달성합니다.

$$
E T=H(X)
$$

분포가 이분이 아니라면 어떻게 될까요? 이 경우 동일한 아이디어를 사용할 수 없습니다. 허프만 코드의 코드 트리가 시작한 분포가 아닌 리프에 대한 이분 분포를 생성하기 때문입니다. 트리의 모든 리프는 $2^{-k}$ 형태의 확률을 가지므로, 이 형태가 아닌 임의의 확률 $p_{i}$는 이 형태의 원자들로 분할해야 합니다. 그런 다음 이러한 원자들을 트리의 리프에 할당할 수 있습니다. 예를 들어, 결과 중 하나 $x$가 $p(x)=\frac{1}{4}$의 확률을 갖는다면, 하나의 원자(레벨 2의 트리 리프)만 필요하지만, $p(x)=\frac{7}{8}=\frac{1}{2}+\frac{1}{4}+\frac{1}{8}$이라면, 레벨 1, 2, 3의 각 리프에 하나씩 세 개의 원자가 필요합니다.

트리의 기댓값 깊이를 최소화하기 위해 가능한 한 큰 확률을 가진 원자를 사용해야 합니다. 따라서 주어진 확률 $p_{i}$에 대해, $p_{i}$보다 작고 $2^{-k}$ 형태인 가장 큰 원자를 찾아 트리에 할당합니다. 그런 다음 나머지를 계산하고 나머지 부분에 들어갈 가장 큰 원자를 찾습니다. 이 과정을 계속하면 모든

<!-- Page 164 -->
확률을 이진 원자(dyadic atoms)로 변환합니다. 이 과정은 확률의 이진 전개를 찾는 것과 동일합니다. 확률 $p_{i}$의 이진 전개를 다음과 같이 나타냅니다.

$$
p_{i}=\sum_{j \geq 1} p_{i}^{(j)}
$$

여기서 $p_{i}^{(j)}=2^{-j}$ 또는 0 입니다. 그러면 전개의 원자는 $\left\{p_{i}^{(j)}\right.$ : $i=1,2, \ldots, m, j \geq 1\}$ 입니다.

$\sum_{i} p_{i}=1$ 이므로, 이 원자들의 확률의 합은 1입니다. 우리는 깊이 $j$에 있는 트리의 잎(leaf)에 $2^{-j}$의 확률을 할당할 것입니다. 원자의 깊이는 Kraft 부등식을 만족하며, 따라서 정리 5.2.1에 의해, 모든 원자를 올바른 깊이에 갖는 이러한 트리를 항상 구성할 수 있습니다. 이 절차를 예제를 통해 설명하겠습니다.

예제 5.11.2 $X$가 다음과 같은 분포를 갖는다고 가정합니다.

$$
X=\left\{\begin{array}{ll}
a & \text { 확률 } \frac{2}{3} \text{ 으로} \\
b & \text { 확률 } \frac{1}{3} \text{ 으로}
\end{array}\right.
$$

이 확률들의 이진 전개를 찾습니다.

$$
\begin{aligned}
& \frac{2}{3}=0.10101010 \ldots 2 \\
& \frac{1}{3}=0.01010101 \ldots 2
\end{aligned}
$$

따라서, 전개의 원자는 다음과 같습니다.

$$
\begin{aligned}
& \frac{2}{3} \rightarrow\left(\frac{1}{2}, \frac{1}{8}, \frac{1}{32}, \ldots\right) \\
& \frac{1}{3} \rightarrow\left(\frac{1}{4}, \frac{1}{16}, \frac{1}{64}, \ldots\right)
\end{aligned}
$$

이것들은 그림 5.8에 표시된 대로 트리에 할당될 수 있습니다.
이 절차는 확률 변수 $X$를 생성하는 트리를 제공합니다. 우리는 이 절차가 최적(최소 기대 깊이를 갖는 트리)임을 주장했지만, 공식적인 증명은 제공하지 않을 것입니다. 대신, 이 절차에 의해 생성된 트리의 기대 깊이를 제한합니다.
<!-- Page 165 -->

그림 5.8. $\left(\frac{2}{3}, \frac{1}{3}\right)$ 분포를 생성하는 트리.

정리 5.11.3. 확률 변수 $X$를 생성하기 위해 최적의 알고리즘이 요구하는 공정 비트의 기대값은 $H(X)$와 $H(X)+2$ 사이에 있습니다:

$$
H(X) \leq E T<H(X)+2
$$

증명: 기대 동전 던지기 횟수에 대한 하한은 정리 5.11.1에서 증명됩니다. 상한에 대해서는 위에서 설명한 절차에 필요한 기대 동전 던지기 횟수에 대한 명시적인 표현을 작성합니다. 모든 확률 $\left(p_{1}, p_{2}, \ldots, p_{m}\right)$을 이진 원자(dyadic atom)로 분할합니다. 예를 들어,

$$
p_{1} \rightarrow\left(p_{1}^{(1)}, p_{1}^{(2)}, \ldots\right)
$$

등등. 이러한 원자(이진 분포를 형성하는)를 사용하여 각 원자에 해당하는 잎을 가진 트리를 구성합니다. 각 원자를 생성하는 데 필요한 동전 던지기 횟수는 트리에서의 깊이이며, 따라서 기대 동전 던지기 횟수는 트리의 기대 깊이와 같으며, 이는 원자의 이진 분포의 엔트로피와 같습니다. 그러므로,

$$
E T=H(Y)
$$

여기서 $Y$는 다음과 같은 분포를 가집니다: $\left(p_{1}^{(1)}, p_{1}^{(2)}, \ldots, p_{2}^{(1)}, p_{2}^{(2)}, \ldots, p_{m}^{(1)}, p_{m}^{(2)}, \ldots\right)$. 이제 $X$가 $Y$의 함수이므로, 우리는 다음을 가집니다:

$$
H(Y)=H(Y, X)=H(X)+H(Y \mid X)
$$
<!-- Page 166 -->
그리고 우리의 목표는 $H(Y \mid X)<2$임을 보이는 것입니다. 이제 이 결과에 대한 대수적 증명을 제시하겠습니다. $Y$의 엔트로피를 확장하면 다음과 같습니다.

$$
\begin{aligned}
H(Y)= & -\sum_{i=1}^{m} \sum_{j \geq 1} p_{i}^{(j)} \log p_{i}^{(j)} \\
& =\sum_{i=1}^{m} \sum_{j: p_{i}^{(j)}>0} j 2^{-j}
\end{aligned}
$$

각 원자(atom)는 0이거나 어떤 $k$에 대해 $2^{-k}$이기 때문입니다. 이제 각 $i$에 해당하는 확장 항을 고려해 보겠습니다. 이를 $T_{i}$라고 부르겠습니다.

$$
T_{i}=\sum_{j: p_{i}^{(j)}>0} j 2^{-j}
$$

$2^{-(n-1)}>p_{i} \geq 2^{-n}$인 $n$을 찾을 수 있습니다. 즉,

$$
n-1<-\log p_{i} \leq n
$$

그러면 $p_{i}^{(j)}>0$인 경우는 $j \geq n$일 때만 해당하므로, (5.121)을 다음과 같이 다시 쓸 수 있습니다.

$$
T_{i}=\sum_{j: j \geq n, p_{i}^{(j)}>0} j 2^{-j}
$$

원자(atom)의 정의를 사용하여 $p_{i}$를 다음과 같이 쓸 수 있습니다.

$$
p_{i}=\sum_{j: j \geq n, p_{i}^{(j)}>0} 2^{-j}
$$

상한을 증명하기 위해 먼저 $T_{i}<-p_{i} \log p_{i}+2 p_{i}$임을 보이겠습니다. 차이를 고려해 보겠습니다.

$$
\begin{aligned}
T_{i}+p_{i} \log p_{i}-2 p_{i} & \stackrel{(a)}{<} T_{i}-p_{i}(n-1)-2 p_{i} \\
& =T_{i}-(n-1+2) p_{i} \\
& =\sum_{j: j \geq n, p_{i}^{(j)}>0} j 2^{-j}-(n+1) \sum_{j: j \geq n, p_{i}^{(j)}>0} 2^{-j} \\
& =\sum_{j: j \geq n, p_{i}^{(j)}>0}(j-n-1) 2^{-j}
\end{aligned}
$$
<!-- Page 167 -->
$$
\begin{aligned}
& =-2^{-n}+0+\sum_{j: j \geq n+2, p_{i}^{(j)}>0}(j-n-1) 2^{-j} \\
& \stackrel{(b)}{=}-2^{-n}+\sum_{k: k \geq 1, p_{i}^{(k+n+1)}>0} k 2^{-(k+n+1)} \\
& \stackrel{(c)}{\leq}-2^{-n}+\sum_{k: k \geq 1} k 2^{-(k+n+1)} \\
& =-2^{-n}+2^{-(n+1)} 2 \\
& =0
\end{aligned}
$$

여기서 (a)는 (5.122)로부터, (b)는 합계에 대한 변수 변경으로부터, 그리고 (c)는 합계의 범위를 늘리는 것으로부터 유도됩니다. 따라서 우리는 다음을 보였습니다.

$$
T_{i}<-p_{i} \log p_{i}+2 p_{i}
$$

$E T=\sum_{i} T_{i}$이므로, 다음이 즉시 도출됩니다.

$$
E T<-\sum_{i} p_{i} \log p_{i}+2 \sum_{i} p_{i}=H(X)+2
$$

이것으로 정리의 증명이 완료됩니다.

따라서, $H(X)+2$의 평균적인 코인 뒤집기로 확률 변수 $X$를 시뮬레이션하는 것으로 충분합니다.

# 요약

Kraft 부등식. 즉시 코드 $\Leftrightarrow \sum D^{-l_{i}} \leq 1$.
McMillan 부등식. 고유하게 복호 가능한 코드 $\Leftrightarrow \sum D^{-l_{i}} \leq 1$.
데이터 압축에 대한 엔트로피 한계

$$
L \triangleq \sum p_{i} l_{i} \geq H_{D}(X)
$$
<!-- Page 168 -->
# Shannon 코드

$$
\begin{gathered}
l_{i}=\left\lceil\log _{D} \frac{1}{p_{i}}\right\rceil \\
H_{D}(X) \leq L<H_{D}(X)+1
\end{gathered}
$$

Huffman 코드

$$
\begin{gathered}
L^{*}=\min _{\sum D^{-l_{i}} \leq 1} \sum p_{i} l_{i} \\
H_{D}(X) \leq L^{*}<H_{D}(X)+1
\end{gathered}
$$

잘못된 코드. $X \sim p(x), l(x)=\left\lceil\log \frac{1}{q(x)}\right\rceil, L=\sum p(x) l(x)$ :

$$
H(p)+D(p \| q) \leq L<H(p)+D(p \| q)+1
$$

확률 과정

$$
\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n} \leq L_{n}<\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n}+\frac{1}{n}
$$

정상 과정

$$
L_{n} \rightarrow H(\mathcal{X})
$$

경쟁적 최적성. Shannon 코드 $l(x)=\left\lceil\log \frac{1}{p(x)}\right\rceil$ 대 다른 모든 코드 $l^{\prime}(x)$ :

$$
\operatorname{Pr}\left(l(X) \geq l^{\prime}(X)+c\right) \leq \frac{1}{2^{c-1}}
$$

## 문제

5.1 고유 복호 가능 및 즉시 복호 가능 코드. 확률 변수 $X$의 인코딩과 관련된 단어 길이의 100제곱의 기댓값을 $L=\sum_{i=1}^{m} p_{i} l_{i}^{100}$이라고 합시다. $L_{1}=\min L$은 모든 즉시 복호 가능 코드에 대한 최소값이고, $L_{2}=\min L$은 모든 고유 복호 가능 코드에 대한 최소값입니다. $L_{1}$과 $L_{2}$ 사이에 어떤 부등식 관계가 존재합니까?
<!-- Page 169 -->
5.2 화성인은 손가락이 몇 개일까요?

$$
S=\binom{S_{1}, \ldots, S_{m}}{p_{1}, \ldots, p_{m}}
$$

$S_i$는 $D$-기호 출력 알파벳의 문자열로 고유 복호화 가능한 방식으로 인코딩됩니다. $m=6$이고 코드어 길이가 $(l_1, l_2, \ldots, l_6)=(1,1,2,3,2,3)$일 때, $D$에 대한 좋은 하한을 찾으십시오. 문제의 제목을 설명하는 것이 좋을 수 있습니다.

5.3 Kraft 부등식의 느슨함. 즉석 코드는 단어 길이 $l_1, l_2, \ldots, l_m$을 가지며, 이는 엄격한 부등식을 만족합니다.

$$
\sum_{i=1}^{m} D^{-l_{i}}<1
$$

코드 알파벳은 $\mathcal{D}=\{0,1,2, \ldots, D-1\}$입니다. $\mathcal{D}^{*}$의 코드어 시퀀스의 임의로 긴 시퀀스가 존재함을 보여주십시오. 이 시퀀스는 코드어 시퀀스로 복호화될 수 없습니다.

5.4 Huffman 코딩. 확률 변수

$$
X=\left(\begin{array}{ccccccc}
x_{1} & x_{2} & x_{3} & x_{4} & x_{5} & x_{6} & x_{7} \\
0.49 & 0.26 & 0.12 & 0.04 & 0.04 & 0.03 & 0.02
\end{array}\right)
$$

를 고려하십시오.
(a) $X$에 대한 이진 Huffman 코드를 찾으십시오.
(b) 이 인코딩에 대한 기대 코드 길이를 찾으십시오.
(c) $X$에 대한 삼진 Huffman 코드를 찾으십시오.

5.5 추가 Huffman 코드. 확률 $( \frac{1}{3}, \frac{1}{5}, \frac{1}{5}, \frac{2}{15}, \frac{2}{15} )$를 갖는 소스에 대한 이진 Huffman 코드를 찾으십시오. 이 코드가 확률 $( \frac{1}{5}, \frac{1}{5}, \frac{1}{5}, \frac{1}{5}, \frac{1}{5} )$를 갖는 소스에 대해서도 최적임을 논증하십시오.

5.6 잘못된 코드. 다음 코드 중 어떤 것이 임의의 확률 할당에 대한 Huffman 코드가 될 수 없습니까?
(a) $\{0,10,11\}$
(b) $\{00,01,10,110\}$
(c) $\{01,10\}$

5.7 Huffman 20 질문. $n$개의 객체 집합을 고려하십시오. $i$번째 객체가 양품이거나 불량품인 경우 $X_i = 1$ 또는 $0$으로 설정합니다. $X_1, X_2, \ldots, X_n$은 독립이며 $\operatorname{Pr}\{X_i=1\}=p_i$이고 $p_1 > p_2 > \cdots > p_n > \frac{1}{2}$입니다. 우리는 모든 불량 객체의 집합을 결정해야 합니다. 생각할 수 있는 모든 예/아니오 질문은 허용됩니다.
<!-- Page 170 -->
(a) 필요한 질문의 평균 최소 개수에 대한 좋은 하한을 제시하십시오.
(b) 자연의 답변이 우리의 질문에 대해 가장 긴 질문 시퀀스를 요구한다면, 마지막 질문은 무엇이어야 합니까 (말로 설명)? 이 질문으로 우리는 어떤 두 집합을 구별하게 됩니까? 압축된 (평균 길이 최소) 질문 시퀀스를 가정하십시오.
(c) 필요한 질문의 평균 최소 개수에 대한 상한을 (하나의 질문 이내로) 제시하십시오.
5.8 마르코프 소스의 간단한 최적 압축. 다음 전이 행렬을 갖는 3-상태 마르코프 프로세스 $U_{1}, U_{2}, \ldots$를 고려하십시오.

| $U_{n-1}$ |  |  |  |
| :--: | :--: | :--: | :--: |
|  | $S_{1}$ | $S_{2}$ | $S_{3}$ |
| $S_{1}$ | $\frac{1}{2}$ | $\frac{1}{4}$ | $\frac{1}{4}$ |
| $S_{2}$ | $\frac{1}{4}$ | $\frac{1}{2}$ | $\frac{1}{4}$ |
| $S_{3}$ | 0 | $\frac{1}{2}$ | $\frac{1}{2}$ |

따라서 $S_{1}$이 $S_{3}$ 다음에 올 확률은 0입니다. 세 개의 코드 $C_{1}, C_{2}, C_{3}$ (각 상태 1, 2 및 3에 대해 하나씩, 각 코드는 $S_{i}$의 집합 요소를 0과 1의 시퀀스로 매핑)를 설계하여 다음 체계를 통해 이 마르코프 프로세스를 최대 압축으로 전송할 수 있습니다.
(a) 현재 기호 $X_{n}=i$를 기록합니다.
(b) 코드 $C_{i}$를 선택합니다.
(c) 다음 기호 $X_{n+1}=j$를 기록하고 $C_{i}$에서 $j$에 해당하는 코드워드를 전송합니다.
(d) 다음 기호에 대해 반복합니다. 이 코딩 체계를 사용하여 이전 상태 $X_{n}=i$에 조건부로 다음 기호의 평균 메시지 길이는 얼마입니까? 무조건부 평균 비트 수는 기호당 얼마입니까? 이를 마르코프 체인의 엔트로피율 $H(\mathcal{U})$와 연관 지으십시오.
5.9 엔트로피보다 1비트 높은 최적 코드 길이. 소스 코딩 정리는 랜덤 변수 $X$에 대한 최적 코드의 기대 길이가 $H(X)+1$보다 작음을 보여줍니다. 최적 코드의 기대 길이가 $H(X)+1$에 가까운 랜덤 변수의 예를 제시하십시오 [즉, 임의의 $\epsilon>0$에 대해, 최적 코드가 $L>H(X)+1-\epsilon$을 갖는 분포를 구성하십시오].
<!-- Page 171 -->
5.10 엔트로피 경계를 달성하는 삼진 코드. 확률 변수 $X$는 $m$개의 값을 가지며 엔트로피 $H(X)$를 갖습니다. 이 소스에 대한 즉석 삼진 코드가 발견되었으며, 평균 길이는 다음과 같습니다.

$$
L=\frac{H(X)}{\log _{2} 3}=H_{3}(X)
$$

(a) $X$의 각 기호가 어떤 $i$에 대한 $3^{-i}$ 형태의 확률을 갖는다는 것을 보이십시오.
(b) $m$이 홀수임을 보이십시오.
5.11 접미사 조건. 어떤 코드워드도 다른 코드워드의 접미사가 되지 않는다는 접미사 조건을 만족하는 코드를 고려하십시오. 접미사 조건 코드는 고유하게 복호화 가능함을 보이고, 접미사 조건을 만족하는 모든 코드에 대한 최소 평균 길이가 해당 확률 변수에 대한 허프만 코드의 평균 길이와 동일함을 보이십시오.
5.12 섀넌 코드와 허프만 코드. 확률이 $\left(\frac{1}{3}, \frac{1}{3}, \frac{1}{4}, \frac{1}{12}\right)$인 네 개의 값을 갖는 확률 변수 $X$를 고려하십시오.
(a) 이 확률 변수에 대한 허프만 코드를 구성하십시오.
(b) 최적 길이의 두 가지 다른 집합이 존재함을 보이십시오. 즉, 코드워드 길이 할당 $(1,2,3,3)$과 $(2,2,2,2)$가 모두 최적임을 보이십시오.
(c) 일부 기호에 대한 코드워드 길이가 섀넌 코드 길이 $\left\lceil\log \frac{1}{p(x)}\right\rceil$를 초과하는 최적 코드가 존재한다고 결론 내리십시오.
5.13 스무고개. 플레이어 A는 우주에서 어떤 물체를 선택하고, 플레이어 B는 일련의 예/아니오 질문으로 그 물체를 식별하려고 시도합니다. 플레이어 B가 플레이어 A의 분포에 대해 최소 기대 길이를 달성하는 코드를 사용할 만큼 영리하다고 가정합니다. 플레이어 B가 물체를 결정하는 데 평균 38.5개의 질문이 필요하다는 것을 관찰합니다. 우주에 있는 물체의 수에 대한 대략적인 하한을 찾으십시오.
5.14 허프만 코드. 확률이 다음과 같은 확률 변수 $X$에 대한 (a) 이진 및 (b) 삼진 허프만 코드를 찾으십시오.

$$
p=\left(\frac{1}{21}, \frac{2}{21}, \frac{3}{21}, \frac{4}{21}, \frac{5}{21}, \frac{6}{21}\right)
$$

(c) 각 경우에 $L=\sum p_{i} l_{i}$를 계산하십시오.
<!-- Page 172 -->
# 5.15 허프만 코드

(a) 다섯 개의 심볼에 대한 다음 분포에 대해 이진 허프만 코드를 구성하십시오: $\mathbf{p}=(0.3,0.3,0.2,0.1,0.1)$. 이 코드의 평균 길이는 얼마입니까?
(b) (a)에서 구성한 코드가 엔트로피 $H\left(\mathbf{p}^{\prime}\right)$와 같은 평균 길이( $\mathbf{p}^{\prime}$ 하에서)를 갖도록 다섯 개의 심볼에 대한 확률 분포 $\mathbf{p}^{\prime}$를 구성하십시오.
5.16 허프만 코드. 확률이 각각 0.5, 0.25, 0.1, 0.05, 0.05, 0.05인 여섯 개의 값 $\{A, B, C, D, E, F\}$을 갖는 확률 변수 $X$를 고려하십시오.
(a) 이 확률 변수에 대한 이진 허프만 코드를 구성하십시오. 평균 길이는 얼마입니까?
(b) 이 확률 변수에 대해 4진 허프만 코드를 구성하십시오 (즉, 네 개의 심볼 ( $a, b, c$ 및 $d$ 라고 부름)의 알파벳에 대한 코드). 이 코드의 평균 길이는 얼마입니까?
(c) 확률 변수에 대한 이진 코드를 구성하는 한 가지 방법은 4진 코드로 시작하여 $a \rightarrow 00, b \rightarrow 01, c \rightarrow 10$, 그리고 $d \rightarrow$ 11 매핑을 사용하여 심볼을 이진으로 변환하는 것입니다. 이 과정을 통해 구성된 위 확률 변수에 대한 이진 코드의 평균 길이는 얼마입니까?
(d) 임의의 확률 변수 $X$에 대해, $L_{H}$를 확률 변수에 대한 이진 허프만 코드의 평균 길이로, $L_{Q B}$를 먼저 4진 허프만 코드를 구축하고 이진으로 변환하여 구성된 평균 길이 코드로 정의합니다. 다음을 보여주십시오.

$$
L_{H} \leq L_{Q B}<L_{H}+2
$$

(e) 예시의 하한은 타이트합니다. 최적의 4진 코드 변환으로 구성된 코드가 최적의 이진 코드이기도 한 예시를 제공하십시오.
(f) 상한 (즉, $L_{Q B}<L_{H}+2$ )은 타이트하지 않습니다. 사실, 더 나은 바운드는 $L_{Q B} \leq L_{H}+1$입니다. 이 바운드를 증명하고, 이 바운드가 타이트한 예시를 제공하십시오.
5.17 데이터 압축. 다음 각 확률 질량 함수에 대해 최적의 이진 코드워드 길이 집합 $l_{1}, l_{2}, \ldots$ ( $\sum p_{i} l_{i}$ 를 최소화하는)를 찾으십시오:
(a) $\mathbf{p}=\left(\frac{10}{41}, \frac{9}{41}, \frac{8}{41}, \frac{7}{41}, \frac{7}{41}\right)$
(b) $\mathbf{p}=\left(\frac{9}{10},\left(\frac{9}{10}\right)\left(\frac{1}{10}\right),\left(\frac{9}{10}\right)\left(\frac{1}{10}\right)^{2},\left(\frac{9}{10}\right)\left(\frac{1}{10}\right)^{3}, \ldots\right)$
<!-- Page 173 -->
5.18 코드의 종류. 코드 $\{0,01\}$을 고려하십시오.
(a) 즉시 해독 가능한 코드입니까?
(b) 고유하게 해독 가능한 코드입니까?
(c) 비특이적 코드입니까?
5.19 Hi-Lo 게임
(a) 컴퓨터는 알려진 확률 질량 함수 $p(x), x \in\{1,2, \ldots, 100\}$에 따라 숫자 $X$를 생성합니다. 플레이어는 "X=i입니까?"라고 질문하고 "예", "너무 높습니다", 또는 "너무 낮습니다"라는 답변을 듣습니다. 총 여섯 번의 질문을 합니다. 이 순서 중에 올바르게 맞히면(즉, "예"라는 답변을 받으면) $v(X)$의 가치를 가진 상금을 받습니다. 플레이어는 기대 상금을 최대화하기 위해 어떻게 진행해야 합니까?
(b) (a) 부분은 정보 이론과 큰 관련이 없습니다. 다음 변형을 고려하십시오: $X \sim p(x)$, 상금 $=v(x), p(x)$는 알려져 있으며, 이전과 같습니다. 그러나 임의의 예/아니오 질문이 순차적으로 주어져 $X$가 결정될 때까지 진행됩니다. ("결정"은 "예"라는 답변을 받는다는 의미가 아닙니다.) 질문당 비용은 1 단위입니다. 플레이어는 어떻게 진행해야 합니까? 기대 상금은 얼마입니까?
(c) (b) 부분을 계속해서, $v(x)$가 고정되어 있지만 $p(x)$는 컴퓨터가 선택할 수 있고(그런 다음 플레이어에게 발표됨) 플레이어의 기대 수익을 최소화하려고 한다면 어떻게 해야 합니까? $p(x)$는 무엇이어야 합니까? 플레이어의 기대 수익은 얼마입니까?
5.20 비용이 있는 Huffman 코드. "Run!", "Help!", "Fire!"와 같은 단어는 자주 사용되기 때문이 아니라, 이러한 단어가 필요한 상황에서 시간이 매우 중요하기 때문에 짧을 수 있습니다. $X=i$일 확률이 $p_{i}, i=1,2, \ldots, m$이라고 가정합니다. $l_{i}$를 $X=i$에 연관된 코드워드에 있는 이진 기호의 수라고 하고, $c_{i}$를 $X=i$일 때 코드워드의 문자당 비용이라고 합니다. 따라서 $X$의 설명에 대한 평균 비용 $C$는 $C=\sum_{i=1}^{m} p_{i} c_{i} l_{i}$입니다.
(a) $\sum 2^{-l_{i}} \leq 1$을 만족하는 모든 $l_{1}, l_{2}, \ldots, l_{m}$에 대해 $C$를 최소화하십시오. $l_{i}$에 대한 암시적인 정수 제약 조건은 무시하십시오. 최소화하는 $l_{1}^{*}, l_{2}^{*}, \ldots, l_{m}^{*}$와 관련 최소값 $C^{*}$를 제시하십시오.
(b) 모든 고유하게 해독 가능한 코드에 대해 $C$를 최소화하기 위해 Huffman 코드 절차를 어떻게 사용하시겠습니까? 이 최소값을 $C_{\text {Huffman }}$이라고 합시다.
<!-- Page 174 -->
(c) 다음을 보일 수 있습니까?

$$
C^{*} \leq C_{\text {Huffman }} \leq C^{*}+\sum_{i=1}^{m} p_{i} c_{i} ?
$$

5.21 고유 복호화 가능 조건. 코드 $C$가 고유 복호화 가능하다고 증명하십시오 (그리고 오직 그럴 때만). 확장

$$
C^{k}\left(x_{1}, x_{2}, \ldots, x_{k}\right)=C\left(x_{1}\right) C\left(x_{2}\right) \cdots C\left(x_{k}\right)
$$

가 모든 $k \geq 1$에 대해 $\lambda^{k}$에서 $D^{*}$로의 일대일 매핑입니다. ("오직 그럴 때만" 부분은 명백합니다.)
5.22 최적 코드의 평균 길이. 확률 $\left\{p_{1}, \ldots, p_{m}\right\}$에 대한 최적 $D$-진 접두 코드의 평균 코드워드 길이 $L\left(p_{1}, \ldots, p_{m}\right)$가 $p_{1}, \ldots, p_{m}$의 연속 함수임을 증명하십시오. 이는 최적 코드가 확률이 변함에 따라 불연속적으로 변함에도 불구하고 참입니다.
5.23 사용되지 않는 코드 시퀀스. Kraft 부등식을 등식으로 만족하지만 접두 조건은 만족하지 않는 가변 길이 코드 $C$를 고려하십시오.
(a) 코드 알파벳 기호의 유한 시퀀스가 코드워드 시퀀스의 접두사가 되지 않음을 증명하십시오.
(b) (선택 사항) 증명 또는 반증: $C$는 무한 복호화 지연을 가집니다.
5.24 균등 분포에 대한 최적 코드. $m$개의 동일 확률 결과를 갖는 확률 변수를 고려하십시오. 이 정보원의 엔트로피는 명백히 $\log _{2} m$ 비트입니다.
(a) 이 소스에 대한 최적의 즉석 이진 코드를 설명하고 평균 코드워드 길이 $L_{m}$을 계산하십시오.
(b) 어떤 $m$ 값에 대해 평균 코드워드 길이 $L_{m}$이 엔트로피 $H=\log _{2} m$과 같습니까?
(c) 우리는 모든 확률 분포에 대해 $L<H+1$임을 알고 있습니다. 가변 길이 코드의 중복도는 $\rho=$ $L-H$로 정의됩니다. 어떤 $m$ 값($2^{k} \leq m \leq 2^{k+1}$)에서 코드의 중복도가 최대화됩니까? 이 최악의 경우 중복도의 극한값은 $m \rightarrow \infty$일 때 무엇입니까?
5.25 최적 코드워드 길이. 최적 가변 길이 코드의 코드워드 길이는 메시지 확률 $\left\{p_{1}, p_{2}, \ldots, p_{m}\right\}$의 복잡한 함수이지만, 덜 확률적인
<!-- Page 175 -->
기호는 더 긴 코드워드로 인코딩됩니다. 메시지 확률이 내림차순으로 주어진다고 가정합니다. $p_{1}>p_{2} \geq \cdots \geq$ $p_{m}$.
(a) 가장 확률이 높은 메시지 기호의 확률이 $p_{1}>\frac{2}{5}$이면 해당 기호는 길이가 1인 코드워드를 할당받아야 함을 모든 이진 허프만 코드에 대해 증명하십시오.
(b) 가장 확률이 높은 메시지 기호의 확률이 $p_{1}<\frac{1}{3}$이면 해당 기호는 길이가 $\geq 2$인 코드워드를 할당받아야 함을 모든 이진 허프만 코드에 대해 증명하십시오.
5.26 병합. 가치가 $W_{1}, W_{2}, \ldots, W_{m}$인 회사들이 다음과 같이 병합됩니다. 가장 가치가 낮은 두 회사가 병합되어 $m-1$개의 회사 목록이 형성됩니다. 병합의 가치는 병합된 두 회사의 가치 합계입니다. 이는 하나의 슈퍼컴퍼니가 남을 때까지 계속됩니다. $V$를 병합 가치의 합계라고 합니다. 따라서 $V$는 보고된 총 병합 거래량을 나타냅니다. 예를 들어, $\mathbf{W}=(3,3,2,2)$이면 병합 결과는 $(3,3,2,2) \rightarrow(4,3,3) \rightarrow(6,4) \rightarrow(10)$이며 $V=$ $4+6+10=20$입니다.
(a) $V$가 하나의 슈퍼컴퍼니로 종료되는 쌍별 병합 시퀀스에 의해 달성 가능한 최소 거래량임을 논증하십시오. (힌트: 허프만 코딩과 비교하십시오.)
(b) $W=\sum W_{i}, \quad \tilde{W}_{i}=W_{i} / W$라고 하고 최소 병합 거래량 $V$가 다음을 만족함을 보이십시오.

$$
W H(\tilde{\mathbf{W}}) \leq V \leq W H(\tilde{\mathbf{W}})+W
$$

5.27 Sardinas-Patterson 고유 복호화 테스트. 코드가 고유하게 복호화될 수 없는 경우는 두 가지 다른 방식으로 코드워드 시퀀스로 해석될 수 있는 유한한 코드 기호 시퀀스가 존재하는 경우입니다. 즉, 다음과 같은 상황이 발생해야 합니다.

여기서 각 $A_{i}$와 각 $B_{i}$는 코드워드입니다. $B_{1}$은 일부 결과 "매달린 접미사"를 가진 $A_{1}$의 접두사여야 합니다. 각 매달린 접미사는 코드워드의 접두사이거나 다른 코드워드를 접두사로 가져야 하며, 이는 또 다른 매달린 접미사를 생성합니다. 마지막으로, 시퀀스의 마지막 매달린 접미사도 코드워드여야 합니다. 따라서 고유 복호화 가능성에 대한 테스트(본질적으로 Sardinas-Patterson 테스트 [456]임)를 설정할 수 있습니다.
<!-- Page 176 -->
다음과 같이 구성합니다. 모든 가능한 늘어나는 접미사의 집합 $S$를 구성합니다. $S$에 어떤 코드어도 포함되어 있지 않을 때만 해당 코드는 고유하게 복호화 가능합니다.
(a) 집합 $S$를 구축하기 위한 정확한 규칙을 명시하십시오.
(b) 코드어 길이가 $l_{i}, i=1,2, \ldots, m$이라고 가정합니다. 집합 $S$의 원소 개수에 대한 좋은 상한을 찾으십시오.
(c) 다음 코드 중 고유하게 복호화 가능한 것을 결정하십시오:
(i) $\{0,10,11\}$
(ii) $\{0,01,11\}$
(iii) $\{0,01,10\}$
(iv) $\{0,01\}$
(v) $\{00,01,10,11\}$
(vi) $\{110,11,10\}$
(vii) $\{110,11,100,00,10\}$
(d) (c)의 각 고유 복호화 가능한 코드에 대해, 가능한 경우, 시작점을 알고 있는 무한 인코딩된 시퀀스를 구성하여 두 가지 다른 방식으로 코드어로 해석될 수 있도록 하십시오. (이는 고유 복호화 가능성이 유한 복호화 가능성을 의미하지 않음을 보여줍니다.) 이러한 시퀀스가 접두사 코드에서는 발생할 수 없음을 증명하십시오.
5.28 Shannon 코드. $m$개의 값 $\{1,2, \ldots, m\}$을 확률 $p_{1}, p_{2}, \ldots, p_{m}$으로 갖는 랜덤 변수 $X$에 대한 코드를 생성하는 다음 방법을 고려하십시오. 확률이 $p_{1} \geq p_{2} \geq \cdots \geq p_{m}$으로 정렬되었다고 가정합니다.

$$
F_{i}=\sum_{k=1}^{i-1} p_{k}
$$

를 $i$보다 작은 모든 심볼의 확률 합으로 정의합니다. 그러면 $i$의 코드어는 $l_{i}=\left\lceil\log \frac{1}{p_{i}}\right\rceil$ 비트로 반올림된 $F_{i} \in[0,1]$의 값입니다.
(a) 이 과정을 통해 구성된 코드가 접두사-자유임을 보이고 평균 길이가

$$
H(X) \leq L<H(X)+1
$$

를 만족함을 보이십시오.
(b) 확률 분포 ( $0.5,0.25$, $0.125,0.125)$에 대한 코드를 구성하십시오.
<!-- Page 177 -->
5.29 이진 분포에 대한 최적 코드. 허프만 코드 트리에 대해, 노드의 확률을 해당 노드 아래의 모든 잎 노드 확률의 합으로 정의합니다. 확률 변수 $X$가 이진 분포에서 추출된다고 가정합니다 [즉, $p(x)=2^{-i}$, 어떤 $i$에 대해, 모든 $x \in \mathcal{X}$]. 이제 이 분포에 대한 이진 허프만 코드를 고려합니다.
(a) 트리의 임의의 노드에 대해, 왼쪽 자식의 확률이 오른쪽 자식의 확률과 같다고 논증하십시오.
(b) $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim p(x)$로 추출된다고 가정합니다. $p(x)$에 대한 허프만 코드를 사용하여, $X_{1}, X_{2}, \ldots, X_{n}$을 비트 시퀀스 $Y_{1}, Y_{2}, \ldots, Y_{k\left(X_{1}, X_{2}, \ldots, X_{n}\right)}$로 매핑합니다. (이 시퀀스의 길이는 결과 $X_{1}, X_{2}, \ldots, X_{n}$에 따라 달라집니다.) 부분 (a)를 사용하여 시퀀스 $Y_{1}, Y_{2}, \ldots$가 공정한 동전 던지기 시퀀스를 형성한다고 논증하십시오 [즉, $\operatorname{Pr}\left\{Y_{i}=0\right\}=\operatorname{Pr}\left\{Y_{i}=1\right\}=\frac{1}{2}$, $Y_{1}, Y_{2}, \ldots, Y_{i-1}$과 독립적임]. 따라서, 코딩된 시퀀스의 엔트로피율은 기호당 1비트입니다.
(c) 엔트로피 한계에 도달하는 임의의 코드에 대한 인코딩된 비트 시퀀스가 압축될 수 없는 이유와 따라서 기호당 1비트의 엔트로피율을 가져야 하는 이유에 대해 직관적인 논증을 제시하십시오.
5.30 상대 엔트로피는 잘못된 코딩의 비용입니다. 확률 변수 $X$가 다섯 개의 가능한 결과 $\{1,2,3,4,5\}$를 갖는다고 가정합니다. 이 확률 변수에 대한 두 분포 $p(x)$와 $q(x)$를 고려합니다.

| 기호 | $p(x)$ | $q(x)$ | $C_{1}(x)$ | $C_{2}(x)$ |
| :--: | :--: | :--: | :--: | :--: |
| 1 | $\frac{1}{2}$ | $\frac{1}{2}$ | 0 | 0 |
| 2 | $\frac{1}{4}$ | $\frac{1}{8}$ | 10 | 100 |
| 3 | $\frac{1}{8}$ | $\frac{1}{8}$ | 110 | 101 |
| 4 | $\frac{1}{16}$ | $\frac{1}{8}$ | 1110 | 110 |
| 5 | $\frac{1}{16}$ | $\frac{1}{8}$ | 1111 | 111 |

(a) $H(p), H(q), D(p \| q), D(q \| p)$를 계산하십시오.
(b) 마지막 두 열은 확률 변수에 대한 코드입니다. $p$에 대한 $C_{1}$의 평균 길이가 엔트로피 $H(p)$와 같음을 확인하십시오. 따라서 $C_{1}$은 $p$에 대해 최적입니다. $C_{2}$가 $q$에 대해 최적임을 확인하십시오.
(c) 이제 분포가 $p$일 때 코드 $C_{2}$를 사용한다고 가정합니다. 코드의 평균 길이는 얼마입니까? 엔트로피 $p$를 얼마나 초과합니까?
(d) 분포가 $q$일 때 코드 $C_{1}$을 사용하면 손실은 얼마입니까?
<!-- Page 178 -->
5.31 비특이 코드. 텍스트의 논의는 즉석 코드에 초점을 맞추었으며, 고유 복호화 가능한 코드로 확장되었습니다. 이 두 가지 모두 코드가 확률 변수의 결과 시퀀스를 인코딩하기 위해 반복적으로 사용되어야 하는 경우에 필요합니다. 그러나 단 하나의 결과만 인코딩해야 하고 복호화 단어의 끝에 도달했음을 알 수 있다면 고유 복호화 가능성은 필요하지 않습니다. 비특이 코드라는 사실만으로도 충분합니다. 예를 들어, 확률 변수 $X$가 세 가지 값 a, b, c를 취한다고 가정하면, 이를 0, 1, 00으로 인코딩할 수 있습니다. 이러한 코드는 비특이적이지만 고유 복호화 가능하지는 않습니다.
이하에서는 확률 변수 $X$가 확률 $p_{1}, p_{2}, \ldots, p_{m}$으로 $m$개의 값을 취하며, 확률이 $p_{1} \geq p_{2} \geq \cdots \geq p_{m}$으로 정렬되어 있다고 가정합니다.
(a) 비특이 이진 코드를 세 개의 기호 0, 1, "STOP"을 갖는 삼진 코드로 간주하여, 확률 변수 $X$에 대한 비특이 코드 $L_{1: 1}$의 기대 길이가 다음 부등식을 만족함을 보이십시오:

$$
L_{1: 1} \geq \frac{H_{2}(X)}{\log _{2} 3}-1
$$

여기서 $H_{2}(X)$는 비트 단위의 $X$의 엔트로피입니다. 따라서 비특이 코드의 평균 길이는 즉석 코드의 평균 길이의 상수 비율 이상입니다.
(b) $L_{\text {INST }}$를 $X$에 대한 최적 즉석 코드의 기대 길이로, $L_{1: 1}^{*}$를 최적 비특이 코드의 기대 길이로 놓습니다. $L_{1: 1}^{*} \leq L_{\text {INST }}^{*} \leq H(X)+1$임을 논증하십시오.
(c) 비특이 코드의 평균 길이가 엔트로피보다 작은 간단한 예를 제시하십시오.
(d) 비특이 코드에 사용할 수 있는 복호화 단어 집합은 $\{0,1$, $00,01,10,11,000, \ldots\}$입니다. $L_{1: 1}=\sum_{i=1}^{m} p_{i} l_{i}$이므로, 가장 확률이 높은 기호에 가장 짧은 복호화 단어를 할당하면 최소화됨을 보이십시오. 따라서 $l_{1}=l_{2}=1, l_{3}=l_{4}=l_{5}=l_{6}=2$, 등입니다. 일반적으로 $l_{i}=\left\lceil\log \left(\frac{i}{2}+1\right)\right\rceil$임을 보이고, 따라서 $L_{1: 1}^{*}=$ $\sum_{i=1}^{m} p_{i}\left\lceil\log \left(\frac{i}{2}+1\right)\right\rceil$임을 보이십시오.
(e) (d)항은 분포에 대한 최적 비특이 코드를 찾는 것이 쉽다는 것을 보여줍니다. 그러나 이 코드의 평균 길이를 다루는 것은 조금 더 까다롭습니다. 이제 이 평균 길이를 제한합니다. (d)항으로부터 $L_{1: 1}^{*} \geq$
<!-- Page 179 -->
$\tilde{L} \triangleq \sum_{i=1}^{m} p_{i} \log \left(\frac{i}{2}+1\right)$입니다. 차이를 고려하십시오.

$$
F(\mathbf{p})=H(X)-\tilde{L}=-\sum_{i=1}^{m} p_{i} \log p_{i}-\sum_{i=1}^{m} p_{i} \log \left(\frac{i}{2}+1\right)
$$

라그랑주 승수법을 사용하여 $F(\mathbf{p})$의 최댓값이 $p_{i}=c /(i+2)$일 때 발생함을 증명하십시오. 여기서 $c=1 /\left(H_{m+2}-\right.$ $H_{2}$ )이고 $H_{k}$는 조화급수의 합입니다.

$$
H_{k} \triangleq \sum_{i=1}^{k} \frac{1}{i}
$$

(이는 상대 엔트로피의 비음수성을 사용하여 수행할 수도 있습니다.)
(f) 다음의 논증을 완성하십시오.

$$
\begin{aligned}
H(X)-L_{1: 1}^{*} & \leq H(X)-\tilde{L} \\
& \leq \log \left(2\left(H_{m+2}-H_{2}\right)\right)
\end{aligned}
$$

이제 (예: Knuth [315] 참조) $H_{k} \approx \ln k$ (더 정확하게는 $H_{k}=\ln k+\gamma+\frac{1}{2 k}-\frac{1}{12 k^{2}}+\frac{1}{120 k^{4}}-\epsilon$, 여기서 $0<\epsilon<1 / 252 n^{6}$, 그리고 $\gamma=$ 오일러 상수 $=0.577 \ldots$)임이 잘 알려져 있습니다. 이를 사용하거나 $\frac{1}{2}$의 적분을 통해 증명할 수 있는 간단한 근사치인 $H_{k} \leq \ln k+1$을 사용하여 $H(X)-L_{1: 1}^{*}<\log \log m+2$임을 보일 수 있습니다. 따라서 다음을 얻습니다.

$$
H(X)-\log \log |\mathcal{X}|-2 \leq L_{1: 1}^{*} \leq H(X)+1
$$

비특이 코드는 즉시 코드보다 훨씬 더 나은 성능을 낼 수 없습니다!
5.32 나쁜 와인. 여섯 병의 와인이 주어졌습니다. 정확히 한 병이 상했다는 것을 알고 있습니다(맛이 끔찍합니다). 병들을 검사한 결과, $i$번째 병이 상했을 확률 $p_{i}$가 $\left(p_{1}, p_{2}, \ldots, p_{6}\right)=\left(\frac{8}{23}, \frac{6}{23}, \frac{4}{23}, \frac{2}{23}, \frac{2}{23}, \frac{1}{23}\right)$으로 주어짐을 결정했습니다. 맛을 보면 상한 와인을 결정할 수 있습니다. 와인을 하나씩 맛본다고 가정합니다. 최소화를 위해 맛보는 순서를 선택하십시오.
<!-- Page 180 -->
마지막 병을 확인하기 위해 필요한 예상 시음 횟수를 기억하십시오. 첫 다섯 개의 와인이 테스트를 통과하면 마지막 와인은 시음할 필요가 없습니다.
(a) 필요한 예상 시음 횟수는 얼마입니까?
(b) 첫 번째로 어떤 병을 시음해야 합니까?

이제 똑똑해지십시오. 첫 번째 샘플의 경우, 신선한 잔에 와인 일부를 섞어 샘플링합니다. 혼합하고 시음하는 과정을 계속하여 불량 병이 확인될 때까지 진행합니다.
(a) 불량 와인을 확인하는 데 필요한 최소 예상 시음 횟수는 얼마입니까?
(b) 첫 번째로 어떤 혼합물을 시음해야 합니까?
5.33 허프만 대 섀넌. 확률 $0.6, 0.3, 0.1$을 갖는 세 가지 값을 갖는 확률 변수 $X$가 있습니다.
(a) $X$에 대한 이진 허프만 코드워드의 길이는 얼마입니까? $X$에 대한 이진 섀넌 코드워드($l(x)=\left\lceil\log \left(\frac{1}{p(x)}\right)\right\rceil$)의 길이는 얼마입니까?
(b) $D$진 알파벳을 사용한 예상 섀넌 코드워드 길이가 $D$진 알파벳을 사용한 예상 허프만 코드워드 길이와 같아지는 가장 작은 정수 $D$는 얼마입니까?
5.34 트리 구성을 위한 허프만 알고리즘. 다음 문제를 고려하십시오: $m$개의 이진 신호 $S_{1}, S_{2}, \ldots, S_{m}$가 시간 $T_{1} \leq T_{2} \leq \cdots \leq T_{m}$에 사용 가능하며, 최종 결과를 가능한 한 빨리 사용할 수 있도록 두 개의 입력 게이트를 사용하여 합 $S_{1} \oplus S_{2} \oplus$ $\cdots \oplus S_{m}$을 찾고 싶습니다. 각 게이트는 한 단위 시간 지연이 있습니다. 간단한 탐욕 알고리즘은 가장 빠른 두 개의 결과를 결합하여 시간 $\max \left(T_{1}, T_{2}\right)+1$에 부분 결과를 형성하는 것입니다. 이제 시간 $\max \left(T_{1}, T_{2}\right)+$ $1, T_{3}, \ldots, T_{m}$에 사용 가능한 $S_{1} \oplus S_{2}, S_{3}, \ldots, S_{m}$에 대한 새로운 문제가 있습니다. 이제 $T$ 목록을 정렬하고 동일한 병합 단계를 다시 적용하여 최종 결과가 나올 때까지 반복할 수 있습니다.
(a) 위 절차가 최종 결과를 가능한 한 빨리 사용할 수 있는 회로를 구성한다는 점에서 최적임을 논증하십시오.
(b) 이 절차가 다음을 최소화하는 트리를 찾음을 보이십시오.

$$
C(T)=\max _{i}\left(T_{i}+l_{i}\right)
$$

여기서 $T_{i}$는 $i$번째 잎에 할당된 결과가 사용 가능한 시간이고 $l_{i}$는 $i$번째 잎에서 루트까지의 경로 길이입니다.
<!-- Page 181 -->
(c) 임의의 트리 $T$에 대해 다음을 증명하십시오.

$$
C(T) \geq \log _{2}\left(\sum_{i} 2^{T_{i}}\right)
$$

(d) 다음을 만족하는 트리가 존재함을 증명하십시오.

$$
C(T) \leq \log _{2}\left(\sum_{i} 2^{T_{i}}\right)+1
$$

따라서 $\log _{2}\left(\sum_{i} 2^{T_{i}}\right)$는 이 문제에 대한 entropy의 유사체입니다.
5.35 난수 생성. 확률 변수 $X$를 생성하고자 합니다.

$$
X=\left\{\begin{array}{ll}
1 & \text { 확률 } p \\
0 & \text { 확률 } 1-p
\end{array}\right.
$$

공정한 동전 던지기 $Z_{1}, Z_{2}, \ldots$가 주어졌습니다. $N$을 $X$를 생성하는 데 필요한 (확률적) 횟수라고 합시다. $X$를 생성하기 위해 $Z_{1}, Z_{2}, \ldots$를 사용하는 좋은 방법을 찾으십시오. $E N \leq 2$임을 증명하십시오.
5.36 최적 단어 길이.
(a) $l=(1,2,2)$가 이진 Huffman 코드의 단어 길이가 될 수 있습니까? $(2,2,3,3)$은 어떻습니까?
(b) 어떤 단어 길이 $l=\left(l_{1}, l_{2}, \ldots\right)$가 이진 Huffman 코드에서 발생할 수 있습니까?
5.37 코드. 다음 코드 중 어느 것이
(a) 고유 복호 가능한가?
(b) 즉석 복호 가능한가?

$$
\begin{aligned}
& C_{1}=\{00,01,0\} \\
& C_{2}=\{00,01,100,101,11\} \\
& C_{3}=\{0,10,110,1110, \ldots\} \\
& C_{4}=\{0,00,000,0000\}
\end{aligned}
$$

5.38 Huffman. $\left(p_{1}, p_{2}, p_{3}, p_{4}, p_{5}\right.$, $\left.p_{6}\right)=\left(\frac{6}{25}, \frac{6}{25}, \frac{4}{25}, \frac{4}{25}, \frac{3}{25}, \frac{3}{25}\right)$에 대한 Huffman $D$-ary 코드를 찾고 기대 단어 길이를 구하십시오.
(a) $D=2$인 경우.
(b) $D=4$인 경우.
<!-- Page 182 -->
5.39 인코딩된 비트의 엔트로피. $C: X \longrightarrow\{0,1\}^{*}$ 를 비특이적이지만 비고유 복호화 가능한 코드라고 가정합니다. $X$ 의 엔트로피를 $H(X)$ 라고 합니다.
(a) $H(C(X))$ 와 $H(X)$ 를 비교하십시오.
(b) $H\left(C\left(X^{n}\right)\right)$ 와 $H\left(X^{n}\right)$ 를 비교하십시오.
5.40 코드 속도. $X$ 를 알파벳 $\{1,2,3\}$ 와 분포를 갖는 확률 변수라고 가정합니다.

$$
X=\left\{\begin{array}{ll}
1 & \text { 확률 } \frac{1}{2} \\
2 & \text { 확률 } \frac{1}{4} \\
3 & \text { 확률 } \frac{1}{4}
\end{array}\right.
$$

$X$ 의 데이터 압축 코드는 다음과 같은 코드워드를 할당합니다.

$$
C(x)= \begin{cases}0 & x=1 \text{인 경우} \\ 10 & x=2 \text{인 경우} \\ 11 & x=3 \text{인 경우}\end{cases}
$$

$X_{1}, X_{2}, \ldots$ 를 이 분포에 따라 독립적이고 동일하게 분포한다고 가정하고, $Z_{1} Z_{2} Z_{3} \cdots=C\left(X_{1}\right) C\left(X_{2}\right) \cdots$ 를 해당 코드워드를 연결하여 생성된 이진 심볼 문자열이라고 합니다. 예를 들어, 122는 01010이 됩니다.
(a) 기호당 엔트로피 속도 $H(\mathcal{X})$ 와 엔트로피 속도 $H(\mathcal{Z})$ 를 비트 단위로 찾으십시오. $Z$ 는 더 이상 압축할 수 없다는 점에 유의하십시오.
(b) 이제 코드를 다음과 같이 설정합니다.

$$
C(x)= \begin{cases}00 & x=1 \text{인 경우} \\ 10 & x=2 \text{인 경우} \\ 01 & x=3 \text{인 경우}\end{cases}
$$

그리고 엔트로피 속도 $H(\mathcal{Z})$ 를 찾으십시오.
(c) 마지막으로, 코드를 다음과 같이 설정합니다.

$$
C(x)= \begin{cases}00 & x=1 \text{인 경우} \\ 1 & x=2 \text{인 경우} \\ 01 & x=3 \text{인 경우}\end{cases}
$$

그리고 엔트로피 속도 $H(\mathcal{Z})$ 를 찾으십시오.
5.41 최적 코드. $p_{1} \geq p_{2} \geq \cdots \geq p_{10}$ 확률에 대한 이진 허프만 코드워드 길이 $l_{1}, l_{2}, \ldots, l_{10}$ 를 가정합니다. 마지막 확률을 분할하여 새로운 분포를 얻는다고 가정합니다.
<!-- Page 183 -->
질량입니다. 확률 $p_{1}, p_{2}, \ldots, p_{9}, \alpha p_{10},(1-\alpha) p_{10}$에 대한 최적 이진 부호어 길이 $\tilde{l_{1}}, \tilde{l_{2}}, \ldots, \tilde{l_{11}}$에 대해 무엇을 말할 수 있습니까? 여기서 $0 \leq \alpha \leq 1$입니다.
5.42 삼진 코드. 다음 부호어 길이 중 어떤 것이 3진 허프만 코드의 단어 길이가 될 수 있고 어떤 것이 될 수 없습니까?
(a) $(1,2,2,2,2)$
(b) $(2,2,2,2,2,2,2,2,3,3,3)$
5.43 조각별 허프만. 확률 변수 $X \sim p(x)$를 설명하는 데 사용하는 부호어가 항상 집합 $\{A, B, C\}$에서 선택된 기호로 시작하고 그 뒤에 이진 숫자 $\{0,1\}$이 오는 경우를 가정합니다. 따라서 첫 번째 기호에 대해서는 삼진 코드를 사용하고 그 이후에는 이진 코드를 사용합니다. 다음 확률 분포에 대한 최적의 고유 복호 가능 코드(최소 기대 기호 수)를 제공하십시오.

$$
p=\left(\frac{16}{69}, \frac{15}{69}, \frac{12}{69}, \frac{10}{69}, \frac{8}{69}, \frac{8}{69}\right)
$$

5.44 허프만. $p=\left(\frac{1}{100}, \frac{1}{100}, \ldots, \frac{1}{100}\right)$의 최적 이진 인코딩의 단어 길이를 찾으십시오.
5.45 무작위 20개의 질문. $X$가 $\{1,2, \ldots, m\}$ 위에서 균등하게 분포한다고 가정합니다. 무작위 질문을 합니다: $X \in S_{1}$입니까? $X \in S_{2}$입니까?... 단 하나의 정수만 남을 때까지. $\{1,2, \ldots, m\}$의 모든 $2^{m}$ 부분 집합 $S$는 질문될 가능성이 동일합니다.
(a) 일반성을 잃지 않고, $X=1$이 무작위 객체라고 가정합니다. 객체 2가 $k$개의 질문에 대해 객체 1과 동일한 답변을 할 확률은 얼마입니까?
(b) 올바른 객체 1과 동일한 질문에 대한 답변을 가진 $\{2,3, \ldots, m\}$의 객체 수의 기대값은 얼마입니까?
(c) $n+\sqrt{n}$개의 무작위 질문을 한다고 가정합니다. 잘못된 객체가 답변과 일치하는 기대값은 얼마입니까?
(d) 마르코프 부등식 $\operatorname{Pr}\{X \geq t \mu\} \leq \frac{1}{t}$를 사용하여 오류 확률(하나 이상의 잘못된 객체가 남는 경우)이 $n \longrightarrow \infty$일 때 0으로 간다는 것을 보여주십시오.

# 역사적 노트

이 장의 자료에 대한 기초는 Shannon의 원본 논문 [469]에서 찾을 수 있으며, 여기서 Shannon은 소스 코딩
<!-- Page 184 -->
이 정리를 제시하고 코드의 간단한 예시를 들었습니다. 그는 Fano에게 귀속된 간단한 코드 구성 절차(문제 5.5.28에 설명됨)를 설명했습니다. 이 방법은 이제 Shannon-Fano 코드 구성 절차라고 불립니다.

고유하게 복호화 가능한 코드에 대한 Kraft 부등식은 McMillan [385]에 의해 처음 증명되었으며, 여기서 제시된 증명은 Karush [306]의 것입니다. Huffman 코딩 절차는 Huffman [283]에 의해 처음 제시되었고 최적임이 증명되었습니다.

최근 몇 년 동안 자기 기록과 같은 특정 응용 프로그램에 맞춰진 소스 코드를 설계하는 데 상당한 관심이 있었습니다. 이러한 경우, 출력 시퀀스가 특정 속성을 만족하도록 코드를 설계하는 것이 목표입니다. 이 문제에 대한 결과 중 일부는 Franaszek [219], Adler 외 [5], Marcus [370]에 의해 설명됩니다.

산술 코딩 절차는 Elias(미발표)가 개발한 Shannon-Fano 코드에 뿌리를 두고 있으며, 이는 Jelinek [297]에 의해 분석되었습니다. 본문에서 설명된 접두사 없는 코드 구성 절차는 Gilbert와 Moore [249]의 것입니다. Shannon-Fano-Elias 방법을 시퀀스로 확장하는 것은 Cover [120]의 열거 방법에 기반하며, Pasco [414]와 Rissanen [441]에 의해 유한 정밀도 산술로 설명되었습니다. Shannon 코드의 경쟁적 최적성은 Cover [125]에서 증명되었고 Feder [203]에 의해 Huffman 코드로 확장되었습니다. 공정한 동전 던지기에서 이산 분포를 생성하는 섹션 5.11은 Knuth와 Yao [317]의 연구를 따릅니다.
<!-- Page 185 -->
# 제 6 장

## 도박과 데이터 압축

정보 이론과 도박은 처음에는 관련이 없어 보일 수 있습니다. 그러나 우리는 경마에서의 투자 성장률과 경마의 엔트로피율 사이에 강력한 이중성이 존재함을 알게 될 것입니다. 실제로 성장률과 엔트로피율의 합은 상수입니다. 이를 증명하는 과정에서, 부가 정보의 재정적 가치가 경마와 부가 정보 간의 상호 정보와 같다고 주장할 것입니다. 경마는 16장에서 연구된 주식 시장 투자와 같은 특별한 경우입니다.

또한, 우리는 동일한 도박꾼 쌍을 사용하여 해당 시퀀스에서의 부의 성장률과 같은 양만큼 확률 변수 시퀀스를 압축하는 방법을 보여줄 것입니다. 마지막으로, 이러한 도박 기법을 사용하여 영어의 엔트로피율을 추정할 것입니다.

### 6.1 경마

$m$ 마리의 말이 경주에 참가한다고 가정합니다. $i$ 번째 말이 확률 $p_{i}$로 이긴다고 합시다. 만약 $i$ 번째 말이 이기면, 배당률은 $o_{i}$입니다 (즉, $i$ 번째 말에 1달러를 투자하면 $i$ 번째 말이 이길 경우 $o_{i}$ 달러를 받고, 질 경우 0달러를 받습니다).

배당률을 설명하는 두 가지 방법이 있습니다: $a$-for-1과 $b$-to-1. 첫 번째는 경주 전에 발생하는 교환을 의미합니다. 도박꾼은 경주 전에 1달러를 걸고 $a$-for-1 배당률에서 자신의 말이 이기면 경주 후에 $a$ 달러를 받게 되고, 그렇지 않으면 아무것도 받지 못합니다. 두 번째는 경주 후에 발생하는 교환을 의미합니다. $b$-to-1 배당률에서 도박꾼은 자신의 말이 지면 경주 후에 1달러를 지불하고, 자신의 말이 이기면 경주 후에 $b$ 달러를 받게 됩니다. 따라서 $b$-to-1 배당률에서의 베팅은 $b=a-1$일 경우 $a$-for-1 배당률에서의 베팅과 동일합니다. 예를 들어, 동전 던지기에서의 공정한 배당률은 2-for-1 또는 1-to-1이며, 이는 동등한 배당률로도 알려져 있습니다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 186 -->
모든 부를 경주마들에게 분배한다고 가정합니다. 여기서 $b_{i}$는 경주마 $i$에 투자된 부의 비율이며, $b_{i} \geq 0$이고 $\sum b_{i}=1$입니다. 그러면 경주마 $i$가 경주에서 이기면, 도박사는 경주마 $i$에 베팅한 부의 양에 $o_{i}$를 곱한 금액을 받게 됩니다. 다른 모든 베팅은 손실됩니다. 따라서 경주가 끝나면 도박사는 경주마 $i$가 이길 경우 부가 $b_{i} o_{i}$ 배가 되며, 이는 확률 $p_{i}$로 발생합니다. 표기상의 편의를 위해 이 장 전체에서 $b(i)$와 $b_{i}$를 상호 교환적으로 사용합니다.

경주가 끝났을 때의 부는 확률 변수이며, 도박사는 이 확률 변수의 값을 "최대화"하기를 원합니다. 최대 기대 수익률(즉, 최대 $\left.p_{i} o_{i}\right)$을 가진 경주마에 모든 것을 거는 것은 유혹적입니다. 하지만 이는 모든 돈을 잃을 수 있으므로 분명히 위험합니다.

이 경주에 대한 반복적인 도박을 고려하면 어느 정도 명확해집니다. 이제 도박사는 돈을 재투자할 수 있으므로, 그의 부는 각 경주의 수익의 곱이 됩니다. $S_{n}$을 $n$번의 경주 후 도박사의 부라고 합시다. 그러면

$$
S_{n}=\prod_{i=1}^{n} S\left(X_{i}\right)
$$

여기서 $S(X)=b(X) o(X)$는 경주마 $X$가 이길 때 도박사의 부가 곱해지는 요인입니다.

정의 경주 상대 수익률 $S(X)=b(X) o(X)$는 경주마 $X$가 경주에서 이길 경우 도박사의 부가 성장하는 요인입니다.

정의 경주마의 두 배 성장률은 다음과 같습니다.

$$
W(\mathbf{b}, \mathbf{p})=E(\log S(X))=\sum_{k=1}^{m} p_{k} \log b_{k} o_{k}
$$

두 배 성장률의 정의는 다음 정리에 의해 정당화됩니다.
정리 6.1.1 경주 결과 $X_{1}, X_{2}, \ldots$가 i.i.d. $\sim p(x)$라고 합시다. 그러면 베팅 전략 $\mathbf{b}$를 사용하는 도박사의 부는 $W(\mathbf{b}, \mathbf{p})$의 비율로 기하급수적으로 성장합니다. 즉,

$$
S_{n} \doteq 2^{n W(\mathbf{b}, \mathbf{p})}
$$
<!-- Page 187 -->
증명: 독립적인 확률 변수의 함수 또한 독립이며, 따라서 $\log S\left(X_{1}\right), \log S\left(X_{2}\right), \ldots$ 는 i.i.d. 입니다. 그런 다음, 대수의 법칙에 따라,

$$
\frac{1}{n} \log S_{n}=\frac{1}{n} \sum_{i=1}^{n} \log S\left(X_{i}\right) \rightarrow E(\log S(X)) \quad \text { 확률적으로. }
$$

따라서,

$$
S_{n} \doteq 2^{n W(\mathbf{b}, \mathbf{p})}
$$

이제 도박꾼의 자산이 $2^{n W(\mathbf{b}, \mathbf{p})}$ 로 증가하므로, 포트폴리오 $\mathbf{b}$ 의 모든 선택에 대해 지수 $W(\mathbf{b}, \mathbf{p})$ 를 최대화하고자 합니다.

정의 최적 두 배율 $W^{*}(\mathbf{p})$ 는 포트폴리오 $\mathbf{b}$ 의 모든 선택에 대한 최대 두 배율입니다:

$$
W^{*}(\mathbf{p})=\max _{\mathbf{b}} W(\mathbf{b}, \mathbf{p})=\max _{\mathbf{b}: b_{i} \geq 0, \sum_{i} b_{i}=1} \sum_{i=1}^{m} p_{i} \log b_{i} o_{i}
$$

제약 조건 $\sum b_{i}=1$ 하에서 $\mathbf{b}$ 의 함수로서 $W(\mathbf{b}, \mathbf{p})$ 를 최대화합니다. 라그랑주 승수법을 사용하여 함수를 작성하고 로그의 밑을 변경하면 (이는 최대화하는 $\mathbf{b}$ 에 영향을 미치지 않습니다), 다음과 같습니다.

$$
J(\mathbf{b})=\sum p_{i} \ln b_{i} o_{i}+\lambda \sum b_{i}
$$

이것을 $b_{i}$ 에 대해 미분하면 다음과 같습니다.

$$
\frac{\partial J}{\partial b_{i}}=\frac{p_{i}}{b_{i}}+\lambda, \quad i=1,2, \ldots, m
$$

최대값을 얻기 위해 편미분을 0으로 설정하면 다음과 같습니다.

$$
b_{i}=-\frac{p_{i}}{\lambda}
$$

이것을 제약 조건 $\sum b_{i}=1$ 에 대입하면 $\lambda=-1$ 이고 $b_{i}=p_{i}$ 가 됩니다. 따라서 $\mathbf{b}=\mathbf{p}$ 가 함수 $J(\mathbf{b})$ 의 정상점임을 결론지을 수 있습니다. 이것이 실제로 최대값임을 증명하는 것은 다소 번거롭습니다.
<!-- Page 188 -->
두 번째 도함수를 사용하지 않고, 많은 문제에 적용되는 방법인 추측 후 검증 방법을 사용합니다. 다음 정리에서 비례 도박 $\mathbf{b}=\mathbf{p}$가 최적임을 검증합니다. 비례 도박은 Kelly 도박으로 알려져 있습니다 [308].

정리 6.1.2 (비례 도박은 로그 최적이다) 최적의 두 배 성장률은 다음과 같이 주어집니다.

$$
W^{*}(\mathbf{p})=\sum p_{i} \log o_{i}-H(\mathbf{p})
$$

이는 비례 도박 방식 $\mathbf{b}^{*}=\mathbf{p}$에 의해 달성됩니다.
증명: 최대값이 명확한 형태로 $W(\mathbf{b}, \mathbf{p})$ 함수를 다시 작성합니다.

$$
\begin{aligned}
W(\mathbf{b}, \mathbf{p}) & =\sum p_{i} \log b_{i} o_{i} \\
& =\sum p_{i} \log \left(\frac{b_{i}}{p_{i}} p_{i} o_{i}\right) \\
& =\sum p_{i} \log o_{i}-H(\mathbf{p})-D(\mathbf{p} \| \mathbf{b}) \\
& \leq \sum p_{i} \log o_{i}-H(\mathbf{p})
\end{aligned}
$$

등호는 $\mathbf{p}=\mathbf{b}$일 때 성립합니다 (즉, 도박꾼이 각 말의 승률에 비례하여 베팅하는 경우).

예제 6.1.1 두 마리의 말이 있는 경우를 고려해 봅시다. 말 1은 확률 $p_{1}$로 이기고, 말 2는 확률 $p_{2}$로 이깁니다. 두 말 모두에 대해 동일한 배당률(2대1)이라고 가정합니다. 그러면 최적의 베팅은 비례 베팅입니다 (즉, $b_{1}=p_{1}, b_{2}=p_{2}$). 최적의 두 배 성장률은 $W^{*}(\mathbf{p})=$ $\sum p_{i} \log o_{i}-H(\mathbf{p})=1-H(\mathbf{p})$이며, 결과적인 자산은 이 비율로 무한대로 증가합니다.

$$
S_{n} \doteq 2^{n(1-H(\mathbf{p}))}
$$

따라서, 도박꾼이 자산을 재투자할 수 있고 현금으로 자산을 보유할 선택권이 없는 경우, i.i.d. 경마 시퀀스에 대해 비례 베팅이 성장률 최적임을 보여주었습니다.

배당률이 특정 분포에 대해 공정할 때 (즉, 트랙 수수료가 없고 $\sum \frac{1}{o_{i}}=1$인 경우) 특별한 경우를 고려합니다. 이 경우 $r_{i}=\frac{1}{o_{i}}$로 작성하며, 여기서 $r_{i}$는 확률 질량 함수로 해석될 수 있습니다.
<!-- Page 189 -->
말에 대한 것입니다. (이것은 북메이커가 승률을 추정한 것입니다.) 이 정의를 사용하면 두 배율을 다음과 같이 작성할 수 있습니다.

$$
\begin{aligned}
W(\mathbf{b}, \mathbf{p}) & =\sum p_{i} \log b_{i} o_{i} \\
& =\sum p_{i} \log \left(\frac{b_{i}}{p_{i}} \frac{p_{i}}{r_{i}}\right) \\
& =D(\mathbf{p} \| \mathbf{r})-D(\mathbf{p} \| \mathbf{b})
\end{aligned}
$$

이 방정식은 상대 엔트로피 거리에 대한 또 다른 해석을 제공합니다. 두 배율은 북메이커의 추정치가 실제 분포로부터의 거리와 도박꾼의 추정치가 실제 분포로부터의 거리 사이의 차이입니다. 따라서 도박꾼은 그의 추정치($\mathbf{b}$로 표현됨)가 북메이커의 추정치보다 나을 경우에만 돈을 벌 수 있습니다.

더욱 특별한 경우는 각 말에 대해 배당률이 1 대 m인 경우입니다. 이 경우 배당률은 균일 분포에 대해 공정하며 최적의 두 배율은 다음과 같습니다.

$$
W^{*}(\mathbf{p})=D\left(\mathbf{p} \| \frac{1}{m}\right)=\log m-H(\mathbf{p})
$$

이 경우 데이터 압축과 두 배율 간의 이중성을 명확하게 볼 수 있습니다.

정리 6.1.3 (보존 정리) 균일한 공정 배당률의 경우,

$$
W^{*}(\mathbf{p})+H(\mathbf{p})=\log m
$$

따라서 두 배율과 엔트로피율의 합은 상수입니다.
엔트로피 감소의 모든 비트는 도박꾼의 부를 두 배로 만듭니다. 낮은 엔트로피 경주가 가장 수익성이 높습니다.

위의 분석에서 우리는 도박꾼이 전액 투자되었다고 가정했습니다. 일반적으로 도박꾼에게 일부 부를 현금으로 보유할 수 있는 옵션을 제공해야 합니다. $b(0)$을 현금으로 보유한 부의 비율로, $b(1), b(2), \ldots, b(m)$을 다양한 말에 건 비율로 둡니다. 그러면 경주가 끝난 후 최종 부와 초기 부의 비율(부 상대값)은 다음과 같습니다.

$$
S(X)=b(0)+b(X) o(X)
$$

이제 최적 전략은 배당률에 따라 달라질 수 있으며 비례 도박의 단순한 형태를 반드시 갖지는 않을 것입니다. 세 가지 하위 사례를 구별합니다.
<!-- Page 190 -->
1. 공정한 배당률 (분포에 대한): $\sum \frac{1}{o_{i}}=1$. 공정한 배당률의 경우, 현금을 보유하는 선택은 분석을 변경하지 않습니다. 이는 $i$번째 말에 $b_{i}=\frac{1}{o_{i}}$로 베팅함으로써 현금을 보유하는 효과를 얻을 수 있기 때문입니다. 그러면 어느 말이 이기든 상관없이 $S(X)=1$이 됩니다. 따라서 도박꾼이 현금으로 따로 보관하는 모든 돈은 말들에게 똑같이 분배될 수 있으며, 도박꾼이 모든 돈을 투자해야 한다는 가정은 분석을 변경하지 않습니다. 비례 베팅이 최적입니다.
2. 초공정 배당률: $\sum \frac{1}{o_{i}}<1$. 이 경우 배당률은 공정한 배당률보다 훨씬 좋으므로, 현금으로 남겨두기보다는 항상 모든 재산을 경주에 투자하고 싶을 것입니다. 이 경주에서도 최적 전략은 비례 베팅입니다. 그러나 $c=$ $1 / \sum \frac{1}{c_{i}}$로 $b_{i}=c \frac{1}{o_{i}}$를 선택하여 네덜란드 북을 형성하도록 b를 선택하는 것이 가능하며, 어느 말이 이기든 상관없이 $o_{i} b_{i}=c$를 얻습니다. 이 할당으로, 확률 1 (즉, 위험 없음)로 재산 $S(X)=1 / \sum \frac{1}{o_{i}}>1$을 갖게 됩니다. 말할 필요도 없이, 실제 생활에서는 이러한 배당률을 거의 찾을 수 없습니다. 덧붙여, 네덜란드 북은 위험이 없지만, 두 배율을 최적화하지는 않습니다.
3. 불공정 배당률: $\sum \frac{1}{o_{i}}>1$. 이것은 실제 생활을 더 잘 나타냅니다. 경마장의 주최자는 모든 베팅에서 수수료를 받습니다. 이 경우 일부 돈만 베팅하고 나머지는 현금으로 남겨두는 것이 최적입니다. 비례 도박은 더 이상 로그 최적이 아닙니다. 최적 전략에 대한 매개변수 형식은 Kuhn-Tucker 조건(문제 6.6.2)을 사용하여 찾을 수 있으며, 간단한 "물 채우기" 해석을 가지고 있습니다.

# 6.2 도박 및 부가 정보

도박꾼이 도박 결과와 관련된 정보를 가지고 있다고 가정해 봅시다. 예를 들어, 도박꾼은 이전 경주의 말 성능에 대한 정보를 가지고 있을 수 있습니다. 이 부가 정보의 가치는 얼마입니까?

이러한 정보의 재정적 가치에 대한 한 가지 정의는 해당 정보로 인해 발생하는 부의 증가입니다. 섹션 6.1에서 설명한 설정에서 정보 가치의 척도는 해당 정보로 인한 두 배율의 증가입니다. 이제 상호 정보와 두 배율 증가 사이의 연결을 도출할 것입니다.

개념을 형식화하기 위해 말 $X \in\{1,2, \ldots, m\}$가 확률 $p(x)$로 경주에서 이기고 1에 대해 $o(x)$의 배당률을 지급한다고 가정해 봅시다. $(X, Y)$가 결합 확률 질량 함수 $p(x, y)$를 갖는다고 가정해 봅시다. $b(x \mid y) \geq 0, \sum_{x} b(x \mid y)=1$을 부가 정보에 의존하는 임의의 조건부 베팅 전략이라고 가정해 봅시다.
<!-- Page 191 -->
$Y$이며, 여기서 $b(x \mid y)$는 $y$가 관찰되었을 때 말 $x$에 거는 부의 비율입니다. 이전과 마찬가지로, $b(x) \geq 0, \sum b(x)=1$은 무조건적인 베팅 방식을 나타냅니다.

무조건적인 배수율과 조건부 배수율을 다음과 같이 정의합니다.

$$
\begin{gathered}
W^{*}(X)=\max _{\mathbf{b}(x)} \sum_{x} p(x) \log b(x) o(x) \\
W^{*}(X \mid Y)=\max _{\mathbf{b}(x \mid y)} \sum_{x, y} p(x, y) \log b(x \mid y) o(x)
\end{gathered}
$$

그리고 다음과 같이 정의합니다.

$$
\Delta W=W^{*}(X \mid Y)-W^{*}(X)
$$

$\left(X_{i}, Y_{i}\right)$가 독립적으로 동일하게 분포된 경마의 경우, 부는 부가 정보가 없을 때 $2^{n W^{*}(X)}$와 같이 성장하고, 부가 정보가 있을 때 $2^{n W^{*}(X \mid Y)}$와 같이 성장함을 관찰합니다.

정리 6.2.1 경마 $X$에 대한 부가 정보 $Y$로 인한 배수율 증가 $\Delta W$는 다음과 같습니다.

$$
\Delta W=I(X ; Y)
$$

증명: 부가 정보가 있을 때, $W^{*}(X \mid Y)$의 최대값은 조건부 비례 도박 [즉, $b^{*}(x \mid y)=p(x \mid y)$]에 의해 달성됩니다. 따라서,

$$
\begin{aligned}
W^{*}(X \mid Y)=\max _{\mathbf{b}(x \mid y)} E[\log S] & =\max _{\mathbf{b}(x \mid y)} \sum p(x, y) \log o(x) b(x \mid y) \\
& =\sum p(x, y) \log o(x) p(x \mid y) \\
& =\sum p(x) \log o(x)-H(X \mid Y)
\end{aligned}
$$

부가 정보가 없을 때, 최적의 배수율은 다음과 같습니다.

$$
W^{*}(X)=\sum p(x) \log o(x)-H(X)
$$

따라서, 부가 정보 $Y$의 존재로 인한 배수율 증가는 다음과 같습니다.

$$
\Delta W=W^{*}(X \mid Y)-W^{*}(X)=H(X)-H(X \mid Y)=I(X ; Y)
$$
<!-- Page 192 -->
따라서, 두 배율의 증가는 부가 정보와 경마 사이의 상호 정보와 같습니다. 놀랍지 않게도, 독립적인 부가 정보는 두 배율을 증가시키지 않습니다.

이 관계는 일반 주식 시장(16장)에도 확장될 수 있습니다. 그러나 이 경우, 부등식 $\Delta W \leq I$만을 보일 수 있으며, 시장이 경마인 경우에만 등식이 성립합니다.

# 6.3 종속 경마 및 엔트로피율

경마에 대한 부가 정보의 가장 일반적인 예는 말들의 과거 성적입니다. 경마가 독립적이라면, 이 정보는 쓸모가 없을 것입니다. 경마 간에 종속성이 있다고 가정하면, 다음 경주의 전략을 결정하기 위해 이전 경주의 결과를 사용할 수 있다면 유효 두 배율을 계산할 수 있습니다.

경마 결과의 시퀀스 $\left\{X_{k}\right\}$가 확률 과정이라고 가정합니다. 각 경주의 전략이 이전 경주의 결과에 의존한다고 가정합니다. 이 경우, 균등한 공정한 배당률에 대한 최적 두 배율은 다음과 같습니다.

$$
\begin{aligned}
& W^{*}\left(X_{k} \mid X_{k-1}, X_{k-2}, \ldots, X_{1}\right) \\
& \quad=E\left[\max _{\mathbf{b}\left(\cdot \mid X_{k-1}, X_{k-2}, \ldots, X_{1}\right)} E\left[\log S\left(X_{k}\right) \mid X_{k-1}, X_{k-2}, \ldots, X_{1}\right]\right] \\
& \quad=\log m-H\left(X_{k} \mid X_{k-1}, X_{k-2}, \ldots, X_{1}\right)
\end{aligned}
$$

이는 $b^{*}\left(x_{k} \mid x_{k-1}, \ldots, x_{1}\right)=p\left(x_{k} \mid x_{k-1}, \ldots, x_{1}\right)$에 의해 달성됩니다.
$n$번의 경주가 끝날 때, 도박꾼의 자산은 다음과 같습니다.

$$
S_{n}=\prod_{i=1}^{n} S\left(X_{i}\right)
$$

그리고 성장률의 지수(1배당률에 대한 $m$을 가정)는 다음과 같습니다.

$$
\begin{aligned}
\frac{1}{n} E \log S_{n} & =\frac{1}{n} \sum E \log S\left(X_{i}\right) \\
& =\frac{1}{n} \sum\left(\log m-H\left(X_{i} \mid X_{i-1}, X_{i-2}, \ldots, X_{1}\right)\right) \\
& =\log m-\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n}
\end{aligned}
$$
<!-- Page 193 -->
수량 $\frac{1}{n} H\left(X_{1}, X_{2}, \ldots, X_{n}\right)$는 경주당 평균 엔트로피입니다. 정상 과정의 엔트로피율 $H(\mathcal{X})$에 대해, (6.35)의 극한은 다음과 같습니다.

$$
\lim _{n \rightarrow \infty} \frac{1}{n} E \log S_{n}+H(\mathcal{X})=\log m
$$

다시 말해, 엔트로피율과 두 배 증가율이 상수라는 결과를 얻습니다.

만약 과정이 에르고딕(ergodic)이라면 (6.36)의 기댓값은 제거될 수 있습니다. 에르고딕 말 경주 시퀀스에 대해 16장에서 다음과 같이 보일 것입니다.

$$
S_{n} \doteq 2^{n W} \quad \text { 확률 } 1 \text{ 로}
$$

여기서 $W=\log m-H(\mathcal{X})$이고

$$
H(\mathcal{X})=\lim \frac{1}{n} H\left(X_{1}, X_{2}, \ldots, X_{n}\right)
$$

예제 6.3.1 (빨강과 검정) 이 예제에서는 카드가 말로 대체되며, 시간이 지남에 따라 결과가 더 예측 가능해집니다. 26장의 빨간 카드와 26장의 검은 카드로 이루어진 덱에서 다음 카드의 색깔에 베팅하는 경우를 고려해 봅시다. 덱을 통과하면서 다음 카드가 빨간색인지 검은색인지에 베팅합니다. 또한 게임이 2 대 1로 지급된다고 가정합니다. 즉, 도박꾼은 올바른 색깔에 베팅한 금액의 두 배를 돌려받습니다. 빨간색과 검은색의 확률이 같을 경우 이는 공정한 배당률입니다.

두 가지 대안적인 베팅 방식을 고려합니다.

1. 순차적으로 베팅하는 경우, 다음 카드의 조건부 확률을 계산하고 비례적으로 베팅할 수 있습니다. 따라서 첫 번째 카드에 대해 (빨강, 검정)에 $\left(\frac{1}{2}, \frac{1}{2}\right)$를 베팅하고, 첫 번째 카드가 검은색이었다면 두 번째 카드에 $\left(\frac{26}{51}, \frac{25}{51}\right)$를 베팅하는 식입니다.
2. 대안적으로, 52장의 카드 전체 시퀀스에 한 번에 베팅할 수 있습니다. 26장의 빨간색과 26장의 검은색 카드로 이루어진 $\binom{52}{26}$개의 가능한 시퀀스가 있으며, 이들은 모두 동일하게 가능성이 있습니다. 따라서 비례 베팅은 이러한 각 시퀀스에 금액의 $1 /\binom{52}{26}$을 걸고 각 베팅을 "연속적으로" 진행하는 것을 의미합니다.

이러한 절차들이 동등하다고 주장할 것입니다. 예를 들어, 52장의 카드 시퀀스의 절반은 빨간색으로 시작하며, 따라서 두 번째 방식에서 빨간색으로 시작하는 시퀀스에 베팅된 금액의 비율도 절반이 되어 첫 번째 방식에서 사용된 비율과 일치합니다. 일반적으로 가능한 결과 각각에 금액의 $1 /\binom{52}{26}$을 베팅하는 것이 각 단계에서

<!-- Page 194 -->
stage에서 해당 stage의 빨간색과 검은색 확률에 비례하는 베팅을 합니다. 각 가능한 출력 시퀀스에 대해 자산의 $1 /\binom{52}{26}$을 베팅하고, 시퀀스에 대한 베팅은 관찰된 시퀀스에서는 $2^{52}$배, 다른 모든 시퀀스에서는 0배로 자산을 증가시키므로 결과 자산은 다음과 같습니다.

$$
S_{52}^{*}=\frac{2^{52}}{\binom{52}{26}}=9.08
$$

매우 흥미롭게도 수익률은 실제 시퀀스에 의존하지 않습니다. 이는 수익률이 모든 시퀀스에 대해 동일하다는 점에서 AEP와 유사합니다. 이러한 의미에서 모든 시퀀스는 일반적입니다.

# 6.4 영어의 엔트로피

정보원의 중요한 예시는 영어 텍스트입니다. 영어가 정상적인 에르고딕 과정인지 즉시 명확하지는 않습니다. 아마도 그렇지 않을 것입니다! 그럼에도 불구하고 우리는 영어의 엔트로피율에 관심을 가질 것입니다. 영어에 대한 다양한 확률적 근사를 논의합니다. 모델의 복잡성을 증가시킴에 따라 영어처럼 보이는 텍스트를 생성할 수 있습니다. 확률적 모델은 영어 텍스트를 압축하는 데 사용될 수 있습니다. 확률적 근사가 좋을수록 압축이 더 잘 됩니다.

논의를 위해 영어의 알파벳은 26개의 글자와 공백 문자로 구성된다고 가정합니다. 따라서 구두점과 대소문자 구분을 무시합니다. 텍스트 샘플에서 수집한 경험적 분포를 사용하여 영어 모델을 구성합니다. 영어에서 글자의 빈도는 균일하지 않습니다. 가장 흔한 글자인 E는 약 $13\%$의 빈도를 가지며, 가장 드문 글자인 Q와 Z는 약 $0.1\%$의 빈도로 나타납니다. 글자 E는 매우 흔하므로 어떤 길이의 문장이라도 이 글자를 포함하지 않는 경우는 드뭅니다. [이에 대한 놀라운 예외는 Ernest Vincent Wright(Lightyear Press, Boston, 1997; 원래 출판 1939년)의 267페이지 소설 Gadsby로, 저자가 의도적으로 글자 E를 사용하지 않았습니다.]

글자 쌍의 빈도 또한 균일하지 않습니다. 예를 들어, 글자 Q는 항상 U 뒤에 옵니다. 가장 빈번한 쌍은 TH로, 일반적으로 약 $3.7\%$의 빈도로 발생합니다. 글자 쌍의 빈도를 사용하여 다른 글자 뒤에 오는 글자의 확률을 추정할 수 있습니다. 이러한 방식으로 진행하면 더 높은 순서의 조건부 확률을 추정하고 언어에 대한 더 복잡한 모델을 구축할 수도 있습니다. 그러나 곧 데이터가 부족해집니다. 예를 들어, 3차 마르코프 근사를 구축하려면 다음의 값을 추정해야 합니다.
<!-- Page 195 -->
$p\left(x_{i} \mid x_{i-1}, x_{i-2}, x_{i-3}\right)$입니다. 이 표에는 $27^{4}=531,441$개의 항목이 있으며, 이러한 확률을 정확하게 추정하기 위해서는 수백만 개의 문자를 처리해야 합니다.

조건부 확률 추정치를 사용하여 이러한 분포에 따라 추출된 무작위 샘플을 생성할 수 있습니다 (난수 생성기 사용). 하지만 텍스트 샘플 (예: 책)을 사용하여 무작위성을 시뮬레이션하는 더 간단한 방법이 있습니다. 예를 들어, 두 번째 순서 모델을 구성하려면 책을 무작위로 열고 페이지에서 무작위로 문자를 선택합니다. 이것이 첫 번째 문자가 됩니다. 다음 문자의 경우, 다시 책을 무작위로 열고 임의의 지점에서 시작하여 첫 번째 문자를 다시 만날 때까지 읽습니다. 그런 다음 그 다음 문자를 두 번째 문자로 취합니다. 이 과정을 진행하면서 다른 페이지를 열고 두 번째 문자를 검색한 다음 그 다음 문자를 세 번째 문자로 취합니다. 이와 같이 진행하면 영어 텍스트의 두 번째 순서 통계를 시뮬레이션하는 텍스트를 생성할 수 있습니다.

다음은 Shannon의 원본 논문 [472]에서 발췌한 영어의 마르코프 근사 예시입니다:

1. 영차 근사. (기호는 독립적이며 균등하게 확률적입니다.)

XFOML RXKHRJFFJUJ ZLPWCFWKCYJ
FFJEYVKCQSGXYD QPAAMKBZAACIBZLHJQD
2. 일차 근사. (기호는 독립적입니다. 문자의 빈도는 영어 텍스트와 일치합니다.)

OCRO HLI RGWR NMIELWIS EU LL NBNESEBYA TH EEI ALHENHTTPA OOBTTVA NAH BRL
3. 이차 근사. (문자 쌍의 빈도는 영어 텍스트와 일치합니다.)

ON IE ANTSOUTINYS ARE T INCTORE ST BE S DEAMY ACHIN D ILONASIVE TUCOOWE AT TEASONARE FUSO TIZIN ANDY TOBE SEACE CTISBE
4. 삼차 근사. (문자 삼중항의 빈도는 영어 텍스트와 일치합니다.)

IN NO IST LAT WHEY CRATICT FROURE BERS GROCID PONDENOME OF DEMONSTURES OF THE REPTAGIN IS REGOACTIONA OF CRE
<!-- Page 196 -->
5. 4차 근사. (네 글자의 빈도가 영어 텍스트와 일치합니다. 각 글자는 이전 세 글자에 의존합니다. 이 문장은 Lucky의 책, Silicon Dreams [366]에서 발췌했습니다.)

THE GENERATED JOB PROVIDUAL BETTER TRAND THE DISPLAYED CODE, ABOVERY UPONDULTS WELL THE CODERST IN THESTICAL IT DO HOCK BOTHE MERG. (INSTATES CONS ERATION. NEVER ANY OF PUBLE AND TO THEORY. EVENTIAL CALLEGAND TO ELAST BENERATED IN WITH PIES AS IS WITH THE )

문자 모델을 계속하는 대신, 단어 모델로 넘어갑니다.
6. 1차 단어 모델. (단어는 독립적으로 선택되지만 영어와 같은 빈도를 가집니다.)

REPRESENTING AND SPEEDILY IS AN GOOD APT OR COME CAN DIFFERENT NATURAL HERE HE THE A IN CAME THE TO OF TO EXPERT GRAY COME TO FURNISHES THE LINE MESSAGE HAD BE THESE.
7. 2차 단어 모델. (단어 전환 확률이 영어 텍스트와 일치합니다.)

THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER THAT THE CHARACTER OF THIS POINT IS THEREFORE ANOTHER METHOD FOR THE LETTERS THAT THE TIME OF WHO EVER TOLD THE PROBLEM FOR AN UNEXPECTED

근사가 영어와 유사해짐에 따라 점점 더 가까워집니다. 예를 들어, 마지막 근사의 긴 구문은 실제 영어 문장에서 쉽게 발생할 수 있었습니다. 더 복잡한 모델을 사용하면 매우 좋은 근사를 얻을 수 있을 것 같습니다. 이러한 근사는 영어의 엔트로피를 추정하는 데 사용될 수 있습니다. 예를 들어, 0차 모델의 엔트로피는 글자당 $\log 27=4.76$ 비트입니다. 모델의 복잡성을 증가시킬수록 영어의 구조를 더 많이 파악하게 되고, 다음 글자의 조건부 불확실성이 감소합니다. 1차 모델은 글자당 4.03 비트의 엔트로피 추정치를 제공하는 반면, 4차 모델은 글자당 2.8 비트의 추정치를 제공합니다. 그러나 4차 모델조차도 영어의 모든 구조를 파악하지는 못합니다. 섹션 6.6에서는 영어의 엔트로피를 추정하는 대안적인 방법을 설명합니다.

영어의 분포는 암호화된 영어 텍스트를 해독하는 데 유용합니다. 예를 들어, 간단한 치환 암호(각 문자가 대체되는 경우)
<!-- Page 197 -->
(다른 문자로) 가장 빈번한 문자를 찾고 그것이 E에 대한 대체라고 추측함으로써 해결할 수 있으며, 계속해서 그렇게 할 수 있습니다. 영어의 중복성은 다른 문자가 해독된 후 누락된 문자를 채우는 데 사용될 수 있습니다. 예를 들어,

TH_R_ _S _NLY _N_ W_Y T_ F_LL _N TH_ V_W_LS _N TH_S S_NT_NC_.
정보 이론에 대한 Shannon의 원래 연구에 대한 영감의 일부는 제2차 세계 대전 중 암호 해독 작업에서 비롯되었습니다. 언어의 엔트로피와 암호 해독의 수학적 이론은 Shannon [481]에 자세히 설명되어 있습니다.

언어의 확률적 모델은 일부 음성 인식 시스템에서도 중요한 역할을 합니다. 일반적으로 사용되는 모델은 삼중항(2차 Markov) 단어 모델로, 앞선 두 단어가 주어졌을 때 다음 단어의 확률을 추정합니다. 음성 신호의 정보는 모델과 결합되어 관찰된 음성을 생성했을 가능성이 가장 높은 단어의 추정치를 생성합니다. 확률적 모델은 영어와 같은 자연어를 지배하는 복잡한 문법 규칙을 명시적으로 통합하지 않더라도 음성 인식에서 놀랍도록 잘 작동합니다.

이 섹션의 기법을 적용하여 음성 및 이미지와 같은 다른 정보 소스의 엔트로피율을 추정할 수 있습니다. 이러한 문제에 대한 흥미로운 비기술적 소개는 Lucky [366]의 책에서 찾을 수 있습니다.

# 6.5 데이터 압축 및 도박

이제 도박과 데이터 압축 사이의 직접적인 연결을 보여줌으로써, 좋은 도박꾼은 또한 좋은 데이터 압축기임을 보여줄 것입니다. 도박꾼이 많은 돈을 버는 모든 시퀀스는 큰 요인으로 압축될 수 있는 시퀀스이기도 합니다. 도박꾼을 데이터 압축기로 사용하는 아이디어는 도박꾼의 베팅이 데이터의 확률 분포에 대한 그의 추정치로 간주될 수 있다는 사실에 기반합니다. 좋은 도박꾼은 확률 분포를 잘 추정할 것입니다. 우리는 분포에 대한 이 추정치를 사용하여 산술 코딩(13.3절)을 수행할 수 있습니다. 이것이 아래 설명된 방식의 본질적인 아이디어입니다.

도박꾼에게는 데이터 압축을 위해 사용될 기계적으로 동일한 쌍둥이가 있다고 가정합니다. 동일한 쌍둥이는 원래 도박꾼과 마찬가지로 가능한 결과 시퀀스에 동일한 베팅을 할 것이며 (따라서 동일한 금액의 돈을 벌 것입니다). 누적 금액
<!-- Page 198 -->
주어진 시퀀스보다 사전순으로 앞에 오는 모든 시퀀스에서 도박사가 벌어들인 금액이 해당 시퀀스의 코드로 사용될 것입니다. 디코더는 동일한 쌍둥이를 사용하여 모든 시퀀스에 대해 도박을 하고, 동일한 누적 금액이 벌어지는 시퀀스를 찾을 것입니다. 이 시퀀스가 디코딩된 시퀀스로 선택될 것입니다.

압축하고자 하는 랜덤 변수 시퀀스를 $X_{1}, X_{2}, \ldots, X_{n}$이라고 하겠습니다. 일반성을 잃지 않고, 랜덤 변수가 이진이라고 가정하겠습니다. 이 시퀀스에 대한 도박은 다음과 같은 베팅 시퀀스로 정의됩니다.

$$
b\left(x_{k+1} \mid x_{1}, x_{2}, \ldots, x_{k}\right) \geq 0, \quad \sum_{x_{k+1}} b\left(x_{k+1} \mid x_{1}, x_{2}, \ldots, x_{k}\right)=1
$$

여기서 $b\left(x_{k+1} \mid x_{1}, x_{2}, \ldots, x_{k}\right)$는 관찰된 과거 $x_{1}, x_{2}, \ldots, x_{k}$가 주어졌을 때 $X_{k+1}=x_{k+1}$ 사건에 대해 시간 $k$에 베팅된 금액의 비율입니다. 베팅은 균일한 배당률(2대1)로 지급됩니다. 따라서 시퀀스 종료 시의 자산 $S_{n}$은 다음과 같이 주어집니다.

$$
\begin{aligned}
S_{n} & =2^{n} \prod_{k=1}^{n} b\left(x_{k} \mid x_{1}, \ldots, x_{k-1}\right) \\
& =2^{n} b\left(x_{1}, x_{2}, \ldots, x_{n}\right)
\end{aligned}
$$

여기서

$$
b\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\prod_{k=1}^{n} b\left(x_{k} \mid x_{k-1}, \ldots, x_{1}\right)
$$

따라서 순차적 도박은 $2^{n}$개의 가능한 시퀀스에 대한 확률(또는 베팅) $b\left(x_{1}, x_{2}, \ldots, x_{n}\right) \geq 0, \sum_{x_{1}, \ldots, x_{n}} b\left(x_{1}, \ldots, x_{n}\right)=1$을 할당하는 것으로 간주될 수도 있습니다.

이 도박은 텍스트 시퀀스의 실제 확률 추정치($\hat{p}\left(x_{1}, \ldots, x_{n}\right)=S_{n} / 2^{n}$)와 해당 시퀀스가 추출된 분포의 엔트로피 추정치($\left[\hat{H}=-\frac{1}{n} \log \hat{p}\right]$)를 모두 도출합니다. 이제 높은 자산 $S_{n}$ 값이 높은 데이터 압축으로 이어진다는 것을 보여주고자 합니다. 구체적으로, 해당 텍스트가 자산 $S_{n}$을 생성하는 경우, 자연스럽게 연관된 결정론적 데이터 압축 방식에서 $\log S_{n}$ 비트를 절약할 수 있다고 주장합니다. 또한 도박이 로그 최적이면 데이터 압축이 Shannon limit $H$에 도달한다고 주장합니다.
<!-- Page 199 -->
다음은 텍스트 $\mathbf{x}=x_{1} x_{2} \cdots x_{n} \in\{0,1\}^{n}$를 $\{0,1\}$로 이루어진 코드 시퀀스 $c_{1} c_{2} \cdots c_{k}$로 매핑하는 데이터 압축 알고리즘을 고려합니다. 압축기와 압축 해제기 모두 $n$을 알고 있습니다. $2^{n}$개의 텍스트 시퀀스를 사전순으로 배열합니다. 예를 들어, $0100101<0101101$입니다. 인코더는 시퀀스 $x^{n}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$를 관찰합니다. 그런 다음 모든 시퀀스 $x^{\prime}(n) \leq x(n)$에 대한 자신의 자산 $S_{n}\left(x^{\prime}(n)\right)$이 얼마였을지 계산하고 $F(x(n))=\sum_{x^{\prime}(n) \leq x(n)} 2^{-n} S_{n}\left(x^{\prime}(n)\right)$를 계산합니다. 명백히 $F(x(n)) \in[0,1]$입니다. $k=$ $\lceil n-\log S_{n}(x(n))\rceil$라고 합시다. 이제 $F(x(n))$를 $k$자리 정확도의 이진 소수로 표현합니다: $\lfloor F(x(n))\rfloor=. c_{1} c_{2} \cdots c_{k}$. 시퀀스 $c(k)=\left(c_{1}, c_{2}, \ldots, c_{k}\right)$가 디코더로 전송됩니다.

디코더 쌍은 $2^{n}$개의 모든 시퀀스 $x^{\prime}(n)$와 관련된 정확한 값 $S\left(x^{\prime}(n)\right)$를 계산할 수 있습니다. 따라서 임의의 시퀀스 $x(n)$까지의 $2^{-n} S\left(x^{\prime}(n)\right)$의 누적 합을 알고 있습니다. 누적 합이 $. c(k)$를 처음 초과하는 지점까지 이 합을 끈기 있게 계산합니다. 누적 합이 구간 $\left[. c_{1} \cdots c_{k}, . c_{1} \ldots c_{k}+(1 / 2)^{k}\right]$에 속하는 첫 번째 시퀀스 $x(n)$는 고유하게 정의되며, $S(x(n)) / 2^{n}$의 크기는 이 시퀀스가 정확히 인코딩된 $x(n)$임을 보장합니다.

따라서 쌍은 $x(n)$를 고유하게 복구합니다. 필요한 비트 수는 $k=\lceil n-\log S(x(n))\rceil$입니다. 절약된 비트 수는 $n-k=$ $\lfloor\log S(x(n))\rfloor$입니다. 비례 도박의 경우 $S(x(n))=2^{n} p(x(n))$입니다. 따라서 기대 비트 수는 $E k=\sum p(x(n))\lceil-\log p(x(n))\rceil \leq$ $H\left(X_{1}, \ldots, X_{n}\right)+1$입니다.

베팅 연산이 결정론적이고 인코더와 디코더 모두가 알고 있다면, $x_{1}, \ldots, x_{n}$를 인코딩하는 데 필요한 비트 수는 $n-\log S_{n}+1$보다 작다는 것을 알 수 있습니다. 또한 $p(x)$를 알고 비례 도박을 사용한다면, 설명 길이 기대값은 $E\left(n-\log S_{n}\right) \leq H\left(X_{1}, \ldots, X_{n}\right)+1$입니다. 따라서 도박 결과는 주어진 인간 인코더-디코더 동일 쌍에 의해 달성되었을 데이터 압축과 정확히 일치합니다.

도박사를 사용하는 데이터 압축 방식은 실제 분포 대신 분포 $b\left(x_{1}, x_{2}, \ldots, x_{n}\right)$를 사용하는 산술 코딩(13.3절)의 아이디어와 유사합니다. 위의 절차는 도박과 데이터 압축 간의 이중성을 보여줍니다. 둘 다 실제 분포의 추정을 포함합니다. 추정이 좋을수록 도박사의 자산 성장률이 높아지고 데이터 압축이 더 좋아집니다.

# 6.6 영어의 엔트로피에 대한 도박 추정

이제 인간 도박사를 사용하여 확률을 추정함으로써 영어의 엔트로피율을 추정합니다. 영어는 27개의 문자로 구성된다고 가정합니다.
<!-- Page 200 -->
(26개의 문자와 공백 기호). 따라서 구두점과 대소문자는 무시합니다. 영어의 엔트로피를 추정하기 위해 두 가지 다른 접근 방식이 제안되었습니다.

1.  **섀넌 추측 게임(Shannon guessing game)**. 이 접근 방식에서는 인간 피험자에게 영어 텍스트 샘플을 제공하고 다음 문자를 추측하도록 요청합니다. 최적의 피험자는 다음 문자의 확률을 추정하고 가장 확률이 높은 문자를 먼저 추측한 다음, 두 번째로 확률이 높은 문자를 추측하는 식으로 진행합니다. 실험자는 다음 문자를 추측하는 데 필요한 추측 횟수를 기록합니다. 피험자는 상당히 큰 텍스트 샘플을 통해 이와 같이 진행합니다. 그런 다음 다음 문자를 추측하는 데 필요한 추측 횟수의 경험적 빈도 분포를 계산할 수 있습니다. 많은 문자는 한 번의 추측만 필요하지만, 단어나 문장의 시작 부분에서는 많은 추측이 필요할 것입니다.
    이제 피험자를 과거 텍스트가 주어졌을 때 결정론적인 추측 선택을 하는 컴퓨터로 모델링할 수 있다고 가정해 보겠습니다. 그러면 동일한 기계와 추측 번호 시퀀스가 있다면 영어 텍스트를 재구성할 수 있습니다. 기계를 실행하고 어떤 위치에서든 추측 횟수가 $k$이면 기계의 $k$번째 추측을 다음 문자로 선택합니다. 따라서 추측 번호 시퀀스의 정보량은 영어 텍스트의 정보량과 같습니다. 추측 시퀀스의 엔트로피는 영어 텍스트의 엔트로피입니다. 샘플이 독립적이라고 가정하여 추측 시퀀스의 엔트로피를 제한할 수 있습니다. 따라서 추측 시퀀스의 엔트로피는 실험의 히스토그램 엔트로피보다 위에서 제한됩니다. 이 실험은 1950년 섀넌[482]에 의해 수행되었으며, 그는 영어 엔트로피에 대해 기호당 1.3 비트의 값을 얻었습니다.
2.  **도박 추정(Gambling estimate)**. 이 접근 방식에서는 인간 피험자에게 영어 텍스트 샘플에서 다음 문자에 대해 도박을 하도록 합니다. 이는 추측하는 경우보다 더 미세한 판단 등급을 허용합니다. 경마의 경우와 마찬가지로 최적의 베팅은 다음 문자의 조건부 확률에 비례합니다. 정답 문자에 대한 지급액은 27 대 1입니다.
    순차적 베팅은 전체 시퀀스에 베팅하는 것과 동등하므로, $n$개의 문자에 대한 지급액을 다음과 같이 쓸 수 있습니다.

$$
S_{n}=(27)^{n} b\left(X_{1}, X_{2}, \ldots, X_{n}\right)
$$

    따라서 $n$ 라운드의 베팅 후 기대 로그 자산은 다음과 같습니다.

$$
E \frac{1}{n} \log S_{n}=\log 27+\frac{1}{n} E \log b\left(X_{1}, X_{2}, \ldots, X_{n}\right)
$$
<!-- Page 201 -->
$$
\begin{aligned}
= & \log 27+\frac{1}{n} \sum_{x^{n}} p\left(x^{n}\right) \log b\left(x^{n}\right) \\
= & \log 27-\frac{1}{n} \sum_{x^{n}} p\left(x^{n}\right) \log \frac{p\left(x^{n}\right)}{b\left(x^{n}\right)} \\
& +\frac{1}{n} \sum_{x^{n}} p\left(x^{n}\right) \log p\left(x^{n}\right) \\
= & \log 27-\frac{1}{n} D\left(p\left(x^{n}\right) \| b\left(x^{n}\right)\right)-\frac{1}{n} H\left(X_{1}, X_{2}, \ldots, X_{n}\right) \\
\leq & \log 27-\frac{1}{n} H\left(X_{1}, X_{2}, \ldots, X_{n}\right) \\
\leq & \log 27-H(\mathcal{X})
\end{aligned}
$$

여기서 $H(\mathcal{X})$는 영어의 entropy rate입니다. 따라서 $\log 27-E \frac{1}{n} \log S_{n}$은 영어의 entropy rate에 대한 상한값입니다. 상한 추정치인 $\tilde{H}(\mathcal{X})=\log 27-\frac{1}{n} \log S_{n}$은 영어가 ergodic이고 도박꾼이 $b\left(x^{n}\right)=p\left(x^{n}\right)$을 사용하면 확률 1로 $H(\mathcal{X})$에 수렴합니다. 12명의 피험자와 Dumas Malone의 저서 Jefferson the Virginian (Little, Brown, Boston, 1948; Shannon이 사용한 출처)에서 발췌한 75개의 문자로 구성된 샘플을 이용한 실험 [131]은 영어의 entropy에 대해 문자당 1.34 비트의 추정치를 산출했습니다.

# 요약

Doubling rate. $W(\mathbf{b}, \mathbf{p})=E(\log S(X))=\sum_{k=1}^{m} p_{k} \log b_{k} o_{k}$.
Optimal doubling rate. $W^{*}(\mathbf{p})=\max _{\mathbf{b}} W(\mathbf{b}, \mathbf{p})$.
Proportional gambling은 log-optimal입니다.

$$
W^{*}(\mathbf{p})=\max _{\mathbf{b}} W(\mathbf{b}, \mathbf{p})=\sum p_{i} \log o_{i}-H(\mathbf{p})
$$

이는 $\mathbf{b}^{*}=\mathbf{p}$에 의해 달성됩니다.
Growth rate. Wealth grows as $S_{n} \doteq 2^{n W^{*}(\mathbf{p})}$.
<!-- Page 202 -->
보존 법칙. 균일한 공정한 배당률의 경우,

$$
H(\mathbf{p})+W^{*}(\mathbf{p})=\log m
$$

부수 정보. 경마 $X$에서, 부수 정보 $Y$로 인한 베팅 비율 증가 $\Delta W$는 다음과 같습니다.

$$
\Delta W=I(X ; Y)
$$

# 문제

6.1 경마. 세 마리의 말이 경주를 합니다. 한 도박꾼이 각 말에 대해 3 대 1의 배당률을 제시합니다. 이는 모든 말이 경주에서 이길 확률이 같다고 가정할 때 공정한 배당률입니다. 실제 승리 확률은 다음과 같이 알려져 있습니다.

$$
\mathbf{p}=\left(p_{1}, p_{2}, p_{3}\right)=\left(\frac{1}{2}, \frac{1}{4}, \frac{1}{4}\right)
$$

각 말에 투자하는 금액을 $\mathbf{b}=\left(b_{1}, b_{2}, b_{3}\right), b_{i} \geq 0, \sum b_{i}=1$이라고 합시다. 따라서 기대 로그 부는 다음과 같습니다.

$$
W(\mathbf{b})=\sum_{i=1}^{3} p_{i} \log 3 b_{i}
$$

(a) $\mathbf{b}$에 대해 이를 최대화하여 $\mathbf{b}^{*}$와 $W^{*}$를 찾으십시오. 따라서 반복되는 경마에서 달성된 부는 확률 1로 $2^{n W^{*}}$와 같이 무한대로 증가해야 합니다.
(b) 대신 가장 가능성 있는 우승마인 1번 말에 모든 돈을 걸면 결국 확률 1로 파산함을 보이십시오.
6.2 불공정한 배당률의 경마. 배당률이 좋지 않다면 (트랙 수수료 때문에), 도박꾼은 주머니에 돈을 남겨두고 싶을 수 있습니다. $b(0)$을 주머니에 있는 금액으로 하고, $b(1), b(2), \ldots, b(m)$을 각각 말 $1,2, \ldots, m$에 베팅한 금액으로 하며, 배당률은 $o(1), o(2), \ldots, o(m)$이고, 승리 확률은 $p(1), p(2), \ldots, p(m)$입니다. 따라서 결과적인 부는 $S(x)=b(0)+b(x) o(x)$이며, 확률은 $p(x), x=$ $1,2, \ldots, m$입니다.
(a) $\sum 1 / o(i)<1$인 경우 $E \log S$를 최대화하는 $\mathbf{b}^{*}$를 찾으십시오.
<!-- Page 203 -->
(b) $\sum 1 / o(i)>1$인 경우 $b^{*}$에 대해 논하십시오. (이 경우 쉬운 닫힌 형태의 해는 없지만, Kuhn-Tucker 조건의 적용으로 "물 채우기" 해가 결과로 나옵니다.)
6.3 카드. 26장의 빨간 카드와 26장의 검은 카드로 구성된 일반 카드 덱을 섞어서 한 번에 한 장씩 복원 없이 나눠줍니다. $X_{i}$를 $i$번째 카드의 색깔이라고 합시다.
(a) $H\left(X_{1}\right)$을 결정하십시오.
(b) $H\left(X_{2}\right)$을 결정하십시오.
(c) $H\left(X_{k} \mid X_{1}, X_{2}, \ldots, X_{k-1}\right)$는 증가합니까, 감소합니까?
(d) $H\left(X_{1}, X_{2}, \ldots, X_{52}\right)$을 결정하십시오.
6.4 도박. 문제 6.6.3의 카드 결과에 대해 순차적으로 도박을 한다고 가정합니다. 2대1의 배당률이 지급됩니다. 따라서 시간 $n$에서의 자산 $S_{n}$은 $S_{n}=2^{n} b\left(x_{1}, x_{2}, \ldots, x_{n}\right)$이며, 여기서 $b\left(x_{1}, x_{2}, \ldots, x_{n}\right)$은 $x_{1}, x_{2}, \ldots, x_{n}$에 베팅된 자산의 비율입니다. $\max _{b(\cdot)} E \log S_{52}$를 찾으십시오.
6.5 공공 배당률 초과 달성. 다음 승률을 가진 세 마리의 말 경주를 고려하십시오.

$$
\left(p_{1}, p_{2}, p_{3}\right)=\left(\frac{1}{2}, \frac{1}{4}, \frac{1}{4}\right)
$$

그리고 (잘못된) 분포에 대한 공정한 배당률

$$
\left(r_{1}, r_{2}, r_{3}\right)=\left(\frac{1}{4}, \frac{1}{4}, \frac{1}{2}\right)
$$

따라서 배당률은 다음과 같습니다.

$$
\left(o_{1}, o_{2}, o_{3}\right)=(4,4,2)
$$

(a) 경주의 엔트로피는 얼마입니까?
(b) 반복적인 플레이에서 복합 자산이 무한대로 증가하도록 하는 베팅 집합 $\left(b_{1}, b_{2}, b_{3}\right)$을 찾으십시오.
6.6 경마. 세 마리의 말이 있는 경주에서 승률은 $\mathbf{p}=$ $\left(p_{1}, p_{2}, p_{3}\right)$이고 배당률은 $\mathbf{o}=(1,1,1)$입니다. 도박꾼은 $\mathbf{b}=$ $\left(b_{1}, b_{2}, b_{3}\right), b_{i} \geq 0, \sum b_{i}=1$로 베팅하며, 여기서 $b_{i}$는 말 $i$에 베팅된 자산의 비율을 나타냅니다. 이 배당률은 매우 나쁩니다. 도박꾼은 이긴 말에 대한 돈을 돌려받고 다른 베팅은 잃습니다. 따라서 독립적인 도박으로 발생하는 시간 $n$에서의 자산 $S_{n}$은 지수적으로 0으로 수렴합니다.
(a) 지수를 찾으십시오.
<!-- Page 204 -->
(b) 최적의 도박 계획 $\mathbf{b}$ (즉, 지수를 최대화하는 베팅 $\mathbf{b}^{*}$)를 찾으십시오.
(c) 부분 (b)에서와 같이 $\mathbf{b}$가 선택되었다고 가정할 때, $S_{n}$이 가장 빠른 속도로 0으로 수렴하게 만드는 분포 $\mathbf{p}$는 무엇입니까?
6.7 경마. 네 마리의 말이 출전하는 경마를 고려하십시오. 각 말은 이길 경우 4 대 1로 지급된다고 가정합니다. 말들의 승률은 $\left\{\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8}\right\}$입니다. 만약 $\$ 100$으로 시작하여 장기적인 성장률을 최대화하기 위해 최적으로 베팅한다면, 각 말에 대한 최적 베팅액은 얼마입니까? 이 전략을 사용했을 때 20번의 경주 후 대략 얼마의 돈을 가지게 됩니까?
6.8 복권. 다음 분석은 여러 주에서 실시하는 복권 게임에 대한 대략적인 근사치입니다. 게임 플레이어는 $\$ 1$을 지불하고 1부터 8까지의 범위에서 숫자 하나를 선택해야 한다고 가정합니다. 매일 말일마다 주 복권 위원회는 같은 범위에서 균등하게 숫자를 선택합니다. 잭팟(즉, 그날 수집된 모든 돈)은 해당 번호를 선택한 모든 사람들에게 분배됩니다. 예를 들어, 오늘 100명이 게임에 참여했고, 그중 10명이 숫자 2를 선택했으며, 당일 추첨에서 2가 선택되었다면, 수집된 $\$ 100$은 10명에게 분배됩니다(즉, 2를 선택한 각 사람은 $\$ 10$을 받고, 나머지는 아무것도 받지 못합니다).
일반 인구는 균등하게 숫자를 선택하지 않습니다. 3과 7과 같은 숫자는 운이 좋다고 여겨져 4나 8보다 인기가 많습니다. 다양한 숫자 $1,2, \ldots, 8$을 선택하는 사람들의 비율을 $\left(f_{1}, f_{2}, \ldots, f_{8}\right)$라고 가정하고, 매일 $n$명이 게임에 참여한다고 가정합니다. 또한 $n$이 매우 크다고 가정하여, 한 사람의 선택이 어떤 숫자에 베팅하는 사람들의 비율에 영향을 미치지 않는다고 가정합니다.
(a) 장기 성장률을 최대화하기 위해 다양한 가능한 티켓에 돈을 분배하는 최적 전략은 무엇입니까? (분수 티켓을 구매할 수 없다는 사실은 무시하십시오.)
(b) 이 게임에서 달성할 수 있는 최적 성장률은 얼마입니까?
(c) 만약 $\left(f_{1}, f_{2}, \ldots, f_{8}\right)=\left(\frac{1}{8}, \frac{1}{8}, \frac{1}{4}, \frac{1}{16}, \frac{1}{16}, \frac{1}{16}, \frac{1}{4}, \frac{1}{16}\right)$이고, $\$ 1$로 시작한다면, 백만장자가 되기까지 얼마나 걸립니까?
6.9 경마. 경마에서 두 배 성장률을 최대화하는 데 관심이 있다고 가정합니다. $p_{1}, p_{2}, \ldots, p_{m}$이 $m$마리의 말의 승률을 나타낸다고 가정합니다. 배당률 $\left(o_{1}, o_{2}, \ldots, o_{m}\right)$은 배당률 $\left(o_{1}^{\prime}, o_{2}^{\prime}, \ldots, o_{m}^{\prime}\right)$보다 높은 두 배 성장률을 언제 제공합니까?
<!-- Page 205 -->
6.10 확률 추정 경주.
(a) 세 마리의 말이 경주합니다. 이들의 승리 확률은 $\left(\frac{1}{2}, \frac{1}{4}, \frac{1}{4}\right)$입니다. 배당률은 각각 4대1, 3대1, 3대1입니다. $W^{*}$를 최적의 두 배율이라고 합시다. 당신이 확률을 $\left(\frac{1}{4}, \frac{1}{2}, \frac{1}{4}\right)$라고 믿는다고 가정합시다. 두 배율을 최대화하려고 한다면, 어떤 두 배율 $W$를 달성하게 됩니까? 확률에 대한 잘못된 추정 때문에 두 배율이 얼마나 감소했습니까 (즉, $\Delta W=W^{*}-W$는 얼마입니까)?
(b) 이제 $m$ 마리의 말이 경주하고, 확률은 $p=\left(p_{1}, p_{2}, \ldots, p_{m}\right)$이고 배당률은 $o=\left(o_{1}, o_{2}, \ldots, o_{m}\right)$입니다. 실제 확률을 $q=\left(q_{1}, q_{2}, \ldots, q_{m}\right)$라고 믿고 두 배율 $W$를 최대화하려고 한다면, $W^{*}-W$는 얼마입니까?
6.11 두 봉투 문제. 한 봉투에는 $b$ 달러가 있고 다른 봉투에는 $2b$ 달러가 있습니다. 금액 $b$는 알려지지 않았습니다. 봉투 하나를 무작위로 선택합니다. $X$를 이 봉투에서 관찰된 금액이라고 하고, $Y$를 다른 봉투에 있는 금액이라고 합시다. $p(x) = \frac{e^{-x}}{\left(e^{-x}+e^{x}\right)}$의 확률로 다른 봉투로 바꾸는 전략을 채택합니다. $Z$를 플레이어가 받는 금액이라고 합시다. 따라서,

$$
\begin{gathered}
(X, Y)= \begin{cases}(b, 2 b) & \text { 확률 } \frac{1}{2} \\
(2 b, b) & \text { 확률 } \frac{1}{2}\end{cases} \\
Z= \begin{cases}X & \text { 확률 } 1-p(x) \\
Y & \text { 확률 } p(x)\end{cases}
\end{gathered}
$$

(a) $E(X)=E(Y)=\frac{3 b}{2}$임을 보이십시오.
(b) $E(Y / X)=\frac{5}{4}$임을 보이십시오. 다른 봉투에 있는 금액의 기대 비율이 $\frac{5}{4}$이므로, 항상 바꾸어야 하는 것처럼 보입니다. (이것이 스위칭 역설의 기원입니다.) 그러나 $E(Y) \neq E(X) E(Y / X)$임을 관찰하십시오. 따라서 $E(Y / X)>1$이지만, $E(Y)>E(X)$라고 결론 내릴 수는 없습니다.
(c) 가장 많은 돈이 들어 있는 봉투의 인덱스를 $J$라고 하고, 알고리즘에 의해 선택된 봉투의 인덱스를 $J^{\prime}$라고 합시다. 임의의 $b$에 대해 $I\left(J ; J^{\prime}\right)>0$임을 보이십시오. 따라서 첫 번째 봉투의 금액은 항상 어떤 봉투를 선택해야 하는지에 대한 정보를 포함합니다.
(d) $E(Z)>E(X)$임을 보이십시오. 따라서 항상 머무르거나 항상 바꾸는 것보다 더 나은 결과를 얻을 수 있습니다. 사실, 이것은 단조 감소하는 스위칭 함수 $p(x)$에 대해 참입니다. $p(x)$에 따라 무작위로 전환함으로써, 더 나쁜 결과에서 더 나은 결과로 거래할 가능성이 더 높습니다.
<!-- Page 206 -->
6.12 도박. 말 경주에서 승률 $p_{1}, p_{2}, \ldots, p_{m}$ 를 찾으십시오:
(a) 주어진 고정된 알려진 배당률 $o_{1}, o_{2}, \ldots, o_{m}$ 에 대해 두 배 성장률 $W^{*}$ 를 최대화하십시오.
(b) 주어진 고정된 배당률 $o_{1}, o_{2}, \ldots$, $o_{m}$ 에 대해 두 배 성장률을 최소화하십시오.
6.13 더치북. 말 경주에서 $m=2$ 마리의 말을 고려하십시오.

$$
\begin{aligned}
X & =1,2 \\
p & =\frac{1}{2}, \frac{1}{2} \\
\text { odds (for one) } & =10,30 \\
\text { bets } & =b, 1-b .
\end{aligned}
$$

배당률은 슈퍼 페어(superfair)입니다.
(a) 어떤 말이 이기든 상관없이 동일한 상금을 보장하는 베팅 $b$ 가 존재합니다. 이러한 베팅을 더치북(Dutch book)이라고 합니다. 이 $b$ 와 관련 부의 계수 $S(X)$ 를 찾으십시오.
(b) 최적의 $b$ 선택에 대한 부의 최대 성장률은 얼마입니까? 더치북의 성장률과 비교하십시오.
6.14 말 경주. 말 경주에서 두 배 성장률을 최대화하는 데 관심이 있다고 가정합니다. $p_{1}, p_{2}, \ldots, p_{m}$ 가 $m$ 마리 말의 승률을 나타낸다고 가정합니다. 어떤 배당률 $\left(o_{1}, o_{2}, \ldots, o_{m}\right)$ 이 배당률 $\left(o_{1}^{\prime}, o_{2}^{\prime}, \ldots, o_{m}^{\prime}\right)$ 보다 높은 두 배 성장률을 제공합니까?
6.15 공정한 말 경주의 엔트로피. $X \sim p(x), x=1,2, \ldots, m$ 를 말 경주의 우승자로 나타냅니다. 배당률 $o(x)$ 가 $p(x)$ 에 대해 공정하다고 가정합니다 [즉, $o(x)=\frac{1}{p(x)}$ ]. $b(x)$ 를 말 $x$ 에 베팅한 금액이라고 가정합니다. $b(x) \geq 0, \sum_{1}^{m} b(x)=1$. 그러면 결과 부의 계수는 $S(x)=b(x) o(x)$ 이고 확률은 $p(x)$ 입니다.
(a) 기대 부 $E S(X)$ 를 찾으십시오.
(b) 부의 최적 성장률 $W^{*}$ 를 찾으십시오.
(c) 다음을 가정합니다.

$$
Y= \begin{cases}1, & X=1 \text { or } 2 \\ 0, & \text { otherwise }\end{cases}
$$

이러한 부가 정보가 베팅 전에 사용 가능하다면, 성장률 $W^{*}$ 를 얼마나 증가시킵니까?
(d) $I(X ; Y)$ 를 찾으십시오.
<!-- Page 207 -->
6.16 음수 경마. 승률이 $p_{1}, p_{2}, \ldots, p_{m}$인 $m$마리의 말 경주를 고려하십시오. 여기서 도박꾼은 특정 말이 지기를 바랍니다. 그는 말에 베팅 $\left(b_{1}, b_{2}, \ldots, b_{m}\right), \sum_{i=1}^{m} b_{i}=1$을 하고, 말이 이기면 베팅 $b_{i}$를 잃고 나머지 금액을 보유합니다. (배당률 없음.) 따라서 $S=\sum_{j \neq i} b_{j}$이며, 확률은 $p_{i}$이고, 제약 조건 $\sum b_{i}=1$ 하에서 $\sum p_{i} \ln \left(1-b_{i}\right)$를 최대화하고자 합니다.
(a) 성장률 최적 투자 전략 $b^{*}$를 찾으십시오. 베팅이 양수라고 가정하지는 않지만, 베팅의 합이 1이 되도록 제약하십시오. (이는 공매 및 마진을 허용합니다.)
(b) 최적 성장률은 얼마입니까?
6.17 상트페테르부르크 역설. 오래전 고대 상트페테르부르크에서 다음과 같은 도박 제안이 큰 논란을 일으켰습니다. $c$ 단위의 참가비를 내면, 도박꾼은 확률 $2^{-k}$로 $2^{k}$ 단위를 받습니다. $k=1,2, \ldots$
(a) 이 게임의 기대 보상이 무한함을 보이십시오. 이러한 이유로 $c=\infty$가 이 게임을 플레이하기 위해 지불해야 하는 "공정한" 가격이라고 주장되었습니다. 대부분의 사람들은 이 답변을 터무니없다고 생각합니다.
(b) 도박꾼이 게임의 일부를 구매할 수 있다고 가정해 봅시다. 예를 들어, 게임에 $c / 2$ 단위를 투자하면, 그는 $1/2$의 지분을 받고 $X / 2$의 수익을 얻습니다. 여기서 $\operatorname{Pr}\left(X=2^{k}\right)=2^{-k}, k=1,2, \ldots$입니다. $X_{1}, X_{2}, \ldots$가 이 분포에 따라 i.i.d.이고 도박꾼이 매번 모든 부를 재투자한다고 가정해 봅시다. 따라서 시간 $n$에서의 그의 부 $S_{n}$은 다음과 같이 주어집니다.

$$
S_{n}=\prod_{i=1}^{n} \frac{X_{i}}{c}
$$

$c<c^{*}$ 또는 $c>c^{*}$에 따라 이 극한이 확률 1로 $\infty$ 또는 0이 됨을 보이십시오. "공정한" 참가비 $c^{*}$를 식별하십시오.
더 현실적으로, 도박꾼은 자신의 돈의 일부 $\bar{b}=1-b$를 주머니에 보관하고 나머지를 상트페테르부르크 게임에 투자하도록 허용되어야 합니다. 그러면 시간 $n$에서의 그의 부는 다음과 같습니다.

$$
S_{n}=\prod_{i=1}^{n}\left(\bar{b}+\frac{b X_{i}}{c}\right)
$$

다음과 같이 정의합니다.

$$
W(b, c)=\sum_{k=1}^{\infty} 2^{-k} \log \left(1-b+\frac{b 2^{k}}{c}\right)
$$
<!-- Page 208 -->
$$
S_{n} \doteq 2^{n W(b, c)}
$$

다음과 같이 정의합니다.

$$
W^{*}(c)=\max _{0 \leq b \leq 1} W(b, c)
$$

$W^{*}(c)$에 대한 질문은 다음과 같습니다.
(a) 최적화 값 $b^{*}$가 1보다 작아지는 진입 수수료 $c$의 값은 무엇입니까?
(b) $b^{*}$는 $c$에 따라 어떻게 변합니까?
(c) $W^{*}(c)$는 $c$에 따라 어떻게 감소합니까?

모든 $c$에 대해 $W^{*}(c)>0$이므로, 모든 진입 수수료 $c$는 공정하다고 결론 내릴 수 있습니다.
6.18 슈퍼 세인트 피터스버그. 마지막으로, 슈퍼 세인트 피터스버그 역설이 있습니다. 여기서 $\operatorname{Pr}\left(X=2^{2^{k}}\right)=2^{-k}, k=1,2, \ldots$입니다. 여기서 기대 로그 부는 모든 $b>0$ 및 모든 $c$에 대해 무한대이며, 도박꾼의 부는 모든 $b>0$에 대해 지수적으로 빠르게 증가합니다. 그러나 이것이 모든 투자 비율 $b$가 동일하게 좋다는 것을 의미하지는 않습니다. 이를 보기 위해, 다른 포트폴리오, 예를 들어 $\mathbf{b}=\left(\frac{1}{2}, \frac{1}{2}\right)$에 대한 상대 성장률을 최대화하고자 합니다. 다음을 최대화하는 고유한 $b$가 존재함을 보여주십시오.

$$
E \ln \frac{\bar{b}+b X / c}{\frac{1}{2}+\frac{1}{2} X / c}
$$

그리고 그 답을 해석하십시오.

# 역사적 참고 자료

경마에서의 도박에 대한 최초의 처리는 Kelly [308]에 의한 것으로, 그는 $\Delta W=I$를 발견했습니다. 로그 최적 포트폴리오는 Bernoulli, Kelly [308], Latané [346], Latané 및 Tuttle [347]의 연구로 거슬러 올라갑니다. 비례 도박은 때때로 Kelly 도박 방식이라고도 합니다. 문제 6.11에서 봉투를 바꾸어 이길 확률을 개선하는 것은 Cover [130]에 기반합니다.

Shannon은 그의 원본 논문 [472]에서 영어에 대한 확률 모델을 연구했습니다. 영어의 entropy rate를 추정하기 위한 그의 추측 게임은 [482]에 설명되어 있습니다. Cover와 King [131]은 영어의 entropy에 대한 도박 추정치를 제공합니다. 세인트 피터스버그 역설의 분석은 Bell과 Cover [39]에서 가져왔습니다. 대안적인 분석은 Feller [208]에서 찾을 수 있습니다.
<!-- Page 209 -->
# 제 7 장

## 채널 용량

$A$가 $B$와 통신한다고 말할 때 무엇을 의미하는가? 우리는 $A$의 물리적 행위가 $B$에서 원하는 물리적 상태를 유도했음을 의미한다. 이러한 정보의 전송은 물리적 과정이며 따라서 제어할 수 없는 주변 잡음과 물리적 신호 전송 과정 자체의 불완전성에 영향을 받는다. 수신자 $B$와 송신자 $A$가 전송된 내용에 동의하면 통신은 성공한다.

이 장에서는 통신 채널을 $n$번 사용할 때 구별 가능한 최대 신호 수를 찾는다. 이 수는 $n$에 따라 기하급수적으로 증가하며, 이 지수는 채널 용량으로 알려져 있다. 채널 용량(구별 가능한 신호 수의 로그)을 최대 상호 정보량으로 특징짓는 것은 정보 이론의 중심적이고 가장 유명한 성공이다.

물리적 신호 시스템의 수학적 유사체는 그림 7.1에 나와 있다. 어떤 유한 알파벳에서 온 소스 심볼은 채널 심볼의 시퀀스로 매핑되고, 이는 채널의 출력 시퀀스를 생성한다. 출력 시퀀스는 무작위이지만 입력 시퀀스에 따라 달라지는 분포를 갖는다. 출력 시퀀스로부터 전송된 메시지를 복구하려고 시도한다.

가능한 각 입력 시퀀스는 출력 시퀀스에 대한 확률 분포를 유도한다. 두 개의 다른 입력 시퀀스가 동일한 출력 시퀀스를 생성할 수 있으므로 입력은 혼동될 수 있다. 다음 몇 섹션에서는 "혼동되지 않는" 입력 시퀀스 부분 집합을 선택하여 특정 출력의 원인이 될 수 있는 매우 가능성 높은 입력이 하나만 있도록 할 수 있음을 보여줄 것이다. 그러면 오류 확률이 무시할 수 있을 정도로 작게 하여 출력에서 입력 시퀀스를 재구성할 수 있다. 소스를 채널에 대한 적절한 "넓게 분리된" 입력 시퀀스로 매핑함으로써 매우 낮은 오류 확률로 메시지를 전송하고 출력에서 소스 메시지를 재구성할 수 있다. 이것이 가능한 최대 속도를 채널 용량이라고 한다.

정의 이산 채널을 입력 알파벳 $\mathcal{X}$와 출력 알파벳 $\mathcal{Y}$ 및 확률 전이 행렬로 구성된 시스템으로 정의한다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 210 -->

그림 7.1. 통신 시스템.
$p(y \mid x)$는 기호 $x$를 보냈을 때 출력 기호 $y$를 관찰할 확률을 나타냅니다. 채널은 출력의 확률 분포가 해당 시점의 입력에만 의존하고 이전 채널 입력 또는 출력과는 조건부로 독립적인 경우 메모리리스(memoryless)라고 합니다.

정의 이산 메모리리스 채널의 "정보" 채널 용량(channel capacity)을 다음과 같이 정의합니다.

$$
C=\max _{p(x)} I(X ; Y)
$$

여기서 최대값은 모든 가능한 입력 분포 $p(x)$에 대해 취해집니다.
곧, 채널 용량에 대한 운영적 정의를 정보가 임의로 낮은 오류 확률로 전송될 수 있는 초당 비트 단위의 최고 속도로 제공할 것입니다. Shannon의 두 번째 정리는 정보 채널 용량이 운영적 채널 용량과 같음을 확립합니다. 따라서 대부분의 채널 용량 논의에서 "정보"라는 단어를 생략합니다.

데이터 압축과 데이터 전송 문제 사이에는 이중성이 존재합니다. 압축 중에는 데이터의 모든 중복성을 제거하여 가능한 가장 압축된 버전을 형성하는 반면, 데이터 전송 중에는 채널의 오류에 대처하기 위해 중복성을 제어된 방식으로 추가합니다. 섹션 7.13에서는 일반적인 통신 시스템을 두 부분으로 나눌 수 있으며 데이터 압축 및 데이터 전송 문제를 별도로 고려할 수 있음을 보여줍니다.

# 7.1 채널 용량의 예시

### 7.1.1 잡음 없는 이진 채널

입력이 정확하게 출력으로 복제되는 채널이 있다고 가정해 보겠습니다(그림 7.2).

이 경우 전송된 모든 비트는 오류 없이 수신됩니다. 따라서 채널 사용당 오류 없는 비트 하나를 전송할 수 있으며, 용량은 다음과 같습니다.
<!-- Page 211 -->

1
그림 7.2. 잡음 없는 이진 채널. $C=1$ 비트.

1 비트입니다. 정보 용량 $C=\max I(X ; Y)=$ 1 비트를 계산할 수도 있으며, 이는 $p(x)=\left(\frac{1}{2}, \frac{1}{2}\right)$를 사용하여 달성됩니다.

# 7.1.2 겹치지 않는 출력을 가진 잡음 채널

이 채널은 두 개의 입력 각각에 대해 두 개의 가능한 출력을 가집니다(그림 7.3). 이 채널은 잡음이 있는 것처럼 보이지만 실제로는 그렇지 않습니다. 채널의 출력이 입력의 무작위 결과임에도 불구하고, 출력으로부터 입력을 결정할 수 있으므로 전송된 모든 비트를 오류 없이 복구할 수 있습니다. 이 채널의 용량 또한 전송당 1 비트입니다. 정보 용량 $C=\max I(X ; Y)=1$ 비트를 계산할 수도 있으며, 이는 $p(x)=\left(\frac{1}{2}, \frac{1}{2}\right)$를 사용하여 달성됩니다.

그림 7.3. 겹치지 않는 출력을 가진 잡음 채널. $C=1$ 비트.
<!-- Page 212 -->
# 7.1.3 노이즈 타이프라이터

이 경우 채널 입력은 출력에서 확률 $\frac{1}{2}$로 변경되지 않고 수신되거나, 확률 $\frac{1}{2}$로 다음 문자로 변환됩니다 (그림 7.4). 입력에 26개의 기호가 있고 격주 입력 기호를 사용하면 각 전송으로 13개의 기호 중 하나를 오류 없이 전송할 수 있습니다. 따라서 이 채널의 용량은 전송당 $\log 13$ 비트입니다. 또한 정보 용량 $C=\max I(X ; Y)=\max (H(Y)-H(Y \mid X))=$ $\max H(Y)-1=\log 26-1=\log 13$을 계산할 수 있으며, 이는 모든 입력에 균일하게 분포된 $p(x)$를 사용하여 달성됩니다.

그림 7.4. 노이즈 타이프라이터. $C=\log 13$ 비트.
<!-- Page 213 -->
# 7.1.4 이진 대칭 채널

그림 7.5에 표시된 이진 대칭 채널(BSC)을 고려하십시오. 이것은 입력 심볼이 확률 $p$로 상호 보완되는 이진 채널입니다. 이것은 오류가 있는 채널의 가장 간단한 모델이지만, 일반적인 문제의 복잡성 대부분을 포착합니다.

오류가 발생하면 0은 1로 수신되고 그 반대도 마찬가지입니다. 수신된 비트는 오류가 발생한 위치를 드러내지 않습니다. 어떤 의미에서 수신된 모든 비트는 신뢰할 수 없습니다. 나중에 우리는 이러한 통신 채널을 사용하여 오류 확률이 임의로 작으면서 0이 아닌 속도로 정보를 전송할 수 있음을 보여줄 것입니다.

상호 정보를 다음과 같이 제한합니다.

$$
\begin{aligned}
I(X ; Y) & =H(Y)-H(Y \mid X) \\
& =H(Y)-\sum p(x) H(Y \mid X=x) \\
& =H(Y)-\sum p(x) H(p) \\
& =H(Y)-H(p) \\
& \leq 1-H(p)
\end{aligned}
$$

여기서 마지막 부등식은 $Y$가 이진 확률 변수이기 때문에 성립합니다. 입력 분포가 균일할 때 등식이 달성됩니다. 따라서 매개변수 $p$를 갖는 이진 대칭 채널의 정보 용량은 다음과 같습니다.

$$
C=1-H(p) \quad \text { 비트. }
$$

그림 7.5. 이진 대칭 채널. $C=1-H(p)$ 비트.
<!-- Page 214 -->
# 7.1.5 이진 삭제 채널

비트가 손상되는 대신 손실되는 이진 대칭 채널의 유사체는 이진 삭제 채널입니다. 이 채널에서는 비트의 일부 $\alpha$가 삭제됩니다. 수신자는 어떤 비트가 삭제되었는지 알고 있습니다. 이진 삭제 채널은 두 개의 입력과 세 개의 출력을 가집니다(그림 7.6).

이진 삭제 채널의 용량은 다음과 같이 계산합니다.

$$
\begin{aligned}
C & =\max _{p(x)} I(X ; Y) \\
& =\max _{p(x)}(H(Y)-H(Y \mid X)) \\
& =\max _{p(x)} H(Y)-H(\alpha)
\end{aligned}
$$

$H(Y)$의 최댓값에 대한 첫 번째 추측은 $\log 3$이겠지만, 입력 분포 $p(x)$의 어떤 선택으로도 이를 달성할 수 없습니다. $E$를 사건 $\{Y=e\}$로 두고, 다음 전개를 사용하면

$$
H(Y)=H(Y, E)=H(E)+H(Y \mid E)
$$

$\operatorname{Pr}(X=1)=\pi$라고 하면,

$$
H(Y)=H((1-\pi)(1-\alpha), \alpha, \pi(1-\alpha))=H(\alpha)+(1-\alpha) H(\pi)
$$

그림 7.6. 이진 삭제 채널.
<!-- Page 215 -->
그러므로

$$
\begin{aligned}
C & =\max _{p(x)} H(Y)-H(\alpha) \\
& =\max _{\pi}(1-\alpha) H(\pi)+H(\alpha)-H(\alpha) \\
& =\max _{\pi}(1-\alpha) H(\pi) \\
& =1-\alpha
\end{aligned}
$$

여기서 $\pi=\frac{1}{2}$일 때 capacity가 달성됩니다.
capacity에 대한 표현은 어느 정도 직관적인 의미를 가집니다. 비트의 $\alpha$ 비율이 channel에서 손실되므로, 우리는 비트의 $1-\alpha$ 비율을 (최대) 복구할 수 있습니다. 그러므로 capacity는 최대 $1-\alpha$입니다. 이 비율을 달성하는 것이 가능한지는 즉시 명확하지 않습니다. 이는 Shannon의 두 번째 정리에 의해 따라올 것입니다.

많은 실제 channel에서 송신자는 수신자로부터 피드백을 받습니다. 이진 삭제 channel에 피드백이 사용 가능하다면, 무엇을 해야 할지는 매우 명확합니다: 비트가 손실되면, 전송될 때까지 재전송하십시오. 비트는 $1-\alpha$의 확률로 전송되므로, 효과적인 전송률은 $1-\alpha$입니다. 이러한 방식으로 피드백을 통해 $1-\alpha$의 capacity를 쉽게 달성할 수 있습니다.

나중에 이 장에서 우리는 피드백의 유무에 관계없이 $1-\alpha$의 비율이 달성할 수 있는 최상의 비율임을 증명할 것입니다. 이것은 피드백이 이산 무기억 채널의 capacity를 증가시키지 않는다는 놀라운 사실의 결과 중 하나입니다.

# 7.2 대칭 채널 (SYMMETRIC CHANNELS)

이진 대칭 채널의 capacity는 전송당 $C=1-H(p)$ 비트이고, 이진 삭제 채널의 capacity는 전송당 $C=1-\alpha$ 비트입니다. 이제 전이 행렬이 다음과 같은 채널을 고려하십시오:

$$
p(y \mid x)=\left[\begin{array}{lll}
0.3 & 0.2 & 0.5 \\
0.5 & 0.3 & 0.2 \\
0.2 & 0.5 & 0.3
\end{array}\right]
$$

여기서 $x$번째 행과 $y$번째 열의 항목은 $x$가 전송될 때 $y$가 수신될 조건부 확률 $p(y \mid x)$를 나타냅니다. 이 channel에서, 확률 전이 행렬의 모든 행은 서로 순열이고 열도 마찬가지입니다. 이러한 channel을 대칭이라고 합니다. 대칭 channel의 또 다른 예는 다음과 같은 형태의 channel입니다.

$$
Y=X+Z \quad(\bmod c)
$$
<!-- Page 216 -->
$Z$는 정수 $\{0,1,2, \ldots, c-1\}$ 상의 어떤 분포를 가지며, $X$는 $Z$와 동일한 알파벳을 가지며, $Z$는 $X$와 독립입니다.

이 두 경우 모두 채널의 용량에 대한 명시적인 표현을 쉽게 찾을 수 있습니다. 전이 행렬의 행을 $\mathbf{r}$이라고 하면 다음과 같습니다.

$$
\begin{aligned}
I(X ; Y) & =H(Y)-H(Y \mid X) \\
& =H(Y)-H(\mathbf{r}) \\
& \leq \log |\mathcal{Y}|-H(\mathbf{r})
\end{aligned}
$$

여기서 출력 분포가 균일할 때 등식이 성립합니다. 그러나 $p(x)=1 /|\mathcal{X}|$는 다음에서 볼 수 있듯이 $Y$ 상의 균일 분포를 달성합니다.

$$
p(y)=\sum_{x \in \mathcal{X}} p(y \mid x) p(x)=\frac{1}{|\mathcal{X}|} \sum p(y \mid x)=c \frac{1}{|\mathcal{X}|}=\frac{1}{|\mathcal{Y}|}
$$

여기서 $c$는 확률 전이 행렬의 한 열에 있는 항목들의 합입니다.

따라서 (7.17)의 채널은 다음과 같은 용량을 가집니다.

$$
C=\max _{p(x)} I(X ; Y)=\log 3-H(0.5,0.3,0.2)
$$

그리고 $C$는 입력 상의 균일 분포에 의해 달성됩니다.
위에 정의된 대칭 채널의 전이 행렬은 이중 확률적입니다. 용량 계산에서 우리는 행들이 서로 순열이고 모든 열 합이 같다는 사실을 사용했습니다.

이러한 속성을 고려하여 대칭 채널 개념의 일반화를 다음과 같이 정의할 수 있습니다.

정의 채널은 채널 전이 행렬 $p(y \mid x)$의 행들이 서로 순열이고 열들이 서로 순열일 때 대칭이라고 합니다. 채널은 약한 대칭이라고 합니다. 만약 전이 행렬 $p(\cdot \mid x)$의 모든 행이 다른 모든 행의 순열이고 모든 열 합 $\sum_{x} p(y \mid x)$이 같을 때입니다.

예를 들어, 전이 행렬이 다음과 같은 채널은

$$
p(y \mid x)=\left(\begin{array}{ccc}
\frac{1}{3} & \frac{1}{6} & \frac{1}{2} \\
\frac{1}{3} & \frac{1}{2} & \frac{1}{6}
\end{array}\right)
$$

약한 대칭이지만 대칭은 아닙니다.
<!-- Page 217 -->
위의 대칭 채널에 대한 유도는 약한 대칭 채널에도 적용됩니다. 약한 대칭 채널에 대한 다음 정리가 있습니다.

정리 7.2.1 약한 대칭 채널의 경우,

$$
C=\log |\mathcal{Y}|-H(\text { 전이 행렬의 행 })
$$

이는 입력 알파벳에 대한 균일 분포에 의해 달성됩니다.

# 7.3 채널 용량의 속성

1. $I(X ; Y) \geq 0$이므로 $C \geq 0$입니다.
2. $C=\max I(X ; Y) \leq \max H(X)=\log |\mathcal{X}|$이므로 $C \leq \log |\mathcal{X}|$입니다.
3. 같은 이유로 $C \leq \log |\mathcal{Y}|$입니다.
4. $I(X ; Y)$는 $p(x)$의 연속 함수입니다.
5. $I(X ; Y)$는 $p(x)$의 오목 함수입니다(정리 2.7.4). $I(X ; Y)$가 닫힌 볼록 집합에 대한 오목 함수이므로 지역 최대값은 전역 최대값입니다. 속성 2와 3에서 최대값은 유한하며, 용량 정의에서 supremum 대신 maximum이라는 용어를 사용하는 것이 정당화됩니다. 그런 다음 표준 비선형 최적화 기법(예: 경사 하강법)을 사용하여 최대값을 찾을 수 있습니다. 사용할 수 있는 몇 가지 방법은 다음과 같습니다.

- 미적분학과 Kuhn-Tucker 조건을 사용한 제약 최대화.
- Frank-Wolfe 경사 하강법 알고리즘.
- Arimoto [25]와 Blahut [65]가 개발한 반복 알고리즘. 이 알고리즘은 10.8절에서 설명합니다.

일반적으로 용량에 대한 닫힌 형태의 해는 없습니다. 그러나 많은 간단한 채널의 경우 대칭과 같은 속성을 사용하여 용량을 계산할 수 있습니다. 앞에서 고려한 예 중 일부는 이 형태입니다.

### 7.4 채널 코딩 정리 미리보기

지금까지 이산 메모리 없는 채널의 정보 용량을 정의했습니다. 다음 섹션에서는 Shannon의 두 번째 정리를 증명할 것입니다.
<!-- Page 218 -->

그림 7.7. $n$번 사용 후의 채널.
이는 용량의 정의에 대한 운영적 의미를 제공하며, 이는 채널을 통해 안정적으로 전송할 수 있는 비트 수입니다. 하지만 먼저 왜 우리가 채널을 통해 $C$ 비트의 정보를 전송할 수 있는지에 대한 직관적인 아이디어를 제공하고자 합니다. 기본적인 아이디어는 블록 길이가 길면 모든 채널이 노이즈가 있는 타자기 채널(그림 7.4)처럼 보이며, 채널에는 출력에서 본질적으로 분리된 시퀀스를 생성하는 입력의 부분 집합이 있다는 것입니다.

각 (일반적인) 입력 $n$-시퀀스에 대해 약 $2^{n H(Y \mid X)}$개의 가능한 $Y$ 시퀀스가 있으며, 이들은 모두 동일하게 가능성이 있습니다(그림 7.7). 우리는 두 개의 $X$ 시퀀스가 동일한 $Y$ 출력 시퀀스를 생성하지 않도록 해야 합니다. 그렇지 않으면 어떤 $X$ 시퀀스가 전송되었는지 결정할 수 없을 것입니다.

가능한 (일반적인) $Y$ 시퀀스의 총 수는 약 $2^{n H(Y)}$입니다. 이 집합은 서로 다른 입력 $X$ 시퀀스에 해당하는 $2^{n H(Y \mid X)}$ 크기의 집합으로 나누어져야 합니다. 분리된 집합의 총 수는 $2^{n(H(Y)-H(Y \mid X))}=2^{n I(X ; Y)}$보다 작거나 같습니다. 따라서 우리는 약 $2^{n I(X ; Y)}$개의 구별 가능한 길이 $n$ 시퀀스를 최대 전송할 수 있습니다.

위의 유도는 용량에 대한 상한을 개략적으로 설명하지만, 위의 논증의 더 강력한 버전은 다음 섹션에서 이 속도 $I$가 임의로 낮은 오류 확률로 달성될 수 있음을 증명하는 데 사용될 것입니다.

Shannon의 두 번째 정리에 대한 증명을 진행하기 전에 몇 가지 정의가 필요합니다.

# 7.5 정의

그림 7.8에 표시된 통신 시스템을 분석합니다.
인덱스 집합 $\{1,2, \ldots, M\}$에서 추출된 메시지 $W$는 신호 $X^{n}(W)$를 생성하며, 이는 수신기에 의해 무작위 시퀀스로 수신됩니다.
<!-- Page 219 -->

그림 7.8. 통신 채널.
$Y^{n} \sim p\left(y^{n} \mid x^{n}\right)$입니다. 수신기는 적절한 복호화 규칙 $\hat{W}=g\left(Y^{n}\right)$을 사용하여 인덱스 $W$를 추측합니다. $\hat{W}$가 전송된 인덱스 $W$와 같지 않으면 수신기는 오류를 범합니다. 이제 이러한 개념을 공식적으로 정의하겠습니다.

정의 이산 채널, $(\mathcal{X}, p(y \mid x), \mathcal{Y})$로 표시되는 것은 두 개의 유한 집합 $\mathcal{X}$와 $\mathcal{Y}$ 및 확률 질량 함수 $p(y \mid x)$의 모음으로 구성됩니다. 각 $x \in \mathcal{X}$에 대해 $p(y \mid x) \geq 0$이고, 모든 $x$에 대해 $\sum_{y} p(y \mid x)=1$이며, $X$는 채널의 입력이고 $Y$는 채널의 출력이라는 해석을 가집니다.

정의 이산 무기억 채널(DMC)의 n번째 확장은 채널 $\left(\mathcal{X}^{n}, p\left(y^{n} \mid x^{n}\right), \mathcal{Y}^{n}\right)$이며, 여기서

$$
p\left(y_{k} \mid x^{k}, y^{k-1}\right)=p\left(y_{k} \mid x_{k}\right), \quad k=1,2, \ldots, n
$$

비고 피드백 없이 채널이 사용되는 경우 [즉, 입력 심볼이 과거 출력 심볼에 의존하지 않는 경우, 즉 $p\left(x_{k} \mid x^{k-1}, y^{k-1}\right)$ $=p\left(x_{k} \mid x^{k-1}\right)$ ], 이산 무기억 채널의 n번째 확장에 대한 채널 전이 함수는 다음과 같이 축소됩니다.

$$
p\left(y^{n} \mid x^{n}\right)=\prod_{i=1}^{n} p\left(y_{i} \mid x_{i}\right)
$$

이산 무기억 채널이라고 할 때, 명시적으로 달리 명시하지 않는 한 피드백이 없는 이산 무기억 채널을 의미합니다.

정의 채널 $(\mathcal{X}, p(y \mid x), \mathcal{Y})$에 대한 $(M, n)$ 코드는 다음으로 구성됩니다.

1. 인덱스 집합 $\{1,2, \ldots, M\}$.
2. 인코딩 함수 $X^{n}:\{1,2, \ldots, M\} \rightarrow \mathcal{X}^{n}$, 이는 코드워드 $x^{n}(1), x^{n}(2), \ldots, x^{n}(M)$를 생성합니다. 코드워드 집합을 코드북이라고 합니다.
<!-- Page 220 -->
3. 디코딩 함수

$$
g: \mathcal{Y}^{n} \rightarrow\{1,2, \ldots, M\}
$$

이는 가능한 모든 수신 벡터에 대해 추측을 할당하는 결정론적 규칙입니다.

정의 (오류의 조건부 확률) 다음을 정의합니다.

$$
\lambda_{i}=\operatorname{Pr}\left(g\left(Y^{n}\right) \neq i \mid X^{n}=x^{n}(i)\right)=\sum_{y^{n}} p\left(y^{n} \mid x^{n}(i)\right) I\left(g\left(y^{n}\right) \neq i\right)
$$

이는 인덱스 $i$가 전송되었을 때의 오류의 조건부 확률이며, 여기서 $I(\cdot)$는 지시 함수입니다.

정의 $(M, n)$ 코드에 대한 최대 오류 확률 $\lambda^{(n)}$은 다음과 같이 정의됩니다.

$$
\lambda^{(n)}=\max _{i \in\{1,2, \ldots, M\}} \lambda_{i}
$$

정의 $(M, n)$ 코드에 대한 (산술) 평균 오류 확률 $P_{e}^{(n)}$은 다음과 같이 정의됩니다.

$$
P_{e}^{(n)}=\frac{1}{M} \sum_{i=1}^{M} \lambda_{i}
$$

만약 인덱스 $W$가 집합 $\{1,2, \ldots, M\}$에 대한 균일 분포에 따라 선택되고, $X^{n}=x^{n}(W)$라면,

$$
P_{e}^{(n)} \triangleq \operatorname{Pr}\left(W \neq g\left(Y^{n}\right)\right)
$$

(즉, $P_{e}^{(n)}$은 오류 확률입니다). 또한 명백하게,

$$
P_{e}^{(n)} \leq \lambda^{(n)}
$$

최대 오류 확률은 평균 오류 확률과 상당히 다르게 동작할 것으로 예상할 수 있습니다. 그러나 다음 섹션에서 우리는 작은 평균 오류 확률이 본질적으로 동일한 비율에서 작은 최대 오류 확률을 의미함을 증명할 것입니다.
<!-- Page 221 -->
$P_{e}^{(n)}$는 (7.32)에서 정의된 바와 같이 오차 확률 $\lambda_{i}$의 수학적 구성일 뿐이며, 메시지가 메시지 집합 $\left\{1,2, \ldots, 2^{M}\right\}$에 대해 균등하게 선택된 경우에만 오차 확률입니다. 그러나 달성 가능성 증명과 반증 모두에서 오차 확률을 제한하기 위해 $W$에 대한 균등 분포를 선택합니다. 이를 통해 $P_{e}^{(n)}$의 동작과 최대 오차 확률 $\lambda^{(n)}$을 확립할 수 있으며, 따라서 채널이 어떻게 사용되든 (즉, $W$의 분포에 관계없이) 채널의 동작을 특성화할 수 있습니다.

정의 $(M, n)$ 코드의 속도 $R$은 다음과 같습니다.

$$
R=\frac{\log M}{n} \quad \text { 전송당 비트. }
$$

정의 속도 $R$은 최대 오차 확률 $\lambda^{(n)}$이 $n \rightarrow \infty$일 때 0으로 수렴하는 $\left(\left\lceil 2^{n R}\right\rceil, n\right)$ 코드의 시퀀스가 존재하는 경우 달성 가능하다고 말합니다.

나중에 $\left(2^{n R}, n\right)$ 코드는 $\left(\left\lceil 2^{n R}\right\rceil, n\right)$ 코드를 의미합니다. 이는 표기법을 단순화할 것입니다.

정의 채널의 용량은 모든 달성 가능한 속도의 상한입니다.

따라서 용량보다 낮은 속도는 충분히 큰 블록 길이에 대해 임의로 작은 오차 확률을 산출합니다.

# 7.6 공동으로 전형적인 시퀀스

대략적으로 말하면, 코드워드 $X^{n}(i)$가 수신 신호 $Y^{n}$과 "공동으로 전형적"인 경우 채널 출력 $Y^{n}$을 $i$번째 인덱스로 디코딩합니다. 이제 공동 전형성의 중요한 개념을 정의하고 $X^{n}(i)$가 $Y^{n}$의 실제 원인인 경우와 그렇지 않은 경우의 공동 전형성 확률을 찾습니다.

정의 분포 $p(x, y)$에 대한 공동 전형적인 시퀀스 $\left\{\left(x^{n}, y^{n}\right)\right\}$의 집합 $A_{\epsilon}^{(n)}$은 경험적 엔트로피가 실제 엔트로피에 $\epsilon$-가까운 $n$-시퀀스의 집합입니다.

$$
\begin{aligned}
A_{\epsilon}^{(n)}= & \left\{\left(x^{n}, y^{n}\right) \in \mathcal{X}^{n} \times \mathcal{Y}^{n}:\right. \\
& \left|-\frac{1}{n} \log p\left(x^{n}\right)-H(X)\right|<\epsilon
\end{aligned}
$$
<!-- Page 222 -->
$$
\begin{aligned}
& \left|-\frac{1}{n} \log p\left(y^{n}\right)-H(Y)\right|<\epsilon \\
& \left|-\frac{1}{n} \log p\left(x^{n}, y^{n}\right)-H(X, Y)\right|<\epsilon
\end{aligned}
$$

where

$$
p\left(x^{n}, y^{n}\right)=\prod_{i=1}^{n} p\left(x_{i}, y_{i}\right)
$$

정리 7.6.1 (결합 AEP) $\left(X^{n}, Y^{n}\right)$가 $p\left(x^{n}, y^{n}\right)=\prod_{i=1}^{n} p\left(x_{i}, y_{i}\right)$에 따라 i.i.d.로 추출된 길이 $n$의 시퀀스라고 가정합니다. 그러면 다음이 성립합니다:

1. $\operatorname{Pr}\left(\left(X^{n}, Y^{n}\right) \in A_{\epsilon}^{(n)}\right) \rightarrow 1$ as $n \rightarrow \infty$.
2. $\left|A_{\epsilon}^{(n)}\right| \leq 2^{n(H(X, Y)+\epsilon)}$.
3. 만약 $\left(\tilde{X}^{n}, \tilde{Y}^{n}\right) \sim p\left(x^{n}\right) p\left(y^{n}\right)$ [즉, $\tilde{X}^{n}$와 $\tilde{Y}^{n}$는 $p\left(x^{n}, y^{n}\right)$와 동일한 주변 분포를 가지며 독립적임], 그러면

$$
\operatorname{Pr}\left(\left(\tilde{X}^{n}, \tilde{Y}^{n}\right) \in A_{\epsilon}^{(n)}\right) \leq 2^{-n(I(X ; Y)-3 \epsilon)}
$$

또한, 충분히 큰 $n$에 대해,

$$
\operatorname{Pr}\left(\left(\tilde{X}^{n}, \tilde{Y}^{n}\right) \in A_{\epsilon}^{(n)}\right) \geq(1-\epsilon) 2^{-n(I(X ; Y)+3 \epsilon)}
$$

# 증명

1. 시퀀스가 일반 집합에 속할 확률이 높다는 것을 보임으로써 시작합니다. 큰 수의 법칙에 의해,

$$
-\frac{1}{n} \log p\left(X^{n}\right) \rightarrow-E[\log p(X)]=H(X) \quad \text { in probability }
$$

따라서, $\epsilon>0$가 주어졌을 때, 모든 $n>n_{1}$에 대해 $n_{1}$이 존재하여 다음이 성립합니다:

$$
\operatorname{Pr}\left(\left|-\frac{1}{n} \log p\left(X^{n}\right)-H(X)\right| \geq \epsilon\right)<\frac{\epsilon}{3}
$$

마찬가지로, 큰 수의 법칙에 의해,

$$
-\frac{1}{n} \log p\left(Y^{n}\right) \rightarrow-E[\log p(Y)]=H(Y) \quad \text { in probability }
$$
<!-- Page 223 -->
그리고
$-\frac{1}{n} \log p\left(X^{n}, Y^{n}\right) \rightarrow-E[\log p(X, Y)]=H(X, Y)$ 확률적으로,
그리고 $n_{2}$와 $n_{3}$가 존재하여, 모든 $n \geq n_{2}$에 대해,

$$
\operatorname{Pr}\left(\left|-\frac{1}{n} \log p\left(Y^{n}\right)-H(Y)\right| \geq \epsilon\right)<\frac{\epsilon}{3}
$$

그리고 모든 $n \geq n_{3}$에 대해,

$$
\operatorname{Pr}\left(\left|-\frac{1}{n} \log p\left(X^{n}, Y^{n}\right)-H(X, Y)\right| \geq \epsilon\right)<\frac{\epsilon}{3}
$$

$n>\max \left\{n_{1}, n_{2}, n_{3}\right\}$를 선택하면, (7.42), (7.45), (7.46)의 집합들의 합집합의 확률은 $\epsilon$보다 작아야 합니다. 따라서 $n$이 충분히 크면, 집합 $A_{\epsilon}^{(n)}$의 확률은 $1-\epsilon$보다 크며, 이는 정리의 첫 번째 부분을 확립합니다.
2. 정리의 두 번째 부분을 증명하기 위해, 우리는 다음과 같이 합니다.

$$
\begin{aligned}
1 & =\sum p\left(x^{n}, y^{n}\right) \\
& \geq \sum_{A_{\epsilon}^{(n)}} p\left(x^{n}, y^{n}\right) \\
& \geq\left|A_{\epsilon}^{(n)}\right| 2^{-n(H(X, Y)+\epsilon)}
\end{aligned}
$$

따라서

$$
\left|A_{\epsilon}^{(n)}\right| \leq 2^{n(H(X, Y)+\epsilon)}
$$

3. 이제 $\tilde{X}^{n}$과 $\tilde{Y}^{n}$이 독립이지만 $X^{n}$과 $Y^{n}$과 동일한 주변 분포를 갖는다고 가정하면,

$$
\begin{aligned}
\operatorname{Pr}\left(\left(\tilde{X}^{n}, \tilde{Y}^{n}\right) \in A_{\epsilon}^{(n)}\right) & =\sum_{\left(x^{n}, y^{n}\right) \in A_{\epsilon}^{(n)}} p\left(x^{n}\right) p\left(y^{n}\right) \\
& \leq 2^{n(H(X, Y)+\epsilon)} 2^{-n(H(X)-\epsilon)} 2^{-n(H(Y)-\epsilon)} \\
& =2^{-n(I(X ; Y)-3 \epsilon)}
\end{aligned}
$$
<!-- Page 224 -->
충분히 큰 $n$에 대해 $\operatorname{Pr}\left(A_{\epsilon}^{(n)}\right) \geq 1-\epsilon$이며, 따라서

$$
\begin{aligned}
1-\epsilon & \leq \sum_{\left(x^{n}, y^{n}\right) \in A_{\epsilon}^{(n)}} p\left(x^{n}, y^{n}\right) \\
& \leq\left|A_{\epsilon}^{(n)}\right| 2^{-n(H(X, Y)-\epsilon)}
\end{aligned}
$$

그리고

$$
\left|A_{\epsilon}^{(n)}\right| \geq(1-\epsilon) 2^{n(H(X, Y)-\epsilon)}
$$

위의 상한과 유사한 논증을 통해, 충분히 큰 $n$에 대해 다음을 보일 수 있습니다.

$$
\begin{aligned}
\operatorname{Pr}\left(\left(\tilde{X}^{n}, \tilde{Y}^{n}\right) \in A_{\epsilon}^{(n)}\right) & =\sum_{A_{\epsilon}^{(n)}} p\left(x^{n}\right) p\left(y^{n}\right) \\
& \geq(1-\epsilon) 2^{n(H(X, Y)-\epsilon)} 2^{-n(H(X)+\epsilon)} 2^{-n(H(Y)+\epsilon)} \\
& =(1-\epsilon) 2^{-n(I(X ; Y)+3 \epsilon)}
\end{aligned}
$$

결합적으로 전형적인 집합은 그림 7.9에 설명되어 있습니다. 약 $2^{n H(X)}$개의 전형적인 $X$ 시퀀스와 약 $2^{n H(Y)}$개의 전형적인 $Y$ 시퀀스가 있습니다. 그러나 $2^{n H(X, Y)}$개의 결합적으로 전형적인 시퀀스만 존재하므로, 모든 전형적인 $X^{n}$과 전형적인 $Y^{n}$의 쌍이 결합적으로 전형적인 것은 아닙니다. 확률은

그림 7.9. 결합적으로 전형적인 시퀀스.
<!-- Page 225 -->
임의로 선택된 쌍이 공동으로 전형적인 경우는 $2^{-n I(X ; Y)}$ 정도입니다. 따라서, 공동으로 전형적인 쌍을 우연히 발견하기 전에 약 $2^{n I(X ; Y)}$개의 그러한 쌍을 고려할 수 있습니다. 이는 약 $2^{n I(X ; Y)}$개의 구별 가능한 신호 $X^{n}$가 있음을 시사합니다.

이를 다르게 보는 방법은 고정된 출력 시퀀스 $Y^{n}$에 대한 공동으로 전형적인 시퀀스의 집합의 관점에서 보는 것입니다. 이 출력 시퀀스는 실제 입력 신호 $X^{n}$로부터 발생하는 것으로 추정됩니다. 이 시퀀스 $Y^{n}$에 대해 약 $2^{n H(X \mid Y)}$개의 조건부로 전형적인 입력 신호가 있습니다. 임의로 선택된 (다른) 입력 신호 $X^{n}$가 $Y^{n}$와 공동으로 전형적일 확률은 약 $2^{n H(X \mid Y)} / 2^{n H(X)}=2^{-n I(X ; Y)}$입니다. 이는 다시 출력 $Y^{n}$를 유발한 코드워드와 혼동될 만한 코드워드 $X^{n}(W)$를 약 $2^{n I(X ; Y)}$개 선택할 수 있음을 시사합니다.

# 7.7 채널 코딩 정리

이제 정보 이론의 가장 기본적인 정리라고 할 수 있는 채널 용량의 달성 가능성을 증명할 것입니다. 이 정리는 1948년 Shannon의 원래 논문에서 처음으로 제시되었고 본질적으로 증명되었습니다. 이 결과는 다소 직관에 반합니다. 채널이 오류를 도입한다면 어떻게 모든 오류를 수정할 수 있습니까? 모든 수정 과정 또한 무한히 오류의 대상이 될 수 있습니다.

Shannon은 채널 용량까지의 모든 속도에서 정보를 채널을 통해 안정적으로 전송할 수 있음을 증명하기 위해 여러 가지 새로운 아이디어를 사용했습니다. 이러한 아이디어는 다음과 같습니다.

- 임의로 작지만 0이 아닌 오류 확률 허용
- 채널을 여러 번 연속적으로 사용하여 대수의 법칙이 적용되도록 함
- 코드북의 무작위 선택에 대한 오류 확률의 평균을 계산하여 확률을 대칭화하고, 이를 통해 적어도 하나의 좋은 코드의 존재를 보여줌

Shannon의 증명 개요는 전형적인 시퀀스의 아이디어에 기반했지만, 증명이 엄밀해진 것은 훨씬 나중이었습니다. 아래의 증명은 전형적인 시퀀스의 속성을 활용하며 지금까지 개발된 증명 중 가장 간단할 것입니다. 모든 증명에서와 마찬가지로, 우리는 동일한 본질적인 아이디어, 즉 무작위 코드 선택, 무작위 코드워드 선택에 대한 평균 오류 확률 계산 등을 사용합니다. 주요 차이점은 디코딩 규칙에 있습니다. 증명에서는 공동 전형성을 사용하여 디코딩합니다. 즉, 수신된 시퀀스와 공동으로 전형적인 코드워드를 찾습니다. 이 속성을 만족하는 고유한 코드워드를 찾으면 해당 단어를 전송된 코드워드로 선언합니다. 속성에 따라

<!-- Page 226 -->
이전에 설명한 공동 전형성(joint typicality)에 따라, 전송된 부호어와 수신된 시퀀스는 확률적으로 관련되어 있으므로 높은 확률로 공동 전형적입니다. 또한, 다른 어떤 부호어가 수신된 시퀀스와 공동 전형적으로 보일 확률은 $2^{-n I}$입니다. 따라서, $2^{n I}$개보다 적은 수의 부호어가 있다면, 높은 확률로 전송된 부호어와 혼동될 수 있는 다른 부호어는 없을 것이며, 오류 확률은 작습니다.

공동 전형 복호화는 최적이 아니지만, 분석이 간단하며 용량(capacity) 이하의 모든 속도에서 달성 가능합니다.

이제 섀넌의 두 번째 정리(Shannon's second theorem)의 완전한 명제와 증명을 제시합니다.

정리 7.7.1 (채널 코딩 정리) 이산 무기억 채널(discrete memoryless channel)에 대해, 용량 $C$ 이하의 모든 속도는 달성 가능합니다. 구체적으로, 모든 속도 $R<C$에 대해, 최대 오류 확률 $\lambda^{(n)} \rightarrow 0$인 $\left(2^{n R}, n\right)$ 부호들의 시퀀스가 존재합니다.

반대로, $\lambda^{(n)} \rightarrow 0$인 모든 $\left(2^{n R}, n\right)$ 부호들의 시퀀스는 $R \leq C$를 만족해야 합니다.

증명: 속도 $R<C$가 달성 가능함을 증명하고, 역정리(converse) 증명은 7.9절로 미룹니다.

달성 가능성: $p(x)$를 고정합니다. $p(x)$ 분포에 따라 무작위로 $\left(2^{n R}, n\right)$ 부호를 생성합니다. 구체적으로, 분포에 따라 독립적으로 $2^{n R}$개의 부호어를 생성합니다.

$$
p\left(x^{n}\right)=\prod_{i=1}^{n} p\left(x_{i}\right)
$$

$2^{n R}$개의 부호어를 행렬의 행으로 나타냅니다.

$$
\mathcal{C}=\left[\begin{array}{cccc}
x_{1}(1) & x_{2}(1) & \cdots & x_{n}(1) \\
\vdots & \vdots & \ddots & \vdots \\
x_{1}\left(2^{n R}\right) & x_{2}\left(2^{n R}\right) & \cdots & x_{n}\left(2^{n R}\right)
\end{array}\right]
$$

이 행렬의 각 항목은 $p(x)$에 따라 i.i.d.로 생성됩니다. 따라서, 특정 부호 $\mathcal{C}$를 생성할 확률은 다음과 같습니다.

$$
\operatorname{Pr}(\mathcal{C})=\prod_{w=1}^{2^{n R}} \prod_{i=1}^{n} p\left(x_{i}(w)\right)
$$
<!-- Page 227 -->
다음 사건들의 순서를 고려하십시오:

1.  $p(x)$에 따라 (7.62)에 설명된 대로 임의의 코드 $\mathcal{C}$가 생성됩니다.
2.  그런 다음 코드 $\mathcal{C}$가 송신자와 수신자 모두에게 공개됩니다. 송신자와 수신자는 채널 전이 행렬 $p(y \mid x)$도 알고 있다고 가정합니다.
3.  균등 분포에 따라 메시지 $W$가 선택됩니다.

    $$
    \operatorname{Pr}(W=w)=2^{-n R}, \quad w=1,2, \ldots, 2^{n R}
    $$

4.  $w$번째 코드워드 $X^{n}(w)$, 즉 $\mathcal{C}$의 $w$번째 행에 해당하는 것이 채널을 통해 전송됩니다.
5.  수신자는 분포에 따라 시퀀스 $Y^{n}$을 수신합니다.

    $$
    P\left(y^{n} \mid x^{n}(w)\right)=\prod_{i=1}^{n} p\left(y_{i} \mid x_{i}(w)\right)
    $$

6.  수신자는 어떤 메시지가 전송되었는지 추측합니다. (오류 확률을 최소화하는 최적의 절차는 최대 사후 확률 복호화입니다. 즉, 수신자는 사후 확률이 가장 높은 메시지를 선택해야 합니다.) 그러나 이 절차는 분석하기 어렵습니다. 대신, 아래에 설명된 공동으로 전형적인 복호화를 사용할 것입니다. 공동으로 전형적인 복호화는 분석하기 쉽고 점근적으로 최적입니다. 공동으로 전형적인 복호화에서 수신자는 다음 조건이 만족되면 인덱스 $\hat{W}$가 전송되었다고 선언합니다:

    *   $\left(X^{n}(\hat{W}), Y^{n}\right)$는 공동으로 전형적입니다.
    *   $\left(X^{n}\left(W^{\prime}\right), Y^{n}\right) \in A_{\epsilon}^{(n)}$인 다른 인덱스 $W^{\prime} \neq \hat{W}$가 없습니다.

    이러한 $\hat{W}$가 존재하지 않거나 여러 개가 있는 경우 오류가 선언됩니다. (이 경우 수신자가 0과 같은 더미 인덱스를 출력한다고 가정할 수 있습니다.)
7.  $\hat{W} \neq W$이면 복호화 오류가 발생합니다. $\mathcal{E}$를 $\{\hat{W} \neq W\}$ 이벤트라고 합시다.

# 오류 확률 분석

개요: 먼저 분석을 개략적으로 설명합니다. 단일 코드에 대한 오류 확률을 계산하는 대신, (7.62)에 따라 무작위로 생성된 모든 코드에 대한 평균을 계산합니다. 코드 구성의 대칭성으로 인해 평균 오류 확률은

<!-- Page 228 -->
전송된 특정 인덱스에 대해. 일반적인 부호어의 경우, 공동으로 전형적인 디코딩을 사용할 때 두 가지 다른 오류 출처가 있습니다. 첫째, 출력 $Y^{n}$이 전송된 부호어와 공동으로 전형적이지 않거나, 둘째, $Y^{n}$과 공동으로 전형적인 다른 부호어가 존재합니다. 전송된 부호어와 수신된 시퀀스가 공동으로 전형적일 확률은 공동 AEP에 의해 보여지듯이 1로 갑니다. 임의의 경쟁 부호어에 대해, 수신된 시퀀스와 공동으로 전형적일 확률은 약 $2^{-n I}$이며, 따라서 오류 확률을 낮게 유지하면서 약 $2^{n I}$개의 부호어를 사용할 수 있습니다. 나중에 최대 오류 확률을 낮추는 코드를 찾기 위해 이 논증을 확장할 것입니다.

오류 확률의 상세 계산: $W$를 $\left\{1,2, \ldots, 2^{n R}\right\}$에서 균등 분포에 따라 추출하고 단계 6에 설명된 대로 공동으로 전형적인 디코딩 $\hat{W}\left(y^{n}\right)$을 사용한다고 가정합니다. 오류 이벤트를 $\mathcal{E}=\left\{\hat{W}\left(Y^{n}\right) \neq W\right\}$로 정의합니다. 모든 부호어에 대해 평균 오류 확률을 계산하고 모든 부호집에 대해 평균을 냅니다. 즉, 다음을 계산합니다.

$$
\begin{aligned}
\operatorname{Pr}(\mathcal{E}) & =\sum_{\mathcal{C}} \operatorname{Pr}(\mathcal{C}) P_{e}^{(n)}(\mathcal{C}) \\
& =\sum_{\mathcal{C}} \operatorname{Pr}(\mathcal{C}) \frac{1}{2^{n R}} \sum_{w=1}^{2^{n R}} \lambda_{w}(\mathcal{C}) \\
& =\frac{1}{2^{n R}} \sum_{w=1}^{2^{n R}} \sum_{\mathcal{C}} \operatorname{Pr}(\mathcal{C}) \lambda_{w}(\mathcal{C})
\end{aligned}
$$

여기서 $P_{e}^{(n)}(\mathcal{C})$는 공동으로 전형적인 디코딩에 대해 정의됩니다. 부호 구성의 대칭성에 의해, 모든 부호에 대해 평균화된 평균 오류 확률은 전송된 특정 인덱스에 의존하지 않습니다 [즉, $\sum_{\mathcal{C}} \operatorname{Pr}(\mathcal{C}) \lambda_{w}(\mathcal{C})$는 $w$에 의존하지 않습니다]. 따라서, 메시지 $W=1$이 전송되었다고 가정해도 일반성을 잃지 않습니다. 왜냐하면

$$
\begin{aligned}
\operatorname{Pr}(\mathcal{E}) & =\frac{1}{2^{n R}} \sum_{w=1}^{2^{n R}} \sum_{\mathcal{C}} \operatorname{Pr}(\mathcal{C}) \lambda_{w}(\mathcal{C}) \\
& =\sum_{\mathcal{C}} \operatorname{Pr}(\mathcal{C}) \lambda_{1}(\mathcal{C}) \\
& =\operatorname{Pr}(\mathcal{E} \mid W=1)
\end{aligned}
$$

다음 이벤트를 정의합니다.

$$
E_{i}=\left\{\left(X^{n}(i), Y^{n}\right) \text { is in } A_{\epsilon}^{(n)}\right\}, \quad i \in\left\{1,2, \ldots, 2^{n R}\right\}
$$
<!-- Page 229 -->
여기서 $E_{i}$는 $i$번째 부호어와 $Y^{n}$이 공동으로 전형적인 사건입니다. $Y^{n}$은 첫 번째 부호어 $X^{n}(1)$을 채널을 통해 전송한 결과임을 기억하십시오.

그러면 디코딩 방식에서 오류가 발생하는 경우는 $E_{1}^{c}$가 발생하거나 (전송된 부호어와 수신된 시퀀스가 공동으로 전형적이지 않은 경우) 또는 $E_{2} \cup E_{3} \cup \cdots \cup E_{2^{n R}}$가 발생하는 경우입니다 (잘못된 부호어가 수신된 시퀀스와 공동으로 전형적인 경우). 따라서 $P(\mathcal{E})$를 $\operatorname{Pr}(\mathcal{E} \mid W=1)$로 표기하면 다음과 같습니다.

$$
\begin{aligned}
\operatorname{Pr}(\mathcal{E} \mid W=1) & =P\left(E_{1}^{c} \cup E_{2} \cup E_{3} \cup \cdots \cup E_{2^{n R}} \mid W=1\right) \\
& \leq P\left(E_{1}^{c} \mid W=1\right)+\sum_{i=2}^{2^{n R}} P\left(E_{i} \mid W=1\right)
\end{aligned}
$$

이는 확률에 대한 사건 합집합의 상한에 의한 것입니다. 이제 공동 AEP에 의해 $P\left(E_{1}^{c} \mid W=1\right) \rightarrow 0$이므로,

$$
P\left(E_{1}^{c} \mid W=1\right) \leq \epsilon \quad \text { for } n \text { sufficiently large. }
$$

부호 생성 과정에 의해 $X^{n}(1)$과 $X^{n}(i)$는 $i \neq 1$일 때 독립적이므로, $Y^{n}$과 $X^{n}(i)$도 독립적입니다. 따라서 $X^{n}(i)$와 $Y^{n}$이 공동으로 전형적일 확률은 공동 AEP에 의해 $\leq 2^{-n(I(X ; Y)-3 \epsilon)}$입니다. 결과적으로,

$$
\begin{aligned}
\operatorname{Pr}(\mathcal{E}) & =\operatorname{Pr}(\mathcal{E} \mid W=1) \leq P\left(E_{1}^{c} \mid W=1\right)+\sum_{i=2}^{2^{n R}} P\left(E_{i} \mid W=1\right) \\
& \leq \epsilon+\sum_{i=2}^{2^{n R}} 2^{-n(I(X ; Y)-3 \epsilon)} \\
& =\epsilon+\left(2^{n R}-1\right) 2^{-n(I(X ; Y)-3 \epsilon)} \\
& \leq \epsilon+2^{3 n \epsilon} 2^{-n(I(X ; Y)-R)} \\
& \leq 2 \epsilon
\end{aligned}
$$

$n$이 충분히 크고 $R<I(X ; Y)-3 \epsilon$인 경우입니다. 따라서 $R<I(X ; Y)$이면, $\epsilon$과 $n$을 선택하여 평균 오류 확률(부호집과 부호어에 대해 평균낸 값)을 $2 \epsilon$보다 작게 만들 수 있습니다.

증명을 마치기 위해 일련의 부호 선택을 통해 이 결론을 강화할 것입니다.

1. 증명에서 $p(x)$를 용량을 달성하는 $X$에 대한 분포인 $p^{*}(x)$로 선택합니다. 그러면 $R<I(X ; Y)$ 조건은 달성 가능 조건인 $R<C$로 대체될 수 있습니다.
<!-- Page 230 -->
2. 코드북에 대한 평균을 제거합니다. 코드북에 대한 평균 오류 확률은 작으므로 $(\leq 2 \epsilon)$, 평균 오류 확률이 작은 코드북 $\mathcal{C}^{*}$가 적어도 하나 존재합니다. 따라서 $\operatorname{Pr}\left(\mathcal{E} \mid \mathcal{C}^{*}\right) \leq 2 \epsilon$입니다. $\mathcal{C}^{*}$의 결정은 모든 $\left(2^{n R}, n\right)$ 코드에 대한 완전 탐색을 통해 달성될 수 있습니다.

$$
\operatorname{Pr}\left(\mathcal{E} \mid \mathcal{C}^{*}\right)=\frac{1}{2^{n R}} \sum_{i=1}^{2^{n R}} \lambda_{i}\left(\mathcal{C}^{*}\right)
$$

(7.63)에 명시된 대로 $\hat{W}$를 균등 분포에 따라 선택했기 때문입니다.
3. 최적 코드북 $\mathcal{C}^{*}$에서 최악의 절반의 코드를 버립니다. 이 코드의 산술 평균 오류 확률 $P_{e}^{(n)}\left(\mathcal{C}^{*}\right)$가 $2 \epsilon$보다 작으므로,

$$
\operatorname{Pr}\left(\mathcal{E} \mid \mathcal{C}^{*}\right) \leq \frac{1}{2^{n R}} \sum \lambda_{i}\left(\mathcal{C}^{*}\right) \leq 2 \epsilon
$$

이는 인덱스 $i$의 절반 이상과 해당 코드 $X^{n}(i)$가 조건부 오류 확률 $\lambda_{i}$가 $4 \epsilon$보다 작아야 함을 의미합니다 (그렇지 않으면, 이 코드 자체들이 합에 $2 \epsilon$보다 더 많이 기여할 것입니다). 따라서 최적 절반의 코드의 최대 오류 확률은 $4 \epsilon$보다 작습니다. 이 코드들을 다시 인덱싱하면 $2^{n R-1}$개의 코드가 있습니다. 코드의 절반을 버리는 것은 속도를 $R$에서 $R-\frac{1}{n}$으로 변경했으며, 이는 $n$이 클 때 무시할 수 있습니다.

이러한 모든 개선 사항을 결합하면, 최대 오류 확률이 $\lambda^{(n)} \leq 4 \epsilon$인 속도 $R^{\prime}=R-\frac{1}{n}$의 코드를 구성했습니다. 이는 용량 이하의 모든 속도의 달성 가능성을 증명합니다.

랜덤 코딩은 정리 7.7.1의 증명 방법이지, 신호 전송 방법이 아닙니다. 코드북은 증명에서 단순히 수학을 대칭화하고 좋은 결정론적 코드의 존재를 보여주기 위해 무작위로 선택됩니다. 우리는 블록 길이 $n$의 모든 코드에 대한 평균이 작은 오류 확률을 갖는다는 것을 증명했습니다. 완전 탐색을 통해 이 집합 내에서 최적의 코드를 찾을 수 있습니다. 덧붙여, 이는 최적 코드의 Kolmogorov 복잡성(14장)이 작은 상수임을 보여줍니다. 이는 최적 코드 $\mathcal{C}^{*}$의 송수신자에게 공개하는 데 채널이 필요하지 않음을 의미합니다. 송수신자는 단순히 채널에 대한 최적의 $\left(2^{n R}, n\right)$ 코드를 사용하기로 합의합니다.

이 정리는 긴 블록 길이에 대해 임의로 작은 오류 확률을 갖는 좋은 코드가 존재함을 보여주지만, 최적 코드를 구성하는 방법을 제공하지는 않습니다. 만약 제안된 방식을 사용한다면
<!-- Page 231 -->
증명에 의해 무작위로 적절한 분포를 가진 코드를 생성하면, 구성된 코드는 긴 블록 길이에 대해 유용할 가능성이 높습니다. 그러나 코드에 어떤 구조도 없다면, 디코딩하기가 매우 어렵습니다 (단순한 테이블 조회 방식은 지수적으로 큰 테이블을 요구합니다). 따라서 이 정리는 실용적인 코딩 방식을 제공하지 않습니다. Shannon의 정보 이론에 관한 최초 논문 이후, 연구자들은 인코딩 및 디코딩이 쉬운 구조화된 코드를 개발하기 위해 노력해 왔습니다. 7.11절에서는 비트 블록에서 하나의 오류를 수정할 수 있는 대수적 오류 정정 코드 계열 중 가장 간단한 Hamming 코드를 논의합니다. Shannon의 논문 이후, 다양한 기법들이 오류 정정 코드를 구성하는 데 사용되었으며, 터보 코드와 함께 가우시안 채널의 용량에 근접했습니다.

# 7.8 무오류 코드

역정리의 증명 개요는 절대 오류가 허용되지 않는 경우의 논증을 통해 가장 명확하게 동기 부여됩니다. 이제 $P_{r}^{(n)}=0$이면 $R \leq C$임을 증명할 것입니다. 무오류 확률을 가진 $\left(2^{n R}, n\right)$ 코드가 있다고 가정합니다 [즉, 디코더 출력 $g\left(Y^{n}\right)$이 확률 1로 입력 인덱스 $W$와 같습니다]. 그러면 입력 인덱스 $W$는 출력 시퀀스에 의해 결정됩니다 [즉, $H\left(W \mid Y^{n}\right)=0$]. 이제 강력한 경계를 얻기 위해 $W$가 $\left\{1,2, \ldots, 2^{n R}\right\}$에 대해 균등하게 분포한다고 임의로 가정합니다. 따라서 $H(W)=n R$입니다. 이제 부등식 문자열을 다음과 같이 작성할 수 있습니다.

$$
\begin{aligned}
n R=H(W) & =\underbrace{H\left(W \mid Y^{n}\right)}_{=0}+I\left(W ; Y^{n}\right) \\
& =I\left(W ; Y^{n}\right) \\
& \stackrel{(a)}{\leq} I\left(X^{n} ; Y^{n}\right) \\
& \stackrel{(b)}{\leq} \sum_{i=1}^{n} I\left(X_{i} ; Y_{i}\right) \\
& \stackrel{(\mathrm{c})}{\leq} n C
\end{aligned}
$$

여기서 (a)는 데이터 처리 부등식에서 비롯되며 ($W \rightarrow X^{n}(W)$ $\rightarrow Y^{n}$이 마르코프 연쇄를 형성함), (b)는 이산 메모리리스 가정(discrete memoryless assumption)을 사용하여 보조 정리 7.9.2에서 증명될 것이며, (c)는 (정보) 용량의 정의에서 비롯됩니다. 따라서 모든 무오류 $\left(2^{n R}, n\right)$ 코드에 대해, 모든 $n$에 대해 다음이 성립합니다.

$$
R \leq C
$$
<!-- Page 232 -->
# 7.9 파노의 부등식과 코딩 정리의 역정리

이제 0 오류 코드에 대해 유도된 증명을 오류 확률이 매우 작은 경우로 확장하겠습니다. 새로운 요소는 조건부 엔트로피에 대한 오류 확률의 하한을 제공하는 파노의 부등식이 될 것입니다. 참조를 위해 새로운 맥락에서 반복되는 파노의 부등식 증명을 기억해 봅시다.

고려 중인 설정을 정의해 보겠습니다. 인덱스 $W$는 집합 $\mathcal{W}=\left\{1,2, \ldots, 2^{n R}\right\}$에서 균일하게 분포되며, 시퀀스 $Y^{n}$은 확률적으로 $W$와 관련이 있습니다. $Y^{n}$으로부터 전송된 인덱스 $W$를 추정합니다. 추정치를 $\hat{W}=g\left(Y^{n}\right)$이라고 합시다. 따라서 $W \rightarrow X^{n}(W) \rightarrow Y^{n} \rightarrow \hat{W}$는 마르코프 연쇄를 형성합니다. 오류 확률은 다음과 같습니다.

$$
\operatorname{Pr}(\hat{W} \neq W)=\frac{1}{2^{n R}} \sum_{i} \lambda_{i}=P_{e}^{(n)}
$$

2.10절에서 증명된 다음 보조정리로 시작하겠습니다.
보조정리 7.9.1 (파노의 부등식) 코드북 $\mathcal{C}$와 $2^{n R}$에 대해 균일하게 분포된 입력 메시지 $W$를 갖는 이산 무기억 채널의 경우, 다음이 성립합니다.

$$
H(W \mid \hat{W}) \leq 1+P_{e}^{(n)} n R
$$

증명: $W$가 균일하게 분포되므로 $P_{e}^{(n)}=\operatorname{Pr}(W \neq \hat{W})$입니다. 크기가 $2^{n R}$인 알파벳에서 $W$에 대해 파노의 부등식(정리 2.10.1)을 적용합니다.

이제 이산 무기억 채널을 여러 번 사용해도 전송당 용량이 증가하지 않음을 보여주는 보조정리를 증명하겠습니다.
보조정리 7.9.2 $Y^{n}$이 용량 $C$의 이산 무기억 채널을 통과한 $X^{n}$의 결과라고 합시다. 그러면

$$
I\left(X^{n} ; Y^{n}\right) \leq n C \quad \text { 모든 } p\left(x^{n}\right) \text{에 대해}
$$

증명

$$
\begin{aligned}
I\left(X^{n} ; Y^{n}\right) & =H\left(Y^{n}\right)-H\left(Y^{n} \mid X^{n}\right) \\
& =H\left(Y^{n}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid Y_{1}, \ldots, Y_{i-1}, X^{n}\right) \\
& =H\left(Y^{n}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{i}\right)
\end{aligned}
$$
<!-- Page 233 -->
이산 무기억 채널(discrete memoryless channel)의 정의에 따라 $Y_{i}$는 $X_{i}$에만 의존하며 다른 모든 것과는 조건부로 독립적입니다. 부등식 시리즈를 계속하면 다음과 같습니다.

$$
\begin{aligned}
I\left(X^{n} ; Y^{n}\right) & =H\left(Y^{n}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{i}\right) \\
& \leq \sum_{i=1}^{n} H\left(Y_{i}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{i}\right) \\
& =\sum_{i=1}^{n} I\left(X_{i} ; Y_{i}\right) \\
& \leq n C
\end{aligned}
$$

여기서 (7.95)는 확률 변수 모음의 엔트로피가 개별 엔트로피의 합보다 작다는 사실에서 비롯되며, (7.97)은 capacity의 정의에서 비롯됩니다. 따라서 채널을 여러 번 사용하는 것이 비트당 정보 capacity를 증가시키지 않음을 증명했습니다.

이제 채널 코딩 정리의 역정리를 증명할 준비가 되었습니다.

증명: 정리 7.7.1 (채널 코딩 정리)의 역정리. 우리는 $\lambda^{(n)} \rightarrow 0$인 모든 $\left(2^{n R}, n\right)$ 코드 시퀀스가 $R \leq C$를 가져야 함을 보여야 합니다. 최대 오류 확률이 0으로 수렴하면 코드 시퀀스에 대한 평균 오류 확률도 0으로 수렴합니다 [즉, $\lambda^{(n)} \rightarrow 0$은 $\left.P_{e}^{(n)} \rightarrow 0\right.$을 의미하며, 여기서 $\left.P_{e}^{(n)}\right.$은 (7.32)에 정의되어 있습니다]. 고정된 인코딩 규칙 $X^{n}(\cdot)$과 고정된 디코딩 규칙 $\hat{W}=g\left(Y^{n}\right)$에 대해, $W \rightarrow X^{n}(W) \rightarrow$ $Y^{n} \rightarrow \hat{W}$입니다. 각 $n$에 대해 $W$를 $\left\{1,2, \ldots, 2^{n R}\right\}$에서 균등 분포에 따라 추출한다고 가정합니다. $W$는 균등 분포를 가지므로, $\operatorname{Pr}(\hat{W} \neq W)=$ $P_{e}^{(n)}=\frac{1}{2^{n R}} \sum_{i} \lambda_{i}$입니다. 따라서,

$$
\begin{aligned}
n R & \stackrel{(\mathrm{a})}{=} H(W) \\
& \stackrel{(\mathrm{b})}{=} H(W \mid \hat{W})+I(W ; \hat{W}) \\
& \stackrel{(\mathrm{c})}{=} 1+P_{e}^{(n)} n R+I(W ; \hat{W}) \\
& \stackrel{(\mathrm{d})}{=} 1+P_{e}^{(n)} n R+I\left(X^{n} ; Y^{n}\right) \\
& \stackrel{(\mathrm{e})}{=} 1+P_{e}^{(n)} n R+n C
\end{aligned}
$$
<!-- Page 234 -->
(a)는 $W$가 $\{1,2, \ldots, 2^{n R}\}$ 위에서 균등 분포를 따른다는 가정에서, (b)는 항등식이며, (c)는 $W$가 최대 $2^{n R}$개의 값을 가질 때의 Fano 부등식이고, (d)는 data-processing inequality이며, (e)는 Lemma 7.9.2에서 비롯됩니다. $n$으로 나누면 다음과 같은 식을 얻습니다.

$$
R \leq P_{e}^{(n)} R+\frac{1}{n}+C
$$

이제 $n \rightarrow \infty$로 보내면, 우변의 첫 두 항이 0으로 수렴하므로,

$$
R \leq C
$$

(7.103)을 다음과 같이 다시 쓸 수 있습니다.

$$
P_{e}^{(n)} \geq 1-\frac{C}{R}-\frac{1}{n R}
$$

이 식은 $R>C$이면, 충분히 큰 $n$에 대해 오류 확률이 0에서 떨어져 있다는 것을 보여줍니다 (따라서 모든 $n$에 대해 그렇습니다. 왜냐하면 만약 작은 $n$에 대해 $P_{e}^{(n)}=0$이라면, 이 코드들을 연결하여 큰 $n$에 대한 $P_{e}^{(n)}=0$인 코드를 구성할 수 있기 때문입니다). 따라서, capacity 이상의 속도에서는 임의로 낮은 오류 확률을 달성할 수 없습니다.

이 converse는 때때로 channel coding theorem의 weak converse라고 불립니다. 또한 strong converse를 증명하는 것도 가능하며, 이는 capacity 이상의 속도에서는 오류 확률이 지수적으로 1로 간다고 말합니다. 따라서, capacity는 매우 명확한 구분점입니다. capacity 이하의 속도에서는 $P_{e}^{(n)} \rightarrow 0$이 지수적으로 일어나고, capacity 이상의 속도에서는 $P_{e}^{(n)} \rightarrow 1$이 지수적으로 일어납니다.

# 7.10 CHANNEL CODING THEOREM CONVERSE의 등식

channel coding theorem과 그 converse를 증명했습니다. 본질적으로 이 정리들은 $R<C$일 때 임의로 낮은 오류 확률로 정보를 전송하는 것이 가능하고, $R>C$일 때 오류 확률이 0에서 떨어져 있다는 것을 말합니다.

converse에서의 등식의 결과를 살펴보는 것은 흥미롭고 보람 있는 일입니다. 이를 통해 capacity를 달성하는 코드의 종류에 대한 아이디어를 얻을 수 있을 것입니다. converse의 단계를 $P_{e}=0$인 경우에 반복하면 다음과 같습니다.

$$
\begin{aligned}
n R & =H(W) \\
& =H(W \mid \hat{W})+I(W ; \hat{W})
\end{aligned}
$$
<!-- Page 235 -->
$$
\begin{aligned}
& =I(W ; \hat{W})) \\
& \stackrel{(a)}{\leq} I\left(X^{n}(W) ; Y^{n}\right) \\
& =H\left(Y^{n}\right)-H\left(Y^{n} \mid X^{n}\right) \\
& =H\left(Y^{n}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{i}\right) \\
& \stackrel{(b)}{\leq} \sum_{i=1}^{n} H\left(Y_{i}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{i}\right) \\
& =\sum_{i=1}^{n} I\left(X_{i} ; Y_{i}\right) \\
& \stackrel{(c)}{\leq} n C .
\end{aligned}
$$

(a)에서의 등식은 data-processing inequality에 따라 $I\left(Y^{n}\right.$; $\left.X^{n}(W) \mid W)=0$이고 $I\left(X^{n} ; Y^{n} \mid \hat{W}\right)=0$일 때 성립합니다. 이는 모든 코드워드가 서로 다르고 $\hat{W}$가 디코딩을 위한 충분 통계량일 때 참입니다. (b)에서의 등식은 $Y_{i}$들이 독립일 때만 성립하며, (c)에서의 등식은 $X_{i}$의 분포가 capacity를 달성하는 $X$ 상의 분포인 $p^{*}(x)$일 때만 성립합니다. 역정리에서의 등식은 이러한 조건들이 만족될 때만 성립합니다. 이는 capacity를 달성하는 zero-error 코드는 서로 다른 코드워드를 가지며, $Y_{i}$들의 분포는 i.i.d.여야 함을 나타냅니다.

$$
p^{*}(y)=\sum_{x} p^{*}(x) p(y \mid x)
$$

이는 최적 분포 $p^{*}(x)$에 의해 유도된 $Y$ 상의 분포입니다. 역정리에서 언급된 분포는 코드워드에 대한 균등 분포에 의해 유도된 $X$와 $Y$ 상의 경험적 분포입니다. 즉,

$$
p\left(x_{i}, y_{i}\right)=\frac{1}{2^{n R}} \sum_{w=1}^{2^{n R}} I\left(X_{i}(w)=x_{i}\right) p\left(y_{i} \mid x_{i}\right)
$$

capacity를 달성하는 코드의 예시에서 이 결과를 확인할 수 있습니다.

1. Noisy typewriter. 이 경우 26개의 문자로 이루어진 입력 알파벳이 있으며, 각 문자는 $\frac{1}{2}$의 확률로 올바르게 출력되거나 다음 문자로 변경됩니다. 이 채널에 대한 capacity $(\log 13)$를 달성하는 간단한 코드는 입력 문자를 격주로 사용하는 것입니다.
<!-- Page 236 -->
두 개의 문자가 혼동될 수 없습니다. 이 경우 블록 길이가 1인 코드워드는 13개입니다. 만약 $\{1,3,5,7, \ldots, 25\}$에서 균일 분포에 따라 i.i.d.로 코드워드를 선택한다면, 채널의 출력 또한 예상대로 i.i.d.이며 $\{1,2, \ldots, 26\}$에서 균일 분포를 따릅니다.
2. 이진 대칭 채널. 임의의 입력 시퀀스가 주어졌을 때, 모든 가능한 출력 시퀀스는 양의 확률을 가지므로, 0의 오류 확률로도 구별할 수 없는 두 개의 코드워드조차도 구별할 수 없을 것입니다. 따라서 BSC의 무오류 용량은 0입니다. 그러나 이 경우에도 유용한 결론을 도출할 수 있습니다. 효율적인 코드는 여전히 $Y$에 대한 분포를 i.i.d. $\sim$ Bernoulli $\left(\frac{1}{2}\right)$처럼 보이도록 유도할 것입니다. 또한, 역방향으로 이어지는 논증으로부터, 용량에 가까운 속도에서는 디코딩 세트가 코드워드에 해당하도록 가능한 출력 시퀀스의 집합을 거의 완전히 덮었음을 알 수 있습니다. 용량 이상의 속도에서는 디코딩 세트가 겹치기 시작하고, 오류 확률을 임의로 작게 만들 수 없게 됩니다.

# 7.11 해밍 코드

채널 코딩 정리는 블록 길이가 충분히 크다면 용량 이하의 속도로 임의로 작은 오류 확률로 정보를 전송할 수 있는 블록 코드의 존재를 약속합니다. Shannon의 원 논문 [471]이 출판된 이후로 사람들은 이러한 코드를 찾아왔습니다. 낮은 오류 확률을 달성하는 것 외에도, 유용한 코드는 "단순해야" 하므로 효율적으로 인코딩 및 디코딩할 수 있어야 합니다.

단순하고 좋은 코드를 찾는 것은 1948년 Shannon의 원 논문 출판 이후로 많은 발전을 이루었습니다. 코딩 이론의 전체 분야가 이러한 탐색 과정에서 개발되었습니다. 1948년 이후 개발된 많은 우아하고 복잡한 코딩 방식을 모두 설명할 수는 없습니다. 가장 기본적인 아이디어를 설명하는 가장 단순한 방식인 Hamming [266]이 개발한 방식을 설명할 것입니다. 이는 대부분의 코드의 근간을 이루는 몇 가지 기본 아이디어를 보여줍니다.

코딩의 목적은 중복성을 도입하여 정보의 일부가 손실되거나 손상되더라도 수신자에서 메시지를 복구할 수 있도록 하는 것입니다. 가장 명백한 코딩 방식은 정보를 반복하는 것입니다. 예를 들어, 1을 보내기 위해 11111을 보내고, 0을 보내기 위해 00000을 보냅니다. 이 방식은 1비트를 보내기 위해 5개의 기호를 사용하므로, 기호당 속도는 $\frac{1}{5}$ 비트입니다. 이 코드가 이진 대칭 채널에서 사용된다면, 최적의 디코딩 방식은 받은 5개의 비트 블록 각각에 대해 다수결 투표를 하는 것입니다. 만약 1이 세 개 이상이면, 우리는 다음과 같이 디코딩합니다.
<!-- Page 237 -->
블록을 1로 디코딩하고, 그렇지 않으면 0으로 디코딩합니다. 세 개 이상의 비트가 변경된 경우에만 오류가 발생합니다. 더 긴 반복 코드를 사용하면 임의로 낮은 오류 확률을 달성할 수 있습니다. 그러나 코드의 속도도 블록 길이에 따라 0으로 수렴하므로, 코드가 "단순"하더라도 실제로 유용한 코드는 아닙니다.

단순히 비트를 반복하는 대신, 각 추가 비트가 정보 비트의 일부 하위 집합에 오류가 있는지 확인하도록 비트를 지능적으로 결합할 수 있습니다. 이에 대한 간단한 예는 패리티 검사 코드입니다. $n-1$개의 정보 비트 블록으로 시작하여, 전체 블록의 패리티가 0이 되도록 (즉, 블록 내 1의 개수가 짝수가 되도록) $n$번째 비트를 선택합니다. 그러면 전송 중에 홀수 개의 오류가 발생하면 수신자는 패리티가 변경되었음을 인지하고 오류를 감지할 수 있습니다. 이것은 오류 감지 코드의 가장 간단한 예입니다. 이 코드는 짝수 개의 오류를 감지하지 못하며 발생하는 오류를 수정하는 방법에 대한 정보를 제공하지 않습니다.

패리티 검사의 아이디어를 확장하여 하나 이상의 패리티 검사 비트를 허용하고 패리티 검사가 정보 비트의 다양한 하위 집합에 의존하도록 할 수 있습니다. 아래에서 설명하는 Hamming 코드는 패리티 검사 코드의 예입니다. 선형 대수의 간단한 아이디어를 사용하여 설명합니다.

Hamming 코드의 원리를 설명하기 위해 블록 길이가 7인 이진 코드를 고려합니다. 모든 연산은 모듈로 2로 수행됩니다. 3개의 길이가 있는 모든 0이 아닌 이진 벡터 집합을 고려합니다. 이를 열로 배열하여 행렬을 형성합니다:

$$
H=\left[\begin{array}{llllll}
0 & 0 & 0 & 1 & 1 & 1 & 1 \\
0 & 1 & 1 & 0 & 0 & 1 & 1 \\
1 & 0 & 1 & 0 & 1 & 0 & 1
\end{array}\right]
$$

$H$의 null 공간에 있는 길이 7의 벡터 집합 (즉, $H$를 곱했을 때 000을 생성하는 벡터)을 고려합니다. 선형 공간 이론에 따르면, $H$의 랭크가 3이므로 $H$의 null 공간은 차원이 4일 것으로 예상됩니다. 이 $2^{4}$개의 코드워드는 다음과 같습니다:

| 0000000 | 0100101 | 1000011 | 1100110 |
| :-- | :-- | :-- | :-- |
| 0001111 | 0101010 | 1001100 | 1101001 |
| 0010110 | 0110011 | 1010101 | 1110000 |
| 0011001 | 0111100 | 1011010 | 1111111 |

코드워드 집합이 행렬의 null 공간이므로, 두 코드워드의 합도 코드워드가 되는 선형성을 가집니다. 따라서 코드워드 집합은 7차원 벡터 공간에서 4차원 선형 부분 공간을 형성합니다.
<!-- Page 238 -->
코드워드를 살펴보면, 모든 0으로 이루어진 코드워드를 제외하고는 어떤 코드워드에서도 최소 3개의 1이 존재함을 알 수 있습니다. 이를 코드의 최소 가중치라고 합니다. 모든 열이 다르므로 두 열이 000이 될 수 없기 때문에 코드의 최소 가중치는 최소 3이어야 합니다. 두 열의 합이 행렬의 열 중 하나가 된다는 사실로부터 최소 거리가 정확히 3임을 알 수 있습니다.

코드가 선형이기 때문에, 두 코드워드의 차이는 또한 코드워드이며, 따라서 두 코드워드는 최소 세 자리에서 다릅니다. 두 코드워드가 다른 최소 자릿수를 코드의 최소 거리라고 합니다. 코드의 최소 거리는 코드워드가 얼마나 떨어져 있는지를 측정하는 척도이며, 채널 출력에서 코드워드가 얼마나 구별될 수 있는지를 결정합니다. 선형 코드의 경우 최소 거리는 최소 가중치와 같습니다. 우리는 큰 최소 거리를 갖는 코드를 개발하는 것을 목표로 합니다.

위에 설명된 코드의 경우, 최소 거리는 3입니다. 따라서 코드워드 $\mathbf{c}$가 한 자리에서 손상되면, 다른 모든 코드워드와 최소 두 자리에서 달라지므로 다른 어떤 코드워드보다 $\mathbf{c}$에 더 가까울 것입니다. 하지만 모든 코드워드를 검색하지 않고 가장 가까운 코드워드를 발견할 수 있을까요?

정답은 '예'입니다. 디코딩을 위해 $H$ 행렬의 구조를 사용할 수 있습니다. 패리티 검사 행렬이라고 불리는 $H$ 행렬은 모든 코드워드 $\mathbf{c}$에 대해 $H \mathbf{c}=0$이라는 속성을 가집니다. $\mathbf{e}_{i}$를 $i$번째 위치에 1을 가지고 나머지 위치는 0인 벡터라고 합시다. 코드워드가 $i$번째 위치에서 손상되면, 수신된 벡터는 $\mathbf{r}=\mathbf{c}+\mathbf{e}_{i}$입니다. 이 벡터에 행렬 $H$를 곱하면 다음과 같습니다.

$$
H \mathbf{r}=H\left(\mathbf{c}+\mathbf{e}_{i}\right)=H \mathbf{c}+H \mathbf{e}_{i}=H \mathbf{e}_{i}
$$

이는 $H$의 $i$번째 열에 해당하는 벡터입니다. 따라서 $H \mathbf{r}$을 보면 벡터의 어느 위치가 손상되었는지 알 수 있습니다. 이 비트를 반전시키면 코드워드를 얻을 수 있습니다. 이는 수신된 시퀀스에서 하나의 오류를 수정하는 간단한 절차를 제공합니다. 우리는 블록 길이 7인 16개의 코드워드를 가진 코드북을 구성했으며, 이는 최대 1개의 오류를 수정할 수 있습니다. 이 코드는 Hamming 코드라고 합니다.

아직 간단한 인코딩 절차를 식별하지 못했습니다. 16개의 메시지 집합에서 코드워드로의 모든 매핑을 사용할 수 있습니다. 그러나 테이블의 코드워드 첫 4비트를 살펴보면, 4비트의 모든 $2^{4}$ 조합을 순환하는 것을 관찰할 수 있습니다. 따라서 이 4비트를 보내고자 하는 메시지의 4비트로 사용할 수 있으며, 나머지 3비트는 코드로 결정됩니다. 일반적으로 선형 코드를 수정하여 매핑을 명시적으로 만들 수 있으므로, 각 코드워드의 첫 $k$ 비트는

<!-- This is an HTML comment and should be preserved. -->
<!-- Page 239 -->
코드워드는 메시지를 나타내며, 마지막 $n-k$ 비트는 패리티 검사 비트입니다. 이러한 코드를 체계적 코드라고 합니다. 코드는 종종 블록 길이 $n$, 정보 비트 수 $k$, 최소 거리 $d$로 식별됩니다. 예를 들어, 위 코드는 $(7,4,3)$ 해밍 코드라고 합니다 (즉, $n=7, k=4$, $d=3$).

해밍 코드가 어떻게 작동하는지 쉽게 이해하는 방법은 벤 다이어그램을 이용하는 것입니다. 그림 7.10과 같이 세 개의 원과 네 개의 교차 영역이 있는 다음 벤 다이어그램을 고려하십시오. 정보 시퀀스 1101을 전송하기 위해 그림에 표시된 대로 4개의 정보 비트를 네 개의 교차 영역에 배치합니다. 그런 다음 나머지 세 영역에 각각 패리티 비트를 배치하여 각 원의 패리티가 짝수가 되도록 합니다 (즉, 각 원에 짝수 개의 1이 있도록 합니다). 따라서 패리티 비트는 그림 7.11에 표시된 대로입니다.

이제 비트 중 하나가 변경되었다고 가정해 봅시다. 예를 들어, 그림 7.12에 표시된 대로 정보 비트 중 하나가 1에서 0으로 변경되었다고 가정해 봅시다. 그러면 두 개의 원 (그림에서 강조 표시됨)에 대해 패리티 제약 조건이 위반되며, 이러한 위반이 주어졌을 때 이를 유발할 수 있는 유일한 단일 비트 오류는 두 원의 교차점 (즉, 변경된 비트)에 있다는 것을 알기 어렵지 않습니다. 마찬가지로 다른 오류 사례를 살펴보면, 이 코드가 수신된 코드워드의 모든 단일 비트 오류를 감지하고 수정할 수 있다는 것을 알기 어렵지 않습니다.

더 큰 행렬 $H$를 구성하기 위해 이 절차를 쉽게 일반화할 수 있습니다. 일반적으로 $H$에 $l$개의 행을 사용하면 얻는 코드는 블록 길이 $n=2^{l}-1$, $k=2^{l}-l-1$, 최소 거리 3을 갖게 됩니다. 이 모든 코드를 해밍 코드라고 하며 하나의 오류를 수정할 수 있습니다.

그림 7.10. 정보 비트가 있는 벤 다이어그램.
<!-- Page 240 -->

그림 7.11. 각 원에 대한 정보 비트와 패리티 비트(짝수 패리티)를 포함하는 벤 다이어그램.

그림 7.12. 정보 비트 중 하나가 변경된 벤 다이어그램.

해밍 코드는 선형 패리티 검사 코드의 가장 간단한 예입니다. 이는 다른 선형 코드의 구성의 기초가 되는 원리를 보여줍니다. 그러나 블록 길이가 길어지면 블록에 하나 이상의 오류가 발생할 가능성이 높습니다. 1950년대 초, Reed와 Solomon은 비이진 채널을 위한 다중 오류 수정 코드 클래스를 발견했습니다. 1950년대 후반, Bose와 Ray-Chaudhuri [72] 및 Hocquenghem [278]은 갈루아 필드 이론을 사용하여 해밍 코드의 아이디어를 일반화하여 임의의 $t$에 대해 $t$ 오류 수정 코드( $B C H$ 코드라고 함)를 구성했습니다. 그 이후로 다양한 저자들이 다른 코드를 개발했으며 효율적인

<!-- Page 241 -->
이러한 코드에 대한 디코딩 알고리즘을 제공합니다. 집적 회로의 등장으로 비교적 복잡한 코드를 하드웨어로 구현하고 Shannon의 채널 용량 정리가 약속한 오류 수정 성능을 일부 실현하는 것이 가능해졌습니다. 예를 들어, 모든 컴팩트 디스크 플레이어에는 두 개의 인터리빙된 $(32,28,5)$ 및 $(28,24,5)$ Reed-Solomon 코드를 기반으로 하는 오류 수정 회로가 포함되어 있어 디코더가 최대 4000개의 오류 버스트를 수정할 수 있습니다.

위에 설명된 모든 코드는 블록 코드입니다. 즉, 정보 비트 블록을 채널 코드워드로 매핑하며 과거 정보 비트에 대한 의존성이 없습니다. 또한 각 출력 블록이 현재 입력 블록뿐만 아니라 과거 입력 중 일부에도 의존하는 코드를 설계할 수도 있습니다. 이러한 코드의 고도로 구조화된 형태를 컨볼루션 코드라고 합니다. 컨볼루션 코드 이론은 지난 40년 동안 상당히 발전했습니다. 자세한 내용은 다루지 않고 관심 있는 독자는 코딩 이론 교재 [69, 356]를 참조하시기 바랍니다.

수년 동안 알려진 코딩 알고리즘 중 어느 것도 Shannon의 채널 용량 정리가 약속한 성능에 근접하지 못했습니다. 교차 확률 $p$를 갖는 이진 대칭 채널의 경우, 길이 $n$ 블록에서 최대 $n p$ 오류를 수정할 수 있고 $n(1-H(p))$ 정보 비트를 갖는 코드가 필요합니다. 예를 들어, 앞에서 제안한 반복 코드는 길이 $n$ 블록에서 최대 $n / 2$ 오류를 수정하지만, 속도는 $n$과 함께 0으로 감소합니다. 1972년까지 길이 $n$ 블록에 대해 $n \alpha$ 오류를 수정할 수 있는 모든 알려진 코드는 점근 속도가 0이었습니다. 1972년에 Justesen [301]은 양의 점근 속도와 블록 길이의 분수로 표현되는 양의 점근 최소 거리를 갖는 코드 클래스를 설명했습니다.

1993년에 Berrou 등의 논문 [57]은 두 개의 인터리빙된 컨볼루션 코드와 병렬 협력 디코더의 조합이 이전 코드보다 훨씬 더 나은 성능을 달성한다는 개념을 도입했습니다. 각 디코더는 각 비트 값에 대한 "의견"을 다른 디코더에 전달하고 다른 디코더의 의견을 사용하여 비트 값 결정을 돕습니다. 이 반복 프로세스는 두 디코더가 비트 값에 동의할 때까지 반복됩니다. 놀라운 사실은 이 반복 절차를 통해 다양한 채널에서 용량에 가까운 속도로 효율적인 디코딩이 가능하다는 것입니다. 또한 Robert Gallager가 그의 논문 [231, 232]에서 소개한 저밀도 패리티 검사(LDPC) 코드 이론에 대한 관심이 다시 살아났습니다. 1997년에 MacKay와 Neal [368]은 터보 코드 디코딩에 사용되는 알고리즘과 유사한 반복 메시지 전달 알고리즘이 LDPC 코드에 대해 높은 확률로 용량에 가까운 속도를 달성할 수 있음을 보여주었습니다. 터보 코드와 LDPC 코드는 모두 활발한 연구 분야로 남아 있으며 무선 및 위성 통신 채널에 적용되었습니다.
<!-- Page 242 -->

그림 7.13. 피드백이 있는 이산 메모리 없는 채널.

# 7.12 피드백 용량

피드백이 있는 채널은 그림 7.13에 나와 있습니다. 수신된 모든 심볼이 즉시 그리고 잡음 없이 송신기로 다시 전송되며, 송신기는 이를 사용하여 다음에 보낼 심볼을 결정할 수 있다고 가정합니다. 피드백을 사용하면 더 나아질 수 있습니까? 놀라운 답은 그렇지 않다는 것이며, 이제 이를 증명할 것입니다. 우리는 $\left(2^{n R}, n\right)$ 피드백 코드를 각 $x_{i}$가 메시지 $W \in 2^{n R}$와 이전 수신 값 $Y_{1}, Y_{2}, \ldots, Y_{i-1}$의 함수인 매핑 시퀀스 $x_{i}\left(W, Y^{i-1}\right)$와 디코딩 함수 시퀀스 $g: \mathcal{Y}^{n} \rightarrow\left\{1,2, \ldots, 2^{n R}\right\}$로 정의합니다. 따라서,

$$
P_{e}^{(n)}=\operatorname{Pr}\left\{g\left(Y^{n}\right) \neq W\right\}
$$

$W$가 $\left\{1,2, \ldots, 2^{n R}\right\}$에 걸쳐 균일하게 분포될 때입니다.
정의 이산 메모리 없는 채널의 피드백 용량 $C_{\mathrm{F} B}$는 피드백 코드로 달성 가능한 모든 속도의 상한입니다.

정리 7.12.1 (피드백 용량)

$$
C_{\mathrm{F} B}=C=\max _{p(x)} I(X ; Y)
$$

증명: 피드백 없는 코드는 피드백 코드의 특수한 경우이므로, 피드백 없이 달성 가능한 모든 속도는 피드백으로 달성 가능하며, 따라서

$$
C_{\mathrm{F} B} \geq C
$$

반대 방향의 부등식을 증명하는 것은 약간 더 까다롭습니다. 피드백 없는 코딩 정리에 대한 역증명에 사용했던 것과 동일한 증명을 사용할 수 없습니다. $X_{i}$가 과거 수신 심볼에 의존하고, $Y_{i}$가 $X_{i}$에만 의존하고 (7.93)에서 미래 $X$와 조건부로 독립적이라는 것이 더 이상 참이 아니므로, 보조 정리 7.9.2는 더 이상 참이 아닙니다.
<!-- Page 243 -->
증명 문제를 해결할 간단한 변경 사항이 있습니다. $X^{n}$을 사용하는 대신, 인덱스 $W$를 사용하고 유사한 일련의 부등식을 증명할 것입니다. $W$가 $\left\{1,2, \ldots, 2^{n R}\right\}$ 위에서 균등하게 분포한다고 가정합니다. 그러면 $\operatorname{Pr}(W \neq \hat{W})=P_{e}^{(n)}$이고

$$
\begin{aligned}
n R=H(W) & =H(W \mid \hat{W})+I(W ; \hat{W}) \\
& \leq 1+P_{e}^{(n)} n R+I(W ; \hat{W}) \\
& \leq 1+P_{e}^{(n)} n R+I\left(W ; Y^{n}\right)
\end{aligned}
$$

이는 Fano의 부등식과 데이터 처리 부등식에 의해 성립합니다. 이제 $I\left(W ; Y^{n}\right)$을 다음과 같이 상한할 수 있습니다.

$$
\begin{aligned}
I\left(W ; Y^{n}\right) & =H\left(Y^{n}\right)-H\left(Y^{n} \mid W\right) \\
& =H\left(Y^{n}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid Y_{1}, Y_{2}, \ldots, Y_{i-1}, W\right) \\
& =H\left(Y^{n}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid Y_{1}, Y_{2}, \ldots, Y_{i-1}, W, X_{i}\right) \\
& =H\left(Y^{n}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{i}\right)
\end{aligned}
$$

$X_{i}$가 $Y_{1}, \ldots, Y_{i-1}$ 및 $W$의 함수이고, $X_{i}$가 주어졌을 때 $Y_{i}$는 $W$ 및 과거 $Y$ 샘플과 독립이기 때문입니다. 계속 진행하면 다음과 같습니다.

$$
\begin{aligned}
I\left(W ; Y^{n}\right) & =H\left(Y^{n}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{i}\right) \\
& \leq \sum_{i=1}^{n} H\left(Y_{i}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{i}\right) \\
& =\sum_{i=1}^{n} I\left(X_{i} ; Y_{i}\right) \\
& \leq n C
\end{aligned}
$$

이는 이산 메모리 없는 채널의 capacity 정의에서 비롯됩니다. 이들을 종합하면 다음과 같은 결과를 얻습니다.

$$
n R \leq P_{e}^{(n)} n R+1+n C
$$
<!-- Page 244 -->
그리고 $n$으로 나누고 $n \rightarrow \infty$로 보내면 다음과 같은 결론을 내릴 수 있습니다.

$$
R \leq C
$$

따라서 피드백이 없는 경우보다 더 높은 속도를 달성할 수 없으며,

$$
C_{F B}=C
$$

이진 삭제 채널의 예에서 보았듯이 피드백은 인코딩 및 디코딩을 단순화하는 데 크게 도움이 될 수 있습니다. 그러나 채널 용량을 증가시킬 수는 없습니다.

# 7.13 소스-채널 분리 정리

이제까지 증명한 두 가지 주요 결과, 즉 데이터 압축 ($R>H$: 정리 5.4.2)과 데이터 전송 ($R<$ $C$: 정리 7.7.1)을 결합할 때입니다. 소스를 채널로 보내기 위해 $H<C$ 조건이 필요충분조건입니까? 예를 들어, 이산 메모리 없는 채널을 통해 디지털화된 음성이나 음악을 보내는 것을 고려해 보십시오. 음성 샘플 시퀀스를 채널 입력으로 직접 매핑하는 코드를 설계하거나, 음성을 가장 효율적인 표현으로 압축한 다음 적절한 채널 코드를 사용하여 채널을 통해 보내는 코드를 설계할 수 있습니다. 데이터 압축은 채널에 의존하지 않고 채널 코딩은 소스 분포에 의존하지 않기 때문에 두 단계 방법을 사용함으로써 무언가를 잃고 있지 않다는 것이 즉시 명확하지는 않습니다.

이 섹션에서는 두 단계 방법이 노이즈가 있는 채널을 통해 정보를 전송하는 다른 어떤 방법보다도 우수함을 증명할 것입니다. 이 결과는 몇 가지 중요한 실용적인 함의를 갖습니다. 이는 통신 시스템 설계를 소스 코딩과 채널 코딩의 두 부분으로 결합하여 고려할 수 있음을 의미합니다. 데이터의 가장 효율적인 표현을 위한 소스 코드를 설계할 수 있습니다. 채널에 적합한 채널 코드를 별도로 독립적으로 설계할 수 있습니다. 이 조합은 두 문제를 함께 고려하여 설계할 수 있는 어떤 것보다도 효율적일 것입니다.

모든 종류의 데이터를 위한 일반적인 표현은 이진 알파벳을 사용합니다. 대부분의 현대 통신 시스템은 디지털이며, 데이터는 공통 채널을 통해 전송하기 위해 이진 표현으로 축소됩니다. 이는 복잡성을 엄청나게 줄여줍니다. ATM 네트워크 및 인터넷과 같은 네트워크는 음성, 비디오 및 디지털 데이터가 동일한 통신 채널을 사용할 수 있도록 공통 이진 표현을 사용합니다.
<!-- Page 245 -->
결과적으로, 두 단계 프로세스가 단일 단계 프로세스만큼 좋다는 것은 너무 명백해 보이지만 항상 그렇지는 않다는 점을 지적하는 것이 적절할 수 있습니다. 분해가 실패하는 다중 사용자 채널의 예가 있습니다. 또한 정리가 오해의 소지가 있는 것처럼 보이는 두 가지 간단한 상황을 고려합니다. 간단한 예는 삭제 채널을 통해 영어 텍스트를 보내는 것입니다. 텍스트의 가장 효율적인 이진 표현을 찾아 채널을 통해 보낼 수 있습니다. 그러나 오류를 디코딩하기가 매우 어려울 것입니다. 그러나 영어 텍스트를 채널을 통해 직접 보내면 글자의 절반까지 손실될 수 있지만 메시지를 이해할 수 있습니다. 마찬가지로, 인간의 귀는 백색 잡음이 있는 경우 매우 높은 잡음 수준에서도 음성을 구별할 수 있게 하는 몇 가지 특이한 속성을 가지고 있습니다. 이러한 경우 압축된 버전 대신 압축되지 않은 음성을 노이즈 채널을 통해 보내는 것이 적절할 수 있습니다. 분명히 소스의 중복성은 채널에 적합합니다.

고려 중인 설정을 정의해 보겠습니다. 우리는 알파벳 $\mathcal{V}$에서 기호를 생성하는 소스 $V$를 가지고 있습니다. 우리는 $V$가 생성하는 확률 과정의 종류에 대해 유한 알파벳에서 생성되고 AEP를 만족한다는 것 외에는 어떠한 가정도 하지 않을 것입니다. 이러한 프로세스의 예로는 i.i.d. 랜덤 변수의 시퀀스와 정상 불변 마르코프 체인의 상태 시퀀스가 있습니다. 섹션 16.8에서 보여주듯이 모든 정상 에르고딕 소스는 AEP를 만족합니다.

수신자가 시퀀스를 재구성할 수 있도록 채널을 통해 기호 시퀀스 $V^{n}=V_{1}, V_{2}, \ldots, V_{n}$를 보내고 싶습니다. 이를 위해 시퀀스를 코드워드 $X^{n}\left(V^{n}\right)$로 매핑하고 채널을 통해 코드워드를 보냅니다. 수신자는 수신된 시퀀스 $Y^{n}$를 보고 보낸 시퀀스 $V^{n}$의 추정값 $\hat{V}^{n}$을 만듭니다. $V^{n} \neq \hat{V^{n}}$이면 수신자는 오류를 범합니다. 오류 확률을 다음과 같이 정의합니다.

$$
\operatorname{Pr}\left(V^{n} \neq \hat{V^{n}}\right)=\sum_{y^{n}} \sum_{v^{n}} p\left(v^{n}\right) p\left(y^{n} \mid x^{n}\left(v^{n}\right)\right) I\left(g\left(y^{n}\right) \neq v^{n}\right)
$$

여기서 $I$는 지시 함수이고 $g\left(y^{n}\right)$는 디코딩 함수입니다. 시스템은 그림 7.14에 나와 있습니다.

이제 공동 소스-채널 코딩 정리를 명시할 수 있습니다.

그림 7.14. 공동 소스 및 채널 코딩.
<!-- Page 246 -->
정리 7.13.1 (소스-채널 코딩 정리) $V_{1}, V_{2}, \ldots V^{n}$이 AEP를 만족하는 유한 알파벳 확률 과정이고 $H(\mathcal{V})<$ $C$이면, 오류 확률 $\operatorname{Pr}\left(\hat{V}^{n} \neq\right.$ $\left.V^{n}\right) \rightarrow 0$인 소스-채널 코드가 존재합니다. 반대로, 모든 정상 확률 과정에 대해 $H(\mathcal{V})>C$이면, 오류 확률은 0에서 벗어나 있으며, 임의로 낮은 오류 확률로 프로세스를 채널을 통해 전송하는 것은 불가능합니다.

증명: 달성 가능성. 증명의 순방향 부분의 핵심은 앞서 설명한 두 단계 인코딩입니다. 확률 과정이 AEP를 만족한다고 가정했으므로, 이는 확률의 대부분을 포함하는 크기 $\leq 2^{n(H(\mathcal{V})+\epsilon)}$의 일반 집합 $A_{\epsilon}^{(n)}$이 존재함을 의미합니다. 일반 집합에 속하는 소스 시퀀스만 인코딩할 것입니다. 다른 모든 시퀀스는 오류를 발생시킵니다. 이는 오류 확률에 최대 $\epsilon$을 기여할 것입니다.

$A_{\epsilon}^{(n)}$에 속하는 모든 시퀀스에 인덱스를 부여합니다. 이러한 시퀀스는 최대 $2^{n(H+\epsilon)}$개이므로, 이를 인덱싱하는 데는 $n(H+\epsilon)$ 비트가 충분합니다. 만약

$$
H(\mathcal{V})+\epsilon=R<C
$$

이면, 수신자에게 원하는 인덱스를 오류 확률이 $\epsilon$보다 작게 전송할 수 있습니다.

수신자는 일반 집합 $A_{\epsilon}^{(n)}$을 열거하고 추정된 인덱스에 해당하는 시퀀스를 선택하여 $V^{n}$을 재구성할 수 있습니다. 이 시퀀스는 높은 확률로 전송된 시퀀스와 일치할 것입니다. 정확히 말하면,

$$
\begin{aligned}
P\left(V^{n} \neq \hat{V}^{n}\right) & \leq P\left(V^{n} \notin A_{\epsilon}^{(n)}\right)+P\left(g\left(Y^{n}\right) \neq V^{n} \mid V^{n} \in A_{\epsilon}^{(n)}\right) \\
& \leq \epsilon+\epsilon=2 \epsilon
\end{aligned}
$$

$n$이 충분히 크면 성립합니다. 따라서 $n$이 충분히 크면 다음과 같은 경우 시퀀스를 낮은 오류 확률로 재구성할 수 있습니다.

$$
H(\mathcal{V})<C
$$

반증: 모든 소스-채널 코드 시퀀스에 대해 $\operatorname{Pr}\left(\hat{V}^{n} \neq V^{n}\right) \rightarrow 0$이 $H(\mathcal{V})$ $\leq C$를 의미함을 보여야 합니다.

$$
\begin{aligned}
& X^{n}\left(V^{n}\right): \mathcal{V}^{n} \rightarrow \mathcal{X}^{n} \\
& g_{n}\left(Y^{n}\right): \mathcal{Y}^{n} \rightarrow \mathcal{V}^{n}
\end{aligned}
$$
<!-- Page 247 -->
따라서 $X^{n}(\cdot)$는 데이터 시퀀스 $V^{n}$에 대한 임의의 (아마도 무작위적인) 코드워드 할당이며, $g_{n}(\cdot)$는 출력 시퀀스 $Y^{n}$에 대한 추정치 $\hat{V}^{n}$의 할당인 디코딩 함수입니다. Fano의 부등식에 따라 다음이 성립해야 합니다.

$$
H\left(V^{n} \mid \hat{V}^{n}\right) \leq 1+\operatorname{Pr}\left(\hat{V}^{n} \neq V^{n}\right) \log \left|\mathcal{V}^{n}\right|=1+\operatorname{Pr}\left(\hat{V}^{n} \neq V^{n}\right) n \log |\mathcal{V}|
$$

따라서 코드에 대해 다음이 성립합니다.

$$
\begin{aligned}
H(\mathcal{V}) & \stackrel{(a)}{\leq} \frac{H\left(V_{1}, V_{2}, \ldots, V_{n}\right)}{n} \\
& =\frac{H\left(V^{n}\right)}{n} \\
& =\frac{1}{n} H\left(V^{n} \mid \hat{V}^{n}\right)+\frac{1}{n} I\left(V^{n} ; \hat{V}^{n}\right) \\
& \stackrel{(b)}{\leq} \frac{1}{n}\left(1+\operatorname{Pr}\left(\hat{V}^{n} \neq V^{n}\right) n \log |\mathcal{V}|\right)+\frac{1}{n} I\left(V^{n} ; \hat{V}^{n}\right) \\
& \stackrel{(c)}{\leq} \frac{1}{n}\left(1+\operatorname{Pr}\left(\hat{V}^{n} \neq V^{n}\right) n \log |\mathcal{V}|\right)+\frac{1}{n} I\left(X^{n} ; Y^{n}\right) \\
& \stackrel{(d)}{\leq} \frac{1}{n}+\operatorname{Pr}\left(\hat{V}^{n} \neq V^{n}\right) \log |\mathcal{V}|+C
\end{aligned}
$$

여기서 (a)는 정상 과정의 엔트로피율의 정의에서 비롯되고, (b)는 Fano의 부등식에서 비롯되며, (c)는 데이터 처리 부등식에서 비롯됩니다 (왜냐하면 $V^{n} \rightarrow X^{n} \rightarrow Y^{n} \rightarrow \hat{V}^{n}$은 마르코프 연쇄를 형성하기 때문입니다). (d)는 채널의 무기억성에서 비롯됩니다. 이제 $n \rightarrow \infty$로 보내면 $\operatorname{Pr}\left(\hat{V}^{n} \neq V^{n}\right) \rightarrow 0$이므로 다음이 성립합니다.

$$
H(\mathcal{V}) \leq C
$$

따라서, 정상 에르고딕 소스를 채널을 통해 전송할 수 있는 것은 소스의 엔트로피율이 채널의 capacity보다 작을 때와 그때뿐입니다. 조인트 소스-채널 분리 정리는 소스 코딩 문제를 채널 코딩 문제와 분리하여 고려할 수 있게 합니다. 소스 코더는 소스의 가장 효율적인 표현을 찾으려고 노력하고, 채널 코더는 채널에 의해 도입된 잡음과 오류에 대처하기 위해 메시지를 인코딩합니다. 분리 정리는 별도의 인코더(그림 7.15)가 조인트 인코더(그림 7.14)와 동일한 비율을 달성할 수 있다고 말합니다.

이 결과로 우리는 정보 이론의 두 가지 기본 정리인 데이터 압축과 데이터 전송을 연결했습니다. 두 결과의 증명을 몇 마디로 요약해 보겠습니다. 데이터
<!-- Page 248 -->

그림 7.15. 소스 코딩과 채널 코딩의 분리.
압축 정리는 AEP의 결과로, 모든 가능한 소스 시퀀스 중에서 확률이 높은 "작은" 부분집합(크기 $2^{n H}$)이 존재하며, 따라서 $H$ 비트/심볼을 사용하여 낮은 오류 확률로 소스를 표현할 수 있음을 보여줍니다. 데이터 전송 정리는 결합 AEP에 기반하며, 긴 블록 길이에 대해 채널의 출력 시퀀스가 입력 코드워드와 결합적으로 전형적일 가능성이 높고, 다른 코드워드는 확률 $\approx 2^{-n I}$로 결합적으로 전형적이라는 사실을 사용합니다. 따라서 약 $2^{n I}$개의 코드워드를 사용해도 오류 확률이 무시할 수 있을 정도로 낮습니다. 소스-채널 분리 정리는 소스 코드와 채널 코드를 별도로 설계하고 그 결과를 결합하여 최적의 성능을 달성할 수 있음을 보여줍니다.

# 요약

채널 용량. 구별 가능한 입력의 수에 대한 로그는 다음과 같이 주어집니다.

$$
C=\max _{p(x)} I(X ; Y)
$$

## 예시

- 이진 대칭 채널: $C=1-H(p)$.
- 이진 삭제 채널: $C=1-\alpha$.
- 대칭 채널: $C=\log |\mathcal{Y}|-H$ (전이 행렬의 행).

## $C$의 속성

1. $0 \leq C \leq \min \{\log |\mathcal{X}|, \log |\mathcal{Y}|\}$.
2. $I(X ; Y)$는 $p(x)$에 대한 연속적인 오목 함수입니다.

결합적 전형성. 분포 $p(x, y)$에 대한 결합적으로 전형적인 시퀀스 $\left\{\left(x^{n}, y^{n}\right)\right\}$의 집합 $A_{\epsilon}^{(n)}$은 다음과 같이 주어집니다.

$$
\begin{gathered}
A_{\epsilon}^{(n)}=\left\{\left(x^{n}, y^{n}\right) \in \mathcal{X}^{n} \times \mathcal{Y}^{n}:\right. \\
\left|-\frac{1}{n} \log p\left(x^{n}\right)-H(X)\right|<\epsilon
\end{gathered}
$$
<!-- Page 249 -->
$$
\begin{aligned}
& \left|-\frac{1}{n} \log p\left(y^{n}\right)-H(Y)\right|<\epsilon \\
& \left|-\frac{1}{n} \log p\left(x^{n}, y^{n}\right)-H(X, Y)\right|<\epsilon
\end{aligned}
$$

여기서 $p\left(x^{n}, y^{n}\right)=\prod_{i=1}^{n} p\left(x_{i}, y_{i}\right)$입니다.
결합 AEP. $\left(X^{n}, Y^{n}\right)$이 $p\left(x^{n}, y^{n}\right)=\prod_{i=1}^{n} p\left(x_{i}, y_{i}\right)$에 따라 i.i.d.로 추출된 길이 $n$의 시퀀스라고 가정합니다. 그러면 다음이 성립합니다.

1. $\operatorname{Pr}\left(\left(X^{n}, Y^{n}\right) \in A_{\epsilon}^{(n)}\right) \rightarrow 1$ as $n \rightarrow \infty$.
2. $\left|A_{\epsilon}^{(n)}\right| \leq 2^{n(H(X, Y)+\epsilon)}$.
3. $\left(\tilde{X}^{n}, \tilde{Y}^{n}\right) \sim p\left(x^{n}\right) p\left(y^{n}\right)$이면, $\operatorname{Pr}\left(\left(\tilde{X}^{n}, \tilde{Y}^{n}\right) \in A_{\epsilon}^{(n)}\right)$ $\leq 2^{-n(I(X ; Y)-3 \epsilon)}$입니다.

채널 코딩 정리. 용량 $C$ 이하의 모든 속도는 달성 가능하며, 용량 이상의 모든 속도는 불가능합니다. 즉, 모든 속도 $R<C$에 대해 오류 확률 $\lambda^{(n)} \rightarrow 0$인 $\left(2^{n R}, n\right)$ 코드 시퀀스가 존재합니다. 반대로, 속도 $R>C$의 경우 $\lambda^{(n)}$은 0에서 벗어나 제한됩니다.

피드백 용량. 피드백은 이산 메모리 없는 채널의 용량을 증가시키지 않습니다 (즉, $C_{\mathrm{F} B}=C$).

소스-채널 정리. 엔트로피율 $H$를 갖는 확률 과정은 $H>C$이면 이산 메모리 없는 채널을 통해 안정적으로 전송될 수 없습니다. 반대로, 프로세스가 AEP를 만족하면 $H<C$이면 소스를 안정적으로 전송할 수 있습니다.

# 문제

7.1 출력 전처리. 전이 확률 $p(y \mid x)$와 채널 용량 $C=\max _{p(x)} I(X ; Y)$를 갖는 통신 채널이 주어졌습니다. 유용한 통계학자가 $\tilde{Y}=g(Y)$를 형성하여 출력을 전처리합니다. 그는 이것이 용량을 엄격하게 개선할 것이라고 주장합니다.
(a) 그가 틀렸음을 증명하십시오.
(b) 어떤 조건에서 용량을 엄격하게 감소시키지 않습니까?
<!-- Page 250 -->
7.2 가산 잡음 채널. 다음 이산 무기억 채널의 채널 용량을 찾으십시오:

여기서 $\operatorname{Pr}\{Z=0\}=\operatorname{Pr}\{Z=a\}=\frac{1}{2}$입니다. $x$의 알파벳은 $\mathbf{X}=$ $\{0,1\}$입니다. $Z$가 $X$와 독립이라고 가정합니다. 채널 용량이 $a$의 값에 따라 달라진다는 점에 유의하십시오.
7.3 기억이 있는 채널이 더 높은 용량을 가집니다. 이진 대칭 채널을 고려하십시오. 여기서 $Y_{i}=X_{i} \oplus Z_{i}$이고, $\oplus$는 $\bmod 2$ 덧셈이며, $X_{i}, Y_{i} \in\{0,1\}$입니다. $\left\{Z_{i}\right\}$가 일정한 주변 확률 $\operatorname{Pr}\left\{Z_{i}=1\right\}=p=1-\operatorname{Pr}\left\{Z_{i}=0\right\}$을 가지지만, $Z_{1}, Z_{2}$, $\ldots, Z_{n}$이 반드시 독립은 아니라고 가정합니다. $Z^{n}$이 입력 $X^{n}$과 독립이라고 가정합니다. $C=1-H(p, 1-p)$라고 할 때, $\max _{p\left(x_{1}, x_{2}, \ldots, x_{n}\right)} I\left(X_{1}, X_{2}, \ldots, X_{n} ; Y_{1}, Y_{2}, \ldots\right.$, $\left.Y_{n}\right) \geq n C$임을 보이십시오.
7.4 채널 용량. 다음 이산 무기억 채널을 고려하십시오: $Y=$ $X+Z(\bmod 11)$, 여기서

$$
Z=\left(\begin{array}{ccc}
1, & 2, & 3 \\
\frac{1}{3}, & \frac{1}{3}, & \frac{1}{3}
\end{array}\right)
$$

이고 $X \in\{0,1, \ldots, 10\}$입니다. $Z$가 $X$와 독립이라고 가정합니다.
(a) 용량을 찾으십시오.
(b) 최적화하는 $p^{*}(x)$는 무엇입니까?
7.5 두 채널을 동시에 사용하기. 두 개의 이산 무기억 채널 $\left(\mathcal{X}_{1}, p\left(y_{1} \mid x_{1}\right), \mathcal{Y}_{1}\right)$와 $\left(\mathcal{X}_{2}, p\left(y_{2} \mid x_{2}\right), \mathcal{Y}_{2}\right)$를 각각 용량 $C_{1}$과 $C_{2}$로 고려합니다. 새로운 채널 $\left(\mathcal{X}_{1} \times \mathcal{X}_{2}, p\left(y_{1} \mid\right.\right.$ $\left.x_{1}\right) \times p\left(y_{2} \mid x_{2}\right), \mathcal{Y}_{1} \times \mathcal{Y}_{2}$ )가 형성되며, 여기서 $x_{1} \in \mathcal{X}_{1}$와 $x_{2} \in \mathcal{X}_{2}$가 동시에 전송되어 $y_{1}, y_{2}$를 생성합니다. 이 채널의 용량을 찾으십시오.
7.6 잡음 있는 타자기. 26개의 키를 가진 타자기를 고려합니다.
(a) 키를 누르면 해당 문자가 인쇄된다면, 비트 단위의 용량 $C$는 얼마입니까?
<!-- Page 251 -->
(b) 이제 키를 누르면 해당 문자 또는 다음 문자(동일한 확률)가 인쇄된다고 가정합니다. 따라서 $A \rightarrow A$ 또는 $B, \ldots, Z \rightarrow$ $Z$ 또는 $A$입니다. 용량은 얼마입니까?
(c) 파트 (b)의 채널에 대해 오류 확률이 0이 되는 블록 길이 1의 가장 높은 속도 코드를 찾으십시오.
7.7 이진 대칭 채널의 캐스케이드. 각 원시 오류 확률 $p$를 갖는 $n$개의 동일한 독립 이진 대칭 채널의 캐스케이드가 다음과 같음을 보여주십시오.

$$
X_{0} \rightarrow \mathrm{BSC} \rightarrow X_{1} \rightarrow \cdots \rightarrow X_{n-1} \rightarrow \mathrm{BSC} \rightarrow X_{n}
$$

오류 확률이 $\frac{1}{2}\left(1-(1-2 p)^{n}\right)$인 단일 BSC와 동등하며, 따라서 $p \neq 0,1$이면 $\lim _{n \rightarrow \infty} I\left(X_{0} ; X_{n}\right)$ $=0$입니다. 중간 단자 $X_{1}, \ldots, X_{n-1}$에서는 인코딩 또는 디코딩이 수행되지 않습니다. 따라서 캐스케이드의 용량은 0으로 수렴합니다.
7.8 Z-채널. Z-채널은 이진 입력 및 출력 알파벳과 다음 행렬로 주어진 전이 확률 $p(y \mid x)$을 갖습니다.

$$
Q=\left[\begin{array}{cc}
1 & 0 \\
1 / 2 & 1 / 2
\end{array}\right] \quad x, y \in\{0,1\}
$$

Z-채널의 용량과 최대화하는 입력 확률 분포를 찾으십시오.
7.9 비최적 코드. 문제 7.8의 Z-채널에 대해 각 코드워드가 공정한 동전 던지기의 시퀀스인 $\left(2^{n R}, n\right)$ 코드를 무작위로 선택한다고 가정합니다. 이것은 용량을 달성하지 못할 것입니다. 블록 길이 $n$이 무한대로 갈 때 평균적으로 무작위로 생성된 코드에 대한 오류 확률 $P_{e}^{(n)}$이 0으로 수렴하는 최대 속도 $R$을 찾으십시오.
7.10 제로 오류 용량. 알파벳 $\{0,1,2,3,4\}$를 갖는 채널은 다음과 같은 형태의 전이 확률을 갖습니다.

$$
p(y \mid x)=\left\{\begin{array}{cl}
1 / 2 & \text { if } y=x \pm 1 \bmod 5 \\
0 & \text { otherwise }
\end{array}\right.
$$

(a) 이 채널의 용량을 비트 단위로 계산하십시오.
<!-- Page 252 -->
(b) 채널의 제로-에러 용량은 제로 확률의 에러로 전송될 수 있는 채널 사용당 비트 수입니다. 명확하게, 이 오각형 채널의 제로-에러 용량은 최소 1비트입니다 (확률 $1/2$로 0 또는 1을 전송). 제로-에러 용량이 1비트보다 크다는 것을 보여주는 블록 코드를 찾으십시오. 제로-에러 용량의 정확한 값을 추정할 수 있습니까? (힌트: 이 채널에 대해 길이 2의 코드를 고려하십시오.) 이 채널의 제로-에러 용량은 Lovasz [365]에 의해 최종적으로 발견되었습니다.

7.11 시변 채널. 시변 이산 무기억 채널을 고려하십시오.
$Y_{1}, Y_{2}, \ldots, Y_{n}$이 $X_{1}, X_{2}, \ldots, X_{n}$이 주어졌을 때 조건부로 독립이며, 조건부 분포가 $p(\mathbf{y} \mid \mathbf{x})=\prod_{i=1}^{n}$ $p_{i}\left(y_{i} \mid x_{i}\right)$로 주어집니다. $\mathbf{X}=\left(X_{1}, X_{2}, \ldots, X_{n}\right), \mathbf{Y}=\left(Y_{1}, Y_{2}, \ldots, Y_{n}\right)$이라고 합시다. $\max _{p(\mathbf{x})} I(\mathbf{X} ; \mathbf{Y})$를 찾으십시오.

7.12 사용되지 않는 심볼. 확률 전이 행렬을 갖는 채널의 용량이

$$
P_{y \mid x}=\left[\begin{array}{ccc}
\frac{2}{3} & \frac{1}{3} & 0 \\
\frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
0 & \frac{1}{3} & \frac{2}{3}
\end{array}\right]
$$

입력 심볼 중 하나에 제로 확률을 부여하는 분포에 의해 달성됨을 보이십시오. 이 채널의 용량은 얼마입니까? 해당 문자가 사용되지 않는 직관적인 이유는 무엇입니까?
7.13 이진 채널에서의 삭제 및 오류. 삭제 및 오류가 모두 있는 이진 입력을 갖는 채널을 고려하십시오. 확률이
<!-- Page 253 -->
오류 확률이 $\epsilon$이고 삭제 확률이 $\alpha$일 때, 채널은 다음과 같습니다.

(a) 이 채널의 용량(capacity)을 구하십시오.
(b) 이진 대칭 채널(binary symmetric channel) $(\alpha=0)$의 경우로 특수화하십시오.
(c) 이진 삭제 채널(binary erasure channel) $(\epsilon=0)$의 경우로 특수화하십시오.
7.14 문자 간 종속성이 있는 채널. 2비트 심볼을 입력받아 다음 매핑에 따라 2비트 출력을 생성하는 이진 알파벳(binary alphabet)에 대한 다음 채널을 고려하십시오: $00 \rightarrow 01, 01 \rightarrow 10, 10 \rightarrow 11$, 그리고 $11 \rightarrow 00$. 따라서, 채널의 입력으로 2비트 시퀀스 01이 주어지면, 출력은 확률 1로 10이 됩니다. $X_{1}, X_{2}$를 두 입력 심볼로, $Y_{1}, Y_{2}$를 해당 출력 심볼로 나타내십시오.
(a) 네 가지 가능한 입력 쌍에 대한 입력 분포의 함수로서 상호 정보량(mutual information) $I\left(X_{1}, X_{2} ; Y_{1}, Y_{2}\right)$을 계산하십시오.
(b) 이 채널에서의 두 번의 전송에 대한 용량(capacity)이 2비트임을 보이십시오.
(c) 최대화하는 입력 분포 하에서 $I\left(X_{1} ; Y_{1}\right)$ $=0$임을 보이십시오. 따라서, 개별 심볼과 해당 출력 간의 상호 정보량(mutual information)을 반드시 최대화하지는 않는, 용량(capacity)을 달성하는 입력 시퀀스에 대한 분포가 존재함을 보이십시오.
7.15 공동으로 전형적인 시퀀스(Jointly typical sequences). 단일 랜덤 변수(random variable)에 대한 전형적인 집합(typical set)에 대해 문제 3.13에서 했던 것처럼, 이진 대칭(binary symmetric)으로 연결된 두 랜덤 변수(random variables) 쌍에 대한 공동으로 전형적인 집합(jointly typical set)을 계산할 것입니다.
<!-- Page 254 -->
채널과 해당 채널에 대한 공동으로 일반적인 디코딩의 오류 확률을 고려합니다.

교차 확률이 0.1인 이진 대칭 채널을 고려합니다. 용량에 도달하는 입력 분포는 균일 분포입니다 [즉, $p(x)=\left(\frac{1}{2}, \frac{1}{2}\right)$ ]. 이 채널에 대한 결합 분포 $p(x, y)$를 제공하는 것은 다음과 같습니다.

| $X$ | 0 | 1 |
| :--: | :--: | :--: |
| 0 | 0.45 | 0.05 |
| 1 | 0.05 | 0.45 |

$Y$의 주변 분포도 $\left(\frac{1}{2}, \frac{1}{2}\right)$입니다.
(a) 위 결합 분포에 대해 $H(X), H(Y), H(X, Y)$, 및 $I(X ; Y)$를 계산하십시오.
(b) $X_{1}, X_{2}, \ldots, X_{n}$이 베르누이 $\left(\frac{1}{2}\right)$ 분포에 따라 i.i.d.로 추출되었다고 가정합니다. 길이 $n$의 가능한 입력 시퀀스 $2^{n}$개 중에서 어떤 것이 일반적입니까 [즉, $\epsilon=$ $0.2$에 대해 $A_{\epsilon}^{(n)}(X)$의 멤버입니까]? $A_{\epsilon}^{(n)}(Y)$에서 일반적인 시퀀스는 무엇입니까?
(c) 공동으로 일반적인 집합 $A_{\epsilon}^{(n)}(X, Y)$는 방정식 (7.35-7.37)을 만족하는 시퀀스의 집합으로 정의됩니다. 처음 두 방정식은 각각 $x^{n}$과 $y^{n}$이 $A_{\epsilon}^{(n)}(X)$ 및 $A_{\epsilon}^{(n)}(Y)$에 속한다는 조건을 나타냅니다. 마지막 조건을 고려하면, 이는 $-\frac{1}{n} \log p\left(x^{n}, y^{n}\right) \in$ $(H(X, Y)-\epsilon, H(X, Y)+\epsilon)$로 다시 작성할 수 있습니다. 시퀀스 $x^{n}$이 $y^{n}$과 다른 위치의 수를 $k$라고 하면 ( $k$는 두 시퀀스의 함수입니다), 다음과 같이 쓸 수 있습니다.

$$
p\left(x^{n}, y^{n}\right)=\prod_{i=1}^{n} p\left(x_{i}, y_{i}\right)
$$
<!-- Page 255 -->
$$
\begin{aligned}
& =(0.45)^{n-k}(0.05)^{k} \\
& =\left(\frac{1}{2}\right)^{n}(1-p)^{n-k} p^{k}
\end{aligned}
$$

이 확률을 바라보는 또 다른 방법은 가산 채널 $Y=X \oplus Z$에서의 이진 대칭 채널을 살펴보는 것입니다. 여기서 $Z$는 확률 $p$로 1이 되는 이진 확률 변수이며, $X$와 독립입니다. 이 경우,

$$
\begin{aligned}
p\left(x^{n}, y^{n}\right) & =p\left(x^{n}\right) p\left(y^{n} \mid x^{n}\right) \\
& =p\left(x^{n}\right) p\left(z^{n} \mid x^{n}\right) \\
& =p\left(x^{n}\right) p\left(z^{n}\right) \\
& =\left(\frac{1}{2}\right)^{n}(1-p)^{n-k} p^{k}
\end{aligned}
$$

$\left(x^{n}, y^{n}\right)$이 공동으로 전형적인 조건과 $x^{n}$이 전형적이고 $z^{n}=y^{n}-x^{n}$이 전형적이라는 조건이 동등함을 보이십시오.
(d) 이제 $n=25$ 및 $\epsilon=0.2$에 대한 $A_{\epsilon}^{(n)}(Z)$의 크기를 계산합니다. 문제 3.13과 마찬가지로, $k$개의 1을 갖는 확률 및 시퀀스 수는 다음과 같습니다.

| $k$ | $\binom{n}{k}$ | $\binom{n}{k} p^{k}(1-p)^{n-k}$ | $-\frac{1}{n} \log p\left(x^{n}\right)$ |
| :--: | --: | :--: | :--: |
| 0 | 1 | 0.071790 | 0.152003 |
| 1 | 25 | 0.199416 | 0.278800 |
| 2 | 300 | 0.265888 | 0.405597 |
| 3 | 2300 | 0.226497 | 0.532394 |
| 4 | 12650 | 0.138415 | 0.659191 |
| 5 | 53130 | 0.064594 | 0.785988 |
| 6 | 177100 | 0.023924 | 0.912785 |
| 7 | 480700 | 0.007215 | 1.039582 |
| 8 | 1081575 | 0.001804 | 1.166379 |
| 9 | 2042975 | 0.000379 | 1.293176 |
| 10 | 3268760 | 0.000067 | 1.419973 |
| 11 | 4457400 | 0.000010 | 1.546770 |
| 12 | 5200300 | 0.000001 | 1.673567 |

[12개보다 많은 1을 갖는 시퀀스는 총 확률이 무시할 수 있을 정도로 작기 때문에 (그리고 전형적인 집합에 속하지 않기 때문에) 생략되었습니다.] 집합 $A_{\epsilon}^{(n)}(Z)$의 크기는 얼마입니까?
<!-- Page 256 -->
(e) 이제 채널 코딩 정리의 증명에서와 같이, 채널에 대한 랜덤 코딩을 고려하십시오. $2^{n R}$개의 부호어 $X^{n}(1), X^{n}(2), \ldots, X^{n}\left(2^{n R}\right)$가 길이 $n$인 $2^{n}$개의 가능한 모든 이진열 중에서 균일하게 선택되었다고 가정합니다. 이 부호어 중 하나가 선택되어 채널을 통해 전송됩니다. 수신기는 수신된 열을 보고 수신된 열과 공동으로 전형적인 부호어를 찾으려고 합니다. 위에서 논의한 바와 같이, 이는 수신된 열 $Y^{n}$이 $Y^{n}-X^{n}(i) \in A_{\epsilon}^{(n)}(Z)$를 만족하는 부호어 $X^{n}(i)$를 찾는 것에 해당합니다. 고정된 부호어 $x^{n}(i)$에 대해, 수신된 열 $Y^{n}$이 $\left(x^{n}(i), Y^{n}\right)$가 공동으로 전형적이 되도록 하는 확률은 얼마입니까?
(f) 이제 특정 수신된 열 $y^{n}=$ $000000 \ldots 0$을 고려하십시오. 길이 $n$인 $2^{n}$개의 가능한 모든 이진열 중에서 균일하게 분포된 랜덤 열 $X^{n}$을 선택한다고 가정합니다. 선택된 열이 이 $y^{n}$과 공동으로 전형적일 확률은 얼마입니까? [힌트: 이는 $y^{n}-x^{n} \in A_{\epsilon}^{(n)}(Z)$를 만족하는 모든 열 $x^{n}$의 확률입니다.]
(g) 이제 길이 $n=25$인 $2^{9}=512$개의 부호어로 구성된 코드를 고려하십시오. 이 부호어들은 길이 $n$인 $2^{n}$개의 모든 열 중에서 균일하게 분포된 랜덤으로 선택되었습니다. 이 부호어 중 하나, 예를 들어 $i=1$에 해당하는 부호어가 선택되어 채널을 통해 전송됩니다. 파트 (e)에서 계산한 바와 같이, 수신된 열은 전송된 부호어와 높은 확률로 공동으로 전형적입니다. 다른 부호어들(전송된 부호어와 독립적으로 랜덤하게 선택된) 중 하나 이상이 수신된 열과 공동으로 전형적일 확률은 얼마입니까? [힌트: 합집합 경계를 사용할 수 있지만, 파트 (f)의 결과와 부호어들의 독립성을 사용하여 이 확률을 정확하게 계산할 수도 있습니다.]
(h) 특정 부호어가 전송되었다고 가정할 때, 오류 확률(채널의 확률 분포와 다른 부호어들의 랜덤 선택에 대해 평균된)은 다음과 같이 쓸 수 있습니다.
$\operatorname{Pr}\left(\operatorname{Error} \mid x^{n}(1)\right.$ sent) $=\sum_{y^{n}: y^{n} \text { causes error }} p\left(y^{n} \mid x^{n}(1)\right)$.
두 가지 종류의 오류가 있습니다. 첫 번째 오류는 수신된 열 $y^{n}$이 전송된 부호어와 공동으로 전형적이지 않은 경우에 발생하고, 두 번째 오류는 수신된 열과 공동으로 전형적인 다른 부호어가 있는 경우에 발생합니다. 앞선 부분들의 결과를 사용하여 이 오류 확률을 계산하십시오.
<!-- Page 257 -->
랜덤 코딩 논증의 대칭성 때문에, 이는 어떤 코드워드가 전송되었는지에 따라 달라지지 않습니다.
위의 계산은 교차 확률이 0.1인 이진 대칭 채널에서 길이 25의 코드워드 512개에 대한 랜덤 코드의 평균 오류 확률이 약 0.34임을 보여줍니다. 이는 상당히 높은 것처럼 보이지만, 그 이유는 우리가 선택한 $\epsilon$ 값이 너무 크기 때문입니다. $A_{\epsilon}^{(n)}$의 정의에서 더 작은 $\epsilon$과 더 큰 $n$을 선택함으로써, 코드의 속도가 $I(X ; Y)-3 \epsilon$보다 작기만 하면 원하는 만큼 오류 확률을 작게 만들 수 있습니다.

또한 문제에서 설명된 디코딩 절차는 최적이 아니라는 점에 유의하십시오. 최적의 디코딩 절차는 최대 우도(즉, 수신된 시퀀스에 가장 가까운 코드워드를 선택하는 것)입니다. 최대 우도 디코딩의 근사치에 기반한 디코딩을 사용하는 랜덤 코드에 대한 평균 오류 확률을 계산할 수 있습니다. 여기서 수신된 시퀀스를 4비트 이하로만 다르게 하는 고유한 코드워드로 디코딩하고, 그렇지 않으면 오류로 선언합니다. 위에서 설명한 공동으로 일반적인 디코딩과 유일한 차이점은 코드워드가 수신된 시퀀스와 동일한 경우입니다! 이 디코딩 방식에 대한 평균 오류 확률은 약 0.285임을 보일 수 있습니다.
7.16 인코더 및 디코더를 채널의 일부로 간주. 교차 확률이 0.1인 이진 대칭 채널을 고려하십시오. 이 채널에 대한 가능한 코딩 방식은 길이 3의 두 개의 코드워드를 사용하여 메시지 $a_{1}$을 000으로, $a_{2}$를 111로 인코딩하는 것입니다. 이 코딩 방식을 사용하면 인코더, 채널 및 디코더를 결합하여 두 개의 입력 $a_{1}$ 및 $a_{2}$와 두 개의 출력 $a_{1}$ 및 $a_{2}$를 갖는 새로운 BSC를 형성하는 것으로 간주할 수 있습니다.
(a) 이 채널의 교차 확률을 계산하십시오.
(b) 이 채널의 용량은 원래 채널의 전송당 비트 수로 얼마입니까?
(c) 교차 확률이 0.1인 원래 BSC의 용량은 얼마입니까?
(d) 임의의 채널에 대해 인코더, 채널 및 디코더를 함께 메시지에서 추정된 메시지로 가는 새로운 채널로 간주하는 것이 원래 채널의 전송당 비트 수 용량을 증가시키지 않는다는 일반적인 결과를 증명하십시오.
7.17 BSC 및 BEC에 대한 길이 3의 코드. 문제 7.16에서는 두 개의 코드워드를 갖는 코드에 대한 오류 확률을 계산했습니다.
<!-- Page 258 -->
길이 3 (000 및 111)을 교차 확률 $\epsilon$을 갖는 이진 대칭 채널을 통해 전송합니다. 이 문제에서는 $\epsilon=0.1$로 가정합니다.
(a) 이 채널에서 길이 3이고 네 개의 코드워드를 갖는 최적의 코드를 찾으십시오. 이 코드의 오류 확률은 얼마입니까? (모든 가능한 수신 시퀀스는 가능한 코드워드에 매핑되어야 함을 유의하십시오.)
(b) 길이 3인 모든 여덟 개의 가능한 시퀀스를 코드워드로 사용한 경우 오류 확률은 얼마입니까?
(c) 이제 삭제 확률이 0.1인 이진 삭제 채널을 고려하십시오. 다시 말하지만, 두 개의 코드워드 000과 111 코드를 사용한 경우, 수신 시퀀스 $00 \mathrm{E}, 0 \mathrm{E} 0, \mathrm{E} 00,0 \mathrm{EE}, \mathrm{E} 0 \mathrm{E}, \mathrm{EE} 0$는 모두 0으로 디코딩되고, 마찬가지로 $11 \mathrm{E}, 1 \mathrm{E} 1$, E11, 1EE, E1E, EE1은 1로 디코딩될 것입니다. EEE 시퀀스를 수신하면 000 또는 111이 전송되었는지 알 수 없으므로 둘 중 하나를 무작위로 선택하고 절반의 확률로 틀립니다. 삭제 채널에서 이 코드의 오류 확률은 얼마입니까?
(d) 삭제 채널에서 사용되는 (a) 및 (b) 부분의 코드에 대한 오류 확률은 얼마입니까?
7.18 채널 용량. 다음 채널의 확률 전이 행렬에 대한 용량을 계산하십시오:
(a) $\mathcal{X}=\mathcal{Y}=\{0,1,2\}$

$$
p(y \mid x)=\left[\begin{array}{cccc}
\frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
\frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
\frac{1}{3} & \frac{1}{3} & \frac{1}{3}
\end{array}\right]
$$

(b) $\mathcal{X}=\mathcal{Y}=\{0,1,2\}$

$$
p(y \mid x)=\left[\begin{array}{ccc}
\frac{1}{2} & \frac{1}{2} & 0 \\
0 & \frac{1}{2} & \frac{1}{2} \\
\frac{1}{2} & 0 & \frac{1}{2}
\end{array}\right]
$$

(c) $\mathcal{X}=\mathcal{Y}=\{0,1,2,3\}$

$$
p(y \mid x)=\left[\begin{array}{cccc}
p & 1-p & 0 & 0 \\
1-p & p & 0 & 0 \\
0 & 0 & q & 1-q \\
0 & 0 & 1-q & q
\end{array}\right]
$$
<!-- Page 259 -->
7.19 캐리어 비둘기 채널의 용량. 요새에 포위되어 동맹군에게 연락할 유일한 수단이 캐리어 비둘기 세트인 군대의 사령관을 고려하십시오. 각 캐리어 비둘기는 하나의 편지(8비트)를 운반할 수 있고, 비둘기는 5분마다 한 번씩 방출되며, 각 비둘기는 목적지에 도달하는 데 정확히 3분이 걸린다고 가정합니다.
(a) 모든 비둘기가 안전하게 도착한다고 가정할 때, 이 링크의 용량은 비트/시간당 얼마입니까?
(b) 이제 적들이 비둘기를 격추하려고 시도하고 그들이 비둘기의 $\alpha$ 비율을 명중시킨다고 가정합니다. 비둘기는 일정한 속도로 보내지므로 수신자는 비둘기가 누락되었을 때를 압니다. 이 링크의 용량은 얼마입니까?
(c) 이제 적이 더 교활해져서 비둘기를 격추할 때마다 무작위 편지(모든 8비트 편지 중에서 균등하게 선택됨)를 담은 더미 비둘기를 보낸다고 가정합니다. 이 링크의 용량은 비트/시간당 얼마입니까?
각 경우에 채널에 대한 적절한 모델을 설정하고 용량을 찾는 방법을 표시하십시오.
7.20 $Y$에 대한 두 개의 독립적인 관찰을 가진 채널. $Y_{1}$과 $Y_{2}$가 $X$가 주어졌을 때 조건부로 독립적이고 조건부로 동일하게 분포한다고 가정합니다.
(a) $I\left(X ; Y_{1}, Y_{2}\right)=2 I\left(X ; Y_{1}\right)-I\left(Y_{1}, Y_{2}\right)$임을 보이십시오.
(b) 채널의 용량이

채널의 두 배보다 작다는 것을 결론지으십시오.

7.21 키가 크고 뚱뚱한 사람들. 방에 있는 사람들의 평균 키가 5피트라고 가정합니다. 평균 몸무게가 100파운드라고 가정합니다.
(a) 인구의 1/3 이상이 15피트 키가 아니라고 주장하십시오.
(b) 방에 있는 300파운드 10피트 사람들의 비율에 대한 상한을 찾으십시오.
<!-- Page 260 -->
7.22 신호 대안이 용량을 낮출 수 있습니까? 채널 전이 행렬에 행을 추가하는 것이 용량을 감소시키지 않음을 보이십시오.
7.23 이진 곱셈 채널
(a) $X$와 $Z$가 0과 1의 값을 갖는 독립적인 이진 확률 변수이고 $Y=XZ$인 채널을 고려하십시오. $Z$는 베르누이($\alpha$)입니다 [즉, $P(Z=1)=\alpha$]. 이 채널의 용량과 $X$에 대한 최대화 분포를 찾으십시오.
(b) 이제 수신자가 $Y$뿐만 아니라 $Z$도 관찰할 수 있다고 가정합니다. 용량은 얼마입니까?
7.24 노이즈 알파벳. 채널을 고려하십시오

$\mathcal{X}=\{0,1,2,3\}$이며, $Y=X+Z$이고, $Z$는 세 개의 서로 다른 정수 값 $\mathcal{Z}=\left\{z_{1}, z_{2}, z_{3}\right\}$에 균등하게 분포되어 있습니다.
(a) $\mathcal{Z}$ 알파벳의 모든 선택에 대한 최대 용량은 얼마입니까? 서로 다른 정수 값 $z_{1}, z_{2}, z_{3}$와 이를 달성하는 $\mathcal{X}$에 대한 분포를 제공하십시오.
(b) $\mathcal{Z}$ 알파벳의 모든 선택에 대한 최소 용량은 얼마입니까? 서로 다른 정수 값 $z_{1}, z_{2}, z_{3}$와 이를 달성하는 $\mathcal{X}$에 대한 분포를 제공하십시오.
7.25 병목 채널. 신호 $X \in \mathcal{X}=\{1,2, \ldots, m\}$가 중간 전이 $X \longrightarrow V \longrightarrow Y$를 거친다고 가정합니다:

여기서 $x=\{1,2, \ldots, m\}, y=\{1,2, \ldots, m\}$, 그리고 $v=\{1,2, \ldots, k\}$입니다. 여기서 $p(v \mid x)$와 $p(y \mid v)$는 임의이며 채널은 전이 확률 $p(y \mid x)=\sum_{v} p(v \mid x) p(y \mid v)$를 갖습니다. $C \leq \log k$임을 보이십시오.
<!-- Page 261 -->
7.26 잡음이 있는 타자기. $x, y \in\{0,1,2,3\}$이고 전이 확률 $p(y \mid x)$가 다음 행렬로 주어지는 채널을 고려하십시오:

$$
\left[\begin{array}{cccc}
\frac{1}{2} & \frac{1}{2} & 0 & 0 \\
0 & \frac{1}{2} & \frac{1}{2} & 0 \\
0 & 0 & \frac{1}{2} & \frac{1}{2} \\
\frac{1}{2} & 0 & 0 & \frac{1}{2}
\end{array}\right]
$$

(a) 이 채널의 capacity를 구하십시오.
(b) 확률 변수 $z=g(y)$를 정의하십시오. 여기서

$$
g(y)=\left\{\begin{array}{ll}
A & \text { if } y \in\{0,1\} \\
B & \text { if } y \in\{2,3\}
\end{array}\right.
$$

다음 두 PMF에 대해 $x$, $I(X ; Z)$를 계산하십시오:
(i)

$$
p(x)= \begin{cases}\frac{1}{2} & \text { if } x \in\{1,3\} \\ 0 & \text { if } x \in\{0,2\}\end{cases}
$$

(ii)

$$
p(x)= \begin{cases}0 & \text { if } x \in\{1,3\} \\ \frac{1}{2} & \text { if } x \in\{0,2\}\end{cases}
$$

(c) $x \in\{0,1,2,3\}, z \in\{A, B\}$이고 전이 확률 $P(z \mid x)$가 다음과 같이 주어지는 $x$와 $z$ 사이의 채널 capacity를 구하십시오.

$$
p(Z=z \mid X=x)=\sum_{g\left(y_{0}\right)=z} P\left(Y=y_{0} \mid X=x\right)
$$

(d) (b)의 (i) 부분의 $X$ 분포에 대해, $X \rightarrow Z \rightarrow Y$는 Markov chain을 형성합니까?
7.27 삭제 채널. $\{\mathcal{X}, p(y \mid x), \mathcal{Y}\}$가 capacity $C$를 갖는 이산 무기억 채널이라고 가정합니다. 이 채널이 $\alpha$개의 기호를 삭제하는 삭제 채널 $\{\mathcal{Y}, p(x \mid y), \mathcal{S}\}$와 즉시 연결된다고 가정합니다.

<!-- Page 262 -->
$\mathcal{S}=\left\{y_{1}, y_{2}, \ldots, y_{m}, e\right\}$이며,

$$
\begin{aligned}
& \operatorname{Pr}\{S=y \mid X=x\}=\bar{\alpha} p(y \mid x), \quad y \in \mathcal{Y} \\
& \operatorname{Pr}\{S=e \mid X=x\}=\alpha
\end{aligned}
$$

이 채널의 capacity를 결정하십시오.
7.28 채널 선택. 두 채널 $\left(\mathcal{X}_{1}, p_{1}\left(y_{1} \mid x_{1}\right), \mathcal{Y}_{1}\right)$와 $\left(\mathcal{X}_{2}, p_{2}\left(y_{2} \mid x_{2}\right), \mathcal{Y}_{2}\right)$의 합집합의 capacity $C$를 찾으십시오. 여기서 각 시간마다 채널 1 또는 채널 2를 통해 기호를 전송할 수 있지만 둘 다 전송할 수는 없습니다. 출력 알파벳이 서로 구별되고 겹치지 않는다고 가정합니다.
(a) $2^{C}=2^{C_{1}}+2^{C_{2}}$임을 보이십시오. 따라서 $2^{C}$는 capacity $C$를 갖는 채널의 유효 알파벳 크기입니다.
(b) $2^{H}=2^{H_{1}}+2^{H_{2}}$인 문제 2.10과 비교하고, (a)를 노이즈 없는 유효 기호의 관점에서 해석하십시오.
(c) 위의 결과를 사용하여 다음 채널의 capacity를 계산하십시오.

7.29 이진 곱셈 채널
(a) $Y=X Z$인 이산 메모리 없는 채널을 고려하십시오. 여기서 $X$와 $Z$는 0과 1의 값을 갖는 독립적인 이진 확률 변수입니다. $P(Z=1)=\alpha$라고 가정합니다. 이 채널의 capacity와 $X$에 대한 최대화 분포를 찾으십시오.
(b) 이제 수신자가 $Y$뿐만 아니라 $Z$도 관찰할 수 있다고 가정합니다. capacity는 얼마입니까?
<!-- Page 263 -->
7.30 노이즈 알파벳. 채널을 고려하십시오.

$\mathcal{X}=\{0,1,2,3\}$이며, 여기서 $Y=X+Z$이고 $Z$는 세 개의 서로 다른 정수 값 $\mathcal{Z}=\left\{z_{1}, z_{2}, z_{3}\right\}$에 균등하게 분포합니다.
(a) $\mathcal{Z}$ 알파벳의 모든 선택에 대한 최대 capacity는 얼마입니까? 서로 다른 정수 값 $z_{1}, z_{2}, z_{3}$와 이를 달성하는 $\mathcal{X}$에 대한 분포를 제시하십시오.
(b) $\mathcal{Z}$ 알파벳의 모든 선택에 대한 최소 capacity는 얼마입니까? 서로 다른 정수 값 $z_{1}, z_{2}, z_{3}$와 이를 달성하는 $\mathcal{X}$에 대한 분포를 제시하십시오.
7.31 소스 및 채널. 우리는 crossover probability $p$를 가진 이진 대칭 채널로 전송하기 위해 Bernoulli $(\alpha)$ 프로세스 $V_{1}, V_{2}, \ldots$를 인코딩하고자 합니다.

오류 확률 $P\left(\hat{V}^{n} \neq\right.$ $V^{n}$ )이 $n \longrightarrow \infty$일 때 0으로 수렴하도록 하는 $\alpha$와 $p$에 대한 조건을 찾으십시오.
7.32 랜덤 20개의 질문. $X$가 $\{1,2, \ldots, m\}$에 균등하게 분포한다고 가정합니다. $m=2^{n}$이라고 가정합니다. 랜덤 질문을 합니다: $X \in S_{1}$입니까? $X \in S_{2}$입니까? $\ldots$ 단 하나의 정수만 남을 때까지. $\{1,2, \ldots, m\}$의 모든 $2^{m}$ 부분집합 $S$는 동일하게 가능성이 있습니다.
(a) $X$를 결정하는 데 필요한 결정론적 질문의 수는 몇 개입니까?
(b) 일반성을 잃지 않고, $X=1$이 랜덤 객체라고 가정합니다. 객체 2가 $k$개의 질문에 대해 객체 1과 동일한 답변을 할 확률은 얼마입니까?
(c) 올바른 객체 1의 질문에 대한 답변과 동일한 답변을 가진 $\{2,3, \ldots, m\}$의 객체의 기대 수는 얼마입니까?
<!-- Page 264 -->
(d) $n+\sqrt{n}$개의 무작위 질문을 한다고 가정합니다. 답과 일치하는 잘못된 객체의 예상 수는 얼마입니까?
(e) 마르코프 부등식 $\operatorname{Pr}\{X \geq t \mu\} \leq \frac{1}{t}$을 사용하여 오류 확률(하나 이상의 잘못된 객체가 남아 있는 경우)이 $n \longrightarrow \infty$일 때 0으로 수렴함을 보이십시오.
7.33 피드백이 있는 BSC. 매개변수 $p$를 갖는 이진 대칭 채널에서 피드백이 사용된다고 가정합니다. $Y$를 받을 때마다 다음 전송이 됩니다. 따라서 $X_{1}$은 $\operatorname{Bern}\left(\frac{1}{2}\right)$이고, $X_{2}=Y_{1}$, $X_{3}=Y_{2}, \ldots, X_{n}=Y_{n-1}$입니다.
(a) $\lim _{n \rightarrow \infty} \frac{1}{n} I\left(X^{n} ; Y^{n}\right)$을 찾으십시오.
(b) 일부 $p$ 값에 대해 이 값이 capacity보다 높을 수 있음을 보이십시오.
(c) 이 피드백 전송 스킴 $X^{n}\left(W, Y^{n}\right)=\left(X_{1}\right.$ $\left.(W), Y_{1}, Y_{2}, \ldots, Y_{m-1}\right)$을 사용하여 달성된 점근 통신 속도는 얼마입니까? 즉, $\lim _{n \rightarrow \infty} \frac{1}{n} I\left(W ; Y^{n}\right)$은 얼마입니까?
7.34 Capacity. 다음의 capacity를 찾으십시오.
(a) 두 개의 병렬 BSC:

(b) BSC와 단일 기호:

<!-- Page 265 -->
(c) BSC와 삼진 채널:

(d) 삼진 채널:

$$
p(y \mid x)=\left[\begin{array}{ccc}
\frac{2}{3} & \frac{1}{3} & 0 \\
0 & \frac{1}{3} & \frac{2}{3}
\end{array}\right]
$$

7.35 Capacity. 채널 $\mathcal{P}$가 $m \times n$ 채널 행렬일 때, capacity $C$를 갖는다고 가정합니다.
(a) 다음의 capacity는 무엇입니까?

$$
\tilde{\mathcal{P}}=\left[\begin{array}{ll}
\mathcal{P} & 0 \\
0 & 1
\end{array}\right] ?
$$

(b) 다음의 capacity는 무엇입니까?

$$
\tilde{\mathcal{P}}=\left[\begin{array}{ll}
\mathcal{P} & 0 \\
0 & I_{k}
\end{array}\right] ?
$$

여기서 $I_{k}$는 $k \times k$ 항등 행렬입니다.
7.36 Memory가 있는 채널. 이산 메모리 없는 채널 $Y_{i}=Z_{i} X_{i}$를 고려하며, 입력 알파벳은 $X_{i} \in\{-1,1\}$입니다.
(a) $\left\{Z_{i}\right\}$가 i.i.d.이고 다음을 만족할 때 이 채널의 capacity는 무엇입니까?

$$
Z_{i}=\left\{\begin{array}{cl}
1, & p=0.5 \\
-1, & p=0.5 ?
\end{array}\right.
$$
<!-- Page 266 -->
이제 메모리가 있는 채널을 고려하십시오. 전송이 시작되기 전에 $Z$는 무작위로 선택되어 모든 시간 동안 고정됩니다. 따라서 $Y_{i}=Z X_{i}$입니다.
(b) 다음의 경우 채널 용량은 얼마입니까?

$$
Z=\left\{\begin{aligned}
1, & p=0.5 \\
-1, & p=0.5 ?
\end{aligned}\right.
$$

7.37 결합 типичность(joint typicality). $\left(X_{i}, Y_{i}, Z_{i}\right)$가 $p(x, y, z)$에 따라 i.i.d.라고 가정합니다. $\left(x^{n}, y^{n}, z^{n}\right)$가 결합 типичность(jointly typical)이라고 말할 것입니다 [$\left(x^{n}, y^{n}, z^{n}\right) \in A_{\epsilon}^{(n)}$로 표기].

- $p\left(x^{n}\right) \in 2^{-n(H(X) \pm \epsilon)}$.
- $p\left(y^{n}\right) \in 2^{-n(H(Y) \pm \epsilon)}$.
- $p\left(z^{n}\right) \in 2^{-n(H(Z) \pm \epsilon)}$.
- $p\left(x^{n}, y^{n}\right) \in 2^{-n(H(X, Y) \pm \epsilon)}$.
- $p\left(x^{n}, z^{n}\right) \in 2^{-n(H(X, Z) \pm \epsilon)}$.
- $p\left(y^{n}, z^{n}\right) \in 2^{-n(H(Y, Z) \pm \epsilon)}$.
- $p\left(x^{n}, y^{n}, z^{n}\right) \in 2^{-n(H(X, Y, Z) \pm \epsilon)}$.

이제 $\left(\tilde{X}^{n}, \tilde{Y}^{n}, \tilde{Z}^{n}\right)$가 $p\left(x^{n}\right) p\left(y^{n}\right) p\left(z^{n}\right)$에 따라 추출되었다고 가정합니다. 따라서 $\tilde{X}^{n}, \tilde{Y}^{n}, \tilde{Z}^{n}$는 $p\left(x^{n}, y^{n}, z^{n}\right)$와 동일한 주변 분포를 가지지만 독립적입니다. 엔트로피 $H(X), H(Y), H(Z), H(X, Y), H(X, Z), H(Y, Z), H(X, Y, Z)$에 대한 (경계값으로) $\operatorname{Pr}\left\{\left(\tilde{X}^{n}, \tilde{Y}^{n}, \tilde{Z}^{n}\right) \in A_{\epsilon}^{(n)}\right\}$를 찾으십시오.

# 역사적 참고 사항

mutual information과 채널 용량과의 관계에 대한 아이디어는 Shannon의 원 논문 [472]에서 개발되었습니다. 이 논문에서 그는 채널 용량 정리를 발표하고 여기서 설명된 논증과 유사한 типичность 시퀀스(typical sequences)를 사용한 증명을 개략적으로 설명했습니다. 최초의 엄밀한 증명은 Feinstein [205]에 의해 이루어졌으며, 그는 오류 확률이 낮은 상태로 전송될 수 있는 코드워드의 수를 찾기 위해 세심한 "쿠키 커팅" 논증을 사용했습니다. 랜덤 코딩 지수(random coding exponent)를 사용한 더 간단한 증명은 Gallager [224]에 의해 개발되었습니다. 우리의 증명은 Cover [121]와 Forney의 미출판 강의 노트 [216]에 기반합니다.

반대는 Fano [201]에 의해 증명되었으며, 그의 이름을 딴 부등식을 사용했습니다. 강한 반대는 Wolfowitz [565]에 의해 처음으로 증명되었으며, типичность 시퀀스와 밀접하게 관련된 기법을 사용했습니다. 채널 용량을 계산하기 위한 반복 알고리즘은 Arimoto [25]와 Blahut [65]에 의해 독립적으로 개발되었습니다.
<!-- Page 267 -->
Shannon은 [474]에서 제로 오류 용량(zero-error capacity)의 개념을 개발했으며, 같은 논문에서 피드백이 이산 무기억 채널(discrete memoryless channel)의 용량을 증가시키지 않는다는 것을 증명했습니다. 제로 오류 용량을 찾는 문제는 본질적으로 조합론적이며, 이 분야의 첫 번째 중요한 결과는 Lovasz [365]에게서 나왔습니다. 제로 오류 용량을 찾는 일반적인 문제는 아직 해결되지 않았습니다. 관련 결과에 대한 개요는 Körner와 Orlitsky [327]를 참조하십시오.

양자 정보 이론은 이 장의 고전 이론에 대한 양자 역학적 대응물로서, 그 자체로 큰 연구 분야로 부상하고 있으며, Bennett와 Shor [49]의 논문 및 Nielsen과 Chuang [395]의 교재에서 잘 개괄되어 있습니다.
<!-- Page 268 -->
.
<!-- Page 269 -->
# 제 8 장

## 차분 엔트로피

이제 연속 확률 변수의 엔트로피인 차분 엔트로피 개념을 소개합니다. 차분 엔트로피는 가장 짧은 설명 길이와도 관련이 있으며, 이산 확률 변수의 엔트로피와 여러 면에서 유사합니다. 하지만 몇 가지 중요한 차이점이 있으며, 이 개념을 사용할 때 주의가 필요합니다.

### 8.1 정의

정의 $X$를 누적 분포 함수 $F(x)=\operatorname{Pr}(X \leq x)$를 갖는 확률 변수라고 합시다. 만약 $F(x)$가 연속이면, 그 확률 변수는 연속이라고 합니다. 도함수가 정의될 때 $f(x)=F^{\prime}(x)$라고 합시다. 만약 $\int_{-\infty}^{\infty} f(x)=1$이면, $f(x)$는 $X$의 확률 밀도 함수라고 불립니다. $f(x)>0$인 집합을 $X$의 지지 집합이라고 합니다.

정의 밀도 $f(x)$를 갖는 연속 확률 변수 $X$의 차분 엔트로피 $h(X)$는 다음과 같이 정의됩니다.

$$
h(X)=-\int_{S} f(x) \log f(x) d x
$$

여기서 $S$는 확률 변수의 지지 집합입니다.
이산적인 경우와 마찬가지로, 차분 엔트로피는 확률 변수의 확률 밀도에만 의존하므로, 차분 엔트로피는 때때로 $h(X)$ 대신 $h(f)$로 표기되기도 합니다.

비고 적분 또는 밀도와 관련된 모든 예에서와 마찬가지로, 존재한다면 해당 명세를 포함해야 합니다. 예시를 구성하는 것은 쉽습니다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 270 -->
밀도 함수가 존재하지 않거나 위 적분이 존재하지 않는 확률 변수에 대한 것입니다.

예제 8.1.1 (균등 분포) 0에서 $a$까지 균등하게 분포된 확률 변수를 고려해 봅시다. 따라서 밀도는 0에서 $a$까지 $1/a$이고 그 외에는 0입니다. 그러면 차분 엔트로피는 다음과 같습니다.

$$
h(X)=-\int_{0}^{a} \frac{1}{a} \log \frac{1}{a} d x=\log a
$$

참고: $a<1$인 경우 $\log a<0$이며 차분 엔트로피는 음수입니다. 따라서 이산 엔트로피와 달리 차분 엔트로피는 음수일 수 있습니다. 그러나 $2^{h(X)}=2^{\log a}=a$는 항상 음수가 아닌 지지 집합의 부피이며, 예상대로입니다.

예제 8.1.2 (정규 분포) $X \sim \phi(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{\frac{-x^{2}}{2 \sigma^{2}}}$라고 가정합니다. 그러면 nats 단위로 차분 엔트로피를 계산하면 다음과 같습니다.

$$
\begin{aligned}
h(\phi) & =-\int \phi \ln \phi \\
& =-\int \phi(x)\left[-\frac{x^{2}}{2 \sigma^{2}}-\ln \sqrt{2 \pi \sigma^{2}}\right] \\
& =\frac{E X^{2}}{2 \sigma^{2}}+\frac{1}{2} \ln 2 \pi \sigma^{2} \\
& =\frac{1}{2}+\frac{1}{2} \ln 2 \pi \sigma^{2} \\
& =\frac{1}{2} \ln e+\frac{1}{2} \ln 2 \pi \sigma^{2} \\
& =\frac{1}{2} \ln 2 \pi e \sigma^{2} \quad \text { nats. }
\end{aligned}
$$

로그의 밑을 변경하면 다음과 같습니다.

$$
h(\phi)=\frac{1}{2} \log 2 \pi e \sigma^{2} \quad \text { bits. }
$$
<!-- Page 271 -->
# 8.2 연속 확률 변수에 대한 AEP

이산 확률 변수에 대한 엔트로피의 중요한 역할 중 하나는 AEP에 있으며, 이는 i.i.d. 확률 변수 시퀀스에 대해 $p\left(X_{1}, X_{2}, \ldots, X_{n}\right)$이 높은 확률로 $2^{-n H(X)}$에 가깝다는 것을 나타냅니다. 이를 통해 일반적인 집합을 정의하고 일반적인 시퀀스의 동작을 특성화할 수 있습니다.

연속 확률 변수에 대해서도 동일한 작업을 수행할 수 있습니다.
정리 8.2.1 $X_{1}, X_{2}, \ldots, X_{n}$이 밀도 $f(x)$에 따라 i.i.d.로 추출된 확률 변수 시퀀스라고 가정합니다. 그러면

$$
-\frac{1}{n} \log f\left(X_{1}, X_{2}, \ldots, X_{n}\right) \rightarrow E[-\log f(X)]=h(X) \quad \text { 확률적으로 }
$$

증명: 증명은 대수의 법칙에서 직접적으로 이어집니다.

이는 일반적인 집합에 대한 다음 정의로 이어집니다.
정의 $\epsilon>0$ 및 임의의 $n$에 대해 다음과 같이 $f(x)$에 대한 일반적인 집합 $A_{\epsilon}^{(n)}$을 정의합니다.
$A_{\epsilon}^{(n)}=\left\{\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in S^{n}:\left|-\frac{1}{n} \log f\left(x_{1}, x_{2}, \ldots, x_{n}\right)-h(X)\right| \leq \epsilon\right\}$,
여기서 $f\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\prod_{i=1}^{n} f\left(x_{i}\right)$입니다.
연속 확률 변수에 대한 일반적인 집합의 속성은 이산 확률 변수에 대한 속성과 유사합니다. 이산 경우의 일반적인 집합의 크기에 대한 유사체는 연속 확률 변수에 대한 일반적인 집합의 부피입니다.

정의 집합 $A \subset \mathcal{R}^{n}$의 부피 $\operatorname{Vol}(A)$는 다음과 같이 정의됩니다.

$$
\operatorname{Vol}(A)=\int_{A} d x_{1} d x_{2} \cdots d x_{n}
$$

정리 8.2.2 일반적인 집합 $A_{\epsilon}^{(n)}$은 다음 속성을 갖습니다.

1. $n$이 충분히 크면 $\operatorname{Pr}\left(A_{\epsilon}^{(n)}\right)>1-\epsilon$입니다.
2. 모든 $n$에 대해 $\operatorname{Vol}\left(A_{\epsilon}^{(n)}\right) \leq 2^{n(h(X)+\epsilon)}$입니다.
3. $n$이 충분히 크면 $\operatorname{Vol}\left(A_{\epsilon}^{(n)}\right) \geq(1-\epsilon) 2^{n(h(X)-\epsilon)}$입니다.
<!-- Page 272 -->
증명: 정리 8.2.1에 의해, $-\frac{1}{n} \log f\left(X^{n}\right)=-\frac{1}{n} \sum \log f\left(X_{i}\right) \rightarrow h(X)$는 확률적으로 성립하며, 이는 속성 1을 확립합니다. 또한,

$$
\begin{aligned}
1 & =\int_{S^{n}} f\left(x_{1}, x_{2}, \ldots, x_{n}\right) d x_{1} d x_{2} \cdots d x_{n} \\
& \geq \int_{A_{\epsilon}^{(n)}} f\left(x_{1}, x_{2}, \ldots, x_{n}\right) d x_{1} d x_{2} \cdots d x_{n} \\
& \geq \int_{A_{\epsilon}^{(n)}} 2^{-n(h(X)+\epsilon)} d x_{1} d x_{2} \cdots d x_{n} \\
& =2^{-n(h(X)+\epsilon)} \int_{A_{\epsilon}^{(n)}} d x_{1} d x_{2} \cdots d x_{n} \\
& =2^{-n(h(X)+\epsilon)} \operatorname{Vol}\left(A_{\epsilon}^{(n)}\right)
\end{aligned}
$$

이를 통해 속성 2를 얻습니다. 우리는 또한 전형적인 집합의 부피가 이보다 크거나 같다고 주장합니다. 만약 $n$이 속성 1이 만족될 만큼 충분히 크다면,

$$
\begin{aligned}
1-\epsilon & \leq \int_{A_{\epsilon}^{(n)}} f\left(x_{1}, x_{2}, \ldots, x_{n}\right) d x_{1} d x_{2} \cdots d x_{n} \\
& \leq \int_{A_{\epsilon}^{(n)}} 2^{-n(h(X)-\epsilon)} d x_{1} d x_{2} \cdots d x_{n} \\
& =2^{-n(h(X)-\epsilon)} \int_{A_{\epsilon}^{(n)}} d x_{1} d x_{2} \cdots d x_{n} \\
& =2^{-n(h(X)-\epsilon)} \operatorname{Vol}\left(A_{\epsilon}^{(n)}\right)
\end{aligned}
$$

이는 속성 3을 확립합니다. 따라서 $n$이 충분히 크다면, 우리는 다음과 같은 결과를 얻습니다.

$$
(1-\epsilon) 2^{n(h(X)-\epsilon)} \leq \operatorname{Vol}\left(A_{\epsilon}^{(n)}\right) \leq 2^{n(h(X)+\epsilon)}
$$

정리 8.2.3 $A_{\epsilon}^{(n)}$ 집합은 확률이 $\geq 1-\epsilon$인 가장 작은 부피를 가진 집합이며, 이는 지수에서 일차 근사입니다.

증명: 이산적인 경우와 동일합니다.
이 정리는 확률의 대부분을 포함하는 가장 작은 집합의 부피가 대략 $2^{n h}$임을 나타냅니다. 이는 $n$차원 부피이므로, 해당 변의 길이는 $\left(2^{n h}\right)^{\frac{1}{n}}=2^{h}$입니다. 이는 다음과 같은 결과를 제공합니다.
<!-- Page 273 -->
차분 엔트로피에 대한 해석: 이는 확률의 대부분을 포함하는 가장 작은 집합의 등가 변 길이에 대한 로그입니다. 따라서 낮은 엔트로피는 확률 변수가 작은 유효 부피에 국한됨을 의미하고 높은 엔트로피는 확률 변수가 널리 퍼져 있음을 나타냅니다.
참고. 엔트로피가 전형적인 집합의 부피와 관련이 있는 것처럼, 피셔 정보라는 양이 있는데, 이는 전형적인 집합의 표면적과 관련이 있습니다. 피셔 정보에 대해서는 섹션 11.10 및 17.8에서 더 자세히 논의합니다.

# 8.3 차분 엔트로피와 이산 엔트로피의 관계

그림 8.1에 설명된 밀도 $f(x)$를 갖는 확률 변수 $X$를 고려하십시오. $X$의 범위를 길이 $\Delta$의 구간으로 나눈다고 가정해 보겠습니다. 구간 내에서 밀도가 연속적이라고 가정합니다. 그러면 평균값 정리에 의해 각 구간 내에 $x_{i}$라는 값이 존재하여 다음을 만족합니다.

$$
f\left(x_{i}\right) \Delta=\int_{i \Delta}^{(i+1) \Delta} f(x) d x
$$

양자화된 확률 변수 $X^{\Delta}$를 고려하십시오. 이는 다음과 같이 정의됩니다.

$$
X^{\Delta}=x_{i} \quad \text { if } i \Delta \leq X<(i+1) \Delta
$$

그림 8.1. 연속 확률 변수의 양자화.
<!-- Page 274 -->
그러면 $X^{\Delta}=x_{i}$일 확률은 다음과 같습니다.

$$
p_{i}=\int_{i \Delta}^{(i+1) \Delta} f(x) d x=f\left(x_{i}\right) \Delta
$$

양자화된 버전의 entropy는 다음과 같습니다.

$$
\begin{aligned}
H\left(X^{\Delta}\right) & =-\sum_{-\infty}^{\infty} p_{i} \log p_{i} \\
& =-\sum_{-\infty}^{\infty} f\left(x_{i}\right) \Delta \log \left(f\left(x_{i}\right) \Delta\right) \\
& =-\sum \Delta f\left(x_{i}\right) \log f\left(x_{i}\right)-\sum f\left(x_{i}\right) \Delta \log \Delta \\
& =-\sum \Delta f\left(x_{i}\right) \log f\left(x_{i}\right)-\log \Delta
\end{aligned}
$$

여기서 $\sum f\left(x_{i}\right) \Delta=\int f(x)=1$입니다. 만약 $f(x) \log f(x)$가 리만 적분 가능하면 (극한이 잘 정의되도록 보장하는 조건 [556]), (8.29)의 첫 번째 항은 리만 적분 가능성의 정의에 따라 $\Delta \rightarrow 0$일 때 $-f(x) \log f(x)$의 적분으로 접근합니다. 이는 다음을 증명합니다.

정리 8.3.1 확률 변수 $X$의 밀도 함수 $f(x)$가 리만 적분 가능하면,

$$
H\left(X^{\Delta}\right)+\log \Delta \rightarrow h(f)=h(X), \quad \text { as } \Delta \rightarrow 0
$$

따라서 연속 확률 변수 $X$의 n-비트 양자화 entropy는 약 $h(X)+n$입니다.

# 예제 8.3.1

1. $X$가 $[0,1]$에서 균등 분포를 따르고 $\Delta=2^{-n}$이라고 하면, $h=0$, $H\left(X^{\Delta}\right)=n$이며, $n$ 비트는 $n$ 비트 정확도로 $X$를 설명하는 데 충분합니다.
2. $X$가 $\left[0, \frac{1}{8}\right]$에서 균등 분포를 따른다면, 소수점 이하 세 번째 비트까지는 모두 0이어야 합니다. $n$ 비트 정확도로 $X$를 설명하는 데는 $n-3$ 비트만 필요하며, 이는 $h(X)=-3$과 일치합니다.
3. $X \sim \mathcal{N}\left(0, \sigma^{2}\right)$이고 $\sigma^{2}=100$인 경우, $n$ 비트 정확도로 $X$를 설명하는 데는 평균적으로 $n+\frac{1}{2} \log \left(2 \pi e \sigma^{2}\right)=n+5.37$ 비트가 필요합니다.
<!-- Page 275 -->
일반적으로 $h(X)+n$은 $n$-비트 정확도로 $X$를 설명하는 데 필요한 평균 비트 수입니다.

이산 확률 변수의 차분 엔트로피는 $-\infty$로 간주될 수 있습니다. $2^{-\infty}=0$이며, 이는 이산 확률 변수의 지지 집합의 부피가 0이라는 아이디어와 일치합니다.

# 8.4 결합 및 조건부 차분 엔트로피

이산적인 경우와 마찬가지로, 단일 확률 변수의 차분 엔트로피 정의를 여러 확률 변수로 확장할 수 있습니다.

정의 밀도 $f\left(x_{1}, x_{2}, \ldots, x_{n}\right)$를 갖는 확률 변수 집합 $X_{1}, X_{2}, \ldots, X_{n}$의 차분 엔트로피는 다음과 같이 정의됩니다.

$$
h\left(X_{1}, X_{2}, \ldots, X_{n}\right)=-\int f\left(x^{n}\right) \log f\left(x^{n}\right) d x^{n}
$$

정의 $X, Y$가 결합 밀도 함수 $f(x, y)$를 갖는 경우, 조건부 차분 엔트로피 $h(X \mid Y)$를 다음과 같이 정의할 수 있습니다.

$$
h(X \mid Y)=-\int f(x, y) \log f(x \mid y) d x d y
$$

일반적으로 $f(x \mid y)=f(x, y) / f(y)$이므로 다음과 같이 쓸 수도 있습니다.

$$
h(X \mid Y)=h(X, Y)-h(Y)
$$

하지만 차분 엔트로피 중 하나라도 무한한 경우에는 주의해야 합니다.
다음 엔트로피 평가는 본문에서 자주 사용됩니다.
정리 8.4.1 (다변수 정규 분포의 엔트로피) $X_{1}, X_{2}, \ldots, X_{n}$이 평균 $\mu$와 공분산 행렬 $K$를 갖는 다변수 정규 분포를 따른다고 가정합니다. 그러면

$$
h\left(X_{1}, X_{2}, \ldots, X_{n}\right)=h\left(\mathcal{N}_{n}(\mu, K)\right)=\frac{1}{2} \log (2 \pi e)^{n}|K| \quad \text { 비트, }
$$

여기서 $|K|$는 $K$의 행렬식을 나타냅니다.
<!-- Page 276 -->
증명: $X_{1}, X_{2}, \ldots, X_{n}$의 확률 밀도 함수는 다음과 같습니다.

$$
f(\mathbf{x})=\frac{1}{(\sqrt{2 \pi})^{n}|K|^{\frac{1}{2}}} e^{-\frac{1}{2}(\mathbf{x}-\mu)^{T} K^{-1}(\mathbf{x}-\mu)}
$$

그러면

$$
\begin{aligned}
h(f)= & -\int f(\mathbf{x})\left[-\frac{1}{2}(\mathbf{x}-\mu)^{T} K^{-1}(\mathbf{x}-\mu)-\ln (\sqrt{2 \pi})^{n}|K|^{\frac{1}{2}}\right] d \mathbf{x} \\
& =\frac{1}{2} E\left[\sum_{i, j}\left(X_{i}-\mu_{i}\right)\left(K^{-1}\right)_{i j}\left(X_{j}-\mu_{j}\right)\right]+\frac{1}{2} \ln (2 \pi)^{n}|K| \\
& =\frac{1}{2} E\left[\sum_{i, j}\left(X_{i}-\mu_{i}\right)\left(X_{j}-\mu_{j}\right)\left(K^{-1}\right)_{i j}\right]+\frac{1}{2} \ln (2 \pi)^{n}|K| \\
& =\frac{1}{2} \sum_{i, j} E\left[\left(X_{j}-\mu_{j}\right)\left(X_{i}-\mu_{i}\right)\right]\left(K^{-1}\right)_{i j}+\frac{1}{2} \ln (2 \pi)^{n}|K| \\
& =\frac{1}{2} \sum_{j} \sum_{i} K_{j i}\left(K^{-1}\right)_{i j}+\frac{1}{2} \ln (2 \pi)^{n}|K| \\
& =\frac{1}{2} \sum_{j}\left(K K^{-1}\right)_{j j}+\frac{1}{2} \ln (2 \pi)^{n}|K| \\
& =\frac{1}{2} \sum_{j} I_{j j}+\frac{1}{2} \ln (2 \pi)^{n}|K| \\
& =\frac{n}{2}+\frac{1}{2} \ln (2 \pi)^{n}|K| \\
& =\frac{1}{2} \ln (2 \pi e)^{n}|K| \quad \text { nats } \\
& =\frac{1}{2} \log (2 \pi e)^{n}|K| \quad \text { bits. }
\end{aligned}
$$

# 8.5 상대 엔트로피 및 상호 정보량

이제 두 가지 친숙한 양인 $D(f \| g)$와 $I(X ; Y)$를 확률 밀도 함수로 확장합니다.
<!-- Page 277 -->
정의 두 밀도 $f$와 $g$ 사이의 상대 엔트로피(또는 Kullback-Leibler 거리) $D(f \| g)$는 다음과 같이 정의됩니다.

$$
D(f \| g)=\int f \log \frac{f}{g}
$$

$D(f \| g)$는 $f$의 지지 집합이 $g$의 지지 집합에 포함될 때만 유한하다는 점에 유의하십시오. [연속성에 의해 동기 부여되어, $0 \log \frac{0}{0}=0$으로 설정합니다.]

정의 결합 밀도 $f(x, y)$를 갖는 두 확률 변수 사이의 상호 정보 $I(X ; Y)$는 다음과 같이 정의됩니다.

$$
I(X ; Y)=\int f(x, y) \log \frac{f(x, y)}{f(x) f(y)} d x d y
$$

정의로부터 명확한 것은 다음과 같습니다.

$$
I(X ; Y)=h(X)-h(X \mid Y)=h(Y)-h(Y \mid X)=h(X)+h(Y)-h(X, Y)
$$

그리고

$$
I(X ; Y)=D(f(x, y) \| f(x) f(y))
$$

$D(f \| g)$와 $I(X ; Y)$의 속성은 이산적인 경우와 동일합니다. 특히, 두 확률 변수 사이의 상호 정보는 양자화된 버전 사이의 상호 정보의 극한이며, 이는 다음과 같습니다.

$$
\begin{aligned}
I\left(X^{\Delta} ; Y^{\Delta}\right) & =H\left(X^{\Delta}\right)-H\left(X^{\Delta} \mid Y^{\Delta}\right) \\
& \approx h(X)-\log \Delta-(h(X \mid Y)-\log \Delta) \\
& =I(X ; Y)
\end{aligned}
$$

더 일반적으로, 확률 변수의 범위에 대한 유한한 분할을 사용하여 상호 정보를 정의할 수 있습니다. $\mathcal{X}$를 확률 변수 $X$의 범위라고 합시다. $\mathcal{X}$의 분할 $\mathcal{P}$는 $\cup_{i} P_{i}=\mathcal{X}$를 만족하는 서로소인 집합 $P_{i}$의 유한한 모음입니다. $\mathcal{P}$에 의한 $X$의 양자화(표기: $[X]_{\mathcal{P}}$)는 다음과 같이 정의되는 이산 확률 변수입니다.

$$
\operatorname{Pr}([X]_{\mathcal{P}}=i)=\operatorname{Pr}\left(X \in P_{i}\right)=\int_{P_{i}} d F(x)
$$

분할 $\mathcal{P}$와 $\mathcal{Q}$를 갖는 두 확률 변수 $X$와 $Y$에 대해, (2.28)을 사용하여 $X$와 $Y$의 양자화된 버전 사이의 상호 정보를 계산할 수 있습니다. 이제 임의의 두 확률 변수 쌍에 대해 상호 정보를 다음과 같이 정의할 수 있습니다.
<!-- Page 278 -->
정의 두 확률 변수 $X$와 $Y$ 간의 상호 정보량은 다음과 같이 주어집니다.

$$
I(X ; Y)=\sup _{\mathcal{P}, \mathcal{Q}} I\left([X]_{\mathcal{P}} ;[Y]_{\mathcal{Q}}\right)
$$

여기서 supremum은 모든 유한 분할 $\mathcal{P}$와 $\mathcal{Q}$에 대한 것입니다.
이것은 원자, 밀도 및 특이 부분을 포함하는 결합 분포에도 항상 적용되는 상호 정보량의 마스터 정의입니다. 또한, 분할 $P$와 $Q$를 계속해서 개선함으로써, 단조 증가하는 수열 $I\left([X]_{P} ;[Y]_{Q}\right) \nearrow I$을 찾을 수 있습니다.

(8.52)와 유사한 논증을 통해, 이 상호 정보량의 정의는 밀도를 갖는 확률 변수에 대해 (8.47)과 동등함을 보일 수 있습니다. 이산 확률 변수의 경우, 이 정의는 (2.28)의 상호 정보량 정의와 동등합니다.

예제 8.5.1 (상관 계수 $\rho$를 갖는 상관된 가우시안 확률 변수 간의 상호 정보량) $(X, Y) \sim \mathcal{N}(0, K)$라고 가정합니다. 여기서

$$
K=\left[\begin{array}{cc}
\sigma^{2} & \rho \sigma^{2} \\
\rho \sigma^{2} & \sigma^{2}
\end{array}\right]
$$

그러면 $h(X)=h(Y)=\frac{1}{2} \log (2 \pi e) \sigma^{2}$이고 $h(X, Y)=\frac{1}{2} \log (2 \pi e)^{2}|K|=$ $\frac{1}{2} \log (2 \pi e)^{2} \sigma^{4}\left(1-\rho^{2}\right)$이므로,

$$
I(X ; Y)=h(X)+h(Y)-h(X, Y)=-\frac{1}{2} \log \left(1-\rho^{2}\right)
$$

$\rho=0$이면, $X$와 $Y$는 독립이며 상호 정보량은 0입니다. $\rho= \pm 1$이면, $X$와 $Y$는 완벽하게 상관되어 있으며 상호 정보량은 무한대입니다.

# 8.6 미분 엔트로피, 상대 엔트로피 및 상호 정보량의 속성

## 정리 8.6.1

$$
D(f \| g) \geq 0
$$

등호는 $f=g$ 거의 모든 곳(a.e.)에서 성립합니다.
증명: $f$의 지지 집합을 $S$라고 합시다. 그러면

$$
\begin{aligned}
-D(f \| g) & =\int_{S} f \log \frac{g}{f} \\
& \leq \log \int_{S} f \frac{g}{f} \quad \text{ (젠센 부등식에 의해) }
\end{aligned}
$$
<!-- Page 279 -->
$$
\begin{aligned}
& =\log \int_{S} g \\
& \leq \log 1=0
\end{aligned}
$$

등식은 Jensen의 부등식에서 등식이 성립할 때, 즉 $f=g$ a.e.일 때 발생합니다.

따름정리 $I(X ; Y) \geq 0$이며, 등식은 $X$와 $Y$가 독립일 때 성립합니다.
따름정리 $\quad h(X \mid Y) \leq h(X)$이며, 등식은 $X$와 $Y$가 독립일 때 성립합니다.
정리 8.6.2 (차분 엔트로피의 연쇄 법칙)

$$
h\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} h\left(X_{i} \mid X_{1}, X_{2}, \ldots, X_{i-1}\right)
$$

증명: 정의로부터 직접 도출됩니다.

# 따름정리

$$
h\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq \sum h\left(X_{i}\right)
$$

등식은 $X_{1}, X_{2}, \ldots, X_{n}$이 독립일 때 성립합니다.
증명: 정리 8.6.2와 정리 8.6.1의 따름정리로부터 직접 도출됩니다.

응용 (Hadamard의 부등식) $\mathbf{X} \sim \mathcal{N}(0, K)$를 다변수 정규 확률 변수로 설정하고 위 부등식의 엔트로피를 계산하면 다음과 같습니다.

$$
|K| \leq \prod_{i=1}^{n} K_{i i}
$$

이는 Hadamard의 부등식입니다. 정보 이론적 부등식(제 17장)으로부터 이러한 방식으로 여러 행렬식 부등식을 도출할 수 있습니다.

## 정리 8.6.3

$$
h(X+c)=h(X)
$$

이동은 차분 엔트로피를 변경하지 않습니다.
증명: 차분 엔트로피의 정의로부터 직접 도출됩니다.
<!-- Page 280 -->
# 정리 8.6.4

$$
h(a X)=h(X)+\log |a|
$$

증명: $Y=a X$라고 합시다. 그러면 $f_{Y}(y)=\frac{1}{|a|} f_{X}\left(\frac{y}{a}\right)$이고,

$$
\begin{aligned}
h(a X) & =-\int f_{Y}(y) \log f_{Y}(y) d y \\
& =-\int \frac{1}{|a|} f_{X}\left(\frac{y}{a}\right) \log \left(\frac{1}{|a|} f_{X}\left(\frac{y}{a}\right)\right) d y \\
& =-\int f_{X}(x) \log f_{X}(x) d x+\log |a| \\
& =h(X)+\log |a|
\end{aligned}
$$

적분에서 변수 변환 후입니다.
마찬가지로 벡터 값 확률 변수에 대해 다음 따름 정리를 증명할 수 있습니다.

## 따름 정리

$$
h(A \mathbf{X})=h(\mathbf{X})+\log |\operatorname{det}(A)| .
$$

이제 다변수 정규 분포가 동일한 공분산을 갖는 모든 분포에 대해 엔트로피를 최대화함을 보입니다.

정리 8.6.5 영평균과 공분산 $K=E \mathbf{X X}^{t}$ (즉, $K_{i j}=E X_{i} X_{j}, 1 \leq i, j \leq n$)을 갖는 확률 벡터 $\mathbf{X} \in \mathbf{R}^{n}$에 대해 $h(\mathbf{X}) \leq \frac{1}{2} \log (2 \pi e)^{n}|K|$이며, 등호는 $\mathbf{X} \sim \mathcal{N}(0, K)$일 때 성립합니다.

증명: 모든 $i, j$에 대해 $\int g(\mathbf{x}) x_{i} x_{j} d \mathbf{x}=K_{i j}$를 만족하는 임의의 밀도 $g(\mathbf{x})$를 사용합시다. $\mu=0$으로 설정한 (8.35)에 주어진 $\mathcal{N}(0, K)$ 벡터의 밀도를 $\phi_{K}$라고 합시다. $\log \phi_{K}(\mathbf{x})$는 이차 형식이고 $\int x_{i} x_{j} \phi_{K}(\mathbf{x}) d \mathbf{x}=K_{i j}$임을 주목하십시오. 그러면

$$
\begin{aligned}
0 & \leq D\left(g \| \phi_{K}\right) \\
& =\int g \log \left(g / \phi_{K}\right) \\
& =-h(g)-\int g \log \phi_{K}
\end{aligned}
$$
<!-- Page 281 -->
$$
\begin{aligned}
& =-h(g)-\int \phi_{K} \log \phi_{K} \\
& =-h(g)+h\left(\phi_{K}\right)
\end{aligned}
$$

여기서 $\int g \log \phi_{K}=\int \phi_{K} \log \phi_{K}$ 라는 치환은 $g$와 $\phi_{K}$가 이차 형식 $\log \phi_{K}(\mathbf{x})$의 동일한 모멘트를 생성한다는 사실로부터 도출됩니다.

특히, 가우시안 분포는 동일한 분산을 갖는 모든 분포에 대해 엔트로피를 최대화합니다. 이는 파노 부등식에 대한 추정의 대응으로 이어집니다. $X$를 미분 엔트로피 $h(X)$를 갖는 확률 변수라고 합시다. $\hat{X}$를 $X$의 추정치라고 하고, $E(X-\hat{X})^{2}$를 예측 오차의 기댓값이라고 합시다. $h(X)$는 nats 단위라고 가정합니다.

정리 8.6.6 (추정 오차 및 미분 엔트로피) 임의의 확률 변수 $X$와 추정량 $\hat{X}$에 대해,

$$
E(X-\hat{X})^{2} \geq \frac{1}{2 \pi e} e^{2 h(X)}
$$

등호는 $X$가 가우시안이고 $\hat{X}$가 $X$의 평균일 때 성립합니다.
증명: 임의의 추정량 $\hat{X}$에 대해,

$$
\begin{aligned}
E(X-\hat{X})^{2} & \geq \min _{\hat{X}} E(X-\hat{X})^{2} \\
& =E(X-E(X))^{2} \\
& =\operatorname{var}(X) \\
& \geq \frac{1}{2 \pi e} e^{2 h(X)}
\end{aligned}
$$

여기서 (8.78)은 $X$의 평균이 $X$에 대한 최적의 추정량이라는 사실로부터 도출되며, 마지막 부등식은 가우시안 분포가 주어진 분산에 대해 최대 엔트로피를 갖는다는 사실로부터 도출됩니다. 등호는 (8.78)에서 $\hat{X}$가 최적의 추정량일 때(즉, $\hat{X}$가 $X$의 평균일 때)만 성립하고, (8.80)에서 등호는 $X$가 가우시안일 때만 성립합니다.

따름 정리 $Y$라는 부가 정보와 추정량 $\hat{X}(Y)$가 주어졌을 때, 다음이 성립합니다.

$$
E(X-\hat{X}(Y))^{2} \geq \frac{1}{2 \pi e} e^{2 h(X \mid Y)}
$$
<!-- Page 282 -->
# 요약

$$
\begin{aligned}
h(X) & =h(f)=-\int_{S} f(x) \log f(x) d x \\
f\left(X^{n}\right) & \doteq 2^{-n h(X)} \\
\operatorname{Vol}\left(A_{\epsilon}^{(n)}\right) & \doteq 2^{n h(X)} \\
H\left([X]_{2^{-n}}\right) & \approx h(X)+n \\
h\left(\mathcal{N}\left(0, \sigma^{2}\right)\right) & =\frac{1}{2} \log 2 \pi e \sigma^{2} \\
h\left(\mathcal{N}_{n}(\mu, K)\right) & =\frac{1}{2} \log (2 \pi e)^{n}|K| . \\
D(f \| g) & =\int f \log \frac{f}{g} \geq 0 \\
h\left(X_{1}, X_{2}, \ldots, X_{n}\right) & =\sum_{i=1}^{n} h\left(X_{i} \mid X_{1}, X_{2}, \ldots, X_{i-1}\right) \\
h(X \mid Y) & \leq h(X) \\
h(a X) & =h(X)+\log |a| \\
I(X ; Y) & =\int f(x, y) \log \frac{f(x, y)}{f(x) f(y)} \geq 0 \\
\max _{E \mathbf{X} \mathbf{X}^{\prime}=K} h(\mathbf{X}) & =\frac{1}{2} \log (2 \pi e)^{n}|K| \\
E(X-\hat{X}(Y))^{2} & \geq \frac{1}{2 \pi e} e^{2 h(X \mid Y)}
\end{aligned}
$$

$2^{n H(X)}$는 이산 확률 변수의 유효 알파벳 크기입니다. $2^{n h(X)}$는 연속 확률 변수의 유효 지지 집합 크기입니다. $2^{C}$는 채널 용량 $C$의 유효 알파벳 크기입니다.

## 문제

8.1 차분 엔트로피. 다음의 차분 엔트로피 $h(X) = -\int f \ln f$를 계산하십시오:
(a) 지수 분포, $f(x)=\lambda e^{-\lambda x}, x \geq 0$.
<!-- Page 283 -->
(b) 라플라스 분포, $f(x)=\frac{1}{2} \lambda e^{-\lambda|x|}$.
(c) $X_{1}$과 $X_{2}$의 합, 여기서 $X_{1}$과 $X_{2}$는 평균이 $\mu_{i}$이고 분산이 $\sigma_{i}^{2}, i=$ 1,2 인 독립 정규 확률 변수입니다.
8.2 행렬식의 오목성. $K_{1}$과 $K_{2}$를 두 개의 대칭 비음수 정방행렬이라고 가정합니다. Ky Fan [199]의 결과를 증명하십시오:

$$
\left|\lambda K_{1}+\bar{\lambda} K_{2}\right| \geq\left|K_{1}\right|^{\lambda}\left|K_{2}\right|^{\bar{\lambda}} \quad \text { for } 0 \leq \lambda \leq 1, \quad \bar{\lambda}=1-\lambda
$$

여기서 $|K|$는 $K$의 행렬식을 나타냅니다. [힌트: $\mathbf{Z}=\mathbf{X}_{\theta}$라고 두십시오. 여기서 $\mathbf{X}_{1} \sim N\left(0, K_{1}\right), \mathbf{X}_{2} \sim N\left(0, K_{2}\right)$이고 $\theta=\operatorname{Bernoulli}(\lambda)$입니다. 그런 다음 $h(\mathbf{Z} \mid \theta) \leq h(\mathbf{Z})$를 사용하십시오.]
8.3 균일 분포 잡음. 채널의 입력 확률 변수 $X$가 구간 $-\frac{1}{2} \leq x \leq+\frac{1}{2}$에 대해 균일하게 분포한다고 가정합니다. 채널의 출력은 $Y=X+Z$이며, 여기서 잡음 확률 변수는 구간 $-a / 2 \leq z \leq$ $+a / 2$에 대해 균일하게 분포합니다.
(a) $a$의 함수로 $I(X ; Y)$를 찾으십시오.
(b) $a=1$일 때, 입력 $X$가 피크 제한(즉, $X$의 범위가 $-\frac{1}{2} \leq x \leq$ $+\frac{1}{2}$로 제한됨)일 때 채널의 용량을 찾으십시오. 어떤 확률 분포가 상호 정보량 $I(X ; Y)$를 최대화합니까?
(c) (선택 사항) 입력 $X$의 범위가 $-\frac{1}{2} \leq x \leq+\frac{1}{2}$로 제한된다고 가정할 때, 모든 $a$ 값에 대해 채널의 용량을 찾으십시오.
8.4 양자화된 확률 변수. 라듐 원자의 붕괴 시간(년)을 세 자리 정확도로 설명하는 데 평균적으로 몇 비트가 필요합니까? 라듐의 반감기는 80년입니다. 반감기는 분포의 중앙값임을 유의하십시오.
8.5 스케일링. $h(\mathbf{X})=-\int f(\mathbf{x}) \log f(\mathbf{x}) d \mathbf{x}$라고 할 때, $h(A \mathbf{X})=\log |\operatorname{det}(A)|+h(\mathbf{X})$임을 보이십시오.
8.6 변분 부등식. 양수 확률 변수 $X$에 대해 다음을 확인하십시오.

$$
\log E_{P}(X)=\sup _{Q}\left[E_{Q}(\log X)-D(Q \| P)\right]
$$

여기서 $E_{P}(X)=\sum_{x} x P(x)$이고 $D(Q \| P)=\sum_{x} Q(x) \log \frac{Q(x)}{P(x)}$이며, supremum은 모든 $Q(x) \geq 0, \sum Q(x)=1$에 대해 취합니다. $J(Q)=E_{Q} \ln X-D(Q \| P)+\lambda(\sum Q(x)-1)$를 극값으로 만드는 것으로 충분합니다.
<!-- Page 284 -->
8.7 이산 엔트로피에 대한 차분 엔트로피의 경계. $X$를 집합 $\mathcal{X}=\left\{a_{1}, a_{2}, \ldots\right\}$ 상의 이산 확률 변수라 하고, $\operatorname{Pr}(X=$ $\left.a_{i}\right)=p_{i}$라고 합시다. 다음을 보이십시오.

$$
H\left(p_{1}, p_{2}, \ldots\right) \leq \frac{1}{2} \log (2 \pi e)\left(\sum_{i=1}^{\infty} p_{i} i^{2}-\left(\sum_{i=1}^{\infty} i p_{i}\right)^{2}+\frac{1}{12}\right)
$$

또한, 모든 순열 $\sigma$에 대하여,

$$
H\left(p_{1}, p_{2}, \ldots\right) \leq \frac{1}{2} \log (2 \pi e)\left(\sum_{i=1}^{\infty} p_{\sigma(i)} i^{2}-\left(\sum_{i=1}^{\infty} i p_{\sigma(i)}\right)^{2}+\frac{1}{12}\right)
$$

[힌트: $\operatorname{Pr}\left(X^{\prime}=i\right)=p_{i}$인 확률 변수 $X^{\prime}$를 구성하십시오. $U$를 균등 분포 $(0,1]$ 확률 변수라 하고, $X^{\prime}$와 $U$가 독립일 때 $Y=X^{\prime}+U$라고 합시다. $Y$에 대한 최대 엔트로피 경계를 사용하여 문제의 경계를 얻으십시오. 이 경계는 Massey(미발표)와 Willems(미발표)에 의한 것입니다.]
8.8 균등 분포 잡음이 있는 채널. 입력 알파벳 $\mathcal{X}=\{0, \pm 1, \pm 2\}$를 갖고 출력 $Y=X+Z$인 가산 채널을 고려하십시오. 여기서 $Z$는 구간 $[-1,1]$ 상에서 균등 분포합니다. 따라서 채널의 입력은 이산 확률 변수이지만 출력은 연속적입니다. 이 채널의 용량 $C=$ $\max _{p(x)} I(X ; Y)$를 계산하십시오.
8.9 가우시안 상호 정보. $(X, Y, Z)$가 공동 가우시안이고 $X \rightarrow Y \rightarrow Z$가 마르코프 연쇄를 형성한다고 가정합니다. $X$와 $Y$의 상관 계수가 $\rho_{1}$이고 $Y$와 $Z$의 상관 계수가 $\rho_{2}$라고 할 때, $I(X ; Z)$를 구하십시오.
8.10 전형적인 집합의 모양. $X_{i}$가 i.i.d. $\sim f(x)$이고,

$$
f(x)=c e^{-x^{4}}
$$

라고 합시다. $h=-\int f \ln f$라고 할 때, 전형적인 집합 $A_{\epsilon}^{(n)}=\left\{x^{n} \in \mathcal{R}^{n}: f\left(x^{n}\right) \in 2^{-n(h \pm \epsilon)}\right\}$의 모양(또는 형태)을 설명하십시오.
8.11 비에르고딕 가우시안 과정. iid 관측 잡음 $\left\{Z_{i}\right\}$가 있는 상수 신호 $V$를 고려하십시오. 따라서 $X_{i}=V+Z_{i}$이며, 여기서 $V \sim N(0, S)$이고 $Z_{i}$는 iid $\sim N(0, N)$입니다. $V$와 $\left\{Z_{i}\right\}$가 독립이라고 가정합니다.
(a) $\left\{X_{i}\right\}$는 정상 과정입니까?
<!-- Page 285 -->
(b) $\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^{n} X_{i}$ 을 구하십시오. 극한값은 랜덤입니까?
(c) $\left\{X_{i}\right\}$ 의 엔트로피율 $h$ 는 무엇입니까?
(d) 최소 평균 제곱 오차 예측자 $\hat{X}_{n+1}\left(X^{n}\right)$ 을 구하고, $\sigma_{\infty}^{2}=\lim _{n \rightarrow \infty} E\left(\hat{X}_{n}-X_{n}\right)^{2}$ 을 구하십시오.
(e) $\left\{X_{i}\right\}$ 는 AEP 를 가집니까? 즉, $-\frac{1}{n} \log f\left(X^{n}\right) \longrightarrow h$ 입니까?

# 역사적 고찰

차분 엔트로피와 이산 엔트로피는 Shannon 의 원 논문 [472] 에서 소개되었습니다. 임의의 확률 변수에 대한 상대 엔트로피와 mutual information 의 일반적인 엄밀한 정의는 Kolmogorov [319] 와 Pinsker [425] 에 의해 개발되었으며, 이들은 mutual information 을 $\sup _{\mathcal{P}, \mathcal{Q}} I\left([X]_{\mathcal{P}} ;[Y]_{\mathcal{Q}}\right)$ 로 정의했습니다. 여기서 supremum 은 모든 유한 분할 $\mathcal{P}$ 와 $\mathcal{Q}$ 에 대한 것입니다.
<!-- Page 286 -->
.
<!-- Page 287 -->
# 가우시안 채널

가장 중요한 연속 알파벳 채널은 그림 9.1에 묘사된 가우시안 채널입니다. 이는 시간 이산 채널로, 시간 $i$에서의 출력 $Y_{i}$는 입력 $X_{i}$와 잡음 $Z_{i}$의 합입니다. 잡음 $Z_{i}$는 분산 $N$을 갖는 가우시안 분포에서 i.i.d.로 추출됩니다. 따라서,

$$
Y_{i}=X_{i}+Z_{i}, \quad Z_{i} \sim \mathcal{N}(0, N)
$$

잡음 $Z_{i}$는 신호 $X_{i}$와 독립이라고 가정합니다. 이 채널은 유선 및 무선 전화 채널, 위성 링크와 같은 일부 일반적인 통신 채널의 모델입니다. 추가적인 조건이 없으면 이 채널의 용량은 무한할 수 있습니다. 잡음 분산이 0이면 수신기는 전송된 심볼을 완벽하게 수신합니다. $X$는 모든 실수 값을 가질 수 있으므로, 채널은 오류 없이 임의의 실수 숫자를 전송할 수 있습니다.

잡음 분산이 0이 아니고 입력에 제약이 없다면, 출력에서 오류 확률이 임의로 작게 구별될 수 있도록 임의로 멀리 떨어진 입력의 무한한 부분 집합을 선택할 수 있습니다. 이러한 방식은 무한한 용량을 갖습니다. 따라서 잡음 분산이 0이거나 입력이 제약되지 않으면 채널의 용량은 무한합니다.

입력에 대한 가장 일반적인 제약은 에너지 또는 전력 제약입니다. 평균 전력 제약을 가정합니다. 채널을 통해 전송되는 모든 코드워드 $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$에 대해 다음을 요구합니다.

$$
\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2} \leq P
$$

이 통신 채널은 무선 및 위성 링크를 포함한 많은 실제 채널을 모델링합니다. 이러한 채널의 가산 잡음은 다양한 원인으로 인해 발생할 수 있습니다. 그러나 중심 극한 정리에 의해,

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 288 -->

그림 9.1. 가우시안 채널.
많은 수의 작은 무작위 효과의 누적 효과는 대략 정규 분포를 따르므로, 가우시안 가정은 많은 상황에서 유효합니다.

먼저 이 채널을 사용하는 간단한 차선의 방법을 분석합니다. 채널을 통해 1비트를 한 번의 채널 사용으로 전송한다고 가정합니다. 전력 제약이 주어졌을 때, 우리가 할 수 있는 최선은 두 개의 레벨, 즉 $+\sqrt{P}$ 또는 $-\sqrt{P}$ 중 하나를 전송하는 것입니다. 수신기는 수신된 해당 $Y$를 보고 두 레벨 중 어느 것이 전송되었는지 결정하려고 합니다. 두 레벨이 동일하게 발생할 가능성이 있다고 가정하면 (정확히 1비트의 정보를 전송하고자 할 때 이러한 경우가 될 것입니다), 최적의 디코딩 규칙은 $Y>0$이면 $+\sqrt{P}$가 전송되었다고 결정하고 $Y<0$이면 $-\sqrt{P}$가 전송되었다고 결정하는 것입니다. 이러한 디코딩 방식의 오류 확률은 다음과 같습니다.

$$
\begin{aligned}
P_{e} & =\frac{1}{2} \operatorname{Pr}(Y<0 \mid X=+\sqrt{P})+\frac{1}{2} \operatorname{Pr}(Y>0 \mid X=-\sqrt{P}) \\
& =\frac{1}{2} \operatorname{Pr}(Z<-\sqrt{P} \mid X=+\sqrt{P})+\frac{1}{2} \operatorname{Pr}(Z>\sqrt{P} \mid X=-\sqrt{P}) \\
& =\operatorname{Pr}(Z>\sqrt{P}) \\
& =1-\Phi(\sqrt{P / N})
\end{aligned}
$$

여기서 $\Phi(x)$는 누적 정규 함수입니다.

$$
\Phi(x)=\int_{-\infty}^{x} \frac{1}{\sqrt{2 \pi}} e^{\frac{-t^{2}}{2}} d t
$$

이러한 방식을 사용하면 가우시안 채널을 교차 확률 $P_{e}$를 갖는 이산 이진 대칭 채널로 변환했습니다. 마찬가지로, 4레벨 입력 신호를 사용하면 가우시안 채널을

<!-- Page 289 -->
연속 채널을 이산 채널로 변환하는 데 사용됩니다. 이산 채널의 주요 장점은 오류 수정을 위한 출력 신호 처리가 용이하다는 것이지만, 양자화 과정에서 일부 정보가 손실됩니다.

# 9.1 가우시안 채널: 정의

이제 채널의 (정보) 용량을 전력 제약을 만족하는 입력에 대한 모든 분포에 대해 입력과 출력 간의 상호 정보량의 최댓값으로 정의합니다.

정의 전력 제약 $P$를 갖는 가우시안 채널의 정보 용량은 다음과 같습니다.

$$
C=\max _{f(x): E X^{2} \leq P} I(X ; Y)
$$

정보 용량은 다음과 같이 계산할 수 있습니다. $I(X ; Y)$를 확장하면 다음과 같습니다.

$$
\begin{aligned}
I(X ; Y) & =h(Y)-h(Y \mid X) \\
& =h(Y)-h(X+Z \mid X) \\
& =h(Y)-h(Z \mid X) \\
& =h(Y)-h(Z)
\end{aligned}
$$

$Z$는 $X$와 독립이기 때문입니다. 이제 $h(Z)=\frac{1}{2} \log 2 \pi e N$입니다. 또한,

$$
E Y^{2}=E(X+Z)^{2}=E X^{2}+2 E X E Z+E Z^{2}=P+N
$$

$X$와 $Z$는 독립이고 $E Z=0$이기 때문입니다. $E Y^{2}=P+N$이 주어졌을 때, $Y$의 엔트로피는 정리 8.6.5에 의해 $\frac{1}{2} \log 2 \pi e(P+N)$으로 제한됩니다 (정규 분포는 주어진 분산에 대해 엔트로피를 최대화합니다).

이 결과를 상호 정보량에 적용하여 다음과 같이 얻습니다.

$$
\begin{aligned}
I(X ; Y) & =h(Y)-h(Z) \\
& \leq \frac{1}{2} \log 2 \pi e(P+N)-\frac{1}{2} \log 2 \pi e N \\
& =\frac{1}{2} \log \left(1+\frac{P}{N}\right)
\end{aligned}
$$
<!-- Page 290 -->
따라서 가우시안 채널의 정보 용량은 다음과 같습니다.

$$
C=\max _{E X^{2} \leq P} I(X ; Y)=\frac{1}{2} \log \left(1+\frac{P}{N}\right)
$$

최댓값은 $X \sim \mathcal{N}(0, P)$일 때 달성됩니다.
이제 이 용량이 채널에 대해 달성 가능한 속도들의 상한임을 보이겠습니다. 논증은 이산 채널에 대한 논증과 유사합니다. 해당 정의부터 시작하겠습니다.

정의 전력 제약 $P$를 갖는 가우시안 채널에 대한 $(M, n)$ 코드는 다음을 포함합니다.

1. 인덱스 집합 $\{1,2, \ldots, M\}$.
2. 부호어 $x^{n}(1), x^{n}(2), \ldots, x^{n}(M)$를 생성하는 인코딩 함수 $x:\{1,2, \ldots, M\} \rightarrow \mathcal{X}^{n}$. 이는 전력 제약 $P$를 만족해야 합니다. 즉, 모든 부호어에 대해

$$
\sum_{i=1}^{n} x_{i}^{2}(w) \leq n P, \quad w=1,2, \ldots, M
$$

3. 디코딩 함수

$$
g: \mathcal{Y}^{n} \rightarrow\{1,2, \ldots, M\}
$$

코드의 속도와 오류 확률은 이산적인 경우에 대해 7장에서와 같이 정의됩니다. 오류 확률의 산술 평균은 다음과 같이 정의됩니다.

$$
P_{e}^{(n)}=\frac{1}{2^{n R}} \sum \lambda_{i}
$$

정의 전력 제약 $P$를 갖는 가우시안 채널에 대해 속도 $R$은 달성 가능하다고 말합니다. 이는 전력 제약을 만족하는 부호어를 갖는 $\left(2^{n R}, n\right)$ 코드의 시퀀스가 존재하며, 최대 오류 확률 $\lambda^{(n)}$이 0으로 수렴함을 의미합니다. 채널의 용량은 달성 가능한 속도들의 상한입니다.

정리 9.1.1 전력 제약 $P$와 잡음 분산 $N$을 갖는 가우시안 채널의 용량은 다음과 같습니다.

$$
C=\frac{1}{2} \log \left(1+\frac{P}{N}\right) \quad \text { 비트/전송. }
$$
<!-- Page 291 -->
고찰 우리는 낮은 오류 확률을 가진 $\left(2^{n C}, n\right)$ 코드를 구성할 수 있는 이유에 대한 타당성 논증을 먼저 제시합니다. 길이 $n$인 임의의 부호어를 고려하십시오. 수신된 벡터는 평균이 실제 부호어이고 분산이 잡음 분산과 같은 정규 분포를 따릅니다. 높은 확률로 수신된 벡터는 실제 부호어 주위의 반지름 $\sqrt{n(N+\epsilon)}$인 구 내에 포함됩니다. 이 구 내의 모든 것을 주어진 부호어에 할당한다면, 이 부호어가 전송될 때 수신된 벡터가 구 밖에 떨어지는 경우에만 오류가 발생하며, 이는 낮은 확률입니다.

마찬가지로 다른 부호어와 해당 복호 구를 선택할 수 있습니다. 이러한 부호어를 몇 개나 선택할 수 있습니까? $n$차원 구의 부피는 $C_{n} r^{n}$ 형태이며, 여기서 $r$은 구의 반지름입니다. 이 경우 각 복호 구의 반지름은 $\sqrt{n N}$입니다. 이 구들은 수신된 벡터 공간 전체에 흩어져 있습니다. 수신된 벡터의 에너지는 $n(P+N)$ 이하이므로 반지름 $\sqrt{n(P+N)}$인 구 내에 있습니다. 이 부피 내에서 서로 겹치지 않는 복호 구의 최대 개수는 다음과 같습니다.

$$
\frac{C_{n}(n(P+N))^{\frac{n}{2}}}{C_{n}(n N)^{\frac{n}{2}}}=2^{\frac{n}{2}} \log \left(1+\frac{P}{N}\right)
$$

그리고 코드의 비율은 $\frac{1}{2} \log \left(1+\frac{P}{N}\right)$입니다. 이 아이디어는 그림 9.2에 설명되어 있습니다.

그림 9.2. 가우시안 채널을 위한 구 충전.
<!-- Page 292 -->
이 구체 충진 논증은 낮은 오류 확률로 $C$보다 높은 비율로 전송하는 것을 기대할 수 없음을 나타냅니다. 그러나 우리는 실제로 거의 이만큼 잘 할 수 있으며, 이는 다음에 증명됩니다.

증명: (달성 가능성). 이산 채널의 경우 채널 코딩 정리 증명에서 사용한 것과 동일한 아이디어, 즉 랜덤 코드와 공동 전형성 디코딩을 사용할 것입니다. 그러나 전력 제약 조건과 변수가 이산적이지 않고 연속적이라는 사실을 고려하기 위해 몇 가지 수정이 필요합니다.

1. 코드북 생성. 전력 제약 조건을 만족하는 모든 코드워드를 포함하는 코드북을 생성하고자 합니다. 이를 보장하기 위해 각 요소가 평균이 0이고 분산이 $P-\epsilon$인 정규 분포에 따라 i.i.d.로 생성된 코드워드를 생성합니다. $n$이 클 때 $\frac{1}{n} \sum X_{i}^{2} \rightarrow P-\epsilon$이므로 코드워드가 전력 제약 조건을 만족하지 못할 확률은 작을 것입니다. $X_{i}(w), i=1,2, \ldots, n$, $w=1,2, \ldots, 2^{n R}$을 i.i.d. $\sim \mathcal{N}(0, P-\epsilon)$으로 생성하여 코드워드 $X^{n}(1), X^{n}(2), \ldots, X^{n}\left(2^{n R}\right) \in \mathcal{R}^{n}$을 형성합니다.
2. 인코딩. 코드북 생성 후 코드북은 송신자와 수신자 모두에게 공개됩니다. 메시지 인덱스 $w$를 전송하기 위해 송신자는 코드북의 $w$번째 코드워드 $X^{n}(w)$를 전송합니다.
3. 디코딩. 수신자는 코드워드 목록 $\left\{X^{n}(w)\right\}$을 살펴보고 수신된 벡터와 공동으로 전형적인 것을 찾습니다. 그러한 코드워드 $X^{n}(w)$가 하나만 있으면 수신자는 전송된 코드워드를 $\hat{W}=w$로 선언합니다. 그렇지 않으면 수신자는 오류를 선언합니다. 선택된 코드워드가 전력 제약 조건을 만족하지 못하는 경우에도 수신자는 오류를 선언합니다.
4. 오류 확률. 일반성을 잃지 않고 코드워드 1이 전송되었다고 가정합니다. 따라서 $Y^{n}=X^{n}(1)+Z^{n}$입니다. 다음 이벤트를 정의합니다.

$$
E_{0}=\left\{\frac{1}{n} \sum_{j=1}^{n} X_{j}^{2}(1)>P\right\}
$$

그리고

$$
E_{i}=\left\{\left(X^{n}(i), Y^{n}\right) \text { is in } A_{\epsilon}^{(n)}\right\}
$$

그러면 오류는 $E_{0}$이 발생하거나(전력 제약 조건이 위반됨) $E_{1}^{c}$이 발생하거나(전송된 코드워드와 수신된 시퀀스가 공동으로 전형적이지 않음) $E_{2} \cup E_{3} \cup \cdots \cup E_{2^{n R}}$이 발생하는 경우(잘못된) 발생합니다.
<!-- Page 293 -->
(코드 단어가 수신된 시퀀스와 함께 일반적으로 해당됩니다). $\mathcal{E}$를 이벤트 $\hat{W} \neq W$로 나타내고 $P$를 $W=1$이 주어졌을 때의 조건부 확률로 나타냅니다. 따라서,

$$
\begin{aligned}
\operatorname{Pr}(\mathcal{E} \mid W=1)=P(\mathcal{E}) & =P\left(E_{0} \cup E_{1}^{c} \cup E_{2} \cup E_{3} \cup \cdots \cup E_{2^{n R}}\right) \\
& \leq P\left(E_{0}\right)+P\left(E_{1}^{c}\right)+\sum_{i=2}^{2^{n R}} P\left(E_{i}\right)
\end{aligned}
$$

확률에 대한 이벤트 결합 바운드를 사용하여. 대수의 법칙에 의해, $P\left(E_{0}\right) \rightarrow 0$ as $n \rightarrow \infty$. 이제, 결합 AEP에 의해 (이는 이산적인 경우에 사용된 것과 동일한 논증을 사용하여 증명될 수 있습니다), $P\left(E_{1}^{c}\right) \rightarrow 0$, 따라서

$$
P\left(E_{1}^{c}\right) \leq \epsilon \quad \text { for } n \text { sufficiently large. }
$$

코드 생성 과정에 의해, $X^{n}(1)$과 $X^{n}(i)$는 독립적이므로, $Y^{n}$과 $X^{n}(i)$도 독립적입니다. 따라서, $X^{n}(i)$와 $Y^{n}$이 결합적으로 일반적일 확률은 결합 AEP에 의해 $\leq 2^{-n(I(X ; Y)-3 \epsilon)}$입니다.
이제 $W$가 $\left\{1,2, \ldots, 2^{n R}\right\}$ 위에서 균등하게 분포한다고 가정하면, 결과적으로,

$$
\operatorname{Pr}(\mathcal{E})=\frac{1}{2^{n R}} \sum \lambda_{i}=P_{\epsilon}^{(n)}
$$

그러면

$$
\begin{aligned}
P_{e}^{(n)} & =\operatorname{Pr}(\mathcal{E})=\operatorname{Pr}(\mathcal{E} \mid W=1) \\
& \leq P\left(E_{0}\right)+P\left(E_{1}^{c}\right)+\sum_{i=2}^{2^{n R}} P\left(E_{i}\right) \\
& \leq \epsilon+\epsilon+\sum_{i=2}^{2^{n R}} 2^{-n(I(X ; Y)-3 \epsilon)} \\
& =2 \epsilon+\left(2^{n R}-1\right) 2^{-n(I(X ; Y)-3 \epsilon)} \\
& \leq 2 \epsilon+2^{3 n \epsilon} 2^{-n(I(X ; Y)-R)} \\
& \leq 3 \epsilon
\end{aligned}
$$
<!-- Page 294 -->
$n$이 충분히 크고 $R<I(X ; Y)-3 \epsilon$인 경우. 이는 좋은 $\left(2^{n R}, n\right)$ 코드의 존재를 증명합니다.

이제 좋은 코드북을 선택하고 최악의 절반의 코드를 삭제함으로써 오류의 최대 확률이 낮은 코드를 얻습니다. 특히, 남아있는 각 코드워드는 전력 제약 조건을 만족합니다 (전력 제약 조건을 만족하지 않는 코드워드는 오류 확률이 1이고 최악의 절반에 속해야 하므로). 따라서 우리는 용량에 임의로 가까운 비율을 달성하는 코드를 구성했습니다. 정리의 순방향 부분은 증명되었습니다. 다음 섹션에서는 달성 가능한 비율이 용량을 초과할 수 없음을 보여줍니다.

# 9.2 가우시안 채널에 대한 코딩 정리의 역

이 섹션에서는 가우시안 채널의 용량이 $C=\frac{1}{2} \log \left(1+\frac{P}{N}\right)$임을 증명함으로써 $R>C$인 비율이 달성될 수 없음을 보여줌으로써 증명을 완료합니다. 이 증명은 이산 채널에 대한 증명과 유사합니다. 새로운 주요 요소는 전력 제약 조건입니다.

증명: (정리 9.1.1의 역). 우리는 $P_{e}^{(n)} \rightarrow 0$이면, 전력 제약 조건 $P$를 갖는 가우시안 채널에 대한 $\left(2^{n R}, n\right)$ 코드 시퀀스에 대해 다음이 성립함을 보여야 합니다.

$$
R \leq C=\frac{1}{2} \log \left(1+\frac{P}{N}\right)
$$

전력 제약 조건을 만족하는 임의의 $\left(2^{n R}, n\right)$ 코드를 고려합니다. 즉,

$$
\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}(w) \leq P
$$

$w=1,2, \ldots, 2^{n R}$에 대해. 이산 경우의 역과 유사하게 진행하여, $W$가 $\left\{1,2, \ldots, 2^{n R}\right\}$ 위에서 균등하게 분포된다고 가정합니다. 인덱스 집합 $W \in\left\{1,2, \ldots, 2^{n R}\right\}$에 대한 균등 분포는 입력 코드워드에 대한 분포를 유도하고, 이는 다시 입력 알파벳에 대한 분포를 유도합니다. 이는 $W \rightarrow X^{n}(W) \rightarrow Y^{n} \rightarrow \hat{W}$에 대한 결합 분포를 지정합니다. 오류 확률과 상호 정보를 관련시키기 위해 파노의 부등식을 적용하여 다음을 얻을 수 있습니다.

$$
H(W \mid \hat{W}) \leq 1+n R P_{e}^{(n)}=n \epsilon_{n}
$$
<!-- Page 295 -->
여기서 $\epsilon_{n} \rightarrow 0$는 $P_{e}^{(n)} \rightarrow 0$일 때 성립합니다. 따라서,

$$
\begin{aligned}
n R=H(W) & =I(W ; \hat{W})+H(W \mid \hat{W}) \\
& \leq I(W ; \hat{W})+n \epsilon_{n} \\
& \leq I\left(X^{n} ; Y^{n}\right)+n \epsilon_{n} \\
& =h\left(Y^{n}\right)-h\left(Y^{n} \mid X^{n}\right)+n \epsilon_{n} \\
& =h\left(Y^{n}\right)-h\left(Z^{n}\right)+n \epsilon_{n} \\
& \leq \sum_{i=1}^{n} h\left(Y_{i}\right)-h\left(Z^{n}\right)+n \epsilon_{n} \\
& =\sum_{i=1}^{n} h\left(Y_{i}\right)-\sum_{i=1}^{n} h\left(Z_{i}\right)+n \epsilon_{n} \\
& =\sum_{i=1}^{n} I\left(X_{i} ; Y_{i}\right)+n \epsilon_{n}
\end{aligned}
$$

여기서 $X_{i}=x_{i}(W)$이며, $W$는 $\left\{1,2, \ldots, 2^{n R}\right\}$에 대한 균등 분포에 따라 추출됩니다. 이제 $P_{i}$를 코드북의 $i$번째 열의 평균 전력이라고 정의합니다. 즉,

$$
P_{i}=\frac{1}{2^{n R}} \sum_{w} x_{i}^{2}(w)
$$

그러면 $Y_{i}=X_{i}+Z_{i}$이고 $X_{i}$와 $Z_{i}$는 독립이므로, $Y_{i}$의 평균 전력 $E Y_{i}{ }^{2}$는 $P_{i}+N$입니다. 따라서 엔트로피는 정규 분포에 의해 최대화되므로,

$$
h\left(Y_{i}\right) \leq \frac{1}{2} \log 2 \pi e\left(P_{i}+N\right)
$$

역정리의 부등식을 계속 적용하면 다음과 같습니다.

$$
\begin{aligned}
n R & \leq \sum\left(h\left(Y_{i}\right)-h\left(Z_{i}\right)\right)+n \epsilon_{n} \\
& \leq \sum\left(\frac{1}{2} \log \left(2 \pi e\left(P_{i}+N\right)\right)-\frac{1}{2} \log 2 \pi e N\right)+n \epsilon_{n} \\
& =\sum \frac{1}{2} \log \left(1+\frac{P_{i}}{N}\right)+n \epsilon_{n}
\end{aligned}
$$
<!-- Page 296 -->
각 코딩워드가 전력 제약을 만족하므로, 이들의 평균 또한 전력 제약을 만족하며, 따라서

$$
\frac{1}{n} \sum_{i} P_{i} \leq P
$$

$f(x)=\frac{1}{2} \log (1+x)$는 $x$에 대한 오목 함수이므로, 젠센 부등식을 적용하여 다음을 얻을 수 있습니다.

$$
\begin{aligned}
\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} \log \left(1+\frac{P_{i}}{N}\right) & \leq \frac{1}{2} \log \left(1+\frac{1}{n} \sum_{i=1}^{n} \frac{P_{i}}{N}\right) \\
& \leq \frac{1}{2} \log \left(1+\frac{P}{N}\right)
\end{aligned}
$$

따라서 $R \leq \frac{1}{2} \log \left(1+\frac{P}{N}\right)+\epsilon_{n}, \epsilon_{n} \rightarrow 0$이며, 필요한 역정리를 얻습니다.
전력 제약이 (9.46)에서 표준 증명에 들어가는 것을 주목하십시오.

# 9.3 대역 제한 채널

무선 네트워크 또는 전화선을 통한 통신의 일반적인 모델은 백색 잡음이 있는 대역 제한 채널입니다. 이것은 연속 시간 채널입니다. 이러한 채널의 출력은 다음의 컨볼루션으로 설명될 수 있습니다.

$$
Y(t)=(X(t)+Z(t)) * h(t)
$$

여기서 $X(t)$는 신호 파형, $Z(t)$는 백색 가우시안 잡음의 파형, 그리고 $h(t)$는 $W$보다 큰 모든 주파수를 차단하는 이상적인 대역 통과 필터의 임펄스 응답입니다. 이 섹션에서는 이러한 채널의 capacity를 계산하기 위한 단순화된 논증을 제시합니다.

먼저 Nyquist [396]와 Shannon [480]의 표현 정리에 따라, 샘플링 속도 $\frac{1}{2 W}$로 대역 제한된 신호를 샘플링하는 것이 샘플로부터 신호를 복원하기에 충분함을 보여줍니다. 직관적으로, 이는 신호가 $W$로 대역 제한되어 있다면, 신호 내 최대 주파수의 반 주기보다 짧은 시간 동안 실질적인 양만큼 변할 수 없다는 사실 때문입니다. 즉, 신호는 $\frac{1}{2 W}$초보다 짧은 시간 간격 동안에는 많이 변할 수 없습니다.
<!-- Page 297 -->
정리 9.3.1 $f(t)$ 함수가 $W$로 대역 제한되어 있다고 가정합니다. 즉, 함수의 스펙트럼이 $W$보다 큰 모든 주파수에 대해 0입니다. 그러면 함수는 $\frac{1}{2 W}$ 초 간격으로 샘플링된 함수에 의해 완전히 결정됩니다.

증명: $F(\omega)$를 $f(t)$의 푸리에 변환이라고 합시다. 그러면

$$
\begin{aligned}
f(t) & =\frac{1}{2 \pi} \int_{-\infty}^{\infty} F(\omega) e^{i \omega t} d \omega \\
& =\frac{1}{2 \pi} \int_{-2 \pi W}^{2 \pi W} F(\omega) e^{i \omega t} d \omega
\end{aligned}
$$

$F(\omega)$가 $-2 \pi W \leq \omega \leq 2 \pi W$ 대역 밖에서는 0이기 때문입니다. $\frac{1}{2 W}$ 초 간격으로 샘플링된 샘플을 고려하면, 샘플 지점에서의 신호 값은 다음과 같이 쓸 수 있습니다.

$$
f\left(\frac{n}{2 W}\right)=\frac{1}{2 \pi} \int_{-2 \pi W}^{2 \pi W} F(\omega) e^{i \omega \frac{n}{2 W}} d \omega
$$

이 방정식의 우변은 또한 함수 $F(\omega)$의 주기적 확장의 푸리에 급수 전개의 계수에 대한 정의이며, 구간 $-2 \pi W$에서 $2 \pi W$를 기본 주기라고 합니다. 따라서 샘플 값 $f\left(\frac{n}{2 W}\right)$는 푸리에 계수를 결정하며, 확장하면 $(-2 \pi W, 2 \pi W)$ 구간에서 $F(\omega)$의 값을 결정합니다. 함수는 푸리에 변환에 의해 고유하게 지정되고, $F(\omega)$가 $W$ 대역 밖에서는 0이므로, 샘플로부터 함수를 고유하게 결정할 수 있습니다.

함수

$$
\operatorname{sinc}(t)=\frac{\sin (2 \pi W t)}{2 \pi W t}
$$

를 고려해 봅시다. 이 함수는 $t=0$에서 1이고 $t=n / 2 W, n \neq 0$에서는 0입니다. 이 함수의 스펙트럼은 $(-W, W)$ 대역에서 일정하며 이 대역 밖에서는 0입니다. 이제 다음과 같이 정의합니다.

$$
g(t)=\sum_{n=-\infty}^{\infty} f\left(\frac{n}{2 W}\right) \operatorname{sinc}\left(t-\frac{n}{2 W}\right)
$$

sinc 함수의 속성으로부터, $g(t)$는 $W$로 대역 제한되며 $t=n / 2 W$에서 $f(n / 2 W)$와 같다는 것을 알 수 있습니다. 유일한

<!-- Page 298 -->
이러한 제약 조건을 만족하는 하나의 함수를 찾으려면 $g(t)=f(t)$여야 합니다. 이는 샘플을 통해 $f(t)$를 명시적으로 표현하는 방법을 제공합니다.

일반적인 함수는 무한한 자유도를 가집니다. 즉, 모든 지점에서 함수의 값을 독립적으로 선택할 수 있습니다. Nyquist-Shannon 샘플링 정리는 대역 제한 함수가 초당 $2W$의 자유도만 가진다는 것을 보여줍니다. 샘플 지점에서의 함수 값은 독립적으로 선택될 수 있으며, 이는 전체 함수를 결정합니다.

함수가 대역 제한되어 있다면 시간 제한은 불가능합니다. 그러나 에너지의 대부분이 대역폭 $W$에 있고 에너지의 대부분이 유한한 시간 간격, 예를 들어 $(0, T)$에 있는 함수를 고려할 수 있습니다. 이러한 함수는 겹친 구면 함수(prolate spheroidal functions)의 기저를 사용하여 설명할 수 있습니다. 이 이론의 자세한 내용은 여기서 다루지 않습니다. 거의 시간 제한되고 거의 대역 제한된 함수 집합에 대해 약 $2TW$개의 정규 직교 기저 함수가 있으며, 이 기저에서의 좌표를 통해 해당 집합 내의 모든 함수를 설명할 수 있다는 것만으로도 충분합니다. 자세한 내용은 Landau, Pollak, Slepian의 일련의 논문 [340, 341, 500]에서 찾을 수 있습니다. 또한, 이러한 기저 벡터에 대한 백색 잡음의 투영은 i.i.d. 가우시안 프로세스를 형성합니다. 위의 논증을 통해 대역 제한되고 시간 제한된 함수를 $2TW$ 차원의 벡터 공간에서의 벡터로 볼 수 있습니다.

이제 대역 제한 채널에서의 통신 문제로 돌아갑니다. 채널이 대역폭 $W$를 가진다고 가정하면, 입력과 출력을 $1/2W$초 간격으로 샘플링하여 표현할 수 있습니다. 각 입력 샘플은 잡음에 의해 해당 출력 샘플로 변환됩니다. 잡음이 백색이고 가우시안이므로, 각 잡음 샘플은 독립적이고 동일하게 분포된 가우시안 확률 변수임을 보일 수 있습니다.

잡음의 파워 스펙트럼 밀도가 와트/헤르츠당 $N_{0}/2$이고 대역폭이 $W$ 헤르츠라면, 잡음의 파워는 $\frac{N_{0}}{2} 2 W=N_{0} W$이며, 시간 $T$ 동안의 $2WT$개의 잡음 샘플 각각은 분산 $N_{0} W T / 2 W T=N_{0} / 2$를 가집니다. 입력을 $2TW$ 차원 공간의 벡터로 보면, 수신 신호는 공분산 $\frac{N_{0}}{2} I$를 가지는 이 점에 대해 구형 정규 분포를 따르는 것을 볼 수 있습니다.

이제 이산 시간 가우시안 채널에 대해 이전에 유도한 이론을 사용할 수 있습니다. 이 이론에서는 이러한 채널의 capacity가 다음과 같음이 입증되었습니다.

$$
C=\frac{1}{2} \log \left(1+\frac{P}{N}\right) \quad \text { 전송당 비트. }
$$

채널이 시간 간격 $[0, T]$ 동안 사용된다고 가정합니다. 이 경우 샘플당 에너지는 $P T / 2 W T=P / 2 W$이고, 샘플당 잡음 분산은
<!-- Page 299 -->
$\frac{N_{0}}{2} 2 W \frac{T}{2 W T}=N_{0} / 2$이며, 따라서 샘플당 용량은 다음과 같습니다.

$$
C=\frac{1}{2} \log \left(\frac{1+\frac{P}{2 W}}{\frac{N_{0}}{2}}\right)=\frac{1}{2} \log \left(1+\frac{P}{N_{0} W}\right) \quad \text { bits per sample. }
$$

초당 $2 W$개의 샘플이 있으므로 채널의 용량은 다음과 같이 다시 작성할 수 있습니다.

$$
C=W \log \left(1+\frac{P}{N_{0} W}\right) \quad \text { bits per second. }
$$

이 방정식은 정보 이론에서 가장 유명한 공식 중 하나입니다. 이는 잡음 스펙트럼 밀도 $N_{0} / 2$ 와트/Hz 및 전력 $P$ 와트의 대역 제한 가우시안 채널의 용량을 제공합니다.

용량 논증 [576]의 보다 정확한 버전에는 채널의 대역폭 $W$ 밖에 있는 에너지의 작은 부분과 시간 간격 $(0, T)$ 밖에 있는 에너지의 작은 부분을 가진 신호를 고려하는 것이 포함됩니다. 그런 다음 대역 외 에너지의 비율이 0으로 가는 극한으로 위의 용량을 얻습니다.

(9.62)에서 $W \rightarrow \infty$로 두면 다음과 같이 얻습니다.

$$
C=\frac{P}{N_{0}} \log _{2} e \quad \text { bits per second }
$$

무한 대역폭, 전력 $P$, 잡음 스펙트럼 밀도 $N_{0} / 2$를 갖는 채널의 용량으로. 따라서 무한 대역폭 채널의 경우 용량은 전력에 따라 선형적으로 증가합니다.

예제 9.3.1 (전화선) 많은 채널의 다중화를 허용하기 위해 전화 신호는 3300 Hz로 대역 제한됩니다. 3300 Hz의 대역폭과 33 dB의 SNR(신호 대 잡음비) (즉, $P / N_{0} W=$ 2000)을 (9.62)에 사용하면 전화 채널의 용량이 초당 약 36,000 비트임을 알 수 있습니다. 실제 모뎀은 전화 채널을 통해 양방향으로 초당 최대 33,600 비트의 전송 속도를 달성합니다. 실제 전화 채널에는 이 용량을 달성하기 위해 보상해야 하는 누화, 간섭, 에코 및 비평탄 채널과 같은 다른 요인이 있습니다.

전화 채널에서 초당 $56 \mathrm{~kb}$를 달성하는 V. 90 모뎀은 이 속도를 단방향으로만 달성하며, 네트워크에서 서버에서 최종 전화 교환기까지의 순수 디지털 채널을 활용합니다. 이 경우 유일한 장애는 이 교환기에서의 디지털-아날로그 변환과 교환기에서 가정까지의 구리 링크의 잡음으로 인한 것입니다.
<!-- Page 300 -->
이러한 손실은 네트워크 내 디지털 신호의 최대 비트 전송률을 $64 \mathrm{~kb} / \mathrm{s}$에서 최상의 전화선 환경에서 $56 \mathrm{~kb} / \mathrm{s}$로 감소시킵니다.

가정집을 전화 교환기와 연결하는 구리선에서 사용 가능한 실제 대역폭은 수 메가헤르츠(MHz) 범위이며, 이는 선의 길이에 따라 달라집니다. 이 대역폭 전체에 걸쳐 주파수 응답은 평탄하지 않습니다. 전체 대역폭을 사용하면 이 채널을 통해 초당 몇 메가비트(Mbps)를 전송할 수 있습니다. DSL(Digital Subscriber Line)과 같은 방식은 전화선의 양 끝에 특수 장비를 사용하여 이를 달성합니다 (전화 교환기에서의 수정이 필요 없는 모뎀과 달리).

# 9.4 병렬 가우시안 채널

이 절에서는 공통 전력 제약 조건 하에 병렬로 $k$개의 독립적인 가우시안 채널을 고려합니다. 목표는 용량을 최대화하기 위해 총 전력을 채널에 분배하는 것입니다. 이 채널은 비백색 가산 가우시안 잡음 채널을 모델링하며, 각 병렬 구성 요소는 다른 주파수를 나타냅니다.

그림 9.3에 설명된 대로 병렬로 가우시안 채널 세트가 있다고 가정합니다. 각 채널의 출력은 입력과 가우시안 잡음의 합입니다. 채널 $j$에 대해,

$$
Y_{j}=X_{j}+Z_{j}, \quad j=1,2, \ldots, k
$$

여기서

$$
Z_{j} \sim \mathcal{N}\left(0, N_{j}\right)
$$

이며, 잡음은 채널 간에 독립적이라고 가정합니다. 총 사용 전력에 대한 공통 전력 제약 조건이 있다고 가정합니다. 즉,

$$
E \sum_{j=1}^{k} X_{j}^{2} \leq P
$$

총 용량을 최대화하기 위해 다양한 채널에 전력을 분배하고자 합니다.

채널 $C$의 정보 용량은 다음과 같습니다.

$$
C=\max _{f\left(x_{1}, x_{2}, \ldots, x_{k}\right): \sum E X_{i}^{2} \leq P} I\left(X_{1}, X_{2}, \ldots, X_{k} ; Y_{1}, Y_{2}, \ldots, Y_{k}\right)
$$
<!-- Page 301 -->

그림 9.3. 병렬 가우시안 채널.

이 채널의 정보 용량(information capacity)을 달성하는 분포를 계산합니다. 정보 용량이 달성 가능한 비율의 상한(supremum)이라는 사실은 단일 가우시안 채널의 용량 정리 증명과 동일한 방법으로 증명될 수 있으며, 여기서는 생략합니다.

$Z_{1}, Z_{2}, \ldots, Z_{k}$는 독립적이므로,

$$
\begin{aligned}
& I\left(X_{1}, X_{2}, \ldots, X_{k} ; Y_{1}, Y_{2}, \ldots, Y_{k}\right) \\
& \quad=h\left(Y_{1}, Y_{2}, \ldots, Y_{k}\right)-h\left(Y_{1}, Y_{2}, \ldots, Y_{k} \mid X_{1}, X_{2}, \ldots, X_{k}\right) \\
& \quad=h\left(Y_{1}, Y_{2}, \ldots, Y_{k}\right)-h\left(Z_{1}, Z_{2}, \ldots, Z_{k} \mid X_{1}, X_{2}, \ldots, X_{k}\right) \\
& \quad=h\left(Y_{1}, Y_{2}, \ldots, Y_{k}\right)-h\left(Z_{1}, Z_{2}, \ldots, Z_{k}\right) \\
& \quad=h\left(Y_{1}, Y_{2}, \ldots, Y_{k}\right)-\sum_{i} h\left(Z_{i}\right) \\
& \quad \leq \sum_{i} h\left(Y_{i}\right)-h\left(Z_{i}\right) \\
& \quad \leq \sum_{i} \frac{1}{2} \log \left(1+\frac{P_{i}}{N_{i}}\right)
\end{aligned}
$$
<!-- Page 302 -->
여기서 $P_{i}=E X_{i}^{2}$이며, $\sum P_{i}=P$입니다. 등식은 다음을 통해 달성됩니다.

$$
\left(X_{1}, X_{2}, \ldots, X_{k}\right) \sim \mathcal{N}\left(0,\left[\begin{array}{cccc}
P_{1} & 0 & \cdots & 0 \\
0 & P_{2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & P_{k}
\end{array}\right]\right)
$$

따라서 문제는 $\sum P_{i}=P$라는 제약 조건 하에서 capacity를 최대화하는 power allotment를 찾는 것으로 축소됩니다. 이는 표준 최적화 문제이며 Lagrange 승수법을 사용하여 해결할 수 있습니다. 함수를 다음과 같이 작성하면

$$
J\left(P_{1}, \ldots, P_{k}\right)=\sum \frac{1}{2} \log \left(1+\frac{P_{i}}{N_{i}}\right)+\lambda\left(\sum P_{i}\right)
$$

이를 $P_{i}$에 대해 미분하면 다음과 같습니다.

$$
\frac{1}{2} \frac{1}{P_{i}+N_{i}}+\lambda=0
$$

또는

$$
P_{i}=v-N_{i}
$$

그러나 $P_{i}$는 음수가 될 수 없으므로 항상 이러한 형태의 해를 찾는 것이 가능하지 않을 수 있습니다. 이 경우 Kuhn-Tucker 조건을 사용하여 다음 해가 capacity를 최대화하는 할당임을 확인할 수 있습니다.

$$
P_{i}=\left(v-N_{i}\right)^{+}
$$

여기서 $v$는 다음을 만족하도록 선택됩니다.

$$
\sum\left(v-N_{i}\right)^{+}=P
$$

여기서 $(x)^{+}$는 $x$의 양수 부분을 나타냅니다.

$$
(x)^{+}=\left\{\begin{array}{ll}
x & \text { if } x \geq 0 \\
0 & \text { if } x<0
\end{array}\right.
$$

이 해는 그림 9.4에서 그래프로 설명됩니다. 수직 레벨은 다양한 channel의 noise 레벨을 나타냅니다. 신호 전력이 0에서 증가함에 따라 가장 낮은 noise를 가진 channel에 전력을 할당합니다.
<!-- Page 303 -->

그림 9.4. 병렬 채널에 대한 물 채우기.
잡음. 사용 가능한 전력이 더욱 증가하면 일부 전력이 더 잡음이 많은 채널에 투입됩니다. 다양한 빈에 전력이 분배되는 과정은 물이 용기에서 스스로를 분배하는 방식과 동일하므로, 이 과정을 때때로 물 채우기라고 합니다.

# 9.5 컬러 가우시안 잡음이 있는 채널

섹션 9.4에서는 서로 다른 채널의 잡음 샘플이 독립적인 병렬 독립 가우시안 채널의 경우를 고려했습니다. 이제 잡음이 종속적인 경우를 고려할 것입니다. 이는 병렬 채널의 경우뿐만 아니라 채널에 기억이 있는 가우시안 잡음이 있는 경우도 나타냅니다. 기억이 있는 채널의 경우, 채널의 연속적인 $n$번의 사용 블록을 종속 잡음이 있는 $n$개의 병렬 채널로 간주할 수 있습니다. 섹션 9.4와 마찬가지로 이 채널에 대한 정보 용량만 계산할 것입니다.

$K_{Z}$를 잡음의 공분산 행렬이라고 하고, $K_{X}$를 입력 공분산 행렬이라고 합시다. 그러면 입력에 대한 전력 제약은 다음과 같이 작성할 수 있습니다.

$$
\frac{1}{n} \sum_{i} E X_{i}^{2} \leq P
$$

또는 동등하게,

$$
\frac{1}{n} \operatorname{tr}\left(K_{X}\right) \leq P
$$
<!-- Page 304 -->
섹션 9.4와 달리 여기서의 전력 제약은 $n$에 의존합니다. 각 $n$에 대해 capacity를 계산해야 합니다.

독립 채널의 경우와 마찬가지로 다음과 같이 쓸 수 있습니다.

$$
\begin{aligned}
I\left(X_{1}, X_{2}, \ldots, X_{n} ; Y_{1}, Y_{2}, \ldots, Y_{n}\right)= & h\left(Y_{1}, Y_{2}, \ldots, Y_{n}\right) \\
& -h\left(Z_{1}, Z_{2}, \ldots, Z_{n}\right)
\end{aligned}
$$

여기서 $h\left(Z_{1}, Z_{2}, \ldots, Z_{n}\right)$는 잡음의 분포에 의해서만 결정되며 입력 분포의 선택에 의존하지 않습니다. 따라서 capacity를 찾는 것은 $h\left(Y_{1}, Y_{2}, \ldots, Y_{n}\right)$를 최대화하는 것과 같습니다. 출력의 엔트로피는 $Y$가 정규 분포일 때 최대화되며, 이는 입력이 정규 분포일 때 달성됩니다. 입력과 잡음이 독립적이므로 출력 $Y$의 공분산은 $K_{Y}=K_{X}+K_{Z}$이고 엔트로피는 다음과 같습니다.

$$
h\left(Y_{1}, Y_{2}, \ldots, Y_{n}\right)=\frac{1}{2} \log \left((2 \pi e)^{n}\left|K_{X}+K_{Z}\right|\right)
$$

이제 문제는 $K_{X}$를 선택하여 $\left|K_{X}+\right.$ $K_{Z} \mid$를 최대화하는 것으로 축소됩니다. 이는 $K_{X}$에 대한 trace 제약 조건을 따릅니다. 이를 위해 $K_{Z}$를 대각 형태로 분해합니다.

$$
K_{Z}=Q \Lambda Q^{t}, \quad \text { 여기서 } Q Q^{t}=I
$$

그러면

$$
\begin{aligned}
\left|K_{X}+K_{Z}\right| & =\left|K_{X}+Q \Lambda Q^{t}\right| \\
& =|Q|\left|Q^{t} K_{X} Q+\Lambda\right|\left|Q^{t}\right| \\
& =\left|Q^{t} K_{X} Q+\Lambda\right| \\
& =|A+\Lambda|
\end{aligned}
$$

여기서 $A=Q^{t} K_{X} Q$입니다. 임의의 행렬 $B$와 $C$에 대해

$$
\operatorname{tr}(B C)=\operatorname{tr}(C B)
$$

이므로

$$
\begin{aligned}
\operatorname{tr}(A) & =\operatorname{tr}\left(Q^{t} K_{X} Q\right) \\
& =\operatorname{tr}\left(Q Q^{t} K_{X}\right) \\
& =\operatorname{tr}\left(K_{X}\right)
\end{aligned}
$$
<!-- Page 305 -->
이제 문제는 $\operatorname{tr}(A) \leq n P$라는 트레이스 제약 조건 하에서 $|A+\Lambda|$를 최대화하는 것으로 축소됩니다.

이제 8장에서 언급된 아다마르 부등식을 적용합니다. 아다마르 부등식은 양의 정부호 행렬 $K$의 행렬식이 대각선 요소들의 곱보다 작거나 같다고 명시합니다. 즉,

$$
|K| \leq \prod_{i} K_{i i}
$$

등호는 행렬이 대각 행렬일 때 성립합니다. 따라서,

$$
|A+\Lambda| \leq \prod_{i}\left(A_{i i}+\lambda_{i}\right)
$$

등호는 $A$가 대각 행렬일 때 성립합니다. $A$는 트레이스 제약 조건에 따라,

$$
\frac{1}{n} \sum_{i} A_{i i} \leq P
$$

이고 $A_{i i} \geq 0$이므로, $\prod_{i}\left(A_{i i}+\lambda_{i}\right)$의 최대값은 다음을 만족할 때 달성됩니다.

$$
A_{i i}+\lambda_{i}=v
$$

그러나 주어진 제약 조건 하에서는 양수 $A_{i i}$로 이 방정식을 항상 만족시키는 것이 가능하지 않을 수 있습니다. 이러한 경우, 표준 쿤-터커 조건에 의해 최적 해는 다음을 설정하는 것에 해당함을 보일 수 있습니다.

$$
A_{i i}=\left(v-\lambda_{i}\right)^{+}
$$

여기서 물 채우기 레벨 $v$는 $\sum A_{i i}=n P$가 되도록 선택됩니다. 이 $A$ 값은 $Y$의 엔트로피를 최대화하고 따라서 상호 정보를 최대화합니다. 위에 설명된 방법과 물 채우기 사이의 연결을 보기 위해 그림 9.4를 사용할 수 있습니다.

가산 가우시안 잡음이 유한 차원 공분산 행렬 $K_{Z}^{(n)}$을 갖는 확률 과정인 채널을 고려합니다. 만약 과정이 정상이라면, 공분산 행렬은 토플리츠 행렬이며, 실수 선 상에서의 고유값의 밀도는 확률 과정의 파워 스펙트럼으로 수렴합니다 [262]. 이 경우, 위의 물 채우기 논증은 스펙트럼 영역에서의 물 채우기로 변환됩니다.

따라서, 잡음이 정상 확률 과정을 형성하는 채널의 경우, 입력 신호는 잡음 스펙트럼이 작은 주파수에서 스펙트럼이 큰 가우시안 과정으로 선택되어야 합니다.
<!-- Page 306 -->

그림 9.5. 스펙트럼 영역에서의 물 채우기.

이는 그림 9.5에 설명되어 있습니다. 잡음 전력 스펙트럼 $N(f)$를 갖는 가산 가우시안 잡음 채널의 용량은 다음과 같이 나타낼 수 있습니다 [233].

$$
C=\int_{-\pi}^{\pi} \frac{1}{2} \log \left(1+\frac{(\nu-N(f))^{+}}{N(f)}\right) d f
$$

여기서 $v$는 $\int(\nu-N(f))^{+} d f=P$가 되도록 선택됩니다.

# 9.6 피드백이 있는 가우시안 채널

7장에서는 피드백이 이산 메모리 없는 채널의 용량을 증가시키지 않지만, 인코딩 또는 디코딩의 복잡성을 크게 줄이는 데 도움이 될 수 있음을 증명했습니다. 백색 잡음이 있는 가산 잡음 채널에도 동일하게 적용됩니다. 이산적인 경우와 마찬가지로, 피드백은 메모리 없는 가우시안 채널의 용량을 증가시키지 않습니다.

그러나 메모리가 있는 채널, 즉 잡음이 시간 순간마다 상관되는 채널의 경우 피드백은 용량을 증가시킵니다. 피드백이 없는 용량은 물 채우기를 사용하여 계산할 수 있지만, 피드백이 있는 용량에 대한 간단한 명시적 특성화를 가지고 있지 않습니다. 이 섹션에서는 잡음 $Z$의 공분산 행렬에 대한 용량 표현을 설명합니다. 이 용량 표현에 대한 역정리를 증명합니다. 그런 다음 피드백으로 인한 용량 증가에 대한 간단한 경계를 도출합니다.

피드백이 있는 가우시안 채널은 그림 9.6에 설명되어 있습니다. 채널의 출력 $Y_{i}$는 다음과 같습니다.

$$
Y_{i}=X_{i}+Z_{i}, \quad Z_{i} \sim \mathcal{N}\left(0, K_{Z}^{(n)}\right)
$$
<!-- Page 307 -->

그림 9.6. 피드백이 있는 가우시안 채널.

피드백은 채널의 입력이 출력의 과거 값에 의존하도록 허용합니다.

피드백이 있는 가우시안 채널에 대한 $\left(2^{n R}, n\right)$ 코드는 매핑 시퀀스 $x_{i}\left(W, Y^{i-1}\right)$로 구성됩니다. 여기서 $W \in\left\{1,2, \ldots, 2^{n R}\right\}$는 입력 메시지이고 $Y^{i-1}$는 출력의 과거 값 시퀀스입니다. 따라서 $x(W, \cdot)$는 코드워드라기보다는 코드 함수입니다. 또한 코드가 전력 제약을 만족해야 합니다.

$$
E\left[\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}\left(w, Y^{i-1}\right)\right] \leq P, \quad w \in\left\{1,2, \ldots, 2^{n R}\right\}
$$

여기서 기댓값은 모든 가능한 노이즈 시퀀스에 대한 것입니다.
입력 $X$와 노이즈 $Z$의 공분산 행렬의 관점에서 가우시안 채널의 capacity를 특성화합니다. 피드백 때문에 $X^{n}$과 $Z^{n}$은 독립이 아닙니다. $X_{i}$는 $Z$의 과거 값에 인과적으로 의존합니다. 다음 섹션에서는 피드백이 있는 가우시안 채널에 대한 역정리를 증명하고 $X$가 가우시안이라고 가정하면 capacity를 달성함을 보여줍니다.

이제 피드백이 있는 채널과 없는 채널의 capacity에 대한 비공식적인 특성화를 명시합니다.

1. 피드백이 있는 경우. 피드백이 있는 시변 가우시안 채널의 전송당 비트 단위 capacity $C_{n, \mathrm{FB}}$는 다음과 같습니다.

$$
C_{n, \mathrm{FB}}=\max _{\frac{1}{n} w\left(K_{X}^{(n)}\right) \leq P} \frac{1}{2 n} \log \frac{\left|K_{X+Z}^{(n)}\right|}{\left|K_{Z}^{(n)}\right|}
$$
<!-- Page 308 -->
최대화는 다음과 같은 형태의 모든 $X^{n}$에 대해 수행됩니다.

$$
X_{i}=\sum_{j=1}^{i-1} b_{i j} Z_{j}+V_{i}, \quad i=1,2, \ldots, n
$$

그리고 $V^{n}$은 $Z^{n}$과 독립입니다. (9.101)에 대한 최대화가 일반성을 잃지 않음을 확인하기 위해, 최대 엔트로피를 달성하는 $X^{n}+Z^{n}$의 분포는 가우시안임을 주목하십시오. $Z^{n}$ 또한 가우시안이므로, ( $X^{n}, Z^{n}, X^{n}+Z^{n}$ )에 대한 결합 가우시안 분포가 (9.100)에서의 최대화를 달성함을 확인할 수 있습니다. 그러나 $Z^{n}=Y^{n}-X^{n}$이므로, $X^{n}$이 $Y^{n}$에 대해 갖는 가장 일반적인 결합 정규 인과적 의존성은 (9.101)의 형태이며, 여기서 $V^{n}$은 혁신 과정의 역할을 합니다. $X=B Z+V$ 및 $Y=X+Z$를 사용하여 (9.100)과 (9.101)을 재구성하면 다음과 같이 쓸 수 있습니다.

$$
C_{n, \mathrm{FB}}=\max \frac{1}{2 n} \log \frac{\left|(B+I) K_{Z}^{(n)}(B+I)^{t}+K_{V}\right|}{\left|K_{Z}^{(n)}\right|}
$$

여기서 최대화는 모든 비음 정방행렬 $K_{V}$와 엄격히 하삼각행렬 $B$에 대해 수행되며, 다음 조건을 만족합니다.

$$
\operatorname{tr}\left(B K_{Z}^{(n)} B^{t}+K_{V}\right) \leq n P
$$

피드백이 허용되지 않으면 $B$는 0입니다.
2. 피드백 없음. 피드백이 없는 시변 가우시안 채널의 용량 $C_{n}$은 다음과 같이 주어집니다.

$$
C_{n}=\max _{\frac{1}{n} \operatorname{tr}\left(K_{X}^{(n)}\right) \leq P} \frac{1}{2 n} \log \frac{\left|K_{X}^{(n)}+K_{Z}^{(n)}\right|}{\left|K_{Z}^{(n)}\right|}
$$

이는 $K_{Z}^{(n)}$의 고유값 $\left\{\lambda_{i}^{(n)}\right\}$에 대한 워터필링으로 축소됩니다. 따라서,

$$
C_{n}=\frac{1}{2 n} \sum_{i=1}^{n} \log \left(1+\frac{\left(\lambda-\lambda_{i}^{(n)}\right)^{+}}{\lambda_{i}^{(n)}}\right)
$$

여기서 $(y)^{+}=\max \{y, 0\}$이고, $\lambda$는 다음을 만족하도록 선택됩니다.

$$
\sum_{i=1}^{n}\left(\lambda-\lambda_{i}^{(n)}\right)^{+}=n P
$$
<!-- Page 309 -->
이제 피드백이 있는 가우시안 채널의 용량에 대한 상한을 증명합니다. 이 상한은 실제로 달성 가능하며 [136], 따라서 용량이지만 여기서는 이를 증명하지 않습니다.

정리 9.6.1 피드백이 있는 가우시안 채널의 경우, $P_{e}^{(n)} \rightarrow 0$인 $\left(2^{n R_{n}}, n\right)$ 코드의 모든 시퀀스에 대한 속도 $R_{n}$은 다음을 만족합니다.

$$
R_{n} \leq C_{n, F B}+\epsilon_{n}
$$

여기서 $\epsilon_{n} \rightarrow 0$은 $n \rightarrow \infty$일 때이며, $C_{n, F B}$는 (9.100)에 정의되어 있습니다.
증명: $W$가 $2^{n R}$에 대해 균등하다고 가정하면, 오류 확률 $P_{e}^{(n)}$은 파노의 부등식에 의해 제한됩니다.

$$
H(W \mid \hat{W}) \leq 1+n R_{n} P_{e}^{(n)}=n \epsilon_{n}
$$

여기서 $\epsilon_{n} \rightarrow 0$은 $P_{e}^{(n)} \rightarrow 0$일 때입니다. 그런 다음 속도를 다음과 같이 제한할 수 있습니다.

$$
\begin{aligned}
n R_{n} & =H(W) \\
& =I(W ; \hat{W})+H(W \mid \hat{W}) \\
& \leq I(W ; \hat{W})+n \epsilon_{n} \\
& \leq I\left(W ; Y^{n}\right)+n \epsilon_{n} \\
& =\sum I\left(W ; Y_{i} \mid Y^{i-1}\right)+n \epsilon_{n} \\
& \stackrel{(\text { a) }}{=} \sum\left(h\left(Y_{i} \mid Y^{i-1}\right)-h\left(Y_{i} \mid W, Y^{i-1}, X_{i}, X^{i-1}, Z^{i-1}\right)\right)+n \epsilon_{n} \\
& \stackrel{(\text { b) }}{=} \sum\left(h\left(Y_{i} \mid Y^{i-1}\right)-h\left(Z_{i} \mid W, Y^{i-1}, X_{i}, X^{i-1}, Z^{i-1}\right)\right)+n \epsilon_{n} \\
& \stackrel{(\text { (C) }}{=} \sum\left(h\left(Y_{i} \mid Y^{i-1}\right)-h\left(Z_{i} \mid Z^{i-1}\right)\right)+n \epsilon_{n} \\
& =h\left(Y^{n}\right)-h\left(Z^{n}\right)+n \epsilon_{n}
\end{aligned}
$$

여기서 (a)는 $X_{i}$가 $W$와 과거 $Y_{i}$의 함수이고, $Z^{i-1}$이 $Y^{i-1}-X^{i-1}$이라는 사실에서 비롯됩니다. (b)는 $Y_{i}=X_{i}+Z_{i}$와 $h(X+Z \mid X)=h(Z \mid X)$라는 사실에서 비롯됩니다. (c)는 $Z_{i}$와 $\left(W, Y^{i-1}, X^{i}\right)$가 $Z^{i-1}$이 주어졌을 때 조건부로 독립이라는 사실에서 비롯됩니다. 계속해서
<!-- Page 310 -->
$n$으로 나눈 후의 부등식 연쇄는 다음과 같습니다.

$$
\begin{aligned}
R_{n} & \leq \frac{1}{n}\left(h\left(Y^{n}\right)-h\left(Z^{n}\right)\right)+\epsilon_{n} \\
& \leq \frac{1}{2 n} \log \frac{\left|K_{Y}^{(n)}\right|}{\left|K_{Z}^{(n)}\right|}+\epsilon_{n} \\
& \leq C_{n, F B}+\epsilon_{n}
\end{aligned}
$$

정규 분포의 엔트로피 최대화 속성에 의해.
우리는 공분산 행렬 $K_{X+Z}^{(n)}$에 대한 가우시안 채널의 피드백 용량에 대한 상한을 증명했습니다. 이제 $K_{X}^{(n)}$ 및 $K_{Z}^{(n)}$에 대한 피드백 용량의 경계를 도출할 것이며, 이는 피드백 없는 용량에 대한 경계를 도출하는 데 사용될 것입니다. 표기의 단순성을 위해 공분산 행렬의 기호에서 위첨자 $n$을 생략하겠습니다.

먼저 행렬과 행렬식에 대한 일련의 보조정리를 증명합니다.
보조정리 9.6.1 $X$와 $Z$가 $n$차원 확률 벡터라고 가정합니다. 그러면

$$
K_{X+Z}+K_{X-Z}=2 K_{X}+2 K_{Z}
$$

# 증명

$$
\begin{aligned}
K_{X+Z} & =E(X+Z)(X+Z)^{t} \\
& =E X X^{t}+E X Z^{t}+E Z X^{t}+E Z Z^{t} \\
& =K_{X}+K_{X Z}+K_{Z X}+K_{Z}
\end{aligned}
$$

마찬가지로,

$$
K_{X-Z}=K_{X}-K_{X Z}-K_{Z X}+K_{Z}
$$

이 두 방정식을 더하면 증명이 완료됩니다.
보조정리 9.6.2 두 개의 $n \times n$ 비음수 정정 행렬 $A$와 $B$에 대해 $A-B$가 비음수 정정 행렬이면 $|A| \geq|B|$입니다.

증명: $C=A-B$라고 둡니다. $B$와 $C$가 비음수 정정 행렬이므로 공분산 행렬로 간주할 수 있습니다. 두 개의 독립적인 정규 확률 벡터 $\mathbf{X}_{1} \sim \mathcal{N}(0, B)$ 및 $\mathbf{X}_{2} \sim \mathcal{N}(0, C)$를 고려합니다. $\mathbf{Y}=\mathbf{X}_{1}+\mathbf{X}_{2}$라고 둡니다.
<!-- Page 311 -->
그러면

$$
\begin{aligned}
h(\mathbf{Y}) & \geq h\left(\mathbf{Y} \mid \mathbf{X}_{2}\right) \\
& =h\left(\mathbf{X}_{1} \mid \mathbf{X}_{2}\right) \\
& =h\left(\mathbf{X}_{1}\right)
\end{aligned}
$$

여기서 부등식은 조건 부여가 차분 엔트로피를 감소시킨다는 사실에서 비롯되며, 마지막 등식은 $\mathbf{X}_{1}$과 $\mathbf{X}_{2}$가 독립이라는 사실에서 비롯됩니다. 정규 확률 변수의 차분 엔트로피에 대한 표현식을 대입하면 다음과 같습니다.

$$
\frac{1}{2} \log (2 \pi e)^{n}|A|>\frac{1}{2} \log (2 \pi e)^{n}|B|
$$

이는 원하는 보조정리에 해당합니다.
보조정리 9.6.3 두 n차원 확률 벡터 $X$와 $Z$에 대해,

$$
\left|K_{X+Z}\right| \leq 2^{n}\left|K_{X}+K_{Z}\right|
$$

증명: 보조정리 9.6.1에 따르면,

$$
2\left(K_{X}+K_{Z}\right)-K_{X+Z}=K_{X-Z} \succeq 0
$$

여기서 $A \succeq 0$는 $A$가 비음정치 행렬임을 의미합니다. 따라서 보조정리 9.6.2를 적용하면 다음과 같습니다.

$$
\left|K_{X+Z}\right| \leq\left|2\left(K_{X}+K_{Z}\right)\right|=2^{n}\left|K_{X}+K_{Z}\right|
$$

이는 원하는 결과입니다.
보조정리 9.6.4 $A, B$가 비음정치 행렬이고 $0 \leq \lambda \leq 1$일 때,

$$
|\lambda A+(1-\lambda) B| \geq|A|^{\lambda}|B|^{1-\lambda}
$$

증명: $\mathbf{X} \sim \mathcal{N}_{n}(0, A)$이고 $\mathbf{Y} \sim \mathcal{N}_{n}(0, B)$라고 합시다. $\mathbf{Z}$를 혼합 확률 벡터라고 합시다.

$$
\mathbf{Z}= \begin{cases}\mathbf{X} & \text { if } \theta=1 \\ \mathbf{Y} & \text { if } \theta=2\end{cases}
$$
<!-- Page 312 -->
여기서

$$
\theta=\left\{\begin{array}{ll}
1 & \text { 확률 } \lambda \text{ 으로} \\
2 & \text { 확률 } 1-\lambda \text{ 으로}
\end{array}\right.
$$

$\mathbf{X}, \mathbf{Y}$, 그리고 $\theta$는 독립이라고 가정합니다. 그러면

$$
K_{Z}=\lambda A+(1-\lambda) B
$$

다음과 같이 관찰됩니다.

$$
\begin{aligned}
\frac{1}{2} \ln (2 \pi e)^{n}|\lambda A+(1-\lambda) B| & \geq h(\mathbf{Z}) \\
& \geq h(\mathbf{Z} \mid \theta) \\
& =\lambda h(\mathbf{X})+(1-\lambda) h(\mathbf{Y}) \\
& =\frac{1}{2} \ln (2 \pi e)^{n}|A|^{\lambda}|B|^{1-\lambda}
\end{aligned}
$$

이는 결과를 증명합니다. 첫 번째 부등식은 공분산 제약 하에서 가우시안의 엔트로피 최대화 속성에서 비롯됩니다.

정의: 랜덤 벡터 $X^{n}$이 $Z^{n}$과 인과적으로 관련되어 있다고 말합니다.

$$
f\left(x^{n}, z^{n}\right)=f\left(z^{n}\right) \prod_{i=1}^{n} f\left(x_{i} \mid x^{i-1}, z^{i-1}\right)
$$

피드백 코드는 필연적으로 인과적으로 관련 $\left(X^{n}, Z^{n}\right)$을 생성한다는 점에 유의하십시오.
정리 9.6.5: $X^{n}$과 $Z^{n}$이 인과적으로 관련되어 있다면,

$$
h\left(X^{n}-Z^{n}\right) \geq h\left(Z^{n}\right)
$$

그리고

$$
\left|K_{X-Z}\right| \geq\left|K_{Z}\right|
$$

여기서 $K_{X-Z}$와 $K_{Z}$는 각각 $X^{n}-Z^{n}$과 $Z^{n}$의 공분산 행렬입니다.

증명: 다음을 가집니다.

$$
h\left(X^{n}-Z^{n}\right) \stackrel{(\text { a) }}{=} \sum_{i=1}^{n} h\left(X_{i}-Z_{i} \mid X^{i-1}-Z^{i-1}\right)
$$
<!-- Page 313 -->
$$
\begin{aligned}
& \text { (b) } \sum_{i=1}^{n} h\left(X_{i}-Z_{i} \mid X^{i-1}, Z^{i-1}, X_{i}\right) \\
& \stackrel{(c)}{=} \sum_{i=1}^{n} h\left(Z_{i} \mid X^{i-1}, Z^{i-1}, X_{i}\right) \\
& \stackrel{(d)}{=} \sum_{i=1}^{n} h\left(Z_{i} \mid Z^{i-1}\right) \\
& \stackrel{(e)}{=} h\left(Z^{n}\right)
\end{aligned}
$$

여기서 (a)는 연쇄 법칙으로부터, (b)는 조건부 엔트로피 $h(A \mid B) \geq h(A \mid B, C)$를 적용하여, (c)는 $X_{i}$의 조건부 결정성과 변환에 대한 미분 엔트로피의 불변성을 이용하여, (d)는 $X^{n}$과 $Z^{n}$의 인과 관계를 이용하여, (e)는 연쇄 법칙으로부터 도출됩니다.

마지막으로, $X^{n}$과 $Z^{n}$이 인과적으로 관련되어 있고 $Z^{n}$ 및 $X^{n}-Z^{n}$에 대한 공분산 행렬이 각각 $K_{Z}$와 $K_{X-Z}$라고 가정합니다. 동일한 공분산 구조를 갖는 다변수 정규 (인과적으로 관련된) 확률 벡터 쌍 $\tilde{X}^{n}, \tilde{Z}^{n}$이 존재합니다. 따라서 (9.148)로부터 다음을 얻습니다.

$$
\begin{aligned}
\frac{1}{2} \ln (2 \pi e)^{n}\left|K_{X-Z}\right| & =h\left(\tilde{X}^{n}-\tilde{Z}^{n}\right) \\
& \geq h\left(\tilde{Z}^{n}\right) \\
& =\frac{1}{2} \ln (2 \pi e)^{n}\left|K_{Z}\right|
\end{aligned}
$$

이를 통해 (9.143)이 증명됩니다.
이제 우리는 피드백이 비백색 가우시안 가산 잡음 채널의 capacity를 최대 0.5 비트 증가시킨다는 것을 증명할 수 있습니다.

# 정리 9.6.2

$$
C_{n, \mathrm{FB}} \leq C_{n}+\frac{1}{2} \quad \text { bits per transmission. }
$$

증명: 모든 보조 정리를 결합하면 다음을 얻습니다.

$$
C_{n, \mathrm{FB}} \leq \max _{\operatorname{tr}\left(K_{X}\right) \leq n P} \frac{1}{2 n} \log \frac{\left|K_{\mathrm{F}}\right|}{\left|K_{Z}\right|}
$$
<!-- Page 314 -->
$$
\begin{aligned}
& \leq \max _{\operatorname{tr}\left(K_{X}\right) \leq n P} \frac{1}{2 n} \log \frac{2^{n}\left|K_{X}+K_{Z}\right|}{\left|K_{Z}\right|} \\
& =\max _{\operatorname{tr}\left(K_{X}\right) \leq n P} \frac{1}{2 n} \log \frac{\left|K_{X}+K_{Z}\right|}{\left|K_{Z}\right|}+\frac{1}{2} \\
& \leq C_{n}+\frac{1}{2} \quad \text { bits per transmission, }
\end{aligned}
$$

여기서 부등식은 각각 Theorem 9.6.1, Lemma 9.6.3, 그리고 피드백 없는 capacity의 정의로부터 도출됩니다.

이제 피드백이 컬러 노이즈 채널의 capacity를 최대 두 배까지 증가시킬 수 있다는 Pinsker의 명제를 증명합니다.

Theorem 9.6.3 $C_{n, \mathrm{FB}} \leq 2 C_{n}$.
Proof: 다음을 보이는 것으로 충분합니다.

$$
\frac{1}{2} \frac{1}{2 n} \log \frac{\left|K_{X+Z}\right|}{\left|K_{Z}\right|} \leq \frac{1}{2 n} \log \frac{\left|K_{X}+K_{Z}\right|}{\left|K_{Z}\right|}
$$

그러면 오른쪽 항을 최대화한 후 왼쪽 항을 최대화함으로써 다음이 성립할 것입니다.

$$
\frac{1}{2} C_{n, \mathrm{FB}} \leq C_{n}
$$

다음과 같이 계산할 수 있습니다.

$$
\begin{aligned}
\frac{1}{2 n} \log \frac{\left|K_{X}+K_{Z}\right|}{\left|K_{Z}\right|} & \stackrel{(a)}{=} \frac{1}{2 n} \log \frac{\left|\frac{1}{2} K_{X+Z}+\frac{1}{2} K_{X-Z}\right|}{\left|K_{Z}\right|} \\
& \stackrel{(b)}{\geq} \frac{1}{2 n} \log \frac{\left|K_{X+Z}\right|^{\frac{1}{2}}\left|K_{X-Z}\right|^{\frac{1}{2}}}{\left|K_{Z}\right|} \\
& \stackrel{(c)}{\geq} \frac{1}{2 n} \log \frac{\left|K_{X+Z}\right|^{\frac{1}{2}}\left|K_{Z}\right|^{\frac{1}{2}}}{\left|K_{Z}\right|} \\
& \stackrel{(d)}{=} \frac{1}{2} \frac{1}{2 n} \log \frac{\left|K_{X+Z}\right|}{\left|K_{Z}\right|}
\end{aligned}
$$

결과가 증명되었습니다. 여기서 (a)는 Lemma 9.6.1에서, (b)는 Lemma 9.6.4의 부등식에서, (c)는 인과관계가 사용된 Lemma 9.6.5에서 도출됩니다.
<!-- Page 315 -->
따라서, 피드백이 있을 때 가우시안 채널 용량은 절반 비트 이상 또는 2배 이상 증가하지 않음을 보여주었습니다. 피드백은 도움이 되지만, 큰 도움이 되지는 않습니다.

# 요약

최대 엔트로피. $\max _{E X^{2}=\alpha} h(X)=\frac{1}{2} \log 2 \pi e \alpha$.
가우시안 채널. $\quad Y_{i}=X_{i}+Z_{i} ; Z_{i} \sim \mathcal{N}(0, N)$; 전력 제약 $\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2} \leq P ;$ 그리고

$$
C=\frac{1}{2} \log \left(1+\frac{P}{N}\right) \quad \text { 전송당 비트. }
$$

대역 제한 백색 가우시안 잡음 채널. 대역폭 $W$; 양방향 전력 스펙트럼 밀도 $N_{0} / 2$; 신호 전력 $P$; 그리고

$$
C=W \log \left(1+\frac{P}{N_{0} W}\right) \quad \text { 초당 비트. }
$$

워터 필링 (k개의 병렬 가우시안 채널). $Y_{j}=X_{j}+Z_{j}, j=1$, $2, \ldots, k ; Z_{j} \sim \mathcal{N}\left(0, N_{j}\right) ; \sum_{j=1}^{k} X_{j}^{2} \leq P ;$ 그리고

$$
C=\sum_{i=1}^{k} \frac{1}{2} \log \left(1+\frac{\left(v-N_{i}\right)^{+}}{N_{i}}\right)
$$

여기서 $v$는 $\sum\left(v-N_{i}\right)^{+}=n P$를 만족하도록 선택됩니다.
가우시안 잡음 채널. $Y_{i}=X_{i}+Z_{i} ; Z^{n} \sim$ $\mathcal{N}\left(0, K_{Z}\right)$; 그리고

$$
C=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} \log \left(1+\frac{\left(v-\lambda_{i}\right)^{+}}{\lambda_{i}}\right)
$$

여기서 $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$는 $K_{Z}$의 고유값이고 $v$는 $\sum_{i}\left(v-\lambda_{i}\right)^{+}=P$를 만족하도록 선택됩니다.

## 피드백 없는 용량

$$
C_{n}=\max _{\operatorname{tr}\left(K_{X}\right) \leq n P} \frac{1}{2 n} \log \frac{\left|K_{X}+K_{Z}\right|}{\left|K_{Z}\right|}
$$
<!-- Page 316 -->
# 피드백을 이용한 용량

$$
C_{n, \mathrm{FB}}=\max _{\operatorname{tr}\left(K_{X}\right) \leq n P} \frac{1}{2 n} \log \frac{\left|K_{X+Z}\right|}{\left|K_{Z}\right|}
$$

## 피드백 상한

$$
\begin{gathered}
C_{n, \mathrm{FB}} \leq C_{n}+\frac{1}{2} \\
C_{n, \mathrm{FB}} \leq 2 C_{n}
\end{gathered}
$$

## 연습문제

9.1 $Y$를 두 번 독립적으로 관찰하는 채널. $Y_{1}$과 $Y_{2}$는 $X$가 주어졌을 때 조건부로 독립이고 조건부로 동일하게 분포한다고 가정합니다.
(a) $I\left(X ; Y_{1}, Y_{2}\right)=2 I\left(X ; Y_{1}\right)-I\left(Y_{1} ; Y_{2}\right)$임을 보이십시오.
(b) 다음 채널의 용량이

다음 채널 용량의 두 배보다 작음을 결론 내리십시오.

9.2 두 번 관찰하는 가우시안 채널

$X$에 대해 두 개의 상관된 관찰을 갖는 일반적인 가우시안 채널을 고려합니다. 즉, $Y=\left(Y_{1}, Y_{2}\right)$이며, 여기서

$$
\begin{aligned}
& Y_{1}=X+Z_{1} \\
& Y_{2}=X+Z_{2}
\end{aligned}
$$
<!-- Page 317 -->
$P$에 대한 전력 제약 조건이 있고, $\left(Z_{1}, Z_{2}\right) \sim \mathcal{N}_{2}(0, K)$이며,

$$
K=\left[\begin{array}{ll}
N & N \rho \\
N \rho & N
\end{array}\right]
$$

일 때, 다음의 용량 $C$를 구하십시오.
(a) $\rho=1$
(b) $\rho=0$
(c) $\rho=-1$
9.3 출력 전력 제약 조건. 기대 출력 전력 제약 조건 $P$를 갖는 가산 백색 가우시안 잡음 채널을 고려하십시오. 즉, $Y=X+Z, Z \sim N\left(0, \sigma^{2}\right), Z$는 $X$와 독립이며, $E Y^{2} \leq P$입니다. 채널 용량을 구하십시오.
9.4 지수 잡음 채널. $Y_{i}=X_{i}+Z_{i}$이며, 여기서 $Z_{i}$는 i.i.d.이고 지수 분포를 따르는 잡음이며 평균은 $\mu$입니다. 신호에 대한 평균 제약 조건($E X_{i} \leq \lambda$)이 있다고 가정하십시오. 이러한 채널의 용량이 $C=\log \left(1+\frac{\lambda}{\mu}\right)$임을 보이십시오.
9.5 페이딩 채널. 가산 잡음 페이딩 채널을 고려하십시오.

$$
Y=X V+Z
$$

여기서 $Z$는 가산 잡음이고, $V$는 페이딩을 나타내는 랜덤 변수이며, $Z$와 $V$는 서로 독립이고 $X$와도 독립입니다. 페이딩 계수 $V$를 알면 용량이 향상됨을 다음을 보여 증명하십시오.

$$
I(X ; Y \mid V) \geq I(X ; Y)
$$

9.6 병렬 채널 및 워터 필링. 두 개의 병렬 가우시안 채널을 고려하십시오.

$$
\binom{Y_{1}}{Y_{2}}=\binom{X_{1}}{X_{2}}+\binom{Z_{1}}{Z_{2}}
$$
<!-- Page 318 -->
$$
\binom{Z_{1}}{Z_{2}} \sim \mathcal{N}\left(0,\left[\begin{array}{cc}
\sigma_{1}^{2} & 0 \\
0 & \sigma_{2}^{2}
\end{array}\right]\right)
$$

이고, 전력 제약 조건 $E\left(X_{1}^{2}+X_{2}^{2}\right) \leq 2 P$이 있다고 가정합니다. 또한 $\sigma_{1}^{2}>\sigma_{2}^{2}$이라고 가정합니다. 어느 전력에서 채널이 잡음 분산 $\sigma_{2}^{2}$을 갖는 단일 채널처럼 동작하는 것을 멈추고, 두 개의 채널 쌍처럼 동작하기 시작합니까?
9.7 다중 경로 가우시안 채널. 전력 제약 조건 $P$를 갖는 가우시안 잡음 채널을 고려합니다. 이 채널에서는 신호가 두 개의 다른 경로를 통과하고 수신된 잡음 신호가 안테나에서 합쳐집니다.

(a) $Z_{1}$과 $Z_{2}$가 공분산 행렬을 갖는 결합 정규 분포를 따를 때 이 채널의 capacity를 구하십시오.

$$
K_{\mathrm{Z}}=\left[\begin{array}{ll}
\sigma^{2} & \rho \sigma^{2} \\
\rho \sigma^{2} & \sigma^{2}
\end{array}\right]
$$

(b) $\rho=0, \rho=1, \rho=-1$일 때의 capacity는 무엇입니까?
9.8 병렬 가우시안 채널. 다음 병렬 가우시안 채널을 고려합니다.

<!-- Page 319 -->
여기서 $Z_{1} \sim \mathcal{N}\left(0, N_{1}\right)$이고 $Z_{2} \sim \mathcal{N}\left(0, N_{2}\right)$는 독립적인 가우시안 랜덤 변수이며 $Y_{i}=X_{i}+Z_{i}$입니다. 두 개의 병렬 채널에 전력을 할당하고자 합니다. $\beta_{1}$과 $\beta_{2}$를 고정한다고 가정합니다. 총 비용 제약 조건 $\beta_{1} P_{1}+\beta_{2} P_{2} \leq \beta$를 고려합니다. 여기서 $P_{i}$는 $i$번째 채널에 할당된 전력이고 $\beta_{i}$는 해당 채널의 단위 전력당 비용입니다. 따라서 $P_{1} \geq 0$ 및 $P_{2} \geq 0$은 비용 제약 조건 $\beta$에 따라 선택될 수 있습니다.
(a) 어떤 $\beta$ 값에서 채널이 단일 채널처럼 작동하는 것을 멈추고 두 개의 채널 쌍처럼 작동하기 시작합니까?
(b) $\beta_{1}=1, \beta_{2}=2, N_{1}=3, N_{2}=2$, 그리고 $\beta=10$일 때 용량을 계산하고 용량을 달성하는 $P_{1}$과 $P_{2}$를 찾으십시오.
9.9 벡터 가우시안 채널. 벡터 가우시안 노이즈 채널을 고려합니다.

$$
Y=X+Z
$$

여기서 $X=\left(X_{1}, X_{2}, X_{3}\right), \quad Z=\left(Z_{1}, Z_{2}, Z_{3}\right), Y=\left(Y_{1}, Y_{2}, Y_{3}\right)$, $E\|X\|^{2} \leq P$, 그리고

$$
Z \sim \mathcal{N}\left(0,\left[\begin{array}{lll}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 2
\end{array}\right]\right)
$$

용량을 찾으십시오. 답은 놀라울 수 있습니다.
9.10 사진 필름의 용량. 흥미로운 답이 나오는 문제입니다. 사진 필름의 용량에 관심이 있습니다. 필름은 은 요오드 결정으로 구성되어 있으며, 밀도는 $\lambda$ 입자/제곱인치로 포아송 분포를 따릅니다. 필름은 은 요오드 입자의 위치를 알지 못한 채 조명됩니다. 그런 다음 현상되며 수신기는 조명된 은 요오드 입자만 봅니다. 셀에 입사된 빛은 은 요오드 입자가 있으면 노출시키고 그렇지 않으면 빈 응답을 생성한다고 가정합니다. 조명되지 않은 은 요오드 입자와 필름의 빈 부분은 그대로 빈 상태로 유지됩니다. 질문은 다음과 같습니다. 이 필름의 용량은 얼마입니까?
다음과 같은 가정을 합니다. 필름을 매우 미세하게 $d A$ 면적의 셀로 나눕니다. 셀당 최대 하나의 은 요오드 입자가 있고, 어떤 은 요오드 입자도 셀 경계와 교차하지 않는다고 가정합니다. 따라서 필름은 교차 확률이 $1-\lambda d A$인 다수의 병렬 이진 비대칭 채널로 간주될 수 있습니다. 이 이진 비대칭 채널의 용량을 $d A$에 대한 첫 번째 차수까지 계산하면 (이것은

<!-- Page 320 -->
필요한 근사치를 사용하면 제곱인치당 비트 단위로 필름의 용량을 계산할 수 있습니다. 물론 이는 $\lambda$에 비례합니다. 질문은 다음과 같습니다. 곱셈 상수는 무엇입니까?
조명기와 수신기 모두 결정의 위치를 알고 있다면 답은 단위 면적당 $\lambda$ 비트가 될 것입니다.
9.11 가우시안 상호 정보. $(X, Y, Z)$가 공동 가우시안이고 $X \rightarrow Y \rightarrow Z$가 마르코프 연쇄를 형성한다고 가정합니다. $X$와 $Y$의 상관 계수가 $\rho_{1}$이고 $Y$와 $Z$의 상관 계수가 $\rho_{2}$라고 가정합니다. $I(X ; Z)$를 찾으십시오.
9.12 시간에 따라 변하는 채널. 기차가 일정한 속도로 역을 출발합니다. 따라서 수신 신호 에너지는 시간 $i$에 따라 $1 / i^{2}$로 감소합니다. 시간 $i$에서의 총 수신 신호는 다음과 같습니다.

$$
Y_{i}=\frac{1}{i} X_{i}+Z_{i}
$$

여기서 $Z_{1}, Z_{2}, \ldots$는 i.i.d. $\sim N(0, N)$입니다. 블록 길이 $n$에 대한 송신기 제약 조건은 다음과 같습니다.

$$
\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}(w) \leq P, \quad w \in\left\{1,2, \ldots, 2^{n R}\right\}
$$

파노의 부등식을 사용하여 이 채널의 용량 $C$가 0과 같음을 보이십시오.
9.13 피드백 용량. $\left(Z_{1}, Z_{2}\right) \sim N(0, K), K=\left[\begin{array}{ll}1 & \rho \\ \rho & 1\end{array}\right]$라고 가정합니다. 트레이스(전력) 제약 조건 $\operatorname{tr}\left(K_{X}\right) \leq 2 P$가 주어졌을 때 피드백 유무에 따른 $\frac{1}{2} \log \frac{\left|K_{X+Z}\right|}{\left|K_{Z}\right|}$의 최댓값을 찾으십시오.
9.14 가산 잡음 채널. 채널 $Y=X+Z$를 고려하십시오. 여기서 $X$는 전력 제약 조건 $P$를 갖는 송신 신호이고, $Z$는 독립적인 가산 잡음이며, $Y$는 수신 신호입니다.

$$
Z= \begin{cases}0 & \text { 확률 } \frac{1}{10} \text{으로} \\ Z^{*} & \text { 확률 } \frac{9}{10} \text{으로}\end{cases}
$$

여기서 $Z^{*} \sim N(0, N)$입니다. 따라서 $Z$는 가우시안 분포와 질량 1이 0에 있는 퇴화 분포의 혼합인 혼합 분포를 갖습니다.
<!-- Page 321 -->
(a) 이 채널의 용량은 얼마입니까? 이는 즐거운 놀라움이 될 것입니다.
(b) 용량을 달성하기 위해 어떻게 신호화하시겠습니까?
9.15 이산 입력, 연속 출력 채널. $\operatorname{Pr}\{X=1\}=p$, $\operatorname{Pr}\{X=0\}=1-p$이고 $Y=X+Z$이며, 여기서 $Z$는 구간 $[0, a], a>1$에서 균등 분포하고 $Z$는 $X$와 독립이라고 가정합니다.
(a) 다음을 계산하십시오.

$$
I(X ; Y)=H(X)-H(X \mid Y)
$$

(b) 이제 다음을 사용하여 다른 방식으로 $I(X ; Y)$를 계산하십시오.

$$
I(X ; Y)=h(Y)-h(Y \mid X)
$$

(c) $p$에 대해 최대화하여 이 채널의 용량을 계산하십시오.
9.16 가우시안 상호 정보. $(X, Y, Z)$가 공동 가우시안이고 $X \rightarrow Y \rightarrow Z$가 마르코프 연쇄를 형성한다고 가정합니다. $X$와 $Y$의 상관 계수가 $\rho_{1}$이고 $Y$와 $Z$의 상관 계수가 $\rho_{2}$라고 가정합니다. $I(X ; Z)$를 구하십시오.
9.17 임펄스 전력. 다음의 가산 백색 가우시안 채널을 고려하십시오.

여기서 $Z_{i} \sim N(0, N)$이고 입력 신호는 평균 전력 제약 $P$를 가집니다.
(a) 모든 전력을 시간 1에 사용한다고 가정합니다 (즉, $E X_{1}^{2}=n P$이고 $i=2,3, \ldots, n$에 대해 $E X_{i}^{2}=0$입니다). 다음을 구하십시오.

$$
\max _{f\left(x^{n}\right)} \frac{I\left(X^{n} ; Y^{n}\right)}{n}
$$

여기서 최대화는 $E X_{1}^{2}=n P$ 및 $i=2,3, \ldots, n$에 대해 $E X_{i}^{2}=0$ 제약 조건 하에서 모든 분포 $f\left(x^{n}\right)$에 대해 수행됩니다.
<!-- Page 322 -->
(b) 다음을 찾으십시오.

$$
\max _{f\left(x^{n}\right): E\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}\right) \leq P} \frac{1}{n} I\left(X^{n} ; Y^{n}\right)
$$

그리고 (a)와 비교하십시오.
9.18 시간 가변 평균을 갖는 가우시안 채널. 다음 가우시안 채널의 용량을 찾으십시오.

$Z_{1}, Z_{2}, \ldots$는 독립이고 $x^{n}(W)$에 대한 전력 제약 $P$가 있다고 가정합니다. 다음의 용량을 찾으십시오.
(a) 모든 $i$에 대해 $\mu_{i}=0$.
(b) $\mu_{i}=e^{i}, \quad i=1,2, \ldots$. $\mu_{i}$는 송신기와 수신기가 알고 있다고 가정합니다.
(c) $\mu_{i}$는 알려지지 않았지만, 모든 $i$에 대해 $\mu_{i}$는 i.i.d. $\sim N\left(0, N_{1}\right)$입니다.
9.19 채널 용량에 대한 매개변수 형식. $m$개의 병렬 가우시안 채널 $Y_{i}=X_{i}+Z_{i}$를 고려하십시오. 여기서 $Z_{i} \sim N\left(0, \lambda_{i}\right)$이고 잡음 $X_{i}$는 독립 확률 변수입니다. 따라서 $C=\sum_{i=1}^{m} \frac{1}{2} \log (1+$ $\left.\frac{\left(\lambda-\lambda_{i}\right)^{+}}{\lambda_{i}}\right)$이며, 여기서 $\lambda$는 $\sum_{i=1}^{m}\left(\lambda-\lambda_{i}\right)^{+}=P$를 만족하도록 선택됩니다. 이를 다음 형식으로 다시 작성할 수 있음을 보이십시오.

$$
\begin{aligned}
& P(\lambda)=\sum_{i: \lambda_{i} \leq \lambda}\left(\lambda-\lambda_{i}\right) \\
& C(\lambda)=\sum_{i: \lambda_{i} \leq \lambda} \frac{1}{2} \log \frac{\lambda}{\lambda_{i}}
\end{aligned}
$$

여기서 $P(\lambda)$는 조각별 선형이고 $C(\lambda)$는 $\lambda$에 대해 조각별 로그입니다.
9.20 강건한 디코딩. 출력 $Y$가 다음으로 주어지는 가산 잡음 채널을 고려하십시오.

$$
Y=X+Z
$$

여기서 채널 입력 $X$는 평균 전력 제한이 있습니다.

$$
E X^{2} \leq P
$$
<!-- Page 323 -->
그리고 잡음 과정 $\left\{Z_{k}\right\}_{k=-\infty}^{\infty}$는 평균 제곱값 $N$을 갖는 $p_{Z}(z)$ (반드시 가우시안일 필요는 없음)의 주변 분포를 갖는 i.i.d.입니다.

$$
E Z^{2}=N
$$

(a) 채널 용량 $C=\max _{E X^{2} \leq P} I(X ; Y)$가 $C_{G}$에 의해 하한선이 정해짐을 보이십시오. 여기서

$$
C_{G}=\frac{1}{2} \log \left(1+\frac{P}{N}\right)
$$

(즉, 백색 가우시안 잡음에 해당하는 용량 $C_{G}$)
(b) 잡음이 비가우시안인 경우, 수신 벡터를 유클리드 거리상 가장 가까운 코드워드로 복호화하는 것은 일반적으로 최적이 아닙니다. 그러나 최적의 최대 우도 또는 결합 전형성 복호화 (실제 잡음 분포에 대한) 대신 근린 이웃 복호화 (최소 유클리드 거리 복호화)를 고수하더라도 비율 $C_{G}$를 달성할 수 있음을 보이십시오.
(c) 잡음이 i.i.d.가 아니라 평균 제곱값 $N$을 갖는 정상적이고 에르고딕인 경우로 결과를 확장하십시오.
(b) 및 (c)에 대한 힌트: 반지름 $\sqrt{n P}$의 $n$차원 구체에 대해 독립적으로 균일 분포에 따라 그려진 코드워드를 갖는 크기 $2^{n R}$의 랜덤 코드북을 고려하십시오.
(a) 대칭성을 이용하여, 잡음 벡터가 주어졌을 때, 오류 확률의 앙상블 평균이 잡음 벡터의 유클리드 노름 $\|\mathbf{z}\|$에만 의존함을 보이십시오.
(b) 기하학적 논증을 사용하여 이 의존성이 단조적임을 보이십시오.
(c) 비율 $R<C_{G}$가 주어졌을 때, $N^{\prime}>N$을 선택하여

$$
R<\frac{1}{2} \log \left(1+\frac{P}{N^{\prime}}\right)
$$

잡음이 i.i.d. $\mathcal{N}\left(0, N^{\prime}\right)$인 경우와 현재 경우를 비교하십시오.
(d) 위의 코드북 앙상블이 가우시안 채널의 용량을 달성할 수 있다는 사실을 사용하여 증명을 마무리하십시오 (그것을 증명할 필요는 없습니다).
<!-- Page 324 -->
9.21 상호 정보량 게임. 다음 채널을 고려하십시오.

이 문제 전체에서 신호 전력

$$
E X=0, \quad E X^{2}=P
$$

및 잡음 전력

$$
E Z=0, \quad E Z^{2}=N
$$

으로 제약하고 $X$와 $Z$가 독립이라고 가정합니다. 채널 용량은 $I(X ; X+Z)$로 주어집니다.
이제 게임에 대해 설명합니다. 잡음 플레이어는 $I(X ; X+Z)$를 최소화하기 위해 $Z$에 대한 분포를 선택하고, 신호 플레이어는 $I(X ; X+Z)$를 최대화하기 위해 $X$에 대한 분포를 선택합니다. $X^{*} \sim \mathcal{N}(0, P), Z^{*} \sim$ $\mathcal{N}(0, N)$이라고 할 때, 가우시안 $X^{*}$와 $Z^{*}$가 안장점 조건을 만족함을 보이십시오.

$$
I\left(X ; X+Z^{*}\right) \leq I\left(X^{*} ; X^{*}+Z^{*}\right) \leq I\left(X^{*} ; X^{*}+Z\right)
$$

따라서,

$$
\begin{aligned}
\min _{Z} \max _{X} I(X ; X+Z) & =\max _{X} \min _{Z} I(X ; X+Z) \\
& =\frac{1}{2} \log \left(1+\frac{P}{N}\right)
\end{aligned}
$$

이며 게임에는 값이 존재합니다. 특히, 어느 플레이어든 정규 분포에서 벗어나면 해당 플레이어의 관점에서 상호 정보량이 악화됩니다. 이에 대한 함의를 논의할 수 있습니까?
참고: 증명의 일부는 섹션 17.8의 엔트로피 전력 부등식에 의존합니다. 이 부등식은 $\mathbf{X}$와 $\mathbf{Y}$가 독립적인 확률 벡터이고 밀도를 가지면 다음과 같다고 말합니다.

$$
2^{\frac{2}{n} h(\mathbf{X}+\mathbf{Y})} \geq 2^{\frac{2}{n} h(\mathbf{X})}+2^{\frac{2}{n} h(\mathbf{Y})}
$$
<!-- Page 325 -->
9.22 노이즈 복구. 표준 가우시안 채널 $Y^{n}=X^{n}+Z^{n}$을 고려합니다. 여기서 $Z_{i}$는 i.i.d. $\sim \mathcal{N}(0, N), i=1,2, \ldots, n$이고, $\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} \leq P$입니다. 여기서 우리는 노이즈 $Z^{n}$을 복구하는 데 관심이 있으며 신호 $X^{n}$에는 신경 쓰지 않습니다. $X^{n}=(0,0, \ldots, 0 )$을 전송함으로써 수신기는 $Y^{n}=Z^{n}$을 얻고 $Z^{n}$의 값을 완전히 결정할 수 있습니다. 우리는 $X^{n}$의 변동성이 어느 정도까지 허용될 수 있으며 여전히 가우시안 노이즈 $Z^{n}$을 복구할 수 있는지 궁금합니다. 채널 사용은 다음과 같습니다.

$R>0$에 대해 송신기가 노이즈 복구에 영향을 주지 않는 방식으로 $2^{n R}$개의 서로 다른 $x^{n}$ 시퀀스 중 하나를 임의로 전송할 수 있다고 주장하십시오.

$$
\operatorname{Pr}\left\{\hat{Z}^{n} \neq Z^{n}\right\} \rightarrow 0 \quad \text { as } n \rightarrow \infty
$$

어떤 $R$에 대해 이것이 가능합니까?

# 역사적 참고 자료

가우시안 채널은 Shannon이 그의 원래 논문 [472]에서 처음 분석했습니다. 컬러 노이즈 가우시안 채널의 용량에 대한 워터-필링(water-filling) 해법은 Shannon [480]에 의해 개발되었고 Pinsker [425]에 의해 상세히 다루어졌습니다. 시간 연속 가우시안 채널은 Wyner [576], Gallager [233], 그리고 Landau, Pollak, Slepian [340, 341, 500]에 의해 다루어졌습니다.

Pinsker [421]와 Ebert [178]는 피드백이 비백색 가우시안 채널의 용량을 최대 두 배로 늘릴 수 있다고 주장했습니다. 본문의 증명은 Cover와 Pombra [136]의 것으로, 그들은 또한 피드백이 비백색 가우시안 채널의 용량을 최대 절반 비트까지 증가시킨다는 것을 보여줍니다. 비백색 가우시안 노이즈 채널에 대한 가장 최근의 피드백 용량 결과는 Kim [314]에 의한 것입니다.
<!-- Page 326 -->
.
<!-- Page 327 -->
# 정보율 왜곡 이론

임의의 실수(real number)를 기술하기 위해서는 무한한 수의 비트가 필요하므로, 유한한 표현으로 연속 확률 변수를 완벽하게 나타내는 것은 불가능합니다. 얼마나 잘 할 수 있을까요? 질문을 적절하게 구성하기 위해서는 소스 표현의 "좋음"을 정의하는 것이 필요합니다. 이는 왜곡 측정값(distortion measure)을 정의함으로써 달성되는데, 이는 확률 변수와 그 표현 간의 거리 측정값입니다. 정보율 왜곡 이론의 기본적인 문제는 다음과 같이 기술될 수 있습니다: 주어진 소스 분포와 왜곡 측정값에 대해, 특정 정보율(rate)에서 달성 가능한 최소 기대 왜곡(expected distortion)은 얼마인가? 또는 동등하게, 특정 왜곡을 달성하기 위해 필요한 최소 정보율 기술은 얼마인가?

이 이론의 가장 흥미로운 측면 중 하나는 개별적인 기술보다 결합된 기술이 더 효율적이라는 것입니다. 코끼리와 닭을 각각 따로 기술하는 것보다 하나의 기술로 함께 기술하는 것이 더 간단합니다. 이는 독립적인 확률 변수의 경우에도 마찬가지입니다. 각 변수에 대해 주어진 왜곡으로 $X_{1}$과 $X_{2}$를 함께 기술하는 것이 각 변수를 개별적으로 기술하는 것보다 더 간단합니다. 왜 독립적인 문제들이 독립적인 해답을 갖지 못하는 것일까요? 그 답은 기하학에서 찾을 수 있습니다. 명백히, 직사각형 격자점(독립적인 기술에서 발생하는)은 공간을 효율적으로 채우지 못합니다.

정보율 왜곡 이론은 이산 확률 변수와 연속 확률 변수 모두에 적용될 수 있습니다. 제5장의 무오류 데이터 압축 이론은 0의 왜곡을 갖는 이산 소스에 적용된 정보율 왜곡 이론의 중요한 특수한 경우입니다. 우리는 유한한 수의 비트로 단일 연속 확률 변수를 표현하는 간단한 문제부터 고려하기 시작합니다.

### 10.1 양자화

이 절에서는 단일 연속 확률 변수에 대한 양자화 문제를 정확하게 해결하는 것이 얼마나 복잡한지를 보여줌으로써 정보율 왜곡의 우아한 이론을 동기 부여합니다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 328 -->
random variable. 연속적인 랜덤 소스는 정확하게 표현하기 위해 무한한 정밀도를 요구하므로, 유한한 속도의 코드를 사용하여 정확하게 재현할 수 없습니다. 따라서 주어진 데이터 속도에 대해 최상의 표현을 찾는 것이 문제입니다.

먼저 소스의 단일 샘플을 표현하는 문제를 고려합니다. 랜덤 변수를 $X$로 나타내고 $X$의 표현을 $\hat{X}(X)$로 표기합니다. $X$를 표현하기 위해 $R$ 비트를 제공받는다면, 함수 $\hat{X}$는 $2^{R}$개의 값을 가질 수 있습니다. 문제는 $\hat{X}$의 최적 값 집합(재현점 또는 코드점이라고 함)과 각 값 $\hat{X}$와 관련된 영역을 찾는 것입니다.

예를 들어, $X \sim \mathcal{N}\left(0, \sigma^{2}\right)$이고 제곱 오차 왜곡 측도를 가정합니다. 이 경우, $\hat{X}$가 최대 $2^{R}$개의 값을 갖고 $E(X-\hat{X}(X))^{2}$를 최소화하는 함수 $\hat{X}(X)$를 찾아야 합니다. $X$를 표현하기 위해 한 비트를 제공받는다면, 해당 비트는 $X>0$인지 아닌지를 구별해야 한다는 것이 명확합니다. 제곱 오차를 최소화하기 위해 각 재현 심볼은 해당 영역의 조건부 평균이어야 합니다. 이는 그림 10.1에 설명되어 있습니다. 따라서,

$$
\hat{X}(x)= \begin{cases}\sqrt{\frac{2}{\pi}} \sigma & \text { if } x \geq 0 \\ -\sqrt{\frac{2}{\pi}} \sigma & \text { if } x<0\end{cases}
$$

샘플을 표현하기 위해 2비트를 제공받는다면 상황은 그렇게 간단하지 않습니다. 명확하게, 우리는 실수선을 네 개의 영역으로 나누고 사용하기를 원합니다.

그림 10.1. 가우시안 랜덤 변수의 1비트 양자화.
<!-- Page 329 -->
각 영역 내의 한 점을 샘플로 나타냅니다. 그러나 표현 영역과 재구성 지점이 무엇인지 즉시 명확하지는 않습니다. 하지만 단일 확률 변수의 양자화에 대한 최적 영역 및 재구성 지점의 두 가지 간단한 속성을 명시할 수 있습니다.

- 재구성 지점 집합 $\{\hat{X}(w)\}$이 주어졌을 때, 왜곡은 소스 확률 변수 $X$를 가장 가까운 표현 $\hat{X}(w)$으로 매핑함으로써 최소화됩니다. 이 매핑에 의해 정의된 $\mathcal{X}$의 영역 집합을 재구성 지점에 의해 정의된 보로노이 또는 디리클레 분할이라고 합니다.
- 재구성 지점은 해당 할당 영역에 대한 조건부 기대 왜곡을 최소화해야 합니다.

이 두 가지 속성을 통해 "좋은" 양자화기를 찾기 위한 간단한 알고리즘을 구성할 수 있습니다. 재구성 지점 집합으로 시작하여 최적의 재구성 영역 집합(왜곡 측정값에 대한 최단 거리 영역)을 찾은 다음, 이러한 영역에 대한 최적 재구성 지점(왜곡이 제곱 오차인 경우 해당 영역의 중심점)을 찾고, 이 새로운 재구성 지점 집합에 대해 반복을 반복합니다. 알고리즘의 각 단계에서 기대 왜곡이 감소하므로 알고리즘은 왜곡의 지역 최소값으로 수렴합니다. 이 알고리즘은 로이드 알고리즘 [363](실수값 확률 변수의 경우) 또는 일반화된 로이드 알고리즘 [358](벡터값 확률 변수의 경우)이라고 하며 양자화 시스템을 설계하는 데 자주 사용됩니다.

단일 확률 변수를 양자화하는 대신, 가우시안 분포에 따라 추출된 $n$개의 i.i.d. 확률 변수 집합이 주어졌다고 가정해 보겠습니다. 이 확률 변수들은 $n R$ 비트를 사용하여 표현됩니다. 소스가 i.i.d.이므로 심볼은 독립적이며, 각 요소의 표현은 별도로 처리해야 하는 독립적인 문제처럼 보일 수 있습니다. 그러나 이는 사실이 아니며, 이는 속도 왜곡 이론의 결과에서 알 수 있습니다. 전체 시퀀스를 $2^{n R}$ 값을 갖는 단일 인덱스로 표현할 것입니다. 전체 시퀀스를 한 번에 처리하면 개별 샘플의 독립적인 양자화보다 동일한 속도에서 더 낮은 왜곡을 달성할 수 있습니다.

# 10.2 정의

소스가 시퀀스 $X_{1}, X_{2}, \ldots, X_{n}$을 생성한다고 가정합니다. 여기서 $X_i$는 i.i.d. $\sim p(x), x \in \mathcal{X}$입니다. 이 장의 증명에서는
<!-- Page 330 -->

그림 10.2. 비율 왜곡 인코더 및 디코더.
알파벳은 유한하지만, 대부분의 증명은 연속 확률 변수로 확장될 수 있습니다. 인코더는 소스 시퀀스 $X^{n}$을 인덱스 $f_{n}\left(X^{n}\right) \in\left\{1,2, \ldots, 2^{n R}\right\}$로 설명합니다. 디코더는 그림 10.2에 설명된 대로 추정값 $\hat{X}^{n} \in \mathcal{X}$으로 $X^{n}$을 표현합니다.

정의 왜곡 함수 또는 왜곡 측정은 다음과 같은 매핑입니다.

$$
d: \mathcal{X} \times \hat{\mathcal{X}} \rightarrow \mathcal{R}^{+}
$$

소스 알파벳-재생 알파벳 쌍의 집합에서 음이 아닌 실수 집합으로의 매핑입니다. 왜곡 $d(x, \hat{x})$는 기호 $x$를 기호 $\hat{x}$로 표현하는 비용의 척도입니다.

정의 왜곡 측정은 최대 왜곡 값이 유한한 경우 유계라고 합니다.

$$
d_{\max } \stackrel{\text { def }}{=} \max _{x \in \mathcal{X}, \hat{x} \in \hat{X}} d(x, \hat{x})<\infty
$$

대부분의 경우, 재생 알파벳 $\hat{\mathcal{X}}$는 소스 알파벳 $\mathcal{X}$와 동일합니다.

일반적인 왜곡 함수의 예시는 다음과 같습니다.

- 해밍(오류 확률) 왜곡. 해밍 왜곡은 다음과 같이 주어집니다.

$$
d(x, \hat{x})= \begin{cases}0 & \text { if } x=\hat{x} \\ 1 & \text { if } x \neq \hat{x}\end{cases}
$$

이는 오류 확률 왜곡으로 이어지는데, 왜냐하면 $\operatorname{Ed}(X, \hat{X})=$ $\operatorname{Pr}(X \neq \hat{X})$이기 때문입니다.
<!-- Page 331 -->
- 제곱-오차 왜곡. 제곱-오차 왜곡은

$$
d(x, \hat{x})=(x-\hat{x})^{2}
$$

연속적인 알파벳에 대해 가장 널리 사용되는 왜곡 측정값입니다. 이 측정값의 장점은 단순성과 최소 제곱 예측과의 관련성입니다. 그러나 이미지 및 음성 코딩과 같은 응용 분야에서는 여러 저자들이 인간 관찰자에게 평균 제곱 오차가 적절한 왜곡 측정값이 아니라고 지적했습니다. 예를 들어, 음성 파형과 약간 시간 이동된 동일한 파형 버전 사이에는 큰 제곱-오차 왜곡이 있지만, 인간 관찰자에게는 둘 다 동일하게 들릴 것입니다.

많은 대안이 제안되었습니다. 음성 코딩에서 인기 있는 왜곡 측정값은 다변수 정규 분포 간의 상대적 엔트로피인 이타쿠라-사이토 거리입니다. 그러나 이미지 코딩에서는 현재 평균 제곱 오차를 왜곡 측정값으로 사용하는 것 외에 실질적인 대안이 없습니다.

왜곡 측정값은 기호별로 정의됩니다. 다음 정의를 사용하여 시퀀스에 대한 정의를 확장합니다.

정의 시퀀스 $x^{n}$과 $\hat{x}^{n}$ 간의 왜곡은 다음과 같이 정의됩니다.

$$
d\left(x^{n}, \hat{x}^{n}\right)=\frac{1}{n} \sum_{i=1}^{n} d\left(x_{i}, \hat{x}_{i}\right)
$$

따라서 시퀀스의 왜곡은 시퀀스 요소의 기호별 왜곡의 평균입니다. 이것이 유일하게 합리적인 정의는 아닙니다. 예를 들어, 두 시퀀스 간의 왜곡을 기호별 왜곡의 최댓값으로 측정하고 싶을 수 있습니다. 아래에서 파생된 이론은 이보다 더 일반적인 왜곡 측정값에는 직접 적용되지 않습니다.

정의 $\left(2^{n R}, n\right)$-비율 왜곡 코드는 인코딩 함수,

$$
f_{n}: \mathcal{X}^{n} \rightarrow\left\{1,2, \ldots, 2^{n R}\right\}
$$

및 디코딩(재생성) 함수,

$$
g_{n}:\left\{1,2, \ldots, 2^{n R}\right\} \rightarrow \hat{\mathcal{X}}^{n}
$$

로 구성됩니다.
<!-- Page 332 -->
$\left(2^{n R}, n\right)$ 코드와 관련된 왜곡은 다음과 같이 정의됩니다.

$$
D=\operatorname{Ed}\left(X^{n}, g_{n}\left(f_{n}\left(X^{n}\right)\right)\right)
$$

여기서 기댓값은 $X$의 확률 분포에 대한 것입니다.

$$
D=\sum_{x^{n}} p\left(x^{n}\right) d\left(x^{n}, g_{n}\left(f_{n}\left(x^{n}\right)\right)\right)
$$

$n$-튜플 $g_{n}(1), g_{n}(2), \ldots, g_{n}\left(2^{n R}\right)$의 집합은 $\hat{X}^{n}(1), \ldots, \hat{X}^{n}\left(2^{n R}\right)$로 표시되며 코드북을 구성하고, $f_{n}^{-1}(1), \ldots, f_{n}^{-1}\left(2^{n R}\right)$는 관련 할당 영역입니다.

$X^{n}$을 양자화된 버전 $\hat{X}^{n}(w)$로 대체하는 것을 설명하기 위해 여러 용어가 사용됩니다. $\hat{X}^{n}$을 $X^{n}$의 벡터 양자화, 재현, 복원, 표현, 소스 코드 또는 추정이라고 부르는 것이 일반적입니다.

정의: 속도 왜곡 쌍 $(R, D)$는 $\lim _{n \rightarrow \infty} \operatorname{Ed}\left(X^{n}, g_{n}\left(f_{n}\left(X^{n}\right)\right)\right) \leq D$를 갖는 $\left(2^{n R}, n\right)$-속도 왜곡 코드 $\left(f_{n}, g_{n}\right)$의 시퀀스가 존재하면 달성 가능하다고 말합니다.

정의: 소스의 속도 왜곡 영역은 달성 가능한 속도 왜곡 쌍 $(R, D)$의 집합의 폐포입니다.

정의: 속도 왜곡 함수 $R(D)$는 주어진 왜곡 $D$에 대해 속도 왜곡 영역에 $(R, D)$가 속하는 속도 $R$의 최소값입니다.

정의: 왜곡 속도 함수 $D(R)$는 주어진 속도 $R$에 대해 속도 왜곡 영역에 $(R, D)$가 속하는 왜곡 $D$의 최소값입니다.

왜곡 속도 함수는 속도 왜곡 영역의 경계를 살펴보는 또 다른 방법을 정의합니다. 두 접근 방식이 동등하지만, 일반적으로 이 경계를 설명하기 위해 왜곡 속도 함수 대신 속도 왜곡 함수를 사용할 것입니다.

이제 소스의 수학적 함수를 정의하며, 이를 정보 속도 왜곡 함수라고 부릅니다. 이 장의 주요 결과는 정보 속도 왜곡 함수가 위에서 정의된 속도 왜곡 함수와 같다는 증명입니다(즉, 특정 왜곡을 달성하는 속도의 최소값입니다).
<!-- Page 333 -->
정의 정보율-왜곡 함수 $R^{(I)}(D)$는 왜곡 측정값 $d(x, \hat{x})$를 갖는 소스 $X$에 대해 다음과 같이 정의됩니다.

$$
R^{(I)}(D)=\min _{p(\hat{x} \mid x): \sum_{(x, \hat{x})} p(x) p(\hat{x} \mid x) d(x, \hat{x}) \leq D} I(X ; \hat{X})
$$

여기서 최소화는 결합 분포 $p(x, \hat{x})=p(x) p(\hat{x} \mid x)$가 기대 왜곡 제약 조건을 만족하는 모든 조건부 분포 $p(\hat{x} \mid x)$에 대해 수행됩니다.

제 7 장의 채널 용량 논의와 유사하게, 우리는 처음에 정보율-왜곡 함수의 속성을 고려하고 일부 간단한 소스 및 왜곡 측정값에 대해 이를 계산합니다. 나중에 우리는 이 함수를 실제로 달성할 수 있음을 증명합니다 (즉, 왜곡 $D$를 갖는 비율 $R^{(I)}(D)$의 코드가 존재합니다). 또한 왜곡 $D$를 달성하는 모든 코드에 대해 $R \geq R^{(I)}(D)$임을 설정하는 역정리를 증명합니다.

율-왜곡 이론의 주요 정리는 다음과 같이 명시될 수 있습니다.
정리 10.2.1 분포 $p(x)$와 유계 왜곡 함수 $d(x, \hat{x})$를 갖는 i.i.d. 소스 $X$에 대한 율-왜곡 함수는 관련 정보율-왜곡 함수와 같습니다. 따라서,

$$
R(D)=R^{(I)}(D)=\min _{p(\hat{x} \mid x): \sum_{(x, \hat{x})} p(x) p(\hat{x} \mid x) d(x, \hat{x}) \leq D} I(X ; \hat{X})
$$

는 왜곡 $D$에서 달성 가능한 최소 비율입니다.
이 정리는 율-왜곡 함수의 운영 정의가 정보 정의와 같음을 보여줍니다. 따라서 이제부터 율-왜곡 함수의 두 정의 모두를 나타내기 위해 $R(D)$를 사용합니다. 정리의 증명으로 넘어가기 전에 일부 간단한 소스 및 왜곡에 대한 정보율-왜곡 함수를 계산합니다.

# 10.3 율-왜곡 함수 계산

### 10.3.1 이진 소스

이제 오류 비율이 $D$ 이하인 베르누이 $(p)$ 소스를 설명하는 데 필요한 설명 비율 $R(D)$를 찾습니다.

정리 10.3.1 햄밍 왜곡을 갖는 베르누이 $(p)$ 소스에 대한 율-왜곡 함수는 다음과 같이 주어집니다.

$$
R(D)= \begin{cases}H(p)-H(D), & 0 \leq D \leq \min \{p, 1-p\} \\ 0, & D>\min \{p, 1-p\}\end{cases}
$$
<!-- Page 334 -->
증명: 햄밍 왜곡 측정값을 갖는 이진 소스 $X \sim \operatorname{Bernoulli}(p)$를 고려하십시오. 일반성을 잃지 않고 $p<\frac{1}{2}$라고 가정할 수 있습니다. 우리는 비율 왜곡 함수를 계산하고자 합니다.

$$
R(D)=\min _{p(\hat{x} \mid x): \sum_{(x, \hat{x})} p(x) p(\hat{x} \mid x) d(x, \hat{x}) \leq D} I(X ; \hat{X})
$$

$\oplus$를 모듈로 2 덧셈으로 나타냅니다. 따라서 $X \oplus \hat{X}=1$은 $X \neq \hat{X}$와 동등합니다. 우리는 $I(X ; \hat{X})$를 직접 최소화하지 않고, 대신 하한을 찾은 다음 이 하한이 달성 가능함을 보입니다. 왜곡 제약을 만족하는 임의의 결합 분포에 대해 다음이 성립합니다.

$$
\begin{aligned}
I(X ; \hat{X}) & =H(X)-H(X \mid \hat{X}) \\
& =H(p)-H(X \oplus \hat{X} \mid \hat{X}) \\
& \geq H(p)-H(X \oplus \hat{X}) \\
& \geq H(p)-H(D)
\end{aligned}
$$

왜냐하면 $\operatorname{Pr}(X \neq \hat{X}) \leq D$이고 $D \leq \frac{1}{2}$일 때 $H(D)$는 $D$에 따라 증가하기 때문입니다. 따라서,

$$
R(D) \geq H(p)-H(D)
$$

이제 왜곡 제약을 충족하고 $I(X ; \hat{X})=R(D)$를 갖는 결합 분포를 찾아 하한이 실제로 비율 왜곡 함수임을 보입니다. $0 \leq D \leq p$에 대해, 그림 10.3에 표시된 이진 대칭 채널을 $(X, \hat{X})$가 갖도록 선택함으로써 (10.19)에서 비율 왜곡 함수의 값을 달성할 수 있습니다.

그림 10.3. 이진 소스에 대한 결합 분포.
<!-- Page 335 -->
채널 입력에서의 $\hat{X}$ 분포를 선택하여 출력 분포 $X$가 지정된 분포가 되도록 합니다. $\operatorname{Pr}(\hat{X}=1)=r$이라고 하면, 다음을 만족하도록 $r$을 선택합니다.

$$
r(1-D)+(1-r) D=p
$$

또는

$$
r=\frac{p-D}{1-2 D}
$$

만약 $D \leq p \leq \frac{1}{2}$이면, $\operatorname{Pr}(\hat{X}=1) \geq 0$이고 $\operatorname{Pr}(\hat{X}=0) \geq 0$입니다. 그러면 다음과 같습니다.

$$
I(X ; \hat{X})=H(X)-H(X \mid \hat{X})=H(p)-H(D)
$$

기대 왜곡은 $\operatorname{Pr}(X \neq \hat{X})=D$입니다.
만약 $D \geq p$이면, 확률 1로 $\hat{X}=0$을 선택함으로써 $R(D)=0$을 달성할 수 있습니다. 이 경우 $I(X ; \hat{X})=0$이고 $D=p$입니다. 마찬가지로, $D \geq 1-p$이면, 확률 1로 $\hat{X}=1$을 설정함으로써 $R(D)=0$을 달성할 수 있습니다. 따라서 이진 소스에 대한 레이트 왜곡 함수는 다음과 같습니다.

$$
R(D)= \begin{cases}H(p)-H(D), & 0 \leq D \leq \min \{p, 1-p\} \\ 0, & D>\min \{p, 1-p\}\end{cases}
$$

이 함수는 그림 10.4에 설명되어 있습니다.

그림 10.4. 베르누이 $\left(\frac{1}{2}\right)$ 소스에 대한 레이트 왜곡 함수.
<!-- Page 336 -->
위의 계산은 전혀 동기가 부여되지 않은 것처럼 보일 수 있습니다. 상호 정보량(mutual information)을 최소화하는 것이 양자화(quantization)와 무슨 관련이 있습니까? 이 질문에 대한 답은 정리 10.2.1을 증명할 때까지 기다려야 합니다.

# 10.3.2 가우시안 소스 (Gaussian Source)

정리 10.2.1은 유계 왜곡 측정(bounded distortion measure)을 갖는 이산 소스(discrete sources)에 대해서만 증명되었지만, 잘 동작하는 연속 소스(continuous sources)와 무계 왜곡 측정(unbounded distortion measures)에 대해서도 증명될 수 있습니다. 이 일반적인 정리를 가정하고, 제곱 오차 왜곡(squared-error distortion)을 갖는 가우시안 소스(Gaussian source)에 대한 레이트 왜곡 함수(rate distortion function)를 계산합니다.

정리 10.3.2 제곱 오차 왜곡(squared-error distortion)을 갖는 $\mathcal{N}\left(0, \sigma^{2}\right)$ 소스(source)의 레이트 왜곡 함수(rate distortion function)는 다음과 같습니다.

$$
R(D)= \begin{cases}\frac{1}{2} \log \frac{\sigma^{2}}{D}, & 0 \leq D \leq \sigma^{2} \\ 0, & D>\sigma^{2}\end{cases}
$$

증명: $X$를 $\sim \mathcal{N}\left(0, \sigma^{2}\right)$라고 합시다. 연속 알파벳(continuous alphabets)으로 확장된 레이트 왜곡 정리(rate distortion theorem)에 의해, 우리는 다음과 같이 얻습니다.

$$
R(D)=\min _{f(\hat{x} \mid x): E(\hat{X}-X)^{2} \leq D} I(X ; \hat{X})
$$

앞선 예시와 마찬가지로, 먼저 레이트 왜곡 함수(rate distortion function)에 대한 하한(lower bound)을 찾은 다음, 이것이 달성 가능함을 증명합니다. $E(X-$ $\hat{X})^{2} \leq D$이므로, 우리는 다음과 같이 관찰합니다.

$$
\begin{aligned}
I(X ; \hat{X}) & =h(X)-h(X \mid \hat{X}) \\
& =\frac{1}{2} \log (2 \pi e) \sigma^{2}-h(X-\hat{X} \mid \hat{X}) \\
& \geq \frac{1}{2} \log (2 \pi e) \sigma^{2}-h(X-\hat{X}) \\
& \geq \frac{1}{2} \log (2 \pi e) \sigma^{2}-h\left(\mathcal{N}\left(0, E(X-\hat{X})^{2}\right)\right) \\
& =\frac{1}{2} \log (2 \pi e) \sigma^{2}-\frac{1}{2} \log (2 \pi e) E(X-\hat{X})^{2} \\
& \geq \frac{1}{2} \log (2 \pi e) \sigma^{2}-\frac{1}{2} \log (2 \pi e) D \\
& =\frac{1}{2} \log \frac{\sigma^{2}}{D}
\end{aligned}
$$
<!-- Page 337 -->
이는 조건화가 엔트로피를 감소시킨다는 사실로부터 (10.28)이 도출되고, 주어진 두 번째 모멘트에 대해 정규 분포가 엔트로피를 최대화한다는 사실로부터 (10.29)가 도출된다는 점에 근거합니다 (정리 8.6.5). 따라서,

$$
R(D) \geq \frac{1}{2} \log \frac{\sigma^{2}}{D}
$$

이 하한을 달성하는 조건부 밀도 $f(\hat{x} \mid x)$를 찾기 위해, 종종 테스트 채널이라고도 불리는 조건부 밀도 $f(x \mid \hat{x})$를 살펴보는 것이 더 편리합니다 (따라서 속도 왜곡과 채널 용량의 이중성을 강조합니다). 이진 경우와 마찬가지로, 하한에서의 등식을 달성하도록 $f(x \mid \hat{x})$를 구성합니다. 그림 10.5에 표시된 결합 분포를 선택합니다. 만약 $D \leq \sigma^{2}$이면, 다음을 선택합니다.

$$
X=\hat{X}+Z, \quad \hat{X} \sim \mathcal{N}\left(0, \sigma^{2}-D\right), \quad Z \sim \mathcal{N}(0, D)
$$

여기서 $\hat{X}$와 $Z$는 독립입니다. 이 결합 분포에 대해 다음을 계산합니다.

$$
I(X ; \hat{X})=\frac{1}{2} \log \frac{\sigma^{2}}{D}
$$

그리고 $E(X-\hat{X})^{2}=D$이므로, (10.33)의 하한을 달성합니다. 만약 $D>\sigma^{2}$이면, 확률 1로 $\hat{X}=0$을 선택하여 $R(D)=0$을 달성합니다. 따라서, 제곱 오차 왜곡에 대한 가우시안 소스의 속도 왜곡 함수는 다음과 같습니다.

$$
R(D)= \begin{cases}\frac{1}{2} \log \frac{\sigma^{2}}{D}, & 0 \leq D \leq \sigma^{2} \\ 0, & D>\sigma^{2}\end{cases}
$$

이는 그림 10.6에 설명되어 있습니다.
(10.36)을 속도에 대한 왜곡을 표현하도록 다시 작성할 수 있습니다.

$$
D(R)=\sigma^{2} 2^{-2 R}
$$

그림 10.5. 가우시안 소스의 결합 분포.
<!-- Page 338 -->

그림 10.6. 가우시안 소스에 대한 속도 왜곡 함수.

설명의 각 비트는 예상 왜곡을 4배로 줄입니다. 1비트 설명으로, 최상의 예상 제곱 오차는 $\sigma^{2} / 4$입니다. 이를 10.1절에서 설명한 $\mathcal{N}\left(0, \sigma^{2}\right)$ 확률 변수의 단순 1비트 양자화 결과와 비교할 수 있습니다. 이 경우, 양수 및 음수 실수 선에 해당하는 두 영역과 각 영역의 중심으로서의 재현점을 사용하면 예상 왜곡은 $\frac{(\pi-2)}{\pi} \sigma^{2}=0.3633 \sigma^{2}$입니다(문제 10.1 참조). 나중에 증명하듯이, 속도 왜곡 한계 $R(D)$는 긴 블록 길이를 고려하여 달성됩니다. 이 예는 각 문제를 개별적으로 고려하여 달성할 수 있는 것보다 여러 왜곡 문제를 연속적으로 고려함으로써(긴 블록 길이) 더 낮은 왜곡을 달성할 수 있음을 보여줍니다. 이는 독립적인 확률 변수를 양자화하기 때문에 다소 놀랍습니다.

# 10.3.3 독립 가우시안 확률 변수의 동시 설명

$X_{i}$가 $\sim \mathcal{N}\left(0, \sigma_{i}^{2}\right)$인 $m$개의 독립적인(그러나 동일하게 분포되지 않은) 정규 확률 소스 $X_{1}, \ldots, X_{m}$를 제곱 오차 왜곡으로 표현하는 경우를 고려하십시오. 이 확률 벡터를 표현하기 위해 $R$비트가 주어졌다고 가정합니다. 총 왜곡을 최소화하기 위해 다양한 구성 요소에 이러한 비트를 어떻게 할당해야 하는지에 대한 질문이 자연스럽게 발생합니다. 정보 속도 왜곡의 정의를 확장하면
<!-- Page 339 -->
벡터의 경우, 다음과 같습니다.

$$
R(D)=\min _{f\left(\hat{x}^{m} \mid x^{m}\right): \operatorname{Ed}\left(X^{m}, \hat{X}^{m}\right) \leq D} I\left(X^{m} ; \hat{X}^{m}\right)
$$

여기서 $d\left(x^{m}, \hat{x}^{m}\right)=\sum_{i=1}^{m}\left(x_{i}-\hat{x}_{i}\right)^{2}$입니다. 이제 앞선 예제의 논증을 사용하여 다음과 같이 나타낼 수 있습니다.

$$
\begin{aligned}
I\left(X^{m} ; \hat{X}^{m}\right) & =h\left(X^{m}\right)-h\left(X^{m} \mid \hat{X}^{m}\right) \\
& =\sum_{i=1}^{m} h\left(X_{i}\right)-\sum_{i=1}^{m} h\left(X_{i} \mid X^{i-1}, \hat{X}^{m}\right) \\
& \geq \sum_{i=1}^{m} h\left(X_{i}\right)-\sum_{i=1}^{m} h\left(X_{i} \mid \hat{X}_{i}\right) \\
& =\sum_{i=1}^{m} I\left(X_{i} ; \hat{X}_{i}\right) \\
& \geq \sum_{i=1}^{m} R\left(D_{i}\right) \\
& =\sum_{i=1}^{m}\left(\frac{1}{2} \log \frac{\sigma_{i}^{2}}{D_{i}}\right)^{+}
\end{aligned}
$$

여기서 $D_{i}=E\left(X_{i}-\hat{X}_{i}\right)^{2}$이며, (10.41)은 조건화가 엔트로피를 감소시킨다는 사실로부터 유도됩니다. (10.41)에서의 등식은 $f\left(x^{m} \mid \hat{x}^{m}\right)=\prod_{i=1}^{m} f\left(x_{i} \mid \hat{x}_{i}\right)$를 선택함으로써 달성될 수 있으며, (10.43)에서의 등식은 앞선 예제와 같이 각 $\hat{X}_{i} \sim \mathcal{N}\left(0, \sigma_{i}^{2}-D_{i}\right)$의 분포를 선택함으로써 달성될 수 있습니다. 따라서, rate distortion function을 찾는 문제는 다음과 같은 최적화 문제로 축소될 수 있습니다 (편의상 nats 사용):

$$
R(D)=\min _{\sum D_{i}=D} \sum_{i=1}^{m} \max \left\{\frac{1}{2} \ln \frac{\sigma_{i}^{2}}{D_{i}}, 0\right\}
$$

라그랑주 승수법을 사용하여, 다음과 같은 함수를 구성합니다.

$$
J(D)=\sum_{i=1}^{m} \frac{1}{2} \ln \frac{\sigma_{i}^{2}}{D_{i}}+\lambda \sum_{i=1}^{m} D_{i}
$$
<!-- Page 340 -->
$D_{i}$에 대해 미분하고 0과 같다고 놓으면 다음과 같습니다.

$$
\frac{\partial J}{\partial D_{i}}=-\frac{1}{2} \frac{1}{D_{i}}+\lambda=0
$$

또는

$$
D_{i}=\lambda^{\prime}
$$

따라서 각 확률 변수에 대한 비트의 최적 할당은 각 확률 변수에 대해 동일한 왜곡을 초래합니다. 이는 상수 $\lambda^{\prime}$이 모든 $i$에 대해 $\sigma_{i}^{2}$보다 작을 경우 가능합니다. 허용되는 총 왜곡 $D$가 증가함에 따라 상수 $\lambda^{\prime}$이 증가하여 일부 $i$에 대해 $\sigma_{i}^{2}$를 초과할 때까지 증가합니다. 이 시점에서 해 (10.48)은 허용되는 왜곡 영역의 경계에 있습니다. 총 왜곡을 증가시키면 (10.46)에서 최소값을 찾기 위해 Kuhn-Tucker 조건을 사용해야 합니다. 이 경우 Kuhn-Tucker 조건은 다음과 같습니다.

$$
\frac{\partial J}{\partial D_{i}}=-\frac{1}{2} \frac{1}{D_{i}}+\lambda
$$

여기서 $\lambda$는 다음을 만족하도록 선택됩니다.

$$
\frac{\partial J}{\partial D_{i}} \begin{cases}=0 & \text { if } D_{i}<\sigma_{i}^{2} \\ \leq 0 & \text { if } D_{i} \geq \sigma_{i}^{2}\end{cases}
$$

Kuhn-Tucker 방정식의 해는 다음 정리에 의해 주어진다는 것을 쉽게 확인할 수 있습니다.

정리 10.3.3 (병렬 가우시안 소스의 비율 왜곡) $X_{i} \sim \mathcal{N}\left(0, \sigma_{i}^{2}\right), i=1,2, \ldots, m$은 독립적인 가우시안 확률 변수이고, 왜곡 척도는 $d\left(x^{m}, \hat{x}^{m}\right)=\sum_{i=1}^{m}\left(x_{i}-\hat{x}_{i}\right)^{2}$라고 가정합니다. 그러면 비율 왜곡 함수는 다음과 같이 주어집니다.

$$
R(D)=\sum_{i=1}^{m} \frac{1}{2} \log \frac{\sigma_{i}^{2}}{D_{i}}
$$

여기서

$$
D_{i}= \begin{cases}\lambda & \text { if } \lambda<\sigma_{i}^{2} \\ \sigma_{i}^{2} & \text { if } \lambda \geq \sigma_{i}^{2}\end{cases}
$$

여기서 $\lambda$는 $\sum_{i=1}^{m} D_{i}=D$를 만족하도록 선택됩니다.
<!-- Page 341 -->

그림 10.7. 독립적인 가우시안 확률 변수에 대한 역 워터-필링.

이는 그림 10.7에 설명된 바와 같이 일종의 역 워터-필링을 발생시킵니다. 상수 $\lambda$를 선택하고 분산이 $\lambda$보다 큰 확률 변수만 설명합니다. 분산이 $\lambda$보다 작은 확률 변수를 설명하는 데는 비트가 사용되지 않습니다. 요약하면, 만약

$$
X \sim \mathcal{N}\left(0,\left[\begin{array}{ccc}
\sigma_{1}^{2} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \sigma_{m}^{2}
\end{array}\right]\right), \text { 그러면 } \hat{X} \sim \mathcal{N}\left(0,\left[\begin{array}{ccc}
\hat{\sigma}_{1}^{2} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \hat{\sigma}_{m}^{2}
\end{array}\right]\right)
$$

이고 $E\left(X_{i}-\hat{X}_{i}\right)^{2}=D_{i}$이며, 여기서 $D_{i}=\min \left\{\lambda, \sigma_{i}^{2}\right\}$입니다. 더 일반적으로, 다변수 정규 벡터에 대한 속도 왜곡 함수는 고유값에 대한 역 워터-필링을 통해 얻을 수 있습니다. 가우시안 확률 과정에도 동일한 논리를 적용할 수 있습니다. 스펙트럼 표현 정리에 따르면, 가우시안 확률 과정은 다양한 주파수 대역의 독립적인 가우시안 과정의 적분으로 표현될 수 있습니다. 스펙트럼에 대한 역 워터-필링은 속도 왜곡 함수를 산출합니다.

# 10.4 속도 왜곡 정리에 대한 역 정리

이 섹션에서는 $R(D)$보다 낮은 속도로 $X$를 설명할 경우 $D$보다 작은 왜곡을 달성할 수 없음을 보여줌으로써 정리 10.2.1의 역 정리를 증명합니다. 여기서

$$
R(D)=\min _{p(\hat{x} \mid x): \sum_{(x, \hat{x})} p(x) p(\hat{x} \mid x) d(x, \hat{x}) \leq D} I(X ; \hat{X})
$$
<!-- Page 342 -->
최소화는 기대 왜곡 제약을 만족하는 결합 분포 $p(x, \hat{x})=p(x) p(\hat{x} \mid x)$를 만족하는 모든 조건부 분포 $p(\hat{x} \mid x)$에 대해 수행됩니다. 역정리를 증명하기 전에 정보율 왜곡 함수의 간단한 속성들을 확립합니다.

정리 10.4.1 ($R(D)$의 볼록성) (10.53)에 주어진율 왜곡 함수 $R(D)$는 $D$의 비증가 볼록 함수입니다.

증명: $R(D)$는 $D$가 증가함에 따라 점점 더 큰 집합에 대한 상호 정보의 최소값입니다. 따라서 $R(D)$는 $D$에 대해 비증가 함수입니다. $R(D)$가 볼록함을 증명하기 위해율 왜곡 곡선 상에 있는 두율 왜곡 쌍 $\left(R_{1}, D_{1}\right)$와 $\left(R_{2}, D_{2}\right)$를 고려합니다. 이 쌍들을 달성하는 결합 분포를 $p_{1}(x, \hat{x})=p(x) p_{1}(\hat{x} \mid x)$와 $p_{2}(x, \hat{x})=$ $p(x) p_{2}(\hat{x} \mid x)$라고 합시다. 분포 $p_{\lambda}=\lambda p_{1}+(1-\lambda) p_{2}$를 고려합니다. 왜곡은 분포의 선형 함수이므로, $D\left(p_{\lambda}\right)=\lambda D_{1}+$ $(1-\lambda) D_{2}$를 얻습니다. 반면에 상호 정보는 조건부 분포의 볼록 함수입니다(정리 2.7.4). 따라서

$$
I_{p_{\lambda}}(X ; \hat{X}) \leq \lambda I_{p_{1}}(X ; \hat{X})+(1-\lambda) I_{p_{2}}(X ; \hat{X})
$$

따라서율 왜곡 함수의 정의에 따라,

$$
\begin{aligned}
R\left(D_{\lambda}\right) & \leq I_{p_{\lambda}}(X ; \hat{X}) \\
& \leq \lambda I_{p_{1}}(X ; \hat{X})+(1-\lambda) I_{p_{2}}(X ; \hat{X}) \\
& =\lambda R\left(D_{1}\right)+(1-\lambda) R\left(D_{2}\right)
\end{aligned}
$$

이는 $R(D)$가 $D$의 볼록 함수임을 증명합니다.

이제 역정리를 증명할 수 있습니다.

증명: (정리 10.2.1의 역정리). 왜곡 측정값 $d(x, \hat{x})$와 왜곡 $\leq D$를 갖는 모든 $\left(2^{n R}, n\right)$율 왜곡 코드에 대해 i.i.d. $\sim p(x)$로 그려진 모든 소스 $X$에 대해 코드의율 $R$이 $R \geq R(D)$를 만족함을 보여야 합니다. 사실, $f_{n}$이 $2^{n R}$개의 값을 최대 $2^{n R}$개까지 취하는 한, 랜덤화된 매핑 $f_{n}$ 및 $g_{n}$에 대해서도 $R \geq R(D)$임을 증명합니다.

(10.7) 및 (10.8)에 주어진 함수 $f_{n}$ 및 $g_{n}$으로 정의된 모든 $\left(2^{n R}, n\right)$율 왜곡 코드를 고려합니다. $X^{n}$에 해당하는 복제된 시퀀스를 $\hat{X}^{n}=\hat{X}^{n}\left(X^{n}\right)=g_{n}\left(f_{n}\left(X^{n}\right)\right)$라고 합시다. $E d\left(X^{n}, \hat{X}^{n}\right) \geq D$라고 가정합니다.
<!-- Page 343 -->
이 코드에 대해 다음과 같은 부등식 연쇄를 얻습니다.

$$
\begin{aligned}
n R & \stackrel{(a)}{\geq} H\left(f_{n}\left(X^{n}\right)\right) \\
& \stackrel{(b)}{\geq} H\left(f_{n}\left(X^{n}\right)\right)-H\left(f_{n}\left(X^{n}\right) \mid X^{n}\right) \\
& =I\left(X^{n} ; f_{n}\left(X^{n}\right)\right) \\
& \stackrel{(c)}{\geq} I\left(X^{n} ; \hat{X}^{n}\right) \\
& =H\left(X^{n}\right)-H\left(X^{n} \mid \hat{X}^{n}\right) \\
& \stackrel{(\mathrm{d})}{=} \sum_{i=1}^{n} H\left(X_{i}\right)-H\left(X^{n} \mid \hat{X}^{n}\right) \\
& \stackrel{(e)}{=} \sum_{i=1}^{n} H\left(X_{i}\right)-\sum_{i=1}^{n} H\left(X_{i} \mid \hat{X}^{n}, X_{i-1}, \ldots, X_{1}\right) \\
& \stackrel{(\mathrm{f})}{\geq} \sum_{i=1}^{n} H\left(X_{i}\right)-\sum_{i=1}^{n} H\left(X_{i} \mid \hat{X}_{i}\right) \\
& =\sum_{i=1}^{n} I\left(X_{i} ; \hat{X}_{i}\right) \\
& \stackrel{(\mathrm{g})}{\geq} \sum_{i=1}^{n} R\left(E d\left(X_{i}, \hat{X}_{i}\right)\right) \\
& =n\left(\frac{1}{n} \sum_{i=1}^{n} R\left(E d\left(X_{i}, \hat{X}_{i}\right)\right)\right) \\
& \stackrel{(\mathrm{h})}{\geq} n R\left(\frac{1}{n} \sum_{i=1}^{n} E d\left(X_{i}, \hat{X}_{i}\right)\right) \\
& \stackrel{(\mathrm{i})}{=} n R\left(E d\left(X^{n}, \hat{X}^{n}\right)\right) \\
& \stackrel{(\mathrm{j})}{=} n R(D)
\end{aligned}
$$

여기서
(a)는 $f_{n}$의 범위가 최대 $2^{n R}$이라는 사실에서 비롯됩니다.
(b)는 $H\left(f_{n}\left(X^{n}\right) \mid X^{n}\right) \geq 0$이라는 사실에서 비롯됩니다.
<!-- Page 344 -->
(c) 데이터 처리 부등식으로부터 도출됩니다.
(d) $X_{i}$가 독립이라는 사실로부터 도출됩니다.
(e) 엔트로피에 대한 연쇄 법칙으로부터 도출됩니다.
(f) 조건 부여가 엔트로피를 감소시킨다는 사실로부터 도출됩니다.
(g) 비율 왜곡 함수의 정의로부터 도출됩니다.
(h) 비율 왜곡 함수의 볼록성 (정리 10.4.1)과 젠센 부등식으로부터 도출됩니다.
(i) 길이 $n$ 블록에 대한 왜곡의 정의로부터 도출됩니다.
(j) $R(D)$가 $D$의 비증가 함수이고 $E d\left(X^{n}, \hat{X}^{n}\right) \leq D$라는 사실로부터 도출됩니다.

이는 임의의 비율 왜곡 코드의 비율 $R$이 해당 코드로 달성된 왜곡 수준 $D=E d\left(X^{n}, \hat{X}^{n}\right)$에서 평가된 비율 왜곡 함수 $R(D)$를 초과함을 보여줍니다.

유사한 논증을 인코딩된 소스가 노이즈 채널을 통과하는 경우에 적용할 수 있으며, 따라서 왜곡을 갖는 소스 채널 분리 정리가 존재합니다.

정리 10.4.1 (왜곡을 갖는 소스-채널 분리 정리) 유한 알파벳 i.i.d. 소스 $V_{1}, V_{2}, \ldots, V_{n}$이 이산 메모리 없는 채널의 $n$개 입력 심볼 시퀀스 $X^{n}$으로 인코딩된다고 가정합니다. 채널의 출력 $Y^{n}$은 복원 알파벳 $\hat{V}^{n}=g\left(Y^{n}\right)$으로 매핑됩니다. 이 결합된 소스 및 채널 코딩 방식에 의해 달성된 평균 왜곡 $D=E d\left(V^{n}, \hat{V}^{n}\right)=\frac{1}{n} \sum_{i=1}^{n} E d\left(V_{i}, \hat{V}_{i}\right)$에 대해, 왜곡 $D$는 $C>R(D)$일 때만 달성 가능합니다.

증명: 문제 10.17을 참조하십시오.

# 10.5 비율 왜곡 함수의 달성 가능성

이제 비율 왜곡 함수의 달성 가능성을 증명합니다. 왜곡 측정에 대해 쌍이 전형적이라는 조건을 추가한 결합 AEP의 수정된 버전으로 시작합니다.
<!-- Page 345 -->
정의
$p(x, \hat{x})$를 $\mathcal{X} \times \hat{\mathcal{X}}$ 상의 결합 확률 분포라 하고, $d(x, \hat{x})$를 $\mathcal{X} \times \hat{\mathcal{X}}$ 상의 왜곡 측정값이라 하자. 임의의 $\epsilon>0$에 대해, 시퀀스 쌍 $\left(x^{n}, \hat{x}^{n}\right)$은 왜곡 $\epsilon$-typical 또는 단순히 왜곡 typical이라고 한다.

$$
\begin{gathered}
\left|-\frac{1}{n} \log p\left(x^{n}\right)-H(X)\right|<\epsilon \\
\left|-\frac{1}{n} \log p\left(\hat{x}^{n}\right)-H(\hat{X})\right|<\epsilon \\
\left|-\frac{1}{n} \log p\left(x^{n}, \hat{x}^{n}\right)-H(X, \hat{X})\right|<\epsilon \\
\left|d\left(x^{n}, \hat{x}^{n}\right)-E d(X, \hat{X})\right|<\epsilon
\end{gathered}
$$

왜곡 typical 시퀀스의 집합을 왜곡 typical 집합이라고 하며 $A_{d, \epsilon}^{(n)}$으로 표기한다.

이는 추가적인 제약 조건으로 왜곡이 기댓값에 가까워야 한다는 조건을 가진 결합 typical 집합(7.6절)의 정의임을 주목하라. 따라서 왜곡 typical 집합은 결합 typical 집합의 부분집합이다 (즉, $A_{d, \epsilon}^{(n)} \subset A_{\epsilon}^{(n)}$ ). 만약 ( $X_{i}, \hat{X}_{i}$ )가 i.i.d $\sim p(x, \hat{x})$로부터 추출된다면, 두 랜덤 시퀀스 간의 왜곡

$$
d\left(X^{n}, \hat{X}^{n}\right)=\frac{1}{n} \sum_{i=1}^{n} d\left(X_{i}, \hat{X}_{i}\right)
$$

는 i.i.d. 랜덤 변수의 평균이며, 대수의 법칙은 높은 확률로 기댓값에 가까워짐을 의미한다. 따라서 다음과 같은 보조 정리가 있다.

보조 정리 10.5.1
$\left(X_{i}, \hat{X}_{i}\right)$가 i.i.d. $\sim p(x, \hat{x})$로부터 추출된다고 하자. 그러면 $n \rightarrow \infty$일 때 $\operatorname{Pr}\left(A_{d, \epsilon}^{(n)}\right) \rightarrow 1$이다.

증명: $A_{d, \epsilon}^{(n)}$의 정의에 있는 네 가지 조건의 합은 모두 i.i.d. 랜덤 변수의 정규화된 합이며, 따라서 대수의 법칙에 의해 확률 1로 각자의 기댓값에 수렴한다. 그러므로 네 가지 조건을 모두 만족하는 시퀀스의 집합은 $n \rightarrow \infty$일 때 확률이 1로 수렴한다.

다음 보조 정리는 왜곡 typical 집합의 정의로부터 직접적으로 도출된다.
<!-- Page 346 -->
10.5.2 보조정리 모든 $\left(x^{n}, \hat{x}^{n}\right) \in A_{d, \epsilon}^{(n)}$에 대하여,

$$
p\left(\hat{x}^{n}\right) \geq p\left(\hat{x}^{n} \mid x^{n}\right) 2^{-n(I(X ; \hat{X})+3 \epsilon)}
$$

증명: $A_{d, \epsilon}^{(n)}$의 정의를 사용하여, 모든 $\left(x^{n}, \hat{x}^{n}\right) \in A_{d, \epsilon}^{(n)}$에 대하여 확률 $p\left(x^{n}\right), p\left(\hat{x}^{n}\right)$ 및 $p\left(x^{n}, \hat{x}^{n}\right)$을 다음과 같이 제한할 수 있습니다.

$$
\begin{aligned}
p\left(\hat{x}^{n} \mid x^{n}\right) & =\frac{p\left(x^{n}, \hat{x}^{n}\right)}{p\left(x^{n}\right)} \\
& =p\left(\hat{x}^{n}\right) \frac{p\left(x^{n}, \hat{x}^{n}\right)}{p\left(x^{n}\right) p\left(\hat{x}^{n}\right)} \\
& \leq p\left(\hat{x}^{n}\right) \frac{2^{-n(H(X, \hat{X})-\epsilon)}}{2^{-n(H(X)+\epsilon)} 2^{-n(H(\hat{X})+\epsilon)}} \\
& =p\left(\hat{x}^{n}\right) 2^{n(I(X ; \hat{X})+3 \epsilon)}
\end{aligned}
$$

따라서 보조정리가 즉시 성립합니다.
또한 다음과 같은 흥미로운 부등식이 필요합니다.
10.5.3 보조정리 $0 \leq x, y \leq 1, n>0$에 대하여,

$$
(1-x y)^{n} \leq 1-x+e^{-y n}
$$

증명: $f(y)=e^{-y}-1+y$라고 합시다. 그러면 $f(0)=0$이고 $y>0$에 대해 $f^{\prime}(y)=-e^{-y}+1>0$이므로, $y>0$에 대해 $f(y)>0$입니다. 따라서 $0 \leq y \leq 1$에 대해 $1-y \leq e^{-y}$이며, 이를 $n$제곱하면 다음과 같습니다.

$$
(1-y)^{n} \leq e^{-y n}
$$

그러므로 $x=1$일 때 보조정리가 만족됩니다. 검토 결과, $x=0$일 때도 부등식이 만족됨이 명확합니다. 미분을 통해 $g_{y}(x)=(1-x y)^{n}$이 $x$에 대한 볼록 함수임을 쉽게 알 수 있습니다. 따라서 $0 \leq x \leq 1$에 대해 다음과 같습니다.

$$
\begin{aligned}
(1-x y)^{n} & =g_{y}(x) \\
& \leq(1-x) g_{y}(0)+x g_{y}(1) \\
& =(1-x) 1+x(1-y)^{n} \\
& \leq 1-x+x e^{-y n} \\
& \leq 1-x+e^{-y n}
\end{aligned}
$$

앞선 증명을 사용하여 정리 10.2.1의 달성 가능성을 증명합니다.
<!-- Page 347 -->
증명: (정리 10.2.1의 달성 가능성). $X_{1}, X_{2}, \ldots, X_{n}$을 i.i.d. $\sim p(x)$로 추출하고 이 소스에 대한 유계 왜곡 측정값으로 $d(x, \hat{x})$를 사용한다고 가정합니다. 이 소스에 대한 비율 왜곡 함수를 $R(D)$라고 합니다. 그러면 임의의 $D$와 임의의 $R>R(D)$에 대해, 비율 $R$과 점근적 왜곡 $D$를 갖는 비율 왜곡 코드 시퀀스의 존재를 증명함으로써 비율 왜곡 쌍 $(R, D)$가 달성 가능하다는 것을 보일 것입니다. $p(\hat{x} \mid x)$를 고정하며, 여기서 $p(\hat{x} \mid x)$는 (10.53)에서 등식을 달성합니다. 따라서 $I(X ; \hat{X})=R(D)$입니다. $p(\hat{x})=\sum_{x} p(x) p(\hat{x} \mid x)$를 계산합니다. $\delta>0$을 선택합니다. 비율 $R$과 왜곡이 $D+\delta$ 이하인 비율 왜곡 코드의 존재를 증명할 것입니다.

코드북 생성: $2^{n R}$개의 시퀀스 $\hat{X}^{n}$으로 구성된 비율 왜곡 코드북 $\mathcal{C}$를 무작위로 생성하며, 이는 $\prod_{i=1}^{n} p\left(\hat{x}_{i}\right)$로부터 i.i.d.로 추출됩니다. 이 코드워드를 $w \in\left\{1,2, \ldots, 2^{n R}\right\}$로 인덱싱합니다. 이 코드북을 인코더와 디코더에게 공개합니다.

인코딩: $X^{n}$을 $w$로 인코딩합니다. 만약 왜곡이 일반적인 집합인 $A_{d, \epsilon}^{(n)}$에 $\left(X^{n}, \hat{X}^{n}(w)\right)$가 속하는 $w$가 존재한다면 그렇게 합니다. 만약 이러한 $w$가 여러 개 존재한다면 가장 작은 것을 보냅니다. 만약 그러한 $w$가 존재하지 않는다면 $w=1$로 설정합니다. 따라서 $n R$ 비트는 공동으로 일반적인 코드워드의 인덱스 $w$를 설명하기에 충분합니다.

디코딩: 복원된 시퀀스는 $\hat{X}^{n}(w)$입니다.
왜곡 계산: 채널 코딩 정리의 경우와 마찬가지로, 코드북 $\mathcal{C}$의 무작위 선택에 대한 기대 왜곡을 다음과 같이 계산합니다.

$$
\bar{D}=E_{X^{n}, \mathcal{C}} d\left(X^{n}, \hat{X}^{n}\right)
$$

여기서 기대값은 코드북의 무작위 선택과 $X^{n}$에 대한 것입니다.

고정된 코드북 $\mathcal{C}$와 $\epsilon>0$의 선택에 대해, 우리는 $x^{n} \in \mathcal{X}^{n}$ 시퀀스를 두 범주로 나눕니다.

- $x^{n}$ 시퀀스 중 왜곡이 일반적인 코드워드 $\hat{X}^{n}(w)$가 $x^{n}$과 존재합니다 [즉, $d\left(x^{n}, \hat{x}^{n}(w)\right)<D+\epsilon$ ]. 이러한 시퀀스의 총 확률은 최대 1이므로, 이러한 시퀀스는 기대 왜곡에 최대 $D+\epsilon$을 기여합니다.
- $x^{n}$ 시퀀스 중 왜곡이 일반적인 코드워드 $\hat{X}^{n}(w)$가 $x^{n}$과 존재하지 않습니다. 이러한 시퀀스의 총 확률을 $P_{e}$라고 합니다. 개별 시퀀스의 왜곡은 $d_{\max }$로 제한되므로, 이러한 시퀀스는 기대 왜곡에 최대 $P_{e} d_{\max }$를 기여합니다.

따라서 총 왜곡을 다음과 같이 제한할 수 있습니다.

$$
E d\left(X^{n}, \hat{X}^{n}\left(X^{n}\right)\right) \leq D+\epsilon+P_{e} d_{\max }
$$
<!-- Page 348 -->
적절한 $\epsilon$을 선택하면 $P_{e}$가 충분히 작을 경우 $D+\delta$보다 작게 만들 수 있습니다. 따라서 $P_{e}$가 작다는 것을 보이면 기대 왜곡은 $D$에 가까워지며 정리가 증명됩니다.

$P_{e}$의 계산: 임의의 코드북 $\mathcal{C}$와 임의로 선택된 소스 시퀀스에 대해, 소스 시퀀스와 왜곡이 전형적인(distortion typical) 코드워드가 존재하지 않을 확률을 경계해야 합니다. $J(\mathcal{C})$를 $\mathcal{C}$의 적어도 하나의 코드워드가 $x^{n}$과 왜곡이 전형적인 소스 시퀀스 $x^{n}$의 집합이라고 하면,

$$
P_{e}=\sum_{\mathcal{C}} P(\mathcal{C}) \sum_{x^{n}: x^{n} \notin J(\mathcal{C})} p\left(x^{n}\right)
$$

이는 코드로 잘 표현되지 않는 모든 시퀀스의 확률이며, 임의로 선택된 코드에 대해 평균화된 것입니다. 합산 순서를 변경함으로써, 이는 $p\left(x^{n}\right)$에 대해 평균화된, 시퀀스 $x^{n}$을 잘 표현하지 못하는 코드북을 선택할 확률로 해석할 수도 있습니다. 따라서,

$$
P_{e}=\sum_{x^{n}} p\left(x^{n}\right) \sum_{\mathcal{C}: x^{n} \notin J(\mathcal{C})} p(\mathcal{C})
$$

다음과 같이 정의합시다.

$$
K\left(x^{n}, \hat{x}^{n}\right)= \begin{cases}1 & \text { if }\left(x^{n}, \hat{x}^{n}\right) \in A_{d, \epsilon}^{(n)} \\ 0 & \text { if }\left(x^{n}, \hat{x}^{n}\right) \notin A_{d, \epsilon}^{(n)}\end{cases}
$$

단일 임의 선택 코드워드 $\hat{X}^{n}$이 고정된 $x^{n}$을 잘 표현하지 못할 확률은 다음과 같습니다.

$$
\operatorname{Pr}\left(\left(x^{n}, \hat{X}^{n}\right) \notin A_{d, \epsilon}^{(n)}\right)=\operatorname{Pr}\left(K\left(x^{n}, \hat{X}^{n}\right)=0\right)=1-\sum_{\hat{x}^{n}} p\left(\hat{x}^{n}\right) K\left(x^{n}, \hat{x}^{n}\right)
$$

따라서 $p\left(x^{n}\right)$에 대해 평균화된 $2^{n R}$개의 독립적으로 선택된 코드워드가 $x^{n}$을 표현하지 못할 확률은 다음과 같습니다.

$$
\begin{aligned}
P_{e} & =\sum_{x^{n}} p\left(x^{n}\right) \sum_{\mathcal{C}: x^{n} \notin J(\mathcal{C})} p(\mathcal{C}) \\
& =\sum_{x^{n}} p\left(x^{n}\right)\left[1-\sum_{\hat{x}^{n}} p\left(\hat{x}^{n}\right) K\left(x^{n}, \hat{x}^{n}\right)\right]^{2^{n R}}
\end{aligned}
$$
<!-- Page 349 -->
이제 괄호 안의 합을 제한하기 위해 렘마 10.5.2를 사용합니다. 렘마 10.5.2로부터 다음이 도출됩니다.

$$
\sum_{\hat{x}^{n}} p\left(\hat{x}^{n}\right) K\left(x^{n}, \hat{x}^{n}\right) \geq \sum_{\hat{x}^{n}} p\left(\hat{x}^{n} \mid x^{n}\right) 2^{-n(I(X ; \hat{X})+3 \epsilon)} K\left(x^{n}, \hat{x}^{n}\right)
$$

따라서

$$
P_{e} \leq \sum_{x^{n}} p\left(x^{n}\right)\left(1-2^{-n(I(X ; \hat{X})+3 \epsilon)} \sum_{\hat{x}^{n}} p\left(\hat{x}^{n} \mid x^{n}\right) K\left(x^{n}, \hat{x}^{n}\right)\right)^{2^{n R}}
$$

이제 (10.98)의 우변 항을 제한하기 위해 렘마 10.5.3을 사용하고 다음을 얻습니다.

$$
\begin{aligned}
& \left(1-2^{-n(I(X ; \hat{X})+3 \epsilon)} \sum_{\hat{x}^{n}} p\left(\hat{x}^{n} \mid x^{n}\right) K\left(x^{n}, \hat{x}^{n}\right)\right)^{2^{n R}} \\
& \quad \leq 1-\sum_{\hat{x}^{n}} p\left(\hat{x}^{n} \mid x^{n}\right) K\left(x^{n}, \hat{x}^{n}\right)+e^{-\left(2^{-n(I(X ; \hat{X})+3 \epsilon)} 2^{n R}\right)}
\end{aligned}
$$

이 부등식을 (10.98)에 대입하면 다음을 얻습니다.

$$
P_{e} \leq 1-\sum_{x^{n}} \sum_{\hat{x}^{n}} p\left(x^{n}\right) p\left(\hat{x}^{n} \mid x^{n}\right) K\left(x^{n}, \hat{x}^{n}\right)+e^{-2^{-n(I(X ; \hat{X})+3 \epsilon)} 2^{n R}}
$$

경계의 마지막 항은 다음과 같습니다.

$$
e^{-2^{n(R-I(X ; \hat{X})-3 \epsilon)}}
$$

이는 $R>I(X ; \hat{X})+3 \epsilon$이면 $n$에 따라 지수적으로 빠르게 0으로 수렴합니다. 따라서 $p(\hat{x} \mid x)$를 속도 왜곡 함수에서 최소값을 달성하는 조건부 분포로 선택하면, $R>R(D)$는 $R>I(X ; \hat{X})$를 의미하며, 마지막 항이 0으로 수렴하도록 $\epsilon$을 충분히 작게 선택할 수 있습니다.

(10.100)의 첫 두 항은 쌍의 시퀀스가 왜곡 전형적이지 않을 확률을 결합 분포 $p\left(x^{n}, \hat{x}^{n}\right)$ 하에서 제공합니다. 따라서 렘마 10.5.1을 사용하면 다음을 얻습니다.

$$
1-\sum_{x^{n}} \sum_{\hat{x}^{n}} p\left(x^{n}, \hat{x}^{n}\right) K\left(x^{n}, \hat{x}^{n}\right)=\operatorname{Pr}\left(\left(X^{n}, \hat{X}^{n}\right) \notin A_{d, \epsilon}^{(n)}\right)<\epsilon
$$
<!-- Page 350 -->
$n$이 충분히 크다고 가정하면. 따라서, 적절한 $\epsilon$과 $n$의 선택을 통해 $P_{e}$를 원하는 만큼 작게 만들 수 있습니다.

그러므로, 임의의 $\delta>0$에 대해, 랜덤하게 선택된 모든 블록 길이 $n$의 코드 $R$에 대해 평균 왜곡이 $D+\delta$보다 작도록 하는 $\epsilon$과 $n$이 존재합니다. 따라서, 이 속도와 블록 길이를 가지며 평균 왜곡이 $D+\delta$보다 작은 코드가 적어도 하나 존재해야 합니다. $\delta$는 임의적이므로, $R>R(D)$이면 $(R, D)$가 달성 가능하다는 것을 보였습니다.

우리는 평균 왜곡이 $D$에 가깝고 속도가 $R(D)$에 가까운 속도 왜곡 코드의 존재를 증명했습니다. 속도 왜곡 정리의 랜덤 코딩 증명과 채널 코딩 정리의 랜덤 코딩 증명 사이의 유사점은 이제 명확합니다. 우리는 문제에 대한 기하학적 통찰력을 제공하는 가우시안 예제를 고려함으로써 이러한 유사점을 더 탐구할 것입니다. 채널 코딩은 구 충전이고 속도 왜곡 코딩은 구 커버링이라는 것이 밝혀졌습니다.

가우시안 채널에 대한 채널 코딩. 가우시안 채널 $Y_{i}=X_{i}+Z_{i}$를 고려합니다. 여기서 $Z_{i}$는 i.i.d. $\sim \mathcal{N}(0, N)$이고 전송된 코드워드의 심볼당 전력에 대한 전력 제약 $P$가 있습니다. $n$번의 전송 시퀀스를 고려합니다. 전력 제약은 전송된 시퀀스가 $\mathcal{R}^{n}$에서 반지름 $\sqrt{n P}$인 구 내에 있음을 의미합니다. 코딩 문제는 이 구 내에서 $2^{n R}$개의 시퀀스 집합을 찾는 것과 동등하며, 이들 중 어느 하나가 다른 하나로 잘못 인식될 확률은 작습니다. 즉, 각 시퀀스 주위의 반지름 $\sqrt{n N}$인 구는 거의 겹치지 않습니다. 이는 반지름 $\sqrt{n N}$인 구로 반지름 $\sqrt{n(P+N)}$인 구를 채우는 것에 해당합니다. 가장 많이 채울 수 있는 구의 수는 부피의 비율, 또는 동등하게 반지름 비율의 $n$제곱이 될 것으로 예상할 수 있습니다. 따라서, 효율적으로 전송할 수 있는 코드워드의 수 $M$이 있다면,

$$
M \leq \frac{(\sqrt{n(P+N)})^{n}}{(\sqrt{n N})^{n}}=\left(\frac{P+N}{N}\right)^{\frac{n}{2}}
$$

채널 코딩 정리의 결과는 $n$이 클 때 이를 효율적으로 수행할 수 있음을 보여줍니다. 약

$$
2^{n C}=\left(\frac{P+N}{N}\right)^{\frac{n}{2}}
$$

개의 코드워드를 찾을 수 있으며, 이 코드워드 주변의 노이즈 구는 거의 겹치지 않습니다 (교집합의 총 부피는 임의로 작게 만들 수 있습니다).
<!-- Page 351 -->
가우시안 소스에 대한 레이트 왜곡. 분산이 $\sigma^{2}$인 가우시안 소스를 고려하십시오. 왜곡 $D$를 갖는 이 소스에 대한 $\left(2^{n R}, n\right)$ 레이트 왜곡 코드는 $\mathcal{R}^{n}$의 $2^{n R}$개의 시퀀스 집합으로, 길이 $n$인 대부분의 소스 시퀀스(반지름 $\sqrt{n \sigma^{2}}$인 구 내에 있는 모든 시퀀스)가 일부 코딩워드로부터 $\sqrt{n D}$ 거리 내에 있는 것을 의미합니다. 다시 말해, 구 채우기 논증에 의해 필요한 최소 코딩워드 수는 다음과 같음이 명확합니다.

$$
2^{n R(D)}=\left(\frac{\sigma^{2}}{D}\right)^{\frac{n}{2}}
$$

레이트 왜곡 정리는 이 최소 레이트가 점근적으로 달성 가능하다는 것을 보여줍니다(즉, 확률이 임의로 작은 집합을 제외한 공간을 덮는 반지름 $\sqrt{n D}$인 구들의 모음이 존재한다는 것을 의미합니다).

위의 기하학적 논증은 채널 전송을 위한 좋은 코드를 레이트 왜곡을 위한 좋은 코드로 변환할 수 있게 합니다. 두 경우 모두, 본질적인 아이디어는 소스 시퀀스의 공간을 채우는 것입니다. 채널 전송에서는 코딩워드 간의 큰 최소 거리를 갖는 코딩워드의 가장 큰 집합을 찾고자 하는 반면, 레이트 왜곡에서는 전체 공간을 덮는 코딩워드의 가장 작은 집합을 찾고자 합니다. 만약 하나에 대한 구 채우기 한계를 충족하는 집합이 있다면, 다른 하나에 대한 구 채우기 한계도 충족할 것입니다. 가우시안의 경우, 적절한 분산을 갖는 가우시안으로 코딩워드를 선택하는 것이 레이트 왜곡과 채널 코딩 모두에 대해 점근적으로 최적입니다.

# 10.6 강한 전형적 시퀀스와 레이트 왜곡

섹션 10.5에서는 평균 왜곡이 $D$에 가까운 레이트 $R(D)$의 레이트 왜곡 코드의 존재를 증명했습니다. 사실, 평균 왜곡이 $D$에 가까울 뿐만 아니라, 왜곡이 $D+\delta$보다 큰 총 확률도 0에 가깝습니다. 이 증명은 섹션 10.5의 증명과 유사합니다. 주요 차이점은 약한 전형적 시퀀스 대신 강한 전형적 시퀀스를 사용한다는 것입니다. 이를 통해 (10.94)에서 임의로 선택된 코딩워드로 잘 표현되지 않는 전형적 소스 시퀀스의 확률에 대한 상한을 제공할 수 있습니다. 이제 강한 전형성을 기반으로 한 대안적인 증명을 개략적으로 설명하며, 이는 레이트 왜곡 정리에 대한 더 강력하고 직관적인 접근 방식을 제공할 것입니다.

강한 전형성을 정의하고 두 시퀀스가 공동으로 전형적일 확률을 제한하는 기본 정리를 인용하는 것으로 시작하겠습니다. 강한 전형성의 속성은 Berger [53]에 의해 소개되었으며
<!-- Page 352 -->
Csiszár와 Körner의 책 [149]에서 자세히 탐구되었습니다. 강한 전형성(11장에서와 같이)을 정의하고 기본적인 보조정리(보조정리 10.6.2)를 명시하겠습니다.

정의 분포 $p(x)$에 대한 시퀀스 $x^{n} \in \mathcal{X}^{n}$는 다음과 같을 때 $\epsilon$-강한 전형성이라고 합니다.

1. $p(a)>0$인 모든 $a \in \mathcal{X}$에 대해 다음이 성립합니다.

$$
\left|\frac{1}{n} N\left(a \mid x^{n}\right)-p(a)\right|<\frac{\epsilon}{|\mathcal{X}|}
$$

2. $p(a)=0$인 모든 $a \in \mathcal{X}$에 대해 $N\left(a \mid x^{n}\right)=0$.
$N\left(a \mid x^{n}\right)$는 시퀀스 $x^{n}$에서 기호 $a$의 출현 횟수입니다.

$x^{n}$이 강한 전형성인 시퀀스 $x^{n} \in \mathcal{X}^{n}$의 집합을 강한 전형성 집합이라고 하며, 문맥에서 확률 변수가 이해되는 경우 $A_{\epsilon}^{*(n)}(X)$ 또는 $A_{\epsilon}^{*(n)}$으로 표시됩니다.

정의 분포 $p(x, y)$에 대한 시퀀스 쌍 $\left(x^{n}, y^{n}\right) \in \mathcal{X}^{n} \times \mathcal{Y}^{n}$는 다음과 같을 때 $\epsilon$ 강한 전형성이라고 합니다.

1. $p(a, b)>0$인 모든 $(a, b) \in \mathcal{X} \times \mathcal{Y}$에 대해 다음이 성립합니다.

$$
\left|\frac{1}{n} N\left(a, b \mid x^{n}, y^{n}\right)-p(a, b)\right|<\frac{\epsilon}{|\mathcal{X}||\mathcal{Y}|}
$$

2. $p(a, b)=0$인 모든 $(a, b) \in \mathcal{X} \times \mathcal{Y}$에 대해 $N\left(a, b \mid x^{n}, y^{n}\right)=0$.
$N\left(a, b \mid x^{n}, y^{n}\right)$는 시퀀스 쌍 $\left(x^{n}, y^{n}\right)$에서 쌍 $(a, b)$의 출현 횟수입니다.

$\left(x^{n}, y^{n}\right)$이 강한 전형성인 시퀀스 $\left(x^{n}, y^{n}\right) \in \mathcal{X}^{n} \times \mathcal{Y}^{n}$의 집합을 강한 전형성 집합이라고 하며, $A_{\epsilon}^{*(n)}(X, Y)$ 또는 $A_{\epsilon}^{*(n)}$으로 표시됩니다. 정의에 따라 $\left(x^{n}, y^{n}\right) \in A_{\epsilon}^{*(n)}(X, Y)$이면 $x^{n} \in A_{\epsilon}^{*(n)}(X)$입니다. 큰 수의 법칙의 강한 법칙으로부터 다음 보조정리는 즉각적입니다.

보조정리 10.6.1 $\left(X_{i}, Y_{i}\right)$가 i.i.d. $\sim p(x, y)$로 추출되었다고 가정합니다. 그러면 $n \rightarrow \infty$일 때 $\operatorname{Pr}\left(A_{\epsilon}^{*(n)}\right)$ $\rightarrow 1$입니다.

독립적으로 추출된 시퀀스가 공동으로 강한 전형성으로 간주될 확률을 제한하는 하나의 기본 결과를 사용할 것입니다.
<!-- Page 353 -->
주어진 시퀀스와 함께 사용됩니다. 정리 7.6.1은 $X^{n}$과 $Y^{n}$을 독립적으로 선택하면, 그들이 약하게 공동으로 전형적일 확률이 $\approx 2^{-n I(X ; Y)}$임을 보여줍니다. 다음 보조정리는 이 결과를 강하게 전형적인 시퀀스로 확장합니다. 이는 임의로 선택된 시퀀스가 고정된 전형적인 $x^{n}$과 공동으로 전형적일 확률에 대한 하한을 제공한다는 점에서 이전 결과보다 더 강력합니다.

보조정리 10.6.2 $Y_{1}, Y_{2}, \ldots, Y_{n}$은 i.i.d. $\sim p(y)$로부터 추출됩니다. $x^{n} \in A_{\epsilon}^{*(n)}(X)$에 대해, $\left(x^{n}, Y^{n}\right) \in A_{\epsilon}^{*(n)}$일 확률은 다음과 같이 제한됩니다.

$$
2^{-n\left(I(X ; Y)+\epsilon_{1}\right)} \leq \operatorname{Pr}\left(\left(x^{n}, Y^{n}\right) \in A_{\epsilon}^{*(n)}\right) \leq 2^{-n\left(I(X ; Y)-\epsilon_{1}\right)}
$$

여기서 $\epsilon_{1}$은 $\epsilon \rightarrow 0$ 및 $n \rightarrow \infty$일 때 0으로 갑니다.
증명: 이 보조정리를 증명하지 않고, 대신 장 끝의 문제 10.16에서 증명을 개략적으로 설명하겠습니다. 본질적으로 증명은 조건부 전형 집합의 크기에 대한 하한을 찾는 것을 포함합니다.

이제 속도 왜곡 함수(rate distortion function)의 달성 가능성으로 직접 진행하겠습니다. 주요 아이디어를 설명하기 위해 개요만 제공하겠습니다. 코드북의 구성 및 인코딩 및 디코딩은 섹션 10.5의 증명과 유사합니다.

증명: $p(\hat{x} \mid x)$를 고정합니다. $p(\hat{x})=\sum_{x} p(x) p(\hat{x} \mid x)$를 계산합니다. $\epsilon>0$을 고정합니다. 나중에 예상 왜곡이 $D+\delta$보다 작도록 $\epsilon$을 적절하게 선택할 것입니다.

코드북 생성: $2^{n R}$개의 시퀀스 $\hat{X}^{n}$으로 구성된 속도 왜곡 코드북 $\mathcal{C}$를 생성합니다. 이 시퀀스들은 i.i.d. $\sim \prod_{i} p\left(\hat{x}_{i}\right)$로부터 추출됩니다. 시퀀스를 $\hat{X}^{n}(1), \ldots, \hat{X}^{n}\left(2^{n R}\right)$로 나타냅니다.

인코딩: 시퀀스 $X^{n}$이 주어졌을 때, $\left(X^{n}, \hat{X}^{n}(w)\right) \in A_{\epsilon}^{*(n)}$ (강한 공동 전형 집합)인 $w$가 존재하면 $w$로 인덱싱합니다. 그러한 $w$가 여러 개 있으면, 사전순으로 첫 번째 것을 보냅니다. 그러한 $w$가 없으면 $w=1$로 설정합니다.

디코딩: 복제된 시퀀스를 $\hat{X}^{n}(w)$로 설정합니다.
왜곡 계산: 섹션 10.5의 증명과 마찬가지로, 코드북의 무작위 선택에 대한 예상 왜곡을 다음과 같이 계산합니다.

$$
\begin{aligned}
D & =E_{X^{n}, \mathcal{C}} d\left(X^{n}, \hat{X}^{n}\right) \\
& =E_{\mathcal{C}} \sum_{x^{n}} p\left(x^{n}\right) d\left(x^{n}, \hat{X}^{n}\left(x^{n}\right)\right) \\
& =\sum_{x^{n}} p\left(x^{n}\right) E_{\mathcal{C}} d\left(x^{n}, \hat{X}^{n}\right)
\end{aligned}
$$
<!-- Page 354 -->

그림 10.8. 속도 왜곡 이론에서의 소스 시퀀스 분류.
여기서 기댓값은 코드북의 무작위 선택에 대한 것입니다. 고정된 코드북 $\mathcal{C}$에 대해, 그림 10.8과 같이 시퀀스 $x^{n} \in \mathcal{X}^{n}$를 세 가지 범주로 나눕니다.

- 비정형 시퀀스 $x^{n} \notin A_{\epsilon}^{*(n)}$. $n$을 충분히 크게 선택하면 이러한 시퀀스의 총 확률을 $\epsilon$보다 작게 만들 수 있습니다. 임의의 두 시퀀스 간의 개별 왜곡은 $d_{\max }$로 제한되므로, 비정형 시퀀스는 기댓값 왜곡에 최대 $\epsilon d_{\max }$만큼 기여할 수 있습니다.
- 정형 시퀀스 $x^{n} \in A_{\epsilon}^{*(n)}$이며, $x^{n}$과 공동으로 정형인 코드워드 $\hat{X}^{n}(w)$가 존재합니다. 이 경우, 소스 시퀀스와 코드워드가 강하게 공동으로 정형이므로, 왜곡이 결합 분포의 함수로서 연속적이라는 사실은 이들이 왜곡 정형이기도 함을 보장합니다. 따라서 이러한 $x^{n}$과 해당 코드워드 간의 왜곡은 $D+\epsilon d_{\max }$로 제한되며, 이러한 시퀀스의 총 확률은 최대 1이므로, 이 시퀀스는 기댓값 왜곡에 최대 $D+\epsilon d_{\max }$만큼 기여합니다.
- 정형 시퀀스 $x^{n} \in A_{\epsilon}^{*(n)}$이며, $x^{n}$과 공동으로 정형인 코드워드 $\hat{X}^{n}$이 존재하지 않습니다. 이러한 시퀀스의 총 확률을 $P_{e}$라고 합시다. 개별 시퀀스의 왜곡은 $d_{\max }$로 제한되므로, 이러한 시퀀스는 기댓값 왜곡에 최대 $P_{e} d_{\max }$만큼 기여합니다.
<!-- Page 355 -->
첫 번째와 세 번째 범주의 시퀀스는 이 속도 왜곡 코드로 잘 표현되지 않을 수 있는 시퀀스입니다. 충분히 큰 $n$에 대해 첫 번째 범주의 시퀀스 확률은 $\epsilon$보다 작습니다. 마지막 범주의 확률은 $P_{e}$이며, 이는 작게 만들 수 있음을 보일 것입니다. 이는 잘 표현되지 않는 시퀀스의 총 확률이 작다는 것을 증명할 것입니다. 차례로, 이를 사용하여 평균 왜곡이 $D$에 가깝다는 것을 보여줍니다.

$P_{e}$의 계산: 주어진 시퀀스 $X^{n}$과 공동으로 전형적인(jointly typical) 코드워드가 없는 확률을 제한해야 합니다. 공동 AEP로부터, $X^{n}$과 임의의 $\hat{X}^{n}$이 공동으로 전형적일 확률은 $\doteq 2^{-n I(X ; \hat{X})}$임을 압니다. 따라서 공동으로 전형적인 $\hat{X}^{n}(w)$의 기대값은 $2^{n R} 2^{-n I(X ; \hat{X})}$이며, 이는 $R>I(X ; \hat{X})$이면 지수적으로 큽니다.

그러나 이것만으로는 $P_{e} \rightarrow 0$임을 보이기에는 충분하지 않습니다. $X^{n}$과 공동으로 전형적인 코드워드가 없는 확률이 0으로 간다는 것을 보여야 합니다. 공동으로 전형적인 코드워드의 기대값이 지수적으로 크다는 사실이 높은 확률로 적어도 하나가 존재함을 보장하지는 않습니다. (10.94)에서와 마찬가지로 오류 확률을 다음과 같이 확장할 수 있습니다.

$$
P_{e}=\sum_{x^{n} \in A_{\epsilon}^{*(n)}} p\left(x^{n}\right)\left[1-\operatorname{Pr}\left(\left(x^{n}, \hat{X}^{n}\right) \in A_{\epsilon}^{*(n)}\right)\right]^{2^{n R}}
$$

Lemma 10.6.2로부터 다음을 얻습니다.

$$
\operatorname{Pr}\left(\left(x^{n}, \hat{X}^{n}\right) \in A_{\epsilon}^{*(n)}\right) \geq 2^{-n\left(I(X ; \hat{X})+\epsilon_{1}\right)}
$$

이를 (10.112)에 대입하고 $(1-x)^{n} \leq e^{-n x}$ 부등식을 사용하면 다음을 얻습니다.

$$
P_{e} \leq e^{-\left(2^{n R} 2^{-n\left(I(X ; \hat{X})+\epsilon_{1}\right)}\right)}
$$

이는 $R>I(X ; \hat{X})+\epsilon_{1}$이면 $n \rightarrow \infty$일 때 0으로 갑니다. 따라서 적절한 $\epsilon$과 $n$의 선택에 대해 모든 잘못 표현된 시퀀스의 총 확률을 원하는 만큼 작게 만들 수 있습니다. 기대 왜곡이 $D$에 가까울 뿐만 아니라, 확률이 1로 가는 상태에서 주어진 시퀀스와의 왜곡이 $D+\delta$보다 작은 코드워드를 찾을 것입니다.

# 10.7 속도 왜곡 함수의 특성화

정보 속도 왜곡 함수를 다음과 같이 정의했습니다.

$$
R(D)=\min _{q(\hat{x} \mid x): \sum_{(x, \hat{x})} p(x) q(\hat{x} \mid x) d(x, \hat{x}) \leq D} I(X ; \hat{X})
$$
<!-- Page 356 -->
이는 기대 왜곡 제약 조건을 만족하는 모든 $q(\hat{x} \mid x)$에 대한 조건부 분포 $p(x) q(\hat{x} \mid x)$에 대한 최소화입니다. 이는 모든 $x$에 대해 $\sum_{\hat{x}} q(\hat{x} \mid x)=1$ 및 $\sum q(\hat{x} \mid x) p(x) d(x, \hat{x}) \leq D$를 만족하는 모든 $q(\hat{x} \mid x) \geq 0$의 볼록 집합에 대한 볼록 함수를 최소화하는 표준 문제입니다.

라그랑주 승수법을 사용하여 해를 구할 수 있습니다. 함수를 다음과 같이 설정합니다.

$$
\begin{aligned}
J(q)= & \sum_{x} \sum_{\hat{x}} p(x) q(\hat{x} \mid x) \log \frac{q(\hat{x} \mid x)}{\sum_{x} p(x) q(\hat{x} \mid x)} \\
& +\lambda \sum_{x} \sum_{\hat{x}} p(x) q(\hat{x} \mid x) d(x, \hat{x}) \\
& +\sum_{x} v(x) \sum_{\hat{x}} q(\hat{x} \mid x)
\end{aligned}
$$

여기서 마지막 항은 $q(\hat{x} \mid x)$가 조건부 확률 질량 함수라는 제약 조건에 해당합니다. $q(\hat{x} \mid x)$에 의해 유도된 $\hat{X}$에 대한 분포를 $q(\hat{x})=\sum_{x} p(x) q(\hat{x} \mid x)$라고 하면 $J(q)$를 다음과 같이 다시 쓸 수 있습니다.

$$
\begin{aligned}
J(q)= & \sum_{x} \sum_{\hat{x}} p(x) q(\hat{x} \mid x) \log \frac{q(\hat{x} \mid x)}{q(\hat{x})} \\
& +\lambda \sum_{x} \sum_{\hat{x}} p(x) q(\hat{x} \mid x) d(x, \hat{x}) \\
& +\sum_{x} v(x) \sum_{\hat{x}} q(\hat{x} \mid x)
\end{aligned}
$$

$q(\hat{x} \mid x)$에 대해 미분하면 다음과 같습니다.

$$
\begin{aligned}
\frac{\partial J}{\partial q(\hat{x} \mid x)}= & p(x) \log \frac{q(\hat{x} \mid x)}{q(\hat{x})}+p(x)-\sum_{x^{\prime}} p\left(x^{\prime}\right) q\left(\hat{x} \mid x^{\prime}\right) \frac{1}{q(\hat{x})} p(x) \\
& +\lambda p(x) d(x, \hat{x})+v(x)=0
\end{aligned}
$$

$\log \mu(x)=v(x) / p(x)$로 설정하면 다음과 같습니다.

$$
p(x)\left[\log \frac{q(\hat{x} \mid x)}{q(\hat{x})}+\lambda d(x, \hat{x})+\log \mu(x)\right]=0
$$
<!-- Page 357 -->
또는

$$
q(\hat{x} \mid x)=\frac{q(\hat{x}) e^{-\lambda d(x, \hat{x})}}{\mu(x)}
$$

$\sum_{\hat{x}} q(\hat{x} \mid x)=1$이므로, 다음이 성립해야 합니다.

$$
\mu(x)=\sum_{\hat{x}} q(\hat{x}) e^{-\lambda d(x, \hat{x})}
$$

또는

$$
q(\hat{x} \mid x)=\frac{q(\hat{x}) e^{-\lambda d(x, \hat{x})}}{\sum_{\hat{x}} q(\hat{x}) e^{-\lambda d(x, \hat{x})}}
$$

이를 $p(x)$를 곱하고 모든 $x$에 대해 합하면 다음을 얻습니다.

$$
q(\hat{x})=q(\hat{x}) \sum_{x} \frac{p(x) e^{-\lambda d(x, \hat{x})}}{\sum_{\hat{x}^{\prime}} q\left(\hat{x}^{\prime}\right) e^{-\lambda d\left(x, \hat{x}^{\prime}\right)}}
$$

만약 $q(\hat{x})>0$이면, 양변을 $q(\hat{x})$로 나눌 수 있으며 다음을 얻습니다.

$$
\sum_{x} \frac{p(x) e^{-\lambda d(x, \hat{x})}}{\sum_{\hat{x}^{\prime}} q\left(\hat{x}^{\prime}\right) e^{-\lambda d\left(x, \hat{x}^{\prime}\right)}}=1
$$

모든 $\hat{x} \in \hat{\mathcal{X}}$에 대해. 이 $|\hat{\mathcal{X}}|$개의 방정식과 왜곡을 정의하는 방정식을 결합하여 $\lambda$와 $|\hat{\mathcal{X}}|$개의 미지수 $q(\hat{x})$를 계산할 수 있습니다. 이를 (10.124)와 함께 사용하여 최적의 조건부 분포를 찾을 수 있습니다.

위의 분석은 $q(\hat{x})$가 제약이 없는 경우(즉, 모든 $\hat{x}$에 대해 $q(\hat{x})>0$) 유효합니다. 부등식 조건 $q(\hat{x})>0$은 Kuhn-Tucker 조건에 의해 다루어지며, 이는 다음으로 축소됩니다.

$$
\begin{aligned}
\frac{\partial J}{\partial q(\hat{x} \mid x)} & =0 \text { if } q(\hat{x} \mid x)>0 \\
& \geq 0 \text { if } q(\hat{x} \mid x)=0
\end{aligned}
$$

미분 값을 대입하면 최소값에 대한 조건은 다음과 같습니다.

$$
\begin{aligned}
\sum_{x} \frac{p(x) e^{-\lambda d(x, \hat{x})}}{\sum_{\hat{x}^{\prime}} q\left(\hat{x}^{\prime}\right) e^{-\lambda d\left(x, \hat{x}^{\prime}\right)}} & =1 & & \text { if } q(\hat{x})>0 \\
& \leq 1 & & \text { if } q(\hat{x})=0
\end{aligned}
$$
<!-- Page 358 -->
이러한 특성화를 통해 주어진 $q(\hat{x})$가 최소화 문제의 해인지 확인할 수 있습니다. 그러나 이러한 방정식으로부터 최적 출력 분포를 구하는 것은 쉽지 않습니다. 다음 섹션에서는 속도 왜곡 함수를 계산하기 위한 반복 알고리즘을 제공합니다. 이 알고리즘은 두 확률 밀도 집합 간의 최소 상대 엔트로피 거리를 찾는 일반 알고리즘의 특수한 경우입니다.

# 10.8 채널 용량 및 속도 왜곡 함수 계산

다음 문제를 고려해 보십시오. 그림 10.9와 같이 $\mathcal{R}^{n}$에 있는 두 개의 볼록 집합 $A$와 $B$가 주어졌을 때, 두 집합 간의 최소 거리를 찾고 싶습니다.

$$
d_{\min }=\min _{a \in A, b \in B} d(a, b)
$$

여기서 $d(a, b)$는 $a$와 $b$ 사이의 유클리드 거리입니다. 이를 수행하기 위한 직관적으로 명확한 알고리즘은 임의의 점 $x \in A$를 선택하고, 가장 가까운 $y \in B$를 찾는 것입니다. 그런 다음 이 $y$를 고정하고 가장 가까운 점을 $A$에서 찾습니다. 이 과정을 반복하면 각 단계에서 거리가 감소한다는 것을 분명히 알 수 있습니다. 두 집합 간의 최소 거리에 수렴합니까? Csiszár와 Tusnády [155]는 집합이 볼록하고 거리가 특정 조건을 만족하면 이 교대 최소화 알고리즘이 실제로 최소값에 수렴한다는 것을 보여주었습니다. 특히, 집합이 확률 분포의 집합이고 거리 측정이 상대 엔트로피인 경우, 알고리즘은 두 분포 집합 간의 최소 상대 엔트로피에 수렴합니다.

그림 10.9. 볼록 집합 간의 거리.
<!-- Page 359 -->
이 알고리즘을 레이트 왜곡에 적용하기 위해, 두 집합 간의 상대 엔트로피의 최소값으로 레이트 왜곡 함수를 다시 작성해야 합니다. 간단한 보조정리로 시작합니다. 이 보조정리의 한 형태는 정리 13.1.1에서 다시 나타나며, 채널 용량 보편 데이터 압축의 이중성을 확립합니다.

보조정리 10.8.1 주어진 결합 분포 $p(x) p(y \mid x)$가 있다고 가정합니다. 그러면 상대 엔트로피 $D(p(x) p(y \mid x) \| p(x)$ $r(y))$를 최소화하는 분포 $r(y)$는 $p(y \mid x)$에 해당하는 주변 분포 $r^{*}(y)$입니다:

$$
D(p(x) p(y \mid x) \| p(x) r^{*}(y))=\min _{r(y)} D(p(x) p(y \mid x) \| p(x) r(y))
$$

여기서 $r^{*}(y)=\sum_{x} p(x) p(y \mid x)$입니다. 또한,

$$
\max _{r(x \mid y)} \sum_{x, y} p(x) p(y \mid x) \log \frac{r(x \mid y)}{p(x)}=\sum_{x, y} p(x) p(y \mid x) \log \frac{r^{*}(x \mid y)}{p(x)}
$$

여기서

$$
r^{*}(x \mid y)=\frac{p(x) p(y \mid x)}{\sum_{x} p(x) p(y \mid x)}
$$

# 증명

$$
\begin{aligned}
& D(p(x) p(y \mid x) \| p(x) r(y))-D(p(x) p(y \mid x) \| p(x) r^{*}(y)) \\
& =\sum_{x, y} p(x) p(y \mid x) \log \frac{p(x) p(y \mid x)}{p(x) r(y)} \\
& \quad-\sum_{x, y} p(x) p(y \mid x) \log \frac{p(x) p(y \mid x)}{p(x) r^{*}(y)} \\
& =\sum_{x, y} p(x) p(y \mid x) \log \frac{r^{*}(y)}{r(y)} \\
& =\sum_{y} r^{*}(y) \log \frac{r^{*}(y)}{r(y)} \\
& =D\left(r^{*} \| r\right) \\
& \geq 0
\end{aligned}
$$
<!-- Page 360 -->
보조정리의 두 번째 부분에 대한 증명은 연습 문제로 남겨둡니다.
이 보조정리를 사용하여 속도 왜곡 함수의 정의에서 최소화를 이중 최소화로 다시 작성할 수 있습니다.

$$
R(D)=\min _{r(\hat{x}) q(\hat{x}|x): \sum p(x) q(\hat{x}|x) d(x, \hat{x}) \leq D} \sum_{x} \sum_{\hat{x}} p(x) q(\hat{x}|x) \log \frac{q(\hat{x}|x)}{r(\hat{x})}
$$

만약 $A$가 왜곡 제약 조건을 만족하는 모든 결합 분포의 집합이고, $B$가 임의의 $r(\hat{x})$를 갖는 곱 분포 $p(x) r(\hat{x})$의 집합이라면, 다음과 같이 쓸 수 있습니다.

$$
R(D)=\min _{q \in B} \min _{p \in A} D(p \| q)
$$

이제 우리는 이 경우 Blahut-Arimoto 알고리즘이라고 불리는 교대 최소화 과정을 적용합니다. 우리는 $\lambda$의 선택과 초기 출력 분포 $r(\hat{x})$로 시작하여 왜곡 제약 조건 하에서 상호 정보를 최소화하는 $q(\hat{x}|x)$를 계산합니다. 이 최소화를 위해 라그랑주 승수법을 사용하여 다음과 같이 얻을 수 있습니다.

$$
q(\hat{x}|x)=\frac{r(\hat{x}) e^{-\lambda d(x, \hat{x})}}{\sum_{\hat{x}} r(\hat{x}) e^{-\lambda d(x, \hat{x})}}
$$

이 조건부 분포 $q(\hat{x}|x)$에 대해, 보조정리 10.8.1에 따라 상호 정보를 최소화하는 출력 분포 $r(\hat{x})$를 계산합니다.

$$
r(\hat{x})=\sum_{x} p(x) q(\hat{x}|x)
$$

이 출력 분포를 다음 반복의 시작점으로 사용합니다. 반복의 각 단계에서 $q(\cdot \mid \cdot)$에 대해 최소화한 다음 $r(\cdot)$에 대해 최소화하면 (10.140)의 우변이 감소합니다. 따라서 극한이 존재하며, Csiszár [139]에 의해 극한이 $R(D)$임이 입증되었습니다. 여기서 $D$와 $R(D)$의 값은 $\lambda$에 따라 달라집니다. 따라서 $\lambda$를 적절하게 선택하면 $R(D)$ 곡선을 얻을 수 있습니다.

채널 용량 계산에도 유사한 절차를 적용할 수 있습니다. 다시 채널 용량의 정의를 다시 작성합니다.

$$
C=\max _{r(x)} I(X ; Y)=\max _{r(x)} \sum_{x} \sum_{y} r(x) p(y \mid x) \log \frac{r(x) p(y \mid x)}{r(x) \sum_{x^{\prime}} r\left(x^{\prime}\right) p\left(y \mid x^{\prime}\right)}
$$
<!-- Page 361 -->
이중 최대화로서, Lemma 10.8.1을 사용하면 다음과 같습니다.

$$
C=\max _{q(x \mid y)} \max _{r(x)} \sum_{x} \sum_{y} r(x) p(y \mid x) \log \frac{q(x \mid y)}{r(x)}
$$

이 경우, Csiszár-Tusnady 알고리즘은 최대화 교대로 수행하는 알고리즘이 됩니다. 즉, 최대화 분포 $r(x)$에 대한 추측으로 시작하여 최적의 조건부 분포를 찾습니다. Lemma 10.8.1에 따르면 다음과 같습니다.

$$
q(x \mid y)=\frac{r(x) p(y \mid x)}{\sum_{x} r(x) p(y \mid x)}
$$

이 조건부 분포에 대해, 라그랑주 승수법을 사용하여 제약 조건이 있는 최대화 문제를 풀어 최적의 입력 분포 $r(x)$를 찾습니다. 최적 입력 분포는 다음과 같습니다.

$$
r(x)=\frac{\prod_{y}(q(x \mid y))^{p(y \mid x)}}{\sum_{x} \prod_{y}(q(x \mid y))^{p(y \mid x)}}
$$

이를 다음 반복의 기초로 사용할 수 있습니다.
채널 용량 및 비율 왜곡 함수 계산을 위한 이러한 알고리즘은 Blahut [65] 및 Arimoto [25]에 의해 확립되었으며, 비율 왜곡 계산에 대한 수렴은 Csiszár [139]에 의해 증명되었습니다. Csiszár와 Tusnady의 교대 최소화 절차는 EM 알고리즘 [166] 및 주식 시장의 로그 최적 포트폴리오를 찾는 알고리즘 [123]을 포함하여 다른 많은 상황에도 적용될 수 있습니다.

# 요약

비율 왜곡. 소스 $X \sim p(x)$ 및 왜곡 측정값 $d(x, \hat{x})$에 대한 비율 왜곡 함수는 다음과 같습니다.

$$
R(D)=\min _{p(\hat{x} \mid x): \sum_{(x, \hat{x})}} \min _{p(x) p(\hat{x} \mid x) d(x, \hat{x}) \leq D} I(X ; \hat{X})
$$

여기서 최소화는 결합 분포 $p(x, \hat{x})=p(x) p(\hat{x} \mid x)$가 기대 왜곡 제약 조건을 만족하는 모든 조건부 분포 $p(\hat{x} \mid x)$에 대해 수행됩니다.
<!-- Page 362 -->
비율 왜곡 정리. 만약 $R>R(D)$이면, 코드워드 수가 $\left|\hat{X}^{n}(\cdot)\right| \leq 2^{n R}$이고 $E d\left(X^{n}, \hat{X}^{n}\left(X^{n}\right)\right) \rightarrow D$인 코드들의 수열 $\hat{X}^{n}\left(X^{n}\right)$이 존재합니다. 만약 $R<R(D)$이면, 그러한 코드는 존재하지 않습니다.

베르누이 소스. 햄밍 왜곡을 갖는 베르누이 소스의 경우,

$$
R(D)=H(p)-H(D)
$$

가우시안 소스. 제곱 오차 왜곡을 갖는 가우시안 소스의 경우,

$$
R(D)=\frac{1}{2} \log \frac{\sigma^{2}}{D}
$$

소스-채널 분리. 비율 왜곡 $R(D)$를 갖는 소스는 채널 용량 $C$를 갖는 채널을 통해 전송되고 왜곡 $D$로 복구될 수 있습니다. 이는 $R(D)<C$일 때 그리고 그때에만 가능합니다.

다변수 가우시안 소스. 유클리드 평균 제곱 오차 왜곡을 갖는 다변수 정규 벡터의 비율 왜곡 함수는 고유값에 대한 역수 워터필링으로 주어집니다.

# 문제

10.1 단일 가우시안 랜덤 변수의 1비트 양자화. $X \sim \mathcal{N}\left(0, \sigma^{2}\right)$이고 왜곡 척도가 제곱 오차라고 가정합니다. 여기서는 블록 설명을 허용하지 않습니다. 1비트 양자화에 대한 최적 재현점은 $\pm \sqrt{\frac{2}{\pi}} \sigma$임을 보이고, 1비트 양자화에 대한 기대 왜곡은 $\frac{\pi-2}{\pi} \sigma^{2}$임을 보이십시오. 이를 $R=1$에 대한 왜곡률 경계 $D=\sigma^{2} 2^{-2 R}$와 비교하십시오.
10.2 무한 왜곡을 갖는 비율 왜곡 함수. $X \sim$ Bernoulli $\left(\frac{1}{2}\right)$이고 왜곡이 다음과 같을 때 비율 왜곡 함수 $R(D)=\min I(X ; \hat{X})$를 구하십시오.

$$
d(x, \hat{x})= \begin{cases}0, & x=\hat{x} \\ 1, & x=1, \hat{x}=0 \\ \infty, & x=0, \hat{x}=1\end{cases}
$$
<!-- Page 363 -->
10.3 이진 소스에 대한 비대칭 왜곡률 왜곡률. $p(\hat{x} \mid x)$를 고정하고 $I(X ; \hat{X})$ 및 $D$를 평가하십시오.

$$
\begin{aligned}
& X \sim \text { Bernoulli }\left(\frac{1}{2}\right) \\
& d(x, \hat{x})=\left[\begin{array}{ll}
0 & a \\
b & 0
\end{array}\right]
\end{aligned}
$$

(률 왜곡 함수는 닫힌 형태로 표현될 수 없습니다.)
10.4 $R(D)$의 속성. 분포 $p_{1}, p_{2}, \ldots, p_{m}$를 가진 이산 소스 $X \in \mathcal{X}=$ $\{1,2, \ldots, m\}$와 왜곡 측정값 $d(i, j)$를 고려하십시오. 이 소스와 왜곡 측정값에 대한 률 왜곡 함수를 $R(D)$라고 합시다. 새로운 왜곡 측정값 $d^{\prime}(i, j)=d(i, j)-w_{i}$와 해당 률 왜곡 함수 $R^{\prime}(D)$를 고려하십시오. $R^{\prime}(D)=R(D+\bar{w})$임을 보이고, 여기서 $\bar{w}=\sum p_{i} w_{i}$이며, 이를 사용하여 $\min _{\hat{x}} d(i, \hat{x})=0$을 가정하는 데 본질적인 일반성 손실이 없음을 보이십시오 (즉, 각 $x \in \mathcal{X}$에 대해, 0 왜곡으로 소스를 재현하는 기호 $\hat{x}$가 하나 존재합니다). 이 결과는 Pinkston [420]에 의한 것입니다.
10.5 햄밍 왜곡을 가진 균일 소스에 대한 률 왜곡률. 집합 $\{1,2, \ldots, m\}$에 균일하게 분포된 소스 $X$를 고려하십시오. 햄밍 왜곡을 가진 이 소스에 대한 률 왜곡 함수를 찾으십시오. 즉,

$$
d(x, \hat{x})= \begin{cases}0 & \text { if } x=\hat{x} \\ 1 & \text { if } x \neq \hat{x}\end{cases}
$$

10.6 률 왜곡 함수에 대한 Shannon 하한. 다음 속성을 만족하는 왜곡 측정값 $d(x, \hat{x})$를 가진 소스 $X$를 고려하십시오: 왜곡 행렬의 모든 열은 집합 $\left\{d_{1}, d_{2}, \ldots, d_{m}\right\}$의 순열입니다. 함수

$$
\phi(D)=\max _{\mathbf{p}: \sum_{i=1}^{m} p_{i} d_{i} \leq D} H(\mathbf{p})
$$

를 정의하십시오. 률 왜곡 함수 [485]에 대한 Shannon 하한은 다음 단계를 통해 증명됩니다:
(a) $\phi(D)$가 $D$의 오목 함수임을 보이십시오.
(b) $E d(X, \hat{X}) \leq D$인 경우 $I(X ; \hat{X})$에 대한 다음 부등식 시리즈를 정당화하십시오.

$$
I(X ; \hat{X})=H(X)-H(X \mid \hat{X})
$$
<!-- Page 364 -->
$$
\begin{aligned}
& =H(X)-\sum_{\hat{x}} p(\hat{x}) H(X \mid \hat{X}=\hat{x}) \\
& \geq H(X)-\sum_{\hat{x}} p(\hat{x}) \phi\left(D_{\hat{x}}\right) \\
& \geq H(X)-\phi\left(\sum_{\hat{x}} p(\hat{x}) D_{\hat{x}}\right) \\
& \geq H(X)-\phi(D)
\end{aligned}
$$

여기서 $D_{\hat{x}}=\sum_{x} p(x \mid \hat{x}) d(x, \hat{x})$입니다.
(c) 다음을 주장하십시오.

$$
R(D) \geq H(X)-\phi(D)
$$

이는 rate distortion function에 대한 Shannon 하한입니다.
(d) 추가로, 소스가 균일 분포를 가지며 왜곡 행렬의 행들이 서로 순열 관계에 있다고 가정하면, $R(D)=H(X)-\phi(D)$ (즉, 하한이 타이트함)입니다.
10.7 삭제 왜곡. $X \sim$ Bernoulli $\left(\frac{1}{2}\right)$를 고려하고, 왜곡 측도가 다음 행렬로 주어진다고 가정합니다.

$$
d(x, \hat{x})=\left[\begin{array}{ccc}
0 & 1 & \infty \\
\infty & 1 & 0
\end{array}\right]
$$

이 소스에 대한 rate distortion function을 계산하십시오. 이 소스에 대해 rate distortion function의 임의의 값을 달성할 수 있는 간단한 방안을 제안할 수 있습니까?
10.8 제곱 오차 왜곡에 대한 rate distortion function의 하한. 평균이 0이고 분산이 $\sigma^{2}$인 연속 확률 변수 $X$와 제곱 오차 왜곡의 경우, 다음을 보이십시오.

$$
h(X)-\frac{1}{2} \log (2 \pi e D) \leq R(D) \leq \frac{1}{2} \log \frac{\sigma^{2}}{D}
$$

상한에 대해서는 다음의 결합 분포를 고려하십시오.
<!-- Page 365 -->
$$
\hat{X}=\frac{\sigma^{2}-D}{\sigma^{2}}(X+Z)
$$

가우시안 확률 변수가 동일한 분산을 가진 다른 확률 변수보다 기술하기 더 어렵습니까, 아니면 더 쉽습니까?
10.9 최적 속도 왜곡 코드의 속성. $R \approx R(D)$인 좋은 $(R, D)$ 속도 왜곡 코드는 소스 $X^{n}$과 표현 $\hat{X}^{n}$의 관계에 엄격한 제약을 줍니다. 등식 조건을 고려하여 부등식 (10.58-10.71)의 연쇄를 검토하고 좋은 코드의 속성으로 해석하십시오. 예를 들어, (10.59)의 등식은 $\hat{X}^{n}$이 $X^{n}$의 결정론적 함수임을 의미합니다.
10.10 속도 왜곡. $X$가 $\mathcal{X}=\{1,2, \ldots, 2 m\}$에서 균일하고

$$
d(x, \hat{x})= \begin{cases}1 & \text { for } x-\hat{x} \text { odd } \\ 0 & \text { for } x-\hat{x} \text { even }\end{cases}
$$

이며, 여기서 $\hat{X}$는 $\hat{\mathcal{X}}=\{1,2, \ldots, 2 m\}$에서 정의됩니다. 속도 왜곡 함수 $R(D)$를 찾고 검증하십시오. (주장에 Shannon 하한을 사용하는 것이 좋습니다.)
10.11 하한.

$$
X \sim \frac{e^{-x^{4}}}{\int_{-\infty}^{\infty} e^{-x^{4}} d x}
$$

이고

$$
\frac{\int x^{4} e^{-x^{4}} d x}{\int e^{-x^{4}} d x}=c
$$

라고 정의합니다. $E X^{4} \leq a$인 모든 밀도에 대해 $\max h(X)$를 $g(a)$로 정의합니다. $R(D)$를 위 밀도와 왜곡 기준 $d(x, \hat{x})=(x-\hat{x})^{4}$를 가진 $X$의 속도 왜곡 함수라고 할 때, $R(D) \geq g(c)-g(D)$임을 보이십시오.
<!-- Page 366 -->
10.12 왜곡 행렬에 열 추가하기. 확률 질량 함수 $p(x)$와 왜곡 함수 $d(x, \hat{x}), x \in \mathcal{X}, \hat{x} \in \hat{\mathcal{X}}$를 갖는 i.i.d. 프로세스에 대한 rate distortion function을 $R(D)$라고 합시다. 이제 $\hat{\mathcal{X}}$에 연관된 왜곡 $d\left(x, \hat{x}_{0}\right), x \in \mathcal{X}$를 갖는 새로운 복원 심볼 $\hat{x}_{0}$를 추가한다고 가정합니다. 이것이 $R(D)$를 증가시키는지 또는 감소시키는지, 그리고 그 이유는 무엇입니까?
10.13 단순화. $\mathcal{X}=\{1,2,3,4\}, \hat{\mathcal{X}}=\{1,2,3,4\}$, $p(i)=\frac{1}{4}, i=1,2,3,4$이고 $X_{1}, X_{2}, \ldots$는 i.i.d. $\sim p(x)$라고 가정합니다. 왜곡 행렬 $d(x, \hat{x})$는 다음과 같이 주어집니다.

|  | 1 | 2 | 3 | 4 |
| :--: | :--: | :--: | :--: | :--: |
| 1 | 0 | 0 | 1 | 1 |
| 2 | 0 | 0 | 1 | 1 |
| 3 | 1 | 1 | 0 | 0 |
| 4 | 1 | 1 | 0 | 0 |

(a) 제로 왜곡으로 프로세스를 설명하는 데 필요한 rate인 $R(0)$를 찾으십시오.
(b) rate distortion function $R(D)$를 찾으십시오. 알파벳 $\mathcal{X}$와 $\hat{\mathcal{X}}$에는 문제의 범위를 축소할 수 있는 관련 없는 구별이 있습니다.
(c) 비균일 분포 $p(i)=p_{i}$, $i=1,2,3,4$가 있다고 가정합니다. $R(D)$는 무엇입니까?
10.14 두 독립 소스의 rate distortion. 두 독립 소스를 개별적으로 압축하는 것보다 동시에 더 잘 압축할 수 있습니까? 다음 문제는 이 질문을 다룹니다. $\left\{X_{i}\right\}$는 왜곡 $d(x, \hat{x})$와 rate distortion function $R_{X}(D)$를 갖는 i.i.d. $\sim p(x)$라고 합시다. 마찬가지로, $\left\{Y_{i}\right\}$는 왜곡 $d(y, \hat{y})$와 rate distortion function $R_{Y}(D)$를 갖는 i.i.d. $\sim p(y)$라고 합시다. 이제 왜곡 $E d(X, \hat{X}) \leq D_{1}$ 및 $E d(Y, \hat{Y}) \leq D_{2}$를 조건으로 프로세스 $\left\{\left(X_{i}, Y_{i}\right)\right\}$를 설명하려고 합니다. 따라서 다음을 만족하는 rate $R_{X, Y}\left(D_{1}, D_{2}\right)$가 있습니다.

$$
R_{X, Y}\left(D_{1}, D_{2}\right)=\min _{p(\hat{x}, \hat{y} \mid x, y): E d(X, \hat{X}) \leq D_{1}, E d(Y, \hat{Y}) \leq D_{2}} I(X, Y ; \hat{X}, \hat{Y})
$$

이제 $\left\{X_{i}\right\}$ 프로세스와 $\left\{Y_{i}\right\}$ 프로세스가 서로 독립이라고 가정합니다.
(a) 다음을 보여주십시오.

$$
R_{X, Y}\left(D_{1}, D_{2}\right) \geq R_{X}\left(D_{1}\right)+R_{Y}\left(D_{2}\right)
$$
<!-- Page 367 -->
(b) 등식이 성립합니까?

이제 질문에 답하십시오.
10.15 왜곡률 함수. 다음을 정의합니다.

$$
D(R)=\min _{p(\hat{x} \mid x): I(X ; \hat{X}) \leq R} E d(X, \hat{X})
$$

왜곡률 함수입니다.
(a) $D(R)$은 $R$에 대해 증가 함수입니까, 감소 함수입니까?
(b) $D(R)$은 $R$에 대해 볼록 함수입니까, 오목 함수입니까?
(c) 왜곡률 함수에 대한 역방향: 이제 $D(R)$에 초점을 맞춰 역방향을 증명하고자 합니다. $X_{1}, X_{2}, \ldots, X_{n}$을 i.i.d. $\sim p(x)$로 가정합니다. $\left(2^{n R}, n\right)$ 속도 왜곡 코드를 $X^{n} \rightarrow i\left(X^{n}\right) \rightarrow \hat{X}^{n}\left(i\left(X^{n}\right)\right)$와 같이 주어진다고 가정하고, 여기서 $i\left(X^{n}\right) \in 2^{n R}$이며, 결과 왜곡이 $D=E d\left(X^{n}, \hat{X}^{n}\right.$ $\left.\left(i\left(X^{n}\right)\right)\right)$라고 가정합니다. 우리는 $D \geq D(R)$임을 보여야 합니다. 증명의 다음 단계에 대한 이유를 제시하십시오.

$$
\begin{aligned}
D & =E d\left(X^{n}, \hat{X}^{n}\left(i\left(X^{n}\right)\right)\right) \\
& \stackrel{(a)}{=} E \frac{1}{n} \sum_{i=1}^{n} d\left(X_{i}, \hat{X}_{i}\right) \\
& \stackrel{(b)}{=} \frac{1}{n} \sum_{i=1}^{n} \operatorname{Ed}\left(X_{i}, \hat{X}_{i}\right) \\
& \stackrel{(c)}{\geq} \frac{1}{n} \sum_{i=1}^{n} D\left(I\left(X_{i} ; \hat{X}_{i}\right)\right) \\
& \stackrel{(d)}{\geq} D\left(\frac{1}{n} \sum_{i=1}^{n} I\left(X_{i} ; \hat{X}_{i}\right)\right) \\
& \stackrel{(e)}{\geq} D\left(\frac{1}{n} I\left(X^{n} ; \hat{X}^{n}\right)\right) \\
& \stackrel{(f)}{\geq} D(R) .
\end{aligned}
$$

10.16 조건부로 전형적인 시퀀스의 확률. 7장에서는 독립적으로 추출된 두 시퀀스 $X^{n}$과 $Y^{n}$이 약하게 공동으로 전형적인 확률을 계산했습니다. 그러나 속도 왜곡 정리를 증명하기 위해서는 다음의 경우에 이 확률을 계산해야 합니다.
<!-- Page 368 -->
하나의 시퀀스는 고정되고 다른 하나는 무작위입니다. 약한 전형성(weak typicality) 기법은 조건부 전형 집합(conditionally typical set)의 평균 크기만 계산할 수 있게 해줍니다. 반면에 강한 전형성(strong typicality)의 아이디어를 사용하면 모든 전형적인 $x^{n}$ 시퀀스에 대해 작동하는 더 강력한 경계를 제공합니다. 우리는 $\operatorname{Pr}\left\{\left(x^{n}, Y^{n}\right) \in A_{\epsilon}^{*(n)}\right\} \approx 2^{-n I(X ; Y)}$ 임을 증명하는 개요를 제시합니다. 이 접근 방식은 Berger [53]에 의해 소개되었으며 Csiszár와 Körner [149]의 책에서 완전히 개발되었습니다.
$\left(X_{i}, Y_{i}\right)$를 i.i.d. $\sim p(x, y)$로 추출한다고 가정합니다. $X$와 $Y$의 주변 분포는 각각 $p(x)$와 $p(y)$입니다.
(a) $A_{\epsilon}^{*(n)}$를 $X$에 대한 강한 전형 집합(strongly typical set)이라고 할 때, 다음을 증명하십시오.

$$
\left|A_{\epsilon}^{*(n)}\right| \doteq 2^{n H(X)}
$$

(힌트: 정리 11.1.1 및 11.1.3.)
(b) 시퀀스 쌍 $\left(x^{n}, y^{n}\right)$의 결합 유형(joint type)은 시퀀스 쌍에서 $\left(x_{i}, y_{i}\right)=(a, b)$인 비율입니다.

$$
p_{x^{n}, y^{n}}(a, b)=\frac{1}{n} N\left(a, b \mid x^{n}, y^{n}\right)=\frac{1}{n} \sum_{i=1}^{n} I\left(x_{i}=a, y_{i}=b\right)
$$

$x^{n}$에 대한 시퀀스 $y^{n}$의 조건부 유형(conditional type)은 시퀀스 쌍에서 $\mathcal{X}$의 각 요소와 함께 $\mathcal{Y}$의 특정 요소가 발생한 비율을 나타내는 확률 행렬입니다. 구체적으로, 조건부 유형 $V_{y^{n} \mid x^{n}}(b \mid a)$는 다음과 같이 정의됩니다.

$$
V_{y^{n} \mid x^{n}}(b \mid a)=\frac{N\left(a, b \mid x^{n}, y^{n}\right)}{N\left(a \mid x^{n}\right)}
$$

조건부 유형의 개수가 $(n+$ 1) ${ }^{|\mathcal{X}||\mathcal{Y}|}$로 제한됨을 증명하십시오.
(c) 시퀀스 $x^{n}$에 대한 조건부 유형 $V$를 갖는 시퀀스 $y^{n} \in \mathcal{Y}^{n}$의 집합을 조건부 유형 클래스 $T_{V}\left(x^{n}\right)$라고 합니다. 다음을 증명하십시오.

$$
\frac{1}{(n+1)^{|\mathcal{X}||\mathcal{Y}|}} 2^{n H(Y \mid X)} \leq\left|T_{V}\left(x^{n}\right)\right| \leq 2^{n H(Y \mid X)}
$$

(d) 시퀀스 $y^{n} \in \mathcal{Y}^{n}$는 조건부 유형이 $V$에 가까운 경우, 조건부 분포 $V(\cdot \mid \cdot)$에 대해 시퀀스 $x^{n}$와 관련하여 $\epsilon$-강한 조건부 전형( $\epsilon$-strongly conditionally typical)이라고 합니다. 조건부 유형은 다음 두 조건을 만족해야 합니다.
<!-- Page 369 -->
(i) 모든 $(a, b) \in \mathcal{X} \times \mathcal{Y}$ 에 대해 $V(b \mid a)>0$ 이면,

$$
\frac{1}{n}\left|N\left(a, b \mid x^{n}, y^{n}\right)-V(b \mid a) N\left(a \mid x^{n}\right)\right| \leq \frac{\epsilon}{|\mathcal{Y}|+1}
$$

(ii) $V(b \mid a)=0$ 인 모든 $(a, b)$ 에 대해 $N\left(a, b \mid x^{n}, y^{n}\right)=0$ 입니다. 이러한 시퀀스들의 집합을 조건부 전형 집합(conditionally typical set)이라고 하며 $A_{\epsilon}^{*(n)}\left(Y \mid x^{n}\right)$ 으로 표기합니다. 주어진 $x^{n} \in \mathcal{X}^{n}$ 에 대해 조건부 전형인 시퀀스 $y^{n}$ 의 개수가 다음으로 제한됨을 보이십시오.

$$
\begin{aligned}
& \frac{1}{(n+1)|\mathcal{X}||\mathcal{Y}|} 2^{n\left(H(Y \mid X)-\epsilon_{1}\right)} \leq\left|A_{\epsilon}^{*(n)}\left(Y \mid x^{n}\right)\right| \\
& \leq(n+1)|\mathcal{X}||\mathcal{Y}| 2^{n\left(H(Y \mid X)+\epsilon_{1}\right)}
\end{aligned}
$$

여기서 $\epsilon \rightarrow 0$ 일 때 $\epsilon_{1} \rightarrow 0$ 입니다.
(e) 결합 분포 $p(x, y)$ 를 갖는 랜덤 변수 쌍 $(X, Y)$ 에 대해, $\epsilon$-강 전형 집합( $\epsilon$-strongly typical set) $A_{\epsilon}^{*(n)}$ 은 다음을 만족하는 시퀀스 $\left(x^{n}, y^{n}\right) \in \mathcal{X}^{n} \times \mathcal{Y}^{n}$ 의 집합입니다.
(i)

$$
\left|\frac{1}{n} N\left(a, b \mid x^{n}, y^{n}\right)-p(a, b)\right|<\frac{\epsilon}{|\mathcal{X}||\mathcal{Y}|}
$$

$p(a, b)>0$ 인 모든 쌍 $(a, b) \in \mathcal{X} \times \mathcal{Y}$ 에 대해.
(ii) $p(a, b)=0$ 인 모든 $(a, b) \in \mathcal{X} \times \mathcal{Y}$ 에 대해 $N\left(a, b \mid x^{n}, y^{n}\right)=0$ 입니다.
$\epsilon$-강 결합 전형 시퀀스( $\epsilon$-strongly jointly typical sequences)의 집합을 $\epsilon$-강 결합 전형 집합이라고 하며 $A_{\epsilon}^{*(n)}(X, Y)$ 로 표기합니다. $(X, Y)$ 를 i.i.d. $\sim p(x, y)$ 로 추출합니다. 적어도 하나의 쌍 $\left(x^{n}, y^{n}\right) \in A_{\epsilon}^{*(n)}(X, Y)$ 가 존재하는 모든 $x^{n}$ 에 대해, $\left(x^{n}, y^{n}\right) \in A_{\epsilon}^{*(n)}$ 을 만족하는 시퀀스 $y^{n}$ 의 집합은 다음을 만족합니다.

$$
\begin{aligned}
& \frac{1}{(n+1)^{|\mathcal{X}||\mathcal{Y}|}} 2^{n(H(Y \mid X)-\delta(\epsilon))} \leq\left|\left\{y^{n}:\left(x^{n}, y^{n}\right) \in A_{\epsilon}^{*(n)}\right\}\right| \\
& \leq(n+1)^{|\mathcal{X}||\mathcal{Y}|} 2^{n(H(Y \mid X)+\delta(\epsilon))}
\end{aligned}
$$

여기서 $\epsilon \rightarrow 0$ 일 때 $\delta(\epsilon) \rightarrow 0$ 입니다. 특히, 다음으로 쓸 수 있습니다.

$$
2^{n(H(Y \mid X)-\epsilon_{2})} \leq\left|\left\{y^{n}:\left(x^{n}, y^{n}\right) \in A_{\epsilon}^{*(n)}\right\}\right| \leq 2^{n(H(Y \mid X)+\epsilon_{2})}
$$
<!-- Page 370 -->
$\epsilon_{2}$를 $\epsilon$과 $n$을 적절히 선택함으로써 임의로 작게 만들 수 있음을 보여줍니다.
(f) $Y_{1}, Y_{2}, \ldots, Y_{n}$이 i.i.d. $\sim \prod p\left(y_{i}\right)$로부터 추출되었다고 가정합니다. $x^{n} \in A_{\epsilon}^{\text {*(n) }}$에 대해, $\left(x^{n}, Y^{n}\right) \in A_{\epsilon}^{\text {*(n) }}$일 확률은 다음과 같이 제한됩니다.

$$
2^{-n\left(I(X ; Y)+\epsilon_{3}\right)} \leq \operatorname{Pr}\left(\left(x^{n}, Y^{n}\right) \in A_{\epsilon}^{\text {*(n) }}\right) \leq 2^{-n\left(I(X ; Y)-\epsilon_{3}\right)}
$$

여기서 $\epsilon_{3}$는 $\epsilon \rightarrow 0$이고 $n \rightarrow \infty$일 때 0으로 갑니다.
10.17 왜곡을 포함한 송수신 분리 정리. $V_{1}$, $V_{2}, \ldots, V_{n}$은 이산 무기억 채널의 $n$개 입력 심볼 시퀀스 $X^{n}$으로 인코딩되는 유한 알파벳 i.i.d. 소스입니다. 채널의 출력 $Y^{n}$은 복원 알파벳 $\hat{V}^{n}=g\left(Y^{n}\right)$으로 매핑됩니다. $D=E d\left(V^{n}, \hat{V}^{n}\right)=\frac{1}{n} \sum_{i=1}^{n}$ $E d\left(V_{i}, \hat{V}_{i}\right)$를 이 결합된 소스 및 채널 코딩 스킴에 의해 달성된 평균 왜곡이라고 합니다.

(a) $C>R(D)$이면, 여기서 $R(D)$는 $V$에 대한 비율 왜곡 함수이며, 평균 왜곡을 $D$에 임의로 가깝게 달성하는 인코더와 디코더를 찾을 수 있음을 보여주십시오.
(b) (역) 평균 왜곡이 $D$와 같으면, 채널의 용량 $C$는 $R(D)$보다 커야 함을 보여주십시오.
10.18 비율 왜곡. $d(x, \hat{x})$를 왜곡 함수라고 합니다. 소스 $X \sim p(x)$가 있습니다. $R(D)$를 관련 비율 왜곡 함수라고 합니다.
(a) $R(D)$를 사용하여 $\tilde{R}(D)$를 찾으십시오. 여기서 $\tilde{R}(D)$는 왜곡 $\tilde{d}(x, \hat{x})=$ $d(x, \hat{x})+a$와 관련된 비율 왜곡 함수이며, 여기서 $a$는 양의 상수입니다. (두 함수는 같지 않습니다.)
(b) 이제 $d(x, \hat{x}) \geq 0$이 모든 $x, \hat{x}$에 대해 성립한다고 가정하고 새로운 왜곡 함수 $d^{*}(x, \hat{x})=b d(x, \hat{x})$를 정의합니다. 여기서 $b$는 0 이상인 어떤 숫자입니다. 관련 비율 왜곡 함수 $R^{*}(D)$를 $R(D)$를 사용하여 찾으십시오.
(c) $X \sim N\left(0, \sigma^{2}\right)$이고 $d(x, \hat{x})=5(x-\hat{x})^{2}+3$이라고 가정합니다. $R(D)$는 무엇입니까?
<!-- Page 371 -->
10.19 두 개의 제약 조건이 있는 속도-왜곡. $X_{i}$가 iid $\sim p(x)$라고 가정합니다. 두 개의 왜곡 함수 $d_{1}(x, \hat{x})$ 및 $d_{2}(x, \hat{x})$가 주어졌습니다. 속도 $R$에서 $X^{n}$을 설명하고 왜곡 $E d_{1}\left(X^{n}, \hat{X}_{1}^{n}\right) \leq D_{1}$ 및 $E d_{2}\left(X^{n}, \hat{X}_{2}^{n}\right) \leq D_{2}$로 재구성하고자 합니다.

$$
\begin{gathered}
X^{n} \longrightarrow i\left(X^{n}\right) \longrightarrow\left(\hat{X}_{1}^{n}(i), \hat{X}_{2}^{n}(i)\right) \\
D_{1}=E d_{1}\left(X_{1}^{n}, \hat{X}_{1}^{n}\right) \\
D_{2}=E d_{2}\left(X_{1}^{n}, \hat{X}_{2}^{n}\right)
\end{gathered}
$$

여기서 $i(\cdot)$는 $2^{n R}$개의 값을 가집니다. 속도-왜곡 함수 $R\left(D_{1}, D_{2}\right)$는 무엇입니까?
10.20 속도-왜곡. 표준 속도-왜곡 문제를 고려합니다. $X_{i}$는 i.i.d. $\sim p(x)$이고, $X^{n} \rightarrow i\left(X^{n}\right) \rightarrow \hat{X}^{n},|i(\cdot)|=2^{n R}$입니다. 두 개의 왜곡 기준 $d_{1}(x, \hat{x})$ 및 $d_{2}(x, \hat{x})$를 고려합니다. 모든 $x \in \mathcal{X}, \hat{x} \in \mathcal{\mathcal { X }}$에 대해 $d_{1}(x, \hat{x}) \leq d_{2}(x, \hat{x})$라고 가정합니다. $R_{1}(D)$와 $R_{2}(D)$를 해당 속도-왜곡 함수라고 합니다.
(a) $R_{1}(D)$와 $R_{2}(D)$ 사이의 부등식 관계를 찾으십시오.
(b) 소스 $\left\{X_{i}\right\}$를 최소 속도 $R$로 설명해야 한다고 가정합니다. 이 속도는 $d_{1}\left(X^{n}, \hat{X}_{1}^{n}\right) \leq D$ 및 $d_{2}\left(X^{n}, \hat{X}_{2}^{n}\right) \leq D$를 달성합니다. 따라서,

$$
X^{n} \rightarrow i\left(X^{n}\right) \rightarrow\left\{\begin{array}{l}
\hat{X}_{1}^{n}\left(i\left(X^{n}\right)\right) \\
\hat{X}_{2}^{n}\left(i\left(X^{n}\right)\right)
\end{array}\right.
$$

이고 $|i(\cdot)|=2^{n R}$입니다.
최소 속도 $R$을 찾으십시오.

# 역사적 참고 사항

속도-왜곡의 개념은 Shannon의 원래 논문 [472]에서 소개되었습니다. 그는 1959년 논문 [485]에서 이 개념을 다시 다루었고, 이 논문에서 첫 번째 속도-왜곡 정리를 증명했습니다. 한편, Kolmogorov와 그의 학파는 1956년에 소련에서 속도-왜곡 이론을 개발하기 시작했습니다. 더 일반적인 소스에 대한 속도-왜곡 정리의 더 강력한 버전은 Berger [52]의 포괄적인 책에서 증명되었습니다.

병렬 가우시안 소스에 대한 속도-왜곡 함수의 역수 물 채우기 해법은 McDonald와 Schultheiss에 의해 확립되었습니다.
<!-- Page 372 -->
[381]. 일반적인 i.i.d. 소스 및 임의의 왜곡 측정에 대한 속도 왜곡 함수 계산을 위한 반복 알고리즘은 Blahut [65], Arimoto [25], Csiszár [139]에 의해 설명되었습니다. 이 알고리즘은 Csiszár와 Tusnády [155]에 의한 일반적인 교대 최소화 알고리즘의 특수한 경우입니다.
<!-- Page 373 -->
# 제 11 장

## 정보 이론과 통계학

이제 정보 이론과 통계학 간의 관계를 탐구합니다. 우리는 대규모 편차 이론에서 강력한 기법인 타입 방법(method of types)을 설명하는 것으로 시작합니다. 타입 방법을 사용하여 희귀 사건의 확률을 계산하고 보편적인 소스 코드의 존재를 보여줍니다. 또한 가설 검정 문제를 고려하고 그러한 검사에 대한 최상의 오차 지수(Chernoff-Stein lemma)를 도출합니다. 마지막으로 분포의 매개변수 추정을 다루고 Fisher 정보의 역할을 설명합니다.

### 11.1 타입 방법

이산 확률 변수에 대한 AEP(제 3 장)는 일반적인 시퀀스의 작은 부분 집합에 주의를 집중시킵니다. 타입 방법은 경험적 분포가 동일한 시퀀스를 고려하는 더욱 강력한 절차입니다. 이 제한을 통해 특정 경험적 분포를 가진 시퀀스의 수와 이 집합의 각 시퀀스의 확률에 대한 강력한 경계를 도출할 수 있습니다. 그런 다음 채널 코딩 정리에 대한 강력한 오차 경계를 도출하고 다양한 속도 왜곡 결과를 증명할 수 있습니다. 타입 방법은 Csiszár와 Körner [149]에 의해 완전히 개발되었으며, 그들은 이 관점에서 대부분의 결과를 얻었습니다.

$\mathcal{X}=$ $\left\{a_{1}, a_{2}, \ldots, a_{|\mathcal{X}|}\right\}$ 알파벳에서 $n$개의 기호 시퀀스를 $X_{1}, X_{2}, \ldots, X_{n}$이라고 합시다. 시퀀스 $x_{1}, x_{2}, \ldots, x_{n}$을 나타내기 위해 $x^{n}$과 $\mathbf{x}$를 상호 교환적으로 사용합니다.

정의 시퀀스 $x_{1}, x_{2}, \ldots, x_{n}$의 타입 $P_{\mathbf{x}}$ (또는 경험적 확률 분포)는 각 기호의 발생 비율입니다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 374 -->
$\mathcal{X}$의 심볼 (즉, 모든 $a \in \mathcal{X}$에 대해 $P_{\mathbf{x}}(a)=N(a \mid \mathbf{x}) / n$이며, 여기서 $N(a \mid \mathbf{x})$는 시퀀스 $\mathbf{x} \in \mathcal{X}^{n}$에서 심볼 $a$가 발생하는 횟수입니다).

시퀀스 $\mathbf{x}$의 타입은 $P_{\mathbf{x}}$로 표기됩니다. 이는 $\mathcal{X}$ 상의 확률 질량 함수입니다. (이 장에서는 대문자를 타입과 분포를 나타내는 데 사용할 것입니다. 또한 확률 질량 함수를 의미하기 위해 분포라는 단어를 느슨하게 사용할 것입니다.)

정의 $\mathcal{R}^{m}$에서의 확률 단순체는 $x_{i} \geq 0, \sum_{i=1}^{m} x_{i}=1$인 점 $\mathbf{x}=$ $\left(x_{1}, x_{2}, \ldots, x_{m}\right) \in \mathcal{R}^{m}$의 집합입니다.

확률 단순체는 $m$차원 공간에서 $(m-1)$차원 다양체입니다. $m=3$일 때, 확률 단순체는 $\left\{\left(x_{1}, x_{2}, x_{3}\right): x_{1} \geq 0, x_{2} \geq 0, x_{3} \geq 0, x_{1}+x_{2}+x_{3}=1\right\}$ (그림 11.1)인 점들의 집합입니다. 이는 $\mathcal{R}^{3}$에서의 삼각형 형태의 2차원 평면이므로, 이 장의 후반부에서 확률 단순체를 나타내기 위해 삼각형을 사용할 것입니다.

정의 $\mathcal{P}_{n}$을 분모가 $n$인 타입들의 집합으로 나타냅니다.
예를 들어, $\mathcal{X}=\{0,1\}$이면, 분모가 $n$인 가능한 타입들의 집합은 다음과 같습니다.

$$
\mathcal{P}_{n}=\left\{(P(0), P(1)):\left(\frac{0}{n}, \frac{n}{n}\right),\left(\frac{1}{n}, \frac{n-1}{n}\right), \ldots,\left(\frac{n}{n}, \frac{0}{n}\right)\right\}
$$

정의 $P \in \mathcal{P}_{n}$이면, 길이 $n$이고 타입이 $P$인 시퀀스들의 집합을 $P$의 타입 클래스라고 하며, $T(P)$로 표기합니다:

$$
T(P)=\left\{\mathbf{x} \in \mathcal{X}^{n}: P_{\mathbf{x}}=P\right\}
$$

타입 클래스는 때때로 $P$의 컴포지션 클래스라고도 불립니다.

그림 11.1. $\mathcal{R}^{3}$에서의 확률 단순체.
<!-- Page 375 -->
예제 11.1.1 $\mathcal{X}=\{1,2,3\}$은 삼진수 알파벳입니다. $\mathbf{x}=11321$이라고 합시다. 그러면 $P_{\mathbf{x}}$의 타입은 다음과 같습니다.

$$
P_{\mathbf{x}}(1)=\frac{3}{5}, \quad P_{\mathbf{x}}(2)=\frac{1}{5}, \quad P_{\mathbf{x}}(3)=\frac{1}{5}
$$

$P_{\mathbf{x}}$의 타입 클래스는 세 개의 1, 하나의 2, 하나의 3을 갖는 길이 5의 모든 시퀀스의 집합입니다. 이러한 시퀀스는 20개이며,

$$
T\left(P_{\mathbf{x}}\right)=\{11123,11132,11213, \ldots, 32111\}
$$

$T(P)$의 원소 수는 다음과 같습니다.

$$
|T(P)|=\binom{5}{3,1,1}=\frac{5!}{3!1!1!}=20
$$

타입 방법의 본질적인 힘은 타입의 수가 $n$에 대해 다항식으로 제한된다는 것을 보여주는 다음 정리에 있습니다.

# 정리 11.1.1

$$
\left|\mathcal{P}_{n}\right| \leq(n+1)^{|\mathcal{X}|}
$$

증명: $P_{\mathbf{x}}$를 지정하는 벡터에는 $|\mathcal{X}|$개의 구성 요소가 있습니다. 각 구성 요소의 분자는 $n+1$개의 값만 가질 수 있습니다. 따라서 타입 벡터에 대해 $(n+1)^{|\mathcal{X}|}$개의 선택지가 있습니다. 물론 이러한 선택은 독립적이지 않습니다(예: 마지막 선택은 다른 선택에 의해 고정됩니다). 그러나 이는 우리가 필요로 하는 상한선으로 충분히 좋습니다.

여기서 중요한 점은 길이 $n$의 타입이 다항식 개수만큼만 존재한다는 것입니다. 시퀀스의 수가 $n$에 대해 지수적이므로, 적어도 하나의 타입은 해당 타입 클래스에 지수적으로 많은 시퀀스를 갖게 됩니다. 사실, 가장 큰 타입 클래스는 전체 시퀀스 집합과 동일한 수의 원소를 갖습니다.

이제 시퀀스 $X_{1}, X_{2}, \ldots, X_{n}$이 분포 $Q(x)$에 따라 i.i.d.로 추출된다고 가정합니다. 다음 정리에서 보여주듯이 동일한 타입을 갖는 모든 시퀀스는 동일한 확률을 갖습니다. $Q^{n}\left(x^{n}\right)=\prod_{i=1}^{n} Q\left(x_{i}\right)$는 $Q$와 관련된 곱셈 분포를 나타냅니다.

정리 11.1.2 $X_{1}, X_{2}, \ldots, X_{n}$이 $Q(x)$에 따라 i.i.d.로 추출된다면, $\mathbf{x}$의 확률은 해당 타입에만 의존하며 다음과 같이 주어집니다.

$$
Q^{n}(\mathbf{x})=2^{-n\left(H\left(P_{\mathbf{x}}\right)+D\left(P_{\mathbf{x}} \| Q\right)\right)}
$$
<!-- Page 376 -->
# 증명

$$
\begin{aligned}
Q^{n}(\mathbf{x}) & =\prod_{i=1}^{n} Q\left(x_{i}\right) \\
& =\prod_{a \in \mathcal{X}} Q(a)^{N(a \mid \mathbf{x})} \\
& =\prod_{a \in \mathcal{X}} Q(a)^{n P_{\mathbf{x}}(a)} \\
& =\prod_{a \in \mathcal{X}} 2^{n P_{\mathbf{x}}(a) \log Q(a)} \\
& =\prod_{a \in \mathcal{X}} 2^{n\left(P_{\mathbf{x}}(a) \log Q(a)-P_{\mathbf{x}}(a) \log P_{\mathbf{x}}(a)+P_{\mathbf{x}}(a) \log P_{\mathbf{x}}(a)\right)} \\
& =2^{n \sum_{a \in \mathcal{X}}\left(-P_{\mathbf{x}}(a) \log \frac{P_{\mathbf{x}}(a)}{Q(a)}+P_{\mathbf{x}}(a) \log P_{\mathbf{x}}(a)\right)} \\
& =2^{n\left(-D\left(P_{\mathbf{x}} \| Q\right)-H\left(P_{\mathbf{x}}\right)\right)}
\end{aligned}
$$

따름정리: 만약 $\mathbf{x}$가 $Q$의 타입 클래스에 속한다면,

$$
Q^{n}(\mathbf{x})=2^{-n H(Q)}
$$

증명: 만약 $\mathbf{x} \in T(Q)$이면, $P_{\mathbf{x}}=Q$이므로 (11.14)에 대입할 수 있습니다.
예제 11.1.2: 공정한 주사위가 각 면이 정확히 $n/6$번 나오는 길이 $n$의 특정 시퀀스를 생성할 확률은 $2^{-n H\left(\frac{1}{6}, \frac{1}{6}, \ldots, \frac{1}{6}\right)}=6^{-n}$입니다. 이는 명백합니다. 그러나 만약 주사위가 확률 질량 함수 $\left(\frac{1}{3}, \frac{1}{3}, \frac{1}{6}, \frac{1}{12}, \frac{1}{12}, 0\right)$를 가진다면, 정확히 이러한 빈도를 갖는 특정 시퀀스를 관찰할 확률은 $n$이 12의 배수일 때 정확히 $2^{-n H\left(\frac{1}{3}, \frac{1}{3}, \frac{1}{6}, \frac{1}{12}, \frac{1}{12}, 0\right)}$입니다. 이는 더 흥미롭습니다.

이제 타입 클래스 $T(P)$의 크기에 대한 추정치를 제시합니다.
정리 11.1.3 (타입 클래스 $T(P)$의 크기) 임의의 타입 $P \in \mathcal{P}_{n}$에 대해,

$$
\frac{1}{(n+1)^{|\mathcal{X}|}} 2^{n H(P)} \leq|T(P)| \leq 2^{n H(P)}
$$

증명: $T(P)$의 정확한 크기는 쉽게 계산할 수 있습니다. 이는 간단한 조합론적 문제입니다. 즉, $n P\left(a_{1}\right), n P\left(a_{2}\right), \ldots$를 배열하는 방법의 수입니다.
<!-- Page 377 -->
$n P\left(a_{|\mathcal{X}|}\right)$개의 객체가 순서대로 나열되어 있으며, 이는 다음과 같습니다.

$$
|T(P)|=\binom{n}{n P\left(a_{1}\right), n P\left(a_{2}\right), \ldots, n P\left(a_{|\mathcal{X}|}\right)}
$$

이 값은 다루기 어렵기 때문에, 그 값에 대한 간단한 지수적 경계를 도출합니다.

지수적 경계에 대한 두 가지 대안적 증명을 제안합니다. 첫 번째 증명은 스털링 공식 [208]을 사용하여 팩토리얼 함수를 경계하고, 몇 가지 대수적 조작 후, 정리의 경계를 얻을 수 있습니다. 대안적 증명을 제시합니다. 먼저 상한을 증명합니다. 타입 클래스는 확률이 $\leq 1$이어야 하므로, 다음과 같습니다.

$$
\begin{aligned}
1 & \geq P^{n}(T(P)) \\
& =\sum_{\mathbf{x} \in T(P)} P^{n}(\mathbf{x}) \\
& =\sum_{\mathbf{x} \in T(P)} 2^{-n H(P)} \\
& =|T(P)| 2^{-n H(P)}
\end{aligned}
$$

정리 11.1.2를 사용합니다. 따라서,

$$
|T(P)| \leq 2^{n H(P)}
$$

이제 하한을 구합니다. 먼저 타입 클래스 $T(P)$가 확률 분포 $P$ 하에서 모든 타입 클래스 중에서 가장 높은 확률을 갖는다는 것을 증명합니다.

$$
P^{n}(T(P)) \geq P^{n}(T(\hat{P})) \quad \text { for all } \hat{P} \in \mathcal{P}_{n}
$$

확률의 비율을 하한합니다.

$$
\begin{aligned}
\frac{P^{n}(T(P))}{P^{n}(T(\hat{P}))} & =\frac{|T(P)|}{\mid T(\hat{P})!} \frac{\prod_{a \in \mathcal{X}} P(a)^{n P(a)}}{\prod_{a \in \mathcal{X}} P(a)^{n \hat{P}(a)}} \\
& =\frac{\left.\binom{n P\left(a_{1}\right), n P\left(a_{2}\right), \ldots, n P\left(a_{|\mathcal{X}|}\right)}{n}\right) \prod_{a \in \mathcal{X}} P(a)^{n P(a)}}{\binom{n}{n \hat{P}\left(a_{1}\right), n \hat{P}\left(a_{2}\right), \ldots, n \hat{P}\left(a_{|\mathcal{X}|}\right)}) \prod_{a \in \mathcal{X}} P(a)^{n \hat{P}(a)}} \\
& =\prod_{a \in \mathcal{X}} \frac{(n \hat{P}(a))!}{(n P(a))!} P(a)^{n(P(a)-\hat{P}(a))}
\end{aligned}
$$
<!-- Page 378 -->
이제 간단한 상한(m ≥ n 및 m < n의 경우를 별도로 고려하여 증명하기 쉬움)을 사용하면

$$
\frac{m!}{n!} \geq n^{m-n}
$$

다음과 같이 얻습니다.

$$
\begin{aligned}
\frac{P^{n}(T(P))}{P^{n}(T(\hat{P}))} & \geq \prod_{a \in \mathcal{X}}(n P(a))^{n \hat{P}(a)-n P(a)} P(a)^{n(P(a)-\hat{P}(a))} \\
& =\prod_{a \in \mathcal{X}} n^{n(\hat{P}(a)-P(a))} \\
& =n^{n\left(\sum_{a \in \mathcal{X}} \hat{P}(a)-\sum_{a \in \mathcal{X}} P(a)\right)} \\
& =n^{n(1-1)} \\
& =1
\end{aligned}
$$

따라서 $P^{n}(T(P)) \geq P^{n}(T(\hat{P}))$입니다. 이 결과로부터 하한은 쉽게 도출됩니다. 왜냐하면

$$
\begin{aligned}
1 & =\sum_{Q \in \mathcal{P}_{n}} P^{n}(T(Q)) \\
& \leq \sum_{Q \in \mathcal{P}_{n}} \max _{Q} P^{n}(T(Q)) \\
& =\sum_{Q \in \mathcal{P}_{n}} P^{n}(T(P)) \\
& \leq(n+1)^{|\mathcal{X}|} P^{n}(T(P)) \\
& =(n+1)^{|\mathcal{X}|} \sum_{\mathbf{x} \in T(P)} P^{n}(\mathbf{x}) \\
& =(n+1)^{|\mathcal{X}|} \sum_{\mathbf{x} \in T(P)} 2^{-n H(P)} \\
& =(n+1)^{|\mathcal{X}|}|T(P)| 2^{-n H(P)}
\end{aligned}
$$

여기서 (11.36)은 Theorem 11.1.1에서, (11.38)은 Theorem 11.1.2에서 도출됩니다.

이진 경우에 대해 약간 더 나은 근사치를 제공합니다.
<!-- Page 379 -->
예제 11.1.3 (이진 알파벳) 이 경우, 타입은 시퀀스에 있는 1의 개수로 정의되며, 따라서 타입 클래스의 크기는 $\binom{n}{k}$입니다. 우리는 다음을 보여줍니다.

$$
\frac{1}{n+1} 2^{n H\left(\frac{k}{n}\right)} \leq\binom{ n}{k} \leq 2^{n H\left(\frac{k}{n}\right)}
$$

이러한 경계는 계승 함수에 대한 스털링 근사(정리 17.5.1)를 사용하여 증명할 수 있습니다. 그러나 우리는 아래에 더 직관적인 증명을 제공합니다.

먼저 상한을 증명합니다. 이항 정리에 따르면, 임의의 $p$에 대해

$$
\sum_{k=0}^{n}\binom{n}{k} p^{k}(1-p)^{n-k}=1
$$

합의 모든 항이 $0 \leq p \leq 1$에 대해 양수이므로, 각 항은 1보다 작습니다. $p=k / n$으로 설정하고 $k$번째 항을 취하면 다음과 같습니다.

$$
\begin{aligned}
1 & \geq\binom{ n}{k}\left(\frac{k}{n}\right)^{k}\left(1-\frac{k}{n}\right)^{n-k} \\
& =\binom{n}{k} 2^{k \log \frac{k}{n}+(n-k) \log \frac{n-k}{n}} \\
& =\binom{n}{k} 2^{n\left(\frac{k}{n} \log \frac{k}{n}+\frac{n-k}{n} \log \frac{n-k}{n}\right)} \\
& =\binom{n}{k} 2^{-n H\left(\frac{k}{n}\right)}
\end{aligned}
$$

따라서,

$$
\binom{n}{k} \leq 2^{n H\left(\frac{k}{n}\right)}
$$

하한에 대해, 매개변수 $n$과 $p$를 갖는 이항 분포를 갖는 확률 변수 $S$를Let $S$ be a random variable with a binomial distribution with parameters $n$ and $p$. $S$의 가장 가능성 있는 값은 $S=\langle n p\rangle$입니다. 이는 다음 사실로부터 쉽게 확인할 수 있습니다.

$$
\frac{P(S=i+1)}{P(S=i)}=\frac{n-i}{i+1} \frac{p}{1-p}
$$

그리고 $i<n p$인 경우와 $i>n p$인 경우를 고려합니다. 그러면 이항 합에 $n+1$개의 항이 있으므로,
<!-- Page 380 -->
$$
\begin{aligned}
1 & =\sum_{k=0}^{n}\binom{n}{k} p^{k}(1-p)^{n-k} \leq(n+1) \max _{k}\binom{n}{k} p^{k}(1-p)^{n-k} \\
& =(n+1)\binom{n}{\langle n p\rangle} p^{\langle n p\rangle}(1-p)^{n-\langle n p\rangle}
\end{aligned}
$$

이제 $p=k / n$이라고 하면, 다음을 얻습니다.

$$
1 \leq(n+1)\binom{n}{k}\left(\frac{k}{n}\right)^{k}\left(1-\frac{k}{n}\right)^{n-k}
$$

이는 (11.45)의 논증에 의해 다음의 동치입니다.

$$
\frac{1}{n+1} \leq\binom{ n}{k} 2^{-n H\left(\frac{k}{n}\right)}
$$

또는

$$
\binom{n}{k} \geq \frac{2^{n H\left(\frac{k}{n}\right)}}{n+1}
$$

두 결과를 결합하면 다음을 알 수 있습니다.

$$
\binom{n}{k} \doteq 2^{n H\left(\frac{k}{n}\right)}
$$

$k \neq 0$ 또는 $n$일 때, 더 정확한 경계는 정리 17.5.1에서 찾을 수 있습니다.
정리 11.1.4 (타입 클래스의 확률) 임의의 $P \in \mathcal{P}_{n}$ 및 임의의 분포 $Q$에 대해, $Q^{n}$ 하에서 타입 클래스 $T(P)$의 확률은 지수에서 일차적으로 $2^{-n D(P \| Q)}$입니다. 더 정확하게는,

$$
\frac{1}{(n+1)^{|\mathcal{X}|}} 2^{-n D(P \| Q)} \leq Q^{n}(T(P)) \leq 2^{-n D(P \| Q)}
$$

증명: 다음을 얻습니다.

$$
\begin{aligned}
Q^{n}(T(P)) & =\sum_{\mathbf{x} \in T(P)} Q^{n}(\mathbf{x}) \\
& =\sum_{\mathbf{x} \in T(P)} 2^{-n(D(P \| Q)+H(P))} \\
& =|T(P)| 2^{-n(D(P \| Q)+H(P))}
\end{aligned}
$$
<!-- Page 381 -->
11.1.2절의 정리에 따라, 11.1.3절에서 도출된 $|T(P)|$의 상한을 사용하면 다음과 같습니다.

$$
\frac{1}{(n+1)^{|\mathcal{X}|}} 2^{-n D(P \| Q)} \leq Q^{n}(T(P)) \leq 2^{-n D(P \| Q)}
$$

타입(type)에 관한 기본 정리를 네 개의 방정식으로 요약할 수 있습니다.

$$
\begin{aligned}
\left|\mathcal{P}_{n}\right| & \leq(n+1)^{|\mathcal{X}|} \\
Q^{n}(\mathbf{x}) & =2^{-n\left(D\left(P_{\mathbf{x}} \| Q\right)+H\left(P_{\mathbf{x}}\right)\right)} \\
|T(P)| & \doteq 2^{n H(P)} \\
Q^{n}(T(P)) & \doteq 2^{-n D(P \| Q)}
\end{aligned}
$$

이 방정식들은 타입의 수가 다항식 개수만큼만 존재하며, 각 타입마다 지수적으로 많은 시퀀스가 존재함을 나타냅니다. 또한, 분포 $Q$ 하에서 임의의 타입 $P$ 시퀀스의 확률에 대한 정확한 공식과 타입 클래스의 확률에 대한 근사 공식을 가지고 있습니다.

이 방정식들을 통해 시퀀스의 타입 속성을 기반으로 긴 시퀀스의 동작을 계산할 수 있습니다. 예를 들어, 특정 분포에 따라 i.i.d.로 추출된 긴 시퀀스의 경우, 시퀀스의 타입은 시퀀스를 생성하는 분포에 가까우며, 이 분포의 속성을 사용하여 시퀀스의 속성을 추정할 수 있습니다. 다음 몇 절에서 다룰 응용 프로그램 중 일부는 다음과 같습니다.

- 대수의 법칙
- 범용 소스 코딩
- 사노프 정리
- 체르노프-스타인 보조정리와 가설 검정
- 조건부 확률 및 극한 정리

# 11.2 대수의 법칙

타입과 타입 클래스의 개념을 통해 대수의 법칙을 대안적으로 기술할 수 있습니다. 실제로 이 개념은 이산적인 경우에 대한 약한 대수의 법칙의 한 버전에 대한 증명으로 사용될 수 있습니다. 타입의 가장 중요한 속성은 타입의 수가 다항식 개수만큼만 존재한다는 것입니다.
<!-- Page 382 -->
각 유형의 시퀀스는 지수적으로 많은 수를 가집니다. 각 유형 클래스의 확률은 유형 $P$와 분포 $Q$ 사이의 상대 엔트로피 거리에 지수적으로 의존하므로, 실제 분포에서 멀리 떨어진 유형 클래스는 지수적으로 작은 확률을 가집니다.

$\epsilon>0$이 주어졌을 때, 분포 $Q^{n}$에 대한 시퀀스의 전형 집합 $T_{Q}^{\epsilon}$을 다음과 같이 정의할 수 있습니다.

$$
T_{Q}^{\epsilon}=\left\{x^{n}: D\left(P_{x^{n}} \| Q\right) \leq \epsilon\right\}
$$

그러면 $x^{n}$이 전형적이지 않을 확률은 다음과 같습니다.

$$
\begin{aligned}
1-Q^{n}\left(T_{Q}^{\epsilon}\right) & =\sum_{P: D(P \| Q)>\epsilon} Q^{n}(T(P)) \\
& \leq \sum_{P: D(P \| Q)>\epsilon} 2^{-n D(P \| Q)} \quad \text { (Theorem 11.1.4) } \\
& \leq \sum_{P: D(P \| Q)>\epsilon} 2^{-n \epsilon} \\
& \leq(n+1)^{|\mathcal{X}|} 2^{-n \epsilon} \quad(\text { Theorem 11.1.1) } \\
& =2^{-n\left(\epsilon-|\mathcal{X}| \frac{\log (n+1)}{n}\right)}
\end{aligned}
$$

이는 $n \rightarrow \infty$일 때 0으로 수렴합니다. 따라서 전형 집합 $T_{Q}^{\epsilon}$의 확률은 $n \rightarrow \infty$일 때 1로 수렴합니다. 이는 제 3장에서 증명된 AEP와 유사하며, 이는 대수의 법칙의 약한 형태입니다. 이제 경험적 분포 $P_{X^{n}}$이 $P$로 수렴함을 증명합니다.

Theorem 11.2.1 $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim P(x)$라고 가정합니다. 그러면

$$
\operatorname{Pr}\left\{D\left(P_{x^{n}} \| P\right)>\epsilon\right\} \leq 2^{-n(\epsilon-|\mathcal{X}| \frac{\log (n+1)}{n})}
$$

결과적으로 $D\left(P_{x^{n}} \| P\right) \rightarrow 0$은 확률 1로 수렴합니다.
Proof: 부등식 (11.69)는 (11.68)에서 증명되었습니다. $n$에 대해 합하면 다음과 같습니다.

$$
\sum_{n=1}^{\infty} \operatorname{Pr}\left\{D\left(P_{x^{n}} \| P\right)>\epsilon\right\}<\infty
$$
<!-- Page 383 -->
따라서 $n$에 대한 사건 $\left\{D\left(P_{x^{n}} \| P\right)>\epsilon\right\}$의 예상 발생 횟수는 유한하며, 이는 보렐-칸텔리 보조정리에 의해 실제 발생 횟수도 확률 1로 유한함을 의미합니다. 그러므로 $D\left(P_{x^{n}} \| P\right) \rightarrow 0$은 확률 1로 수렴합니다.

이제 3장에서보다 더 강력한 전형성(typicality) 버전을 정의합니다.
정의 강한 전형 집합 $A_{\epsilon}^{*(n)}$을 표본 빈도가 실제 값에 가까운 $\mathcal{X}^{n}$의 시퀀스 집합으로 정의합니다.

$$
A_{\epsilon}^{*(n)}=\left\{\begin{array}{cl}
\mathbf{x} \in \mathcal{X}^{n}: & \left|\frac{1}{n} N(a \mid \mathbf{x})-P(a)\right|<\frac{\epsilon}{\left|\mathcal{X}\right|}, \quad \text { if } P(a)>0 \\
& N(a \mid \mathbf{x})=0 \quad \text { if } P(a)=0
\end{array}\right\}
$$

따라서 전형 집합은 모든 구성 요소에서 실제 확률과의 차이가 $\epsilon /|\mathcal{X}|$를 초과하지 않는 유형을 가진 시퀀스로 구성됩니다. 대수의 법칙에 따라 강한 전형 집합의 확률은 $n \rightarrow \infty$일 때 1로 수렴합니다. 강한 전형성으로 얻어지는 추가적인 능력은 특히 범용 코딩,율 왜곡 이론 및 대규모 편차 이론에서 더 강력한 결과를 증명하는 데 유용합니다.

# 11.3 범용 소스 코딩

허프만 코딩은 알려진 분포 $p(x)$를 가진 i.i.d. 소스를 엔트로피 한계 $H(X)$로 압축합니다. 그러나 코드가 잘못된 분포 $q(x)$에 대해 설계된 경우 $D(p \| q)$의 페널티가 발생합니다. 따라서 허프만 코딩은 가정된 분포에 민감합니다.

실제 분포 $p(x)$를 알 수 없을 때 어떤 압축을 달성할 수 있습니까? 엔트로피 $H(X)<R$을 가진 모든 i.i.d. 소스를 설명하기에 충분한 속도 $R$의 범용 코드가 존재합니까? 놀라운 대답은 예입니다. 이 아이디어는 유형(type)의 방법에 기반합니다. 유형 $P$를 가진 $2^{n H(P)}$개의 시퀀스가 있습니다. 분모가 $n$인 유형은 다항식 개수만 있으므로, $H\left(P_{x^{n}}\right)<R$인 유형 $P_{x^{n}}$을 가진 모든 시퀀스 $x^{n}$을 열거하는 데는 대략 $n R$ 비트가 필요합니다. 따라서 이러한 모든 시퀀스를 설명함으로써, 엔트로피 $H(Q)<R$을 가진 어떤 분포 $Q$에서 발생할 가능성이 있는 모든 시퀀스를 설명할 준비가 됩니다. 정의부터 시작하겠습니다.

정의 분포 $Q$를 알 수 없는 소스 $X_{1}, X_{2}, \ldots$, $X_{n}$에 대한 속도 $R$의 고정 속도 블록 코드는 두 개의 매핑으로 구성됩니다. 인코더는 다음과 같습니다.

$$
f_{n}: \mathcal{X}^{n} \rightarrow\left\{1,2, \ldots, 2^{n R}\right\}
$$
<!-- Page 384 -->
디코더는 다음과 같습니다.

$$
\phi_{n}:\left\{1,2, \ldots, 2^{n R}\right\} \rightarrow \mathcal{X}^{n}
$$

여기서 $R$은 코드의 속도라고 합니다. 분포 $Q$에 대한 코드의 오류 확률은 다음과 같습니다.

$$
P_{e}^{(n)}=Q^{n}\left(X^{n}: \phi_{n}\left(f_{n}\left(X^{n}\right)\right) \neq X^{n}\right)
$$

정의: 소스에 대한 속도 $R$ 블록 코드는 함수 $f_{n}$과 $\phi_{n}$이 분포 $Q$에 의존하지 않고, $R>H(Q)$일 때 $P_{e}^{(n)} \rightarrow 0$이면 보편적이라고 합니다.

이제 Csiszár와 Körner [149]에 의한 보편 인코딩 방식 중 하나를 설명하겠습니다. 이 방식은 타입 $P$의 시퀀스 수가 엔트로피에 따라 지수적으로 증가한다는 사실과 타입의 수가 다항식으로만 존재한다는 사실에 기반합니다.

정리 11.3.1: $P_{e}^{(n)} \rightarrow 0$이 모든 소스 $Q$에 대해 $H(Q)<R$을 만족하는 $\left(2^{n R}, n\right)$ 보편 소스 코드 시퀀스가 존재합니다.

증명: 코드의 속도 $R$을 고정합니다.

$$
R_{n}=R-|\mathcal{X}| \frac{\log (n+1)}{n}
$$

시퀀스 집합을 고려합니다.

$$
A=\left\{\mathbf{x} \in \mathcal{X}^{n}: H\left(P_{\mathbf{x}}\right) \leq R_{n}\right\}
$$

그러면

$$
\begin{aligned}
|A| & =\sum_{P \in \mathcal{P}_{n}: H(P) \leq R_{n}}|T(P)| \\
& \leq \sum_{P \in \mathcal{P}_{n}: H(P) \leq R_{n}} 2^{n H(P)} \\
& \leq \sum_{P \in \mathcal{P}_{n}: H(P) \leq R_{n}} 2^{n R_{n}} \\
& \leq(n+1)^{|\mathcal{X}|} 2^{n R_{n}} \\
& =2^{n\left(R_{n}+|\mathcal{X}| \frac{\log (n+1)}{n}\right)} \\
& =2^{n R}
\end{aligned}
$$
<!-- Page 385 -->
$A$의 원소들을 인덱싱함으로써, 인코딩 함수 $f_{n}$을 다음과 같이 정의합니다.

$$
f_{n}(\mathbf{x})= \begin{cases}\text { A에 있는 } \mathbf{x} \text{의 인덱스 } & \text { if } x \in A \\ 0 & \text { otherwise }\end{cases}
$$

디코딩 함수는 각 인덱스를 $A$의 해당 원소에 매핑합니다. 따라서 $A$의 모든 원소는 올바르게 복구되며, 나머지 모든 시퀀스는 오류를 발생시킵니다. 올바르게 복구되는 시퀀스의 집합은 그림 11.2에 나와 있습니다.

이제 이 인코딩 방식이 보편적임을 보이겠습니다. $X_{1}, X_{2}, \ldots, X_{n}$의 분포가 $Q$이고 $H(Q)<R$이라고 가정합니다. 그러면 디코딩 오류 확률은 다음과 같이 주어집니다.

$$
\begin{aligned}
P_{e}^{(n)} & =1-Q^{n}(A) \\
& =\sum_{P: H(P)>R_{n}} Q^{n}(T(P)) \\
& \leq(n+1)^{|\mathcal{X}|} \max _{P: H(P)>R_{n}} Q^{n}(T(P)) \\
& \leq(n+1)^{|\mathcal{X}|} 2^{-n \min _{P: H(P)>R_{n}} D(P \| Q)}
\end{aligned}
$$

$R_{n} \uparrow R$이고 $H(Q)<R$이므로, 모든 $n \geq n_{0}$에 대해 $R_{n}>H(Q)$인 $n_{0}$이 존재합니다. 그러면 $n \geq n_{0}$에 대해 $\min _{P: H(P)>R_{n}} D(P \| Q)$는 0보다 커야 하며, 오류 확률 $P_{e}^{(n)}$은 $n \rightarrow \infty$일 때 지수적으로 빠르게 0으로 수렴합니다.

그림 11.2. 보편적 코드와 확률 단체. 타입이 원 밖에 있는 각 시퀀스는 해당 인덱스로 인코딩됩니다. 이러한 시퀀스는 $2^{n R}$개보다 적습니다. 타입이 원 안에 있는 시퀀스는 0으로 인코딩됩니다.
<!-- Page 386 -->

그림 11.3. 범용 코드의 오차 지수.

반면에, 분포 $Q$가 엔트로피 $H(Q)$가 비율 $R$보다 큰 분포인 경우, 시퀀스는 높은 확률로 집합 $A$ 밖에 있는 타입을 가질 것입니다. 따라서 이러한 경우 오차 확률은 1에 가깝습니다.

오차 확률의 지수는 다음과 같습니다.

$$
D_{R, Q}^{*}=\min _{P: H(P)>R} D(P \| Q)
$$

이는 그림 11.3에 설명되어 있습니다.
여기서 설명된 범용 코딩 방식은 여러 가지 방식 중 하나일 뿐입니다. 이는 i.i.d. 분포 집합에 대해 범용적입니다. Lempel-Ziv 알고리즘과 같이 모든 에르고딕 소스에 대한 가변율 범용 코드인 다른 방식들도 있습니다. 섹션 13.4에서 논의된 Lempel-Ziv 알고리즘은 영어 텍스트나 컴퓨터 소스 코드와 같이 간단하게 모델링할 수 없는 데이터를 압축하는 데 실용적으로 자주 사용됩니다.

확률 분포에 특화된 허프만 코드를 사용하는 것이 왜 필요한지 궁금할 수 있습니다. 범용 코드를 사용하면 무엇을 잃게 될까요? 범용 코드는 동일한 성능을 얻기 위해 특정 확률 분포에 대해 설계된 코드보다 더 긴 블록 길이가 필요합니다. 이 블록 길이 증가는 인코더 및 디코더의 복잡성 증가라는 대가를 치르게 됩니다. 따라서 소스의 분포를 알고 있다면 분포별 코드가 최선입니다.

# 11.4 대규모 편차 이론

대규모 편차 이론의 주제는 예시를 통해 설명될 수 있습니다. $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. 베르누이 $\left(\frac{1}{3}\right)$에서 추출되었다면, $\frac{1}{n} \sum X_{i}$가 $\frac{1}{3}$ 근처일 확률은 얼마입니까? 이는 작은 편차(예상 결과로부터의 편차)입니다.
<!-- Page 387 -->
확률은 1에 가깝습니다. 이제 $X_{1}, X_{2}, \ldots, X_{n}$이 베르누이 $\left(\frac{1}{3}\right)$일 때 $\frac{1}{n} \sum X_{i}$가 $\frac{3}{4}$보다 클 확률은 얼마입니까? 이는 큰 편차이며 확률은 지수적으로 작습니다. 중심 극한 정리를 사용하여 지수를 추정할 수 있지만, 이는 몇 표준 편차 이상에서는 좋지 않은 근사입니다. $\frac{1}{n} \sum X_{i}=\frac{3}{4}$는 $P_{\mathbf{x}}=\left(\frac{1}{4}, \frac{3}{4}\right)$와 동등하다는 점에 유의합니다. 따라서 $\bar{X}_{n}$이 $\frac{3}{4}$에 가까울 확률은 타입 $P_{X}$가 $\left(\frac{3}{4}, \frac{1}{4}\right)$에 가까울 확률입니다. 이 큰 편차의 확률은 $\approx 2^{-n D\left(\left(\frac{3}{4}, \frac{1}{4}\right) \|\left(\frac{1}{3}, \frac{1}{3}\right)\right)}$이 될 것입니다. 이 섹션에서는 비정상적인 타입 집합의 확률을 추정합니다.

$E$를 확률 질량 함수 집합의 부분집합이라고 합시다. 예를 들어, $E$는 평균 $\mu$를 갖는 확률 질량 함수 집합일 수 있습니다. 표기법을 약간 남용하여 다음과 같이 씁니다.

$$
Q^{n}(E)=Q^{n}\left(E \cap \mathcal{P}_{n}\right)=\sum_{\mathbf{x}: P_{\mathbf{x}} \in E \cap \mathcal{P}_{n}} Q^{n}(\mathbf{x})
$$

만약 $E$가 $Q$의 상대 엔트로피 이웃을 포함한다면, 큰 수의 법칙 (정리 11.2.1)에 의해 $Q^{n}(E) \rightarrow 1$이 됩니다. 반면에, 만약 $E$가 $Q$ 또는 $Q$의 이웃을 포함하지 않는다면, 큰 수의 법칙에 의해 $Q^{n}(E) \rightarrow 0$이 지수적으로 빠르게 됩니다. 지수를 계산하기 위해 타입의 방법을 사용할 것입니다.

먼저 고려하고 있는 집합 $E$의 종류에 대한 몇 가지 예를 들어 보겠습니다. 예를 들어, 관찰을 통해 $g(X)$의 표본 평균이 $\alpha$보다 크거나 같다고 가정합니다 [즉, $\frac{1}{n} \sum_{i} g\left(x_{i}\right) \geq \alpha$ ]. 이 사건은 $P_{\mathbf{X}} \in E \cap \mathcal{P}_{n}$ 사건과 동등하며, 여기서

$$
E=\left\{P: \sum_{a \in \mathcal{X}} g(a) P(a) \geq \alpha\right\}
$$

왜냐하면

$$
\begin{aligned}
\frac{1}{n} \sum_{i=1}^{n} g\left(x_{i}\right) \geq \alpha & \Leftrightarrow \sum_{a \in \mathcal{X}} P_{\mathbf{X}}(a) g(a) \geq \alpha \\
& \Leftrightarrow P_{\mathbf{X}} \in E \cap \mathcal{P}_{n}
\end{aligned}
$$

따라서,

$$
\operatorname{Pr}\left(\frac{1}{n} \sum_{i=1}^{n} g\left(X_{i}\right) \geq \alpha\right)=Q^{n}\left(E \cap \mathcal{P}_{n}\right)=Q^{n}(E)
$$
<!-- Page 388 -->

그림 11.4. 확률 단순체와 Sanov의 정리.

여기서 $E$는 그림 11.4에 묘사된 확률 벡터 공간의 반공간입니다.

정리 11.4.1 (Sanov의 정리) $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim Q(x)$라고 가정합니다. $E \subseteq \mathcal{P}$를 확률 분포의 집합이라고 가정합니다. 그러면

$$
Q^{n}(E)=Q^{n}\left(E \cap \mathcal{P}_{n}\right) \leq(n+1)^{|\mathcal{X}|} 2^{-n D\left(P^{*}| | Q\right)}
$$

여기서

$$
P^{*}=\arg \min _{P \in E} D(P \| Q)
$$

는 상대 엔트로피에서 $Q$에 가장 가까운 $E$ 내의 분포입니다.
추가적으로, 집합 $E$가 내부의 폐포라면,

$$
\frac{1}{n} \log Q^{n}(E) \rightarrow-D\left(P^{*}| | Q\right)
$$

증명: 먼저 상한을 증명합니다:

$$
\begin{aligned}
Q^{n}(E) & =\sum_{P \in E \cap \mathcal{P}_{n}} Q^{n}(T(P)) \\
& \leq \sum_{P \in E \cap \mathcal{P}_{n}} 2^{-n D(P \| Q)}
\end{aligned}
$$
<!-- Page 389 -->
$$
\begin{aligned}
& \leq \sum_{P \in E \cap \mathcal{P}_{n}} \max _{P \in E \cap \mathcal{P}_{n}} 2^{-n D(P \| Q)} \\
& =\sum_{P \in E \cap \mathcal{P}_{n}} 2^{-n \min _{P \in E \cap \mathcal{P}_{n}} D(P \| Q)} \\
& \leq \sum_{P \in E \cap \mathcal{P}_{n}} 2^{-n \min _{P \in E} D(P \| Q)} \\
& =\sum_{P \in E \cap \mathcal{P}_{n}} 2^{-n D\left(P^{*} \| Q\right)} \\
& \leq(n+1)^{|\mathcal{X}|} 2^{-n D\left(P^{*} \| Q\right)}
\end{aligned}
$$

여기서 마지막 부등식은 Theorem 11.1.1에 의해 도출됩니다. $P^{*}$가 $\mathcal{P}_{n}$의 원소가 아닐 수도 있음에 유의하십시오. 이제 하한에 도달할 차례이며, 이를 위해서는 "좋은" 집합 $E$가 필요합니다. 그래야 모든 큰 $n$에 대해 $E \cap \mathcal{P}_{n}$ 내에서 $P^{*}$에 가까운 분포를 찾을 수 있습니다. 만약 $E$가 내부의 폐포라고 가정한다면 (따라서 내부는 공집합이 아니어야 합니다), $\cup_{n} \mathcal{P}_{n}$이 모든 분포의 집합에서 조밀하므로, 어떤 $n_{0}$에 대해 모든 $n \geq n_{0}$에 대해 $E \cap \mathcal{P}_{n}$은 공집합이 아니게 됩니다. 그러면 $P_{n} \in E \cap \mathcal{P}_{n}$이고 $D\left(P_{n} \| Q\right) \rightarrow D\left(P^{*} \| Q\right)$인 분포의 수열 $P_{n}$을 찾을 수 있습니다. 각 $n \geq n_{0}$에 대해,

$$
\begin{aligned}
Q^{n}(E) & =\sum_{P \in E \cap \mathcal{P}_{n}} Q^{n}(T(P)) \\
& \geq Q^{n}\left(T\left(P_{n}\right)\right) \\
& \geq \frac{1}{(n+1)^{|\mathcal{X}|}} 2^{-n D\left(P_{n} \| Q\right)}
\end{aligned}
$$

결과적으로,

$$
\begin{aligned}
& \liminf \frac{1}{n} \log Q^{n}(E) \geq \liminf \left(-\frac{|\mathcal{X}| \log (n+1)}{n}-D\left(P_{n} \| Q\right)\right) \\
& \quad=-D\left(P^{*} \| Q\right)
\end{aligned}
$$

이를 상한과 결합하면 정리가 확립됩니다.

이 논증은 양자화를 사용하여 연속 분포로 확장될 수 있습니다.
<!-- Page 390 -->
# 11.5 사노프 정리의 예시

$\operatorname{Pr}\left\{\frac{1}{n} \sum_{i=1}^{n} g_{j}\left(X_{i}\right) \geq \alpha_{j}, j=1,2, \ldots, k\right\}$를 찾고자 한다고 가정합니다. 그러면 집합 $E$는 다음과 같이 정의됩니다.

$$
E=\left\{P: \sum_{a} P(a) g_{j}(a) \geq \alpha_{j}, j=1,2, \ldots, k\right\}
$$

$E$에서 $Q$에 가장 가까운 분포를 찾기 위해 (11.108)의 제약 조건을 만족하면서 $D(P \| Q)$를 최소화합니다. 라그랑주 승수법을 사용하여 함수를 구성합니다.

$$
J(P)=\sum_{x} P(x) \log \frac{P(x)}{Q(x)}+\sum_{i} \lambda_{i} \sum_{x} P(x) g_{i}(x)+v \sum_{x} P(x)
$$

그런 다음 미분하여 $Q$에 가장 가까운 분포가 다음과 같은 형태임을 계산합니다.

$$
P^{*}(x)=\frac{Q(x) e^{\sum_{i} \lambda_{i} g_{i}(x)}}{\sum_{a \in \mathcal{X}} Q(a) e^{\sum_{i} \lambda_{i} g_{i}(a)}}
$$

여기서 상수 $\lambda_{i}$는 제약 조건을 만족하도록 선택됩니다. $Q$가 균등 분포인 경우, $P^{*}$는 최대 엔트로피 분포입니다. $P^{*}$가 실제로 최소값임을 확인하는 것은 12장에서 주어진 논증과 동일한 종류의 논증으로부터 따릅니다.

몇 가지 구체적인 예를 고려해 봅시다:
예시 11.5.1 (주사위) 공정한 주사위를 $n$번 던진다고 가정합니다. 던진 결과의 평균이 4 이상일 확률은 얼마입니까? 사노프 정리에 따르면 다음과 같습니다.

$$
Q^{n}(E) \doteq 2^{-n D\left(P^{*}| | Q\right)}
$$

여기서 $P^{*}$는 다음을 만족하는 모든 분포 $P$에 대해 $D(P \| Q)$를 최소화합니다.

$$
\sum_{i=1}^{6} i P(i) \geq 4
$$
<!-- Page 391 -->
(11.110)에 따르면, $P^{*}$는 다음과 같은 형태를 가집니다.

$$
P^{*}(x)=\frac{2^{\lambda x}}{\sum_{i=1}^{6} 2^{\lambda i}}
$$

여기서 $\lambda$는 $\sum i P^{*}(i)=4$를 만족하도록 선택됩니다. 수치적으로 풀면 $\lambda=0.2519, P^{*}=(0.1031,0.1227,0.1461,0.1740,0.2072,0.2468)$를 얻게 되며, 따라서 $D\left(P^{*}| | Q\right)=0.0624$ bit입니다. 그러므로 10000번의 던지기 평균이 4 이상일 확률은 약 $2^{-624}$입니다.

예제 11.5.2 (동전) 공정한 동전이 있고 1000번의 시행에서 700번 이상 앞면이 나올 확률을 추정한다고 가정해 봅시다. 이 문제는 예제 11.5.1과 유사합니다. 확률은 다음과 같습니다.

$$
P\left(\bar{X}_{n} \geq 0.7\right) \doteq 2^{-n D\left(P^{*}| | Q\right)}
$$

여기서 $P^{*}$는 $(0.7,0.3)$ 분포이고 $Q$는 $(0.5,0.5)$ 분포입니다. 이 경우 $D\left(P^{*}| | Q\right)=1-H\left(P^{*}\right)=1-H(0.7)=0.119$입니다. 따라서 1000번의 시행에서 700번 이상 앞면이 나올 확률은 약 $2^{-119}$입니다.

예제 11.5.3 (상호 의존성) 주어진 결합 분포를 $Q(x, y)$라고 하고, $Q$의 주변 분포로부터 형성된 연관된 곱 분포를 $Q_{0}(x, y)=Q(x) Q(y)$라고 합시다. 우리는 $Q_{0}$에 따라 추출된 표본이 $Q$에 따라 결합 분포된 것처럼 "보일" 가능성을 알고 싶습니다. 따라서 $\left(X_{i}, Y_{i}\right)$를 i.i.d. $\sim Q_{0}(x, y)=Q(x) Q(y)$라고 정의합니다. 우리는 제7.6절에서 정의한 대로 결합 전형성을 정의합니다. 즉, $\left(x^{n}, y^{n}\right)$는 결합 분포 $Q(x, y)$에 대해 결합 전형적입니다. 이는 표본 엔트로피가 실제 값에 가까울 때입니다.

$$
\begin{aligned}
& \left|-\frac{1}{n} \log Q\left(x^{n}\right)-H(X)\right| \leq \epsilon \\
& \left|-\frac{1}{n} \log Q\left(y^{n}\right)-H(Y)\right| \leq \epsilon
\end{aligned}
$$

그리고

$$
\left|-\frac{1}{n} \log Q\left(x^{n}, y^{n}\right)-H(X, Y)\right| \leq \epsilon
$$

우리는 곱 분포 하에서 $Q$에 대해 결합 전형적으로 보이는 쌍 $\left(x^{n}, y^{n}\right)$를 볼 확률을 계산하고자 합니다. 즉, $\left(x^{n}, y^{n}\right)$

<!-- Page 392 -->
(11.115)-(11.117)을 만족합니다. 따라서 $\left(x^{n}, y^{n}\right)$은 $Q(x, y)$에 대해 공동으로 전형적(jointly typical)이라고 합니다. 여기서

$$
\begin{aligned}
E=\{P(x, y): & \left\lvert\,-\sum_{x, y} P(x, y) \log Q(x)-H(X)\right. \leq \epsilon \\
& \left\lvert\,-\sum_{x, y} P(x, y) \log Q(y)-H(Y)\right. \leq \epsilon \\
& \left\lvert\,-\sum_{x, y} P(x, y) \log Q(x, y)-H(X, Y)\right. \leq \epsilon\}
\end{aligned}
$$

Sanov의 정리를 사용하면 확률은 다음과 같습니다.

$$
Q_{0}^{n}(E) \doteq 2^{-n D\left(P^{*}| | Q_{0}\right)}
$$

여기서 $P^{*}$는 상대 엔트로피(relative entropy)에서 $Q_{0}$에 가장 가까운 제약 조건을 만족하는 분포입니다. 이 경우, $\epsilon \rightarrow 0$이 되면 (문제 11.10), $P^{*}$는 공동 분포 $Q$이고 $Q_{0}$는 곱셈 분포(product distribution)임을 확인할 수 있습니다. 따라서 확률은 $2^{-n D(Q(x, y) \| Q(x) Q(y))}=2^{-n I(X ; Y)}$가 되며, 이는 제7장에서 공동 AEP에 대해 도출된 결과와 동일합니다.

다음 섹션에서는 특정 분포 집합 $E$에 속하는 타입(type)이 주어졌을 때 결과 시퀀스의 경험적 분포(empirical distribution)를 고려합니다. 우리는 집합 $E$의 확률이 $Q$에 가장 가까운 원소의 거리인 $D\left(P^{*}| | Q\right)$에 의해 본질적으로 결정될 뿐만 아니라, 조건부 타입(conditional type)이 본질적으로 $P^{*}$가 된다는 것을 보일 것입니다. 따라서 집합 $E$에 속한다는 조건 하에서, 타입은 $P^{*}$에 가까울 가능성이 매우 높습니다.

# 11.6 조건부 극한 정리

분포 $Q$ 하에서 타입 집합의 확률은 집합에서 $Q$에 가장 가까운 원소의 확률에 본질적으로 의해 결정된다는 것이 입증되었습니다. 확률은 첫 번째 근사에서 지수 부분에 대해 $2^{-n D^{*}}$이며, 여기서

$$
D^{*}=\min _{P \in E} D(P \| Q)
$$

이는 타입 집합의 확률이 각 타입의 확률의 합이기 때문이며, 이는 가장 큰 항에 집합의 크기를 곱한 값으로 제한됩니다.
<!-- Page 393 -->

그림 11.5. 상대 엔트로피에 대한 피타고라스 정리.
항의 개수. 항의 개수가 시퀀스 길이에 대한 다항식이므로, 합은 지수에서 가장 큰 항과 일차적으로 같습니다.

이제 우리는 집합 $E$의 확률이 가장 가까운 타입 $P^{*}$의 확률과 본질적으로 동일할 뿐만 아니라, $P^{*}$에서 멀리 떨어진 다른 타입들의 총 확률이 무시할 수 있다는 것을 보여주기 위해 주장을 강화합니다. 이는 관찰된 타입이 $P^{*}$에 매우 높은 확률로 가깝다는 것을 의미합니다. 우리는 이것을 조건부 극한 정리라고 부릅니다.

이 결과를 증명하기 전에, $D(P \| Q)$의 기하학에 대한 통찰력을 제공하는 "피타고라스" 정리를 증명합니다. $D(P \| Q)$는 메트릭이 아니므로, 거리의 직관적인 속성 중 많은 부분이 $D(P \| Q)$에 대해 유효하지 않습니다. 다음 정리는 $D(P \| Q)$가 유클리드 거리의 제곱처럼 작동하는 방식을 보여줍니다 (그림 11.5).

정리 11.6.1 닫힌 볼록 집합 $E \subset \mathcal{P}$와 분포 $Q \notin E$에 대해, $Q$에 대한 최소 거리를 달성하는 분포 $P^{*} \in E$가 존재한다고 가정합니다. 즉,

$$
D\left(P^{*}| | Q\right)=\min _{P \in E} D(P \| Q)
$$

그러면 모든 $P \in E$에 대해 다음이 성립합니다.

$$
D(P \| Q) \geq D\left(P \| P^{*}\right)+D\left(P^{*}| | Q\right)
$$
<!-- Page 394 -->
참고. 이 정리는 주로 다음과 같이 사용됩니다. $P_{n} \in E$가 $D\left(P_{n} \| Q\right) \rightarrow D\left(P^{*} \| Q\right)$를 생성한다고 가정해 봅시다. 그러면 피타고라스 정리에 의해 $D\left(P_{n} \| P^{*}\right) \rightarrow 0$도 성립합니다.

증명: 임의의 $P \in E$를 고려하십시오.

$$
P_{\lambda}=\lambda P+(1-\lambda) P^{*}
$$

라고 합시다. 그러면 $P_{\lambda} \rightarrow P^{*}$는 $\lambda \rightarrow 0$일 때 성립합니다. 또한 $E$가 볼록 집합이므로 $0 \leq \lambda \leq 1$에 대해 $P_{\lambda} \in E$입니다. $D\left(P^{*} \| Q\right)$는 $P^{*} \rightarrow P$ 경로를 따라 $D\left(P_{\lambda} \| Q\right)$의 최소값이므로, $\lambda$에 대한 함수로서 $D\left(P_{\lambda} \| Q\right)$의 미분은 $\lambda=0$에서 음수가 아닙니다. 이제

$$
D_{\lambda}=D\left(P_{\lambda} \| Q\right)=\sum P_{\lambda}(x) \log \frac{P_{\lambda}(x)}{Q(x)}
$$

이고

$$
\frac{d D_{\lambda}}{d \lambda}=\sum\left(\left(P(x)-P^{*}(x)\right) \log \frac{P_{\lambda}(x)}{Q(x)}+\left(P(x)-P^{*}(x)\right)\right)
$$

입니다. $\lambda=0$으로 설정하면 $P_{\lambda}=P^{*}$가 되고 $\sum P(x)=\sum P^{*}$ $(x)=1$이라는 사실을 사용하면 다음과 같습니다.

$$
\begin{aligned}
0 & \leq\left(\frac{d D_{\lambda}}{d \lambda}\right)_{\lambda=0} \\
& =\sum\left(P(x)-P^{*}(x)\right) \log \frac{P^{*}(x)}{Q(x)} \\
& =\sum P(x) \log \frac{P^{*}(x)}{Q(x)}-\sum P^{*}(x) \log \frac{P^{*}(x)}{Q(x)} \\
& =\sum P(x) \log \frac{P(x)}{Q(x)} \frac{P^{*}(x)}{P(x)}-\sum P^{*}(x) \log \frac{P^{*}(x)}{Q(x)} \\
& =D(P \| Q)-D\left(P \| P^{*}\right)-D\left(P^{*} \| Q\right)
\end{aligned}
$$

이는 정리를 증명합니다.
상대 엔트로피 $D(P \| Q)$가 유클리드 거리의 제곱처럼 동작한다는 점에 유의하십시오. $\mathcal{R}^{n}$에 볼록 집합 $E$가 있다고 가정해 봅시다. 집합 밖에 있는 점을 $A$라고 하고, $A$에 가장 가까운 집합 안의 점을 $B$라고 하고, 임의의 점을 $C$라고 하면,
<!-- Page 395 -->

그림 11.6. 거리 제곱에 대한 삼각 부등식.
집합 내의 다른 점입니다. 그러면 선 $B A$와 $B C$ 사이의 각도는 둔각이어야 하며, 이는 $l_{A C}^{2} \geq l_{A B}^{2}+l_{B C}^{2}$를 의미합니다. 이는 정리 11.6.1과 동일한 형태입니다. 이는 그림 11.6에 설명되어 있습니다.

이제 상대 엔트로피에서의 수렴이 $\mathcal{L}_{1}$ 노름에서의 수렴을 보여주는 유용한 보조 정리를 증명합니다.

정의 두 분포 간의 $\mathcal{L}_{1}$ 거리는 다음과 같이 정의됩니다.

$$
\left\|P_{1}-P_{2}\right\|_{1}=\sum_{a \in \mathcal{X}}\left|P_{1}(a)-P_{2}(a)\right|
$$

$P_{1}(x)>P_{2}(x)$인 집합을 $A$라고 합시다. 그러면

$$
\begin{aligned}
\left\|P_{1}-P_{2}\right\|_{1} & =\sum_{x \in \mathcal{X}}\left|P_{1}(x)-P_{2}(x)\right| \\
& =\sum_{x \in A}\left(P_{1}(x)-P_{2}(x)\right)+\sum_{x \in A^{c}}\left(P_{2}(x)-P_{1}(x)\right) \\
& =P_{1}(A)-P_{2}(A)+P_{2}\left(A^{c}\right)-P_{1}\left(A^{c}\right) \\
& =P_{1}(A)-P_{2}(A)+1-P_{2}(A)-1+P_{1}(A) \\
& =2\left(P_{1}(A)-P_{2}(A)\right)
\end{aligned}
$$
<!-- Page 396 -->
또한 다음을 참고하십시오.

$$
\max _{B \subseteq \mathcal{X}}\left(P_{1}(B)-P_{2}(B)\right)=P_{1}(A)-P_{2}(A)=\frac{\left\|P_{1}-P_{2}\right\|_{1}}{2}
$$

(11.137)의 좌변을 $P_{1}$과 $P_{2}$ 사이의 변동 거리(variational distance)라고 합니다.

# Lemma 11.6.1

$$
D\left(P_{1} \| P_{2}\right) \geq \frac{1}{2 \ln 2}\left\|P_{1}-P_{2}\right\|_{1}^{2}
$$

증명: 먼저 이진(binary) 경우에 대해 증명합니다. 매개변수 $p$와 $q$를 가지며 $p \geq q$인 두 이진 분포를 고려합니다. 다음을 보일 것입니다.

$$
p \log \frac{p}{q}+(1-p) \log \frac{1-p}{1-q} \geq \frac{4}{2 \ln 2}(p-q)^{2}
$$

두 변의 차이 $g(p, q)$는 다음과 같습니다.

$$
g(p, q)=p \log \frac{p}{q}+(1-p) \log \frac{1-p}{1-q}-\frac{4}{2 \ln 2}(p-q)^{2}
$$

그러면

$$
\begin{aligned}
\frac{d g(p, q)}{d q} & =-\frac{p}{q \ln 2}+\frac{1-p}{(1-q) \ln 2}-\frac{4}{2 \ln 2} 2(q-p) \\
& =\frac{q-p}{q(1-q) \ln 2}-\frac{4}{\ln 2}(q-p) \\
& \leq 0
\end{aligned}
$$

$q(1-q) \leq \frac{1}{4}$이고 $q \leq p$이므로 성립합니다. $q=p$일 때 $g(p, q)=0$이므로, $q \leq p$에 대해 $g(p, q) \geq 0$이며, 이는 이진 경우에 대한 lemma를 증명합니다.

일반적인 경우에 대해, 임의의 두 분포 $P_{1}$과 $P_{2}$에 대해 다음을 정의합니다.

$$
A=\left\{x: P_{1}(x)>P_{2}(x)\right\}
$$

집합 $A$의 지시자(indicator)인 새로운 이진 확률 변수 $Y=\phi(X)$를 정의하고, $Y$의 분포를 $\hat{P}_{1}$과 $\hat{P}_{2}$라고 합시다. 따라서 $\hat{P}_{1}$과 $\hat{P}_{2}$는 $P_{1}$과 $P_{2}$의 양자화된(quantized) 버전과 일치합니다. 그러면 데이터 처리(data-processing)에 의해

<!-- Page 397 -->
상대 엔트로피에 대한 부등식(이는 상호 정보에 대한 데이터 처리 부등식과 같은 방식으로 증명됩니다)을 적용하면 다음과 같습니다.

$$
\begin{aligned}
D\left(P_{1} \| P_{2}\right) & \geq D\left(\hat{P}_{1} \| \hat{P}_{2}\right) \\
& \geq \frac{4}{2 \ln 2}\left(P_{1}(A)-P_{2}(A)\right)^{2} \\
& =\frac{1}{2 \ln 2}\left\|P_{1}-P_{2}\right\|_{1}^{2}
\end{aligned}
$$

(11.137)에 의해 보조 정리가 증명됩니다.
이제 조건부 극한 정리의 증명을 시작할 수 있습니다. 먼저 사용된 방법을 개략적으로 설명합니다. 장의 시작 부분에 명시된 바와 같이, 본질적인 아이디어는 $Q$ 하에서의 타입의 확률이 타입과 $Q$ 간의 거리에 지수적으로 의존한다는 것이며, 따라서 더 멀리 떨어진 타입은 지수적으로 발생 가능성이 낮다는 것입니다. $E$ 내의 타입 집합을 두 범주로 나눕니다. 하나는 $P^{*}$와 거의 같은 거리인 타입이고, 다른 하나는 $2 \delta$ 더 멀리 떨어진 타입입니다. 두 번째 집합은 첫 번째 집합보다 지수적으로 낮은 확률을 가지므로, 첫 번째 집합은 확률이 1로 수렴하는 조건부 확률을 가집니다. 그런 다음 피타고라스 정리를 사용하여 첫 번째 집합의 모든 요소가 $P^{*}$에 가깝다는 것을 확립하여 정리를 증명할 것입니다.

다음 정리는 최대 엔트로피 원리의 중요한 강화입니다.

정리 11.6.2 (조건부 극한 정리) $E$를 $\mathcal{P}$의 닫힌 볼록 부분집합이라 하고, $Q$를 $E$에 속하지 않는 분포라 합니다. $X_{1}, X_{2}, \ldots, X_{n}$을 i.i.d. $\sim Q$로 추출된 이산 확률 변수라 합니다. $P^{*}$가 $\min _{P \in E}$ $D(P \| Q)$를 달성한다고 가정합니다. 그러면

$$
\operatorname{Pr}\left(X_{1}=a \mid P_{X^{n}} \in E\right) \rightarrow P^{*}(a)
$$

확률적으로 $n \rightarrow \infty$일 때, 즉 시퀀스의 타입이 $E$에 속한다는 조건 하에서 $X_{1}$의 조건부 분포는 큰 $n$에 대해 $P^{*}$에 가깝습니다.

예제 11.6.1 $X_{i}$가 i.i.d. $\sim Q$이면,

$$
\operatorname{Pr}\left\{X_{1}=a \left\lvert\, \frac{1}{n} \sum X_{i}^{2} \geq \alpha\right.\right\} \rightarrow P^{*}(a)
$$
<!-- Page 398 -->
$P^{*}(a)$는 $\sum P(a) a^{2} \geq \alpha$를 만족하는 $P$에 대해 $D(P \| Q)$를 최소화합니다. 이 최소화는 다음과 같은 결과를 낳습니다.

$$
P^{*}(a)=Q(a) \frac{e^{\lambda a^{2}}}{\sum_{a} Q(a) e^{\lambda a^{2}}}
$$

여기서 $\lambda$는 $\sum P^{*}(a) a^{2}=\alpha$를 만족하도록 선택됩니다. 따라서 제곱의 합에 대한 제약 조건이 주어졌을 때 $X_{1}$에 대한 조건부 분포는 원래 확률 질량 함수와 최대 엔트로피 확률 질량 함수(이 경우 가우시안 분포)의 곱(정규화됨)입니다.

정리 증명: 집합을 정의합니다.

$$
S_{t}=\{P \in \mathcal{P}: D(P \| Q) \leq t\}
$$

$D(P \| Q)$는 $P$에 대한 볼록 함수이므로 집합 $S_{t}$는 볼록합니다.

$$
D^{*}=D\left(P^{*}| | Q\right)=\min _{P \in E} D(P \| Q)
$$

$D(P \| Q)$는 $P$에 대해 엄격하게 볼록하므로 $P^{*}$는 고유합니다. 이제 집합을 정의합니다.

$$
A=S_{D^{*}+2 \delta} \cap E
$$

그리고

$$
B=E-S_{D^{*}+2 \delta} \cap E
$$

따라서 $A \cup B=E$입니다. 이 집합들은 그림 11.7에 나와 있습니다. 그러면

$$
\begin{aligned}
Q^{n}(B) & =\sum_{P \in E \cap \mathcal{P}_{n}: D(P \| Q)>D^{*}+2 \delta} Q^{n}(T(P)) \\
& \leq \sum_{P \in E \cap \mathcal{P}_{n}: D(P \| Q)>D^{*}+2 \delta} 2^{-n D(P \| Q)} \\
& \leq \sum_{P \in E \cap \mathcal{P}_{n}: D(P \| Q)>D^{*}+2 \delta} 2^{-n\left(D^{*}+2 \delta\right)} \\
& \leq(n+1)^{|\mathcal{X}|} 2^{-n\left(D^{*}+2 \delta\right)}
\end{aligned}
$$
<!-- Page 399 -->

그림 11.7. 조건부 극한 정리.
유형의 수가 다항식 개수만큼만 존재하기 때문입니다. 다른 한편으로는,

$$
\begin{aligned}
Q^{n}(A) & \geq Q^{n}\left(S_{D^{*}+\delta} \cap E\right) \\
& =\sum_{P \in E \cap \mathcal{P}_{n}: D(P \| Q) \leq D^{*}+\delta} Q^{n}(T(P)) \\
& \geq \sum_{P \in E \cap \mathcal{P}_{n}: D(P \| Q) \leq D^{*}+\delta} \frac{1}{(n+1)^{|\mathcal{X}|}} 2^{-n D(P \| Q)} \\
& \geq \frac{1}{(n+1)^{|\mathcal{X}|}} 2^{-n\left(D^{*}+\delta\right)} \quad \text { 충분히 큰 } n \text{에 대하여, }
\end{aligned}
$$

합이 항 중 하나보다 크고, 충분히 큰 $n$에 대해 $S_{D^{*}+\delta} \cap E \cap \mathcal{P}_{n}$에 적어도 하나의 유형이 존재하기 때문입니다. 그러면, 충분히 큰 $n$에 대해,

$$
\begin{aligned}
\operatorname{Pr}\left(P_{X^{n}} \in B \mid P_{X^{n}} \in E\right) & =\frac{Q^{n}(B \cap E)}{Q^{n}(E)} \\
& \leq \frac{Q^{n}(B)}{Q^{n}(A)} \\
& \leq \frac{(n+1)^{|\mathcal{X}|} 2^{-n\left(D^{*}+2 \delta\right)}}{\frac{1}{(n+1)^{|\mathcal{X}|} 2^{-n\left(D^{*}+\delta\right)}} } \\
&=(n+1)^{2|\mathcal{X}|} 2^{-n \delta}
\end{aligned}
$$
<!-- Page 400 -->
$n \rightarrow \infty$일 때 0으로 수렴합니다. 따라서 $B$의 조건부 확률은 $n \rightarrow \infty$일 때 0으로 수렴하며, 이는 $A$의 조건부 확률이 1로 수렴함을 의미합니다.

이제 $A$의 모든 멤버가 상대 엔트로피에서 $P^{*}$에 가깝다는 것을 보이겠습니다. $A$의 모든 멤버에 대해,

$$
D(P \| Q) \leq D^{*}+2 \delta
$$

따라서 "피타고라스" 정리(정리 11.6.1)에 의해,

$$
D\left(P \| P^{*}\right)+D\left(P^{*}| | Q\right) \leq D(P \| Q) \leq D^{*}+2 \delta
$$

이는 다시 다음을 의미합니다.

$$
D\left(P \| P^{*}\right) \leq 2 \delta
$$

$D\left(P^{*}| | Q\right)=D^{*}$이므로. 따라서 $P_{\mathbf{x}} \in A$는 $D\left(P_{\mathbf{x}} \| Q\right) \leq D^{*}+2 \delta$를 의미하며, 따라서 $D\left(P_{\mathbf{x}} \| P^{*}\right) \leq 2 \delta$를 의미합니다. 결과적으로, $\operatorname{Pr}\left\{P_{X^{n}} \in A \mid P_{X^{n}}\right.$ $\in E\} \rightarrow 1$이므로,

$$
\operatorname{Pr}\left(D\left(P_{X^{n}} \| P^{*}\right) \leq 2 \delta \mid P_{X^{n}} \in E\right) \rightarrow 1
$$

$n \rightarrow \infty$일 때. 보조정리 11.6.1에 의해, 상대 엔트로피가 작다는 사실은 $\mathcal{L}_{1}$ 거리가 작다는 것을 의미하며, 이는 다시 $\max _{a \in \mathcal{X}}$ $\left|P_{X^{n}}(a)-P^{*}(a)\right|$가 작다는 것을 의미합니다. 따라서, $\operatorname{Pr}\left(\left|P_{X^{n}}(a)-P^{*}(a)\right| \geq \epsilon \mid P_{X^{n}} \in E\right) \rightarrow$ 0으로 수렴합니다 $n \rightarrow \infty$일 때. 대안적으로, 이는 다음과 같이 작성될 수 있습니다.

$$
\operatorname{Pr}\left(X_{1}=a \mid P_{X^{n}} \in E\right) \rightarrow P^{*}(a) \quad \text { 확률적으로, } a \in \mathcal{X}
$$

이 정리에서는 주변 분포가 $n \rightarrow \infty$일 때 $P^{*}$로 수렴한다는 것만 증명했습니다. 유사한 논증을 사용하여 이 정리의 더 강력한 버전을 증명할 수 있습니다.

$$
\begin{aligned}
\operatorname{Pr}\left(X_{1}\right. & \left.=a_{1}, X_{2}=a_{2}, \ldots, X_{m}\right) \\
& \left.=a_{m} \mid P_{X^{n}} \in E\right) \rightarrow \prod_{i=1}^{m} P^{*}\left(a_{i}\right) \quad \text { 확률적으로. }
\end{aligned}
$$

이는 고정된 $m$에 대해 $n \rightarrow \infty$일 때 성립합니다. $m=n$의 경우 이 결과는 성립하지 않습니다. 끝 효과가 있기 때문입니다. 시퀀스의 유형이 $E$에 속한다는 점을 고려할 때, 시퀀스의 마지막 요소는 나머지 요소로부터 결정될 수 있으며, 요소들은 더 이상 독립적이지 않습니다. 조건부 극한
<!-- Page 401 -->
정리는 첫 번째 몇 개의 요소가 공통 분포 $P^{*}$와 점근적으로 독립적이라고 명시합니다.

예제 11.6.2 조건부 극한 정리의 예로, $n$개의 공정한 주사위를 던지는 경우를 고려해 봅시다. 주사위 결과의 합이 $4n$을 초과한다고 가정합니다. 그러면 조건부 극한 정리에 의해, 첫 번째 주사위가 숫자 $a \in\{1,2, \ldots, 6\}$을 나타낼 확률은 약 $P^{*}(a)$입니다. 여기서 $P^{*}(a)$는 $E$에서의 분포이며, 균일 분포에 가장 가까운 분포입니다. 여기서 $E=\left\{P: \sum P(a) a \geq 4\right\}$입니다. 이는 다음과 같은 최대 엔트로피 분포입니다.

$$
P^{*}(x)=\frac{2^{\lambda x}}{\sum_{i=1}^{6} 2^{\lambda i}}
$$

여기서 $\lambda$는 $\sum i P^{*}(i)=4$가 되도록 선택됩니다 (12장 참조). 여기서 $P^{*}$는 첫 번째 (또는 다른) 주사위에 대한 조건부 분포입니다. 명백히, 검사된 처음 몇 개의 주사위는 지수 분포에 따라 독립적으로 추출된 것처럼 작동할 것입니다.

# 11.7 가설 검정

통계학의 표준적인 문제 중 하나는 관찰된 데이터에 대한 두 가지 대안적 설명을 결정하는 것입니다. 예를 들어, 의료 검사에서 새로운 약이 효과적인지 여부를 검증하고자 할 수 있습니다. 마찬가지로, 일련의 동전 던지기는 동전이 편향되었는지 여부를 드러낼 수 있습니다.

이러한 문제들은 일반적인 가설 검정 문제의 예입니다. 가장 간단한 경우, 우리는 두 개의 i.i.d. 분포 중에서 결정해야 합니다. 일반적인 문제는 다음과 같이 설명될 수 있습니다.

문제 11.7.1 $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim Q(x)$라고 가정합니다. 두 가지 가설을 고려합니다.

- $H_{1}: Q=P_{1}$.
- $H_{2}: Q=P_{2}$.

일반적인 결정 함수 $g\left(x_{1}, x_{2}, \ldots, x_{n}\right)$를 고려합니다. 여기서 $g\left(x_{1}\right.$, $\left.x_{2}, \ldots, x_{n}\right)=1$은 $H_{1}$이 수락됨을 의미하고 $g\left(x_{1}, x_{2}, \ldots, x_{n}\right)=2$는 $H_{2}$가 수락됨을 의미합니다. 함수가 두 개의 값만 취하므로, 테스트는 $g\left(x_{1}, x_{2}, \ldots, x_{n}\right)$이 1인 집합 $A$를 지정하여 지정할 수도 있습니다. 이 집합의 여집합은 $g\left(x_{1}, x_{2}, \ldots, x_{n}\right)$이 2의 값을 갖는 집합입니다. 두 가지 오류 확률을 정의합니다.

$$
\alpha=\operatorname{Pr}\left(g\left(X_{1}, X_{2}, \ldots, X_{n}\right)=2 \mid H_{1} \text { true }\right)=P_{1}^{n}\left(A^{c}\right)
$$
<!-- Page 402 -->
그리고

$$
\beta=\operatorname{Pr}\left(g\left(X_{1}, X_{2}, \ldots, X_{n}\right)=1 \mid H_{2} \text { true }\right)=P_{2}^{n}(A)
$$

일반적으로 두 확률을 모두 최소화하고자 하지만, 상충 관계가 존재합니다. 따라서 다른 오류 확률에 대한 제약 조건을 만족하면서 한 오류 확률을 최소화합니다. 이 문제에 대한 오류 확률의 최적 달성 가능한 오류 지수는 Chernoff-Stein lemma에 의해 주어집니다.

먼저 두 가설 간의 최적 검정의 형태를 도출하는 Neyman-Pearson lemma를 증명합니다. 이 결과는 이산 분포에 대해 유도하며, 연속 분포에 대해서도 동일한 결과를 유도할 수 있습니다.

정리 11.7.1 (Neyman-Pearson lemma) $X_{1}, X_{2}, \ldots, X_{n}$이 확률 질량 함수 $Q$에 따라 i.i.d.로 추출되었다고 가정합니다. $Q=P_{1}$ 대 $Q=P_{2}$의 가설에 해당하는 결정 문제를 고려합니다. $T \geq 0$에 대해 영역을 다음과 같이 정의합니다.

$$
A_{n}(T)=\left\{x^{n}: \frac{P_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right)}{P_{2}\left(x_{1}, x_{2}, \ldots, x_{n}\right)}>T\right\}
$$

다음과 같이 정의합니다.

$$
\alpha^{*}=P_{1}^{n}\left(A_{n}^{c}(T)\right), \quad \beta^{*}=P_{2}^{n}\left(A_{n}(T)\right)
$$

이는 결정 영역 $A_{n}$에 해당하는 오류 확률입니다. $\alpha$와 $\beta$에 해당하는 오류 확률을 갖는 다른 결정 영역을 $B_{n}$이라고 할 때, 만약 $\alpha \leq \alpha^{*}$이면 $\beta \geq \beta^{*}$입니다.

증명: $A=A_{n}(T)$를 (11.176)에 정의된 영역이라고 하고, $B \subseteq \mathcal{X}^{n}$를 다른 승인 영역이라고 합시다. $A$와 $B$의 결정 영역에 대한 지시 함수를 각각 $\phi_{A}$와 $\phi_{B}$라고 합시다. 그러면 모든 $\mathbf{x}=$ $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in \mathcal{X}^{n}$에 대해 다음이 성립합니다.

$$
\left(\phi_{A}(\mathbf{x})-\phi_{B}(\mathbf{x})\right)\left(P_{1}(\mathbf{x})-T P_{2}(\mathbf{x})\right) \geq 0
$$

이는 $\mathbf{x} \in A$와 $\mathbf{x} \notin A$의 경우를 각각 고려하여 확인할 수 있습니다. 이를 전개하여 전체 공간에 대해 합산하면 다음과 같습니다.

$$
0 \leq \sum\left(\phi_{A} P_{1}-T \phi_{A} P_{2}-P_{1} \phi_{B}+T P_{2} \phi_{B}\right)
$$
<!-- Page 403 -->
$$
\begin{aligned}
& =\sum_{A}\left(P_{1}-T P_{2}\right)-\sum_{B}\left(P_{1}-T P_{2}\right) \\
& =\left(1-\alpha^{*}\right)-T \beta^{*}-(1-\alpha)+T \beta \\
& =T\left(\beta-\beta^{*}\right)-\left(\alpha^{*}-\alpha\right)
\end{aligned}
$$

$T \geq 0$이므로, 정리를 증명하였습니다.
Neyman-Pearson lemma는 두 가설에 대한 최적 검정의 형태가 다음과 같음을 나타냅니다.

$$
\frac{P_{1}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{P_{2}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}>T
$$

이것은 가능도비 검정이며, $\frac{P_{1}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{P_{2}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}$는 가능도비라고 불립니다. 예를 들어, 두 Gaussian 분포 [즉, $f_{1}=\mathcal{N}\left(1, \sigma^{2}\right)$와 $f_{2}=\mathcal{N}\left(-1, \sigma^{2}\right)$ 사이의 검정]에서 가능도비는 다음과 같습니다.

$$
\begin{aligned}
\frac{f_{1}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{f_{2}\left(X_{1}, X_{2}, \ldots, X_{n}\right)} & =\frac{\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{\left(X_{i}-1\right)^{2}}{2 \sigma^{2}}}}{\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{\left(X_{i}+1\right)^{2}}{2 \sigma^{2}}}} \\
& =e^{+\frac{2 \sum_{i=1}^{n} X_{i}}{\sigma^{2}}} \\
& =e^{+\frac{2 n \bar{X}_{n}}{\sigma^{2}}}
\end{aligned}
$$

따라서 가능도비 검정은 표본 평균 $\bar{X}_{n}$을 임계값과 비교하는 것으로 구성됩니다. 두 오류 확률을 같게 하려면 $T=1$로 설정해야 합니다. 이는 그림 11.8에 나와 있습니다.

정리 11.7.1에서 최적 검정이 가능도비 검정임을 보였습니다. 로그 가능도비를 다음과 같이 다시 쓸 수 있습니다.

$$
\begin{aligned}
L\left(X_{1}, X_{2}, \ldots, X_{n}\right) & =\log \frac{P_{1}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{P_{2}\left(X_{1}, X_{2}, \ldots, X_{n}\right)} \\
& =\sum_{i=1}^{n} \log \frac{P_{1}\left(X_{i}\right)}{P_{2}\left(X_{i}\right)} \\
& =\sum_{a \in \mathcal{X}} n P_{X^{n}}(a) \log \frac{P_{1}(a)}{P_{2}(a)} \\
& =\sum_{a \in \mathcal{X}} n P_{X^{n}}(a) \log \frac{P_{1}(a)}{P_{2}(a)} \frac{P_{X^{n}}(a)}{P_{X^{n}}(a)}
\end{aligned}
$$
<!-- Page 404 -->

그림 11.8. 두 개의 가우시안 분포 간의 검정.

$$
\begin{aligned}
= & \sum_{a \in \mathcal{X}} n P_{X^{n}}(a) \log \frac{P_{X^{n}}(a)}{P_{2}(a)} \\
& -\sum_{a \in \mathcal{X}} n P_{X^{n}}(a) \log \frac{P_{X^{n}}(a)}{P_{1}(a)} \\
= & n D\left(P_{X^{n}} \| P_{2}\right)-n D\left(P_{X^{n}} \| P_{1}\right)
\end{aligned}
$$

표본 유형과 두 분포 각각 간의 상대 엔트로피 거리의 차이입니다. 따라서 가능도 비율 검정

$$
\frac{P_{1}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{P_{2}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}>T
$$

은 다음과 동등합니다.

$$
D\left(P_{X^{n}} \| P_{2}\right)-D\left(P_{X^{n}} \| P_{1}\right)>\frac{1}{n} \log T
$$

검정을 가설 $H_{1}$을 선택하는 것에 해당하는 유형의 단순체 영역을 지정하는 것으로 간주할 수 있습니다. 최적 영역은 (11.194)의 형태이며, 여기서 영역의 경계는 거리 간의 차이가 상수인 유형의 집합입니다. 이 경계는 유클리드 기하학에서 수직 이등분선의 유사체입니다. 검정은 그림 11.9에 설명되어 있습니다.

이제 Sanov의 정리에 기반한 비공식적인 논증을 통해 다른 오류 확률을 얻기 위한 임계값을 선택하는 방법을 보여드리겠습니다. 가설 1이 받아들여지는 집합을 $B$라고 합시다. 확률

<!-- Page 405 -->

그림 11.9. 확률 단체에서의 우도비 검정.
제1종 오류의 확률은 다음과 같습니다.

$$
\alpha_{n}=P_{1}^{n}\left(P_{X^{n}} \in B^{c}\right)
$$

집합 $B^{c}$는 볼록 집합이므로, Sanov의 정리를 사용하여 오류 확률이 $B^{c}$의 가장 가까운 원소와 $P_{1}$ 사이의 상대 엔트로피에 의해 본질적으로 결정됨을 보일 수 있습니다. 따라서,

$$
\alpha_{n} \doteq 2^{-n D\left(P_{1}^{*} \| P_{1}\right)}
$$

여기서 $P_{1}^{*}$는 $B^{c}$의 분포 $P_{1}$에 가장 가까운 원소입니다. 마찬가지로,

$$
\beta_{n} \doteq 2^{-n D\left(P_{2}^{*} \| P_{2}\right)}
$$

여기서 $P_{2}^{*}$는 $B$의 분포 $P_{2}$에 가장 가까운 원소입니다.
이제 제약 조건 $D\left(P \| P_{2}\right)-$ $D\left(P \| P_{1}\right) \geq \frac{1}{n} \log T$ 하에서 $D\left(P \| P_{2}\right)$를 최소화하면 $P_{2}$에 가장 가까운 $B$ 내의 타입이 결정됩니다. 라그랑주 승수법을 사용하여 제약 조건 $D\left(P \| P_{2}\right)-D\left(P \| P_{1}\right)=$ $\frac{1}{n} \log T$ 하에서 $D\left(P \| P_{2}\right)$의 최소화를 설정하면 다음과 같습니다.

$$
J(P)=\sum P(x) \log \frac{P(x)}{P_{2}(x)}+\lambda \sum P(x) \log \frac{P_{1}(x)}{P_{2}(x)}+v \sum P(x)
$$

$P(x)$에 대해 미분하고 0으로 설정하면 다음과 같습니다.

$$
\log \frac{P(x)}{P_{2}(x)}+1+\lambda \log \frac{P_{1}(x)}{P_{2}(x)}+v=0
$$
<!-- Page 406 -->
이러한 방정식들을 풀면, 다음과 같은 형태의 최소 $P$를 얻습니다.

$$
P_{2}^{*}=P_{\lambda^{*}}=\frac{P_{1}^{\lambda}(x) P_{2}^{1-\lambda}(x)}{\sum_{a \in \mathcal{X}} P_{1}^{\lambda}(a) P_{2}^{1-\lambda}(a)}
$$

여기서 $\lambda$는 $D\left(P_{\lambda^{*}} \| P_{1}\right)-D\left(P_{\lambda^{*}} \| P_{2}\right)=\frac{1}{n} \log T$가 되도록 선택됩니다.
식 (11.200)의 대칭성으로부터, $P_{1}^{*}=P_{2}^{*}$이고 오류 확률은 상대 엔트로피 $D\left(P^{*}\left\|P_{1}\right)\right.$ 및 $D\left(P^{*}\left\|P_{2}\right)\right.$에 의해 주어지는 지수와 함께 지수적으로 거동한다는 것을 명확히 알 수 있습니다. 또한, $\lambda \rightarrow 1$일 때 $P_{\lambda} \rightarrow P_{1}$이고, $\lambda \rightarrow 0$일 때 $P_{\lambda} \rightarrow P_{2}$임을 식에서 알 수 있습니다. $\lambda$가 변함에 따라 $P_{\lambda}$가 그리는 곡선은 심플렉스 내의 측지선입니다. 여기서 $P_{\lambda}$는 정규화된 볼록 조합이며, 조합은 지수 안에 있습니다 (그림 11.9).

다음 섹션에서는 두 가지 오류 유형 중 하나가 임의로 느리게 0으로 가는 경우의 최적 오류 지수를 계산합니다 (Chernoff-Stein lemma). 또한 두 오류 확률의 가중 합을 최소화하고 Chernoff 정보 경계를 얻을 것입니다.

# 11.8 CHERNOFF-STEIN LEMMA

두 오류 확률 중 하나를 고정시키고 다른 하나를 가능한 한 작게 만드는 경우의 가설 검정을 고려합니다. 다른 오류 확률은 상대 엔트로피와 동일한 지수율을 갖는 지수적으로 작은 값임을 보일 것입니다. 증명 방법은 AEP의 상대 엔트로피 버전을 사용합니다.

정리 11.8.1 (상대 엔트로피에 대한 AEP) $X_{1}, X_{2}, \ldots, X_{n}$이 $P_{1}(x)$에 따라 i.i.d.로 추출된 확률 변수들의 시퀀스이고, $P_{2}(x)$가 $\mathcal{X}$에 대한 다른 분포라고 가정합니다. 그러면

$$
\frac{1}{n} \log \frac{P_{1}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{P_{2}\left(X_{1}, X_{2}, \ldots, X_{n}\right)} \rightarrow D\left(P_{1} \| P_{2}\right) \quad \text { 확률적으로. }
$$

증명: 이는 큰 수의 법칙으로부터 직접적으로 도출됩니다.

$$
\frac{1}{n} \log \frac{P_{1}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{P_{2}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}=\frac{1}{n} \log \frac{\prod_{i=1}^{n} P_{1}\left(X_{i}\right)}{\prod_{i=1}^{n} P_{2}\left(X_{i}\right)}
$$
<!-- Page 407 -->
$$
\begin{aligned}
& =\frac{1}{n} \sum_{i=1}^{n} \log \frac{P_{1}\left(X_{i}\right)}{P_{2}\left(X_{i}\right)} \\
& \rightarrow E_{P_{1}} \log \frac{P_{1}(X)}{P_{2}(X)} \text { in probability } \\
& =D\left(P_{1} \| P_{2}\right)
\end{aligned}
$$

일반적인 AEP와 마찬가지로, 경험적 상대 엔트로피가 기댓값에 가까운 상대 엔트로피 전형 시퀀스를 정의할 수 있습니다.

정의 고정된 $n$과 $\epsilon>0$에 대해, 시퀀스 $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in \mathcal{X}^{n}$는 다음을 만족할 때 상대 엔트로피 전형이라고 합니다.

$$
D\left(P_{1} \| P_{2}\right)-\epsilon \leq \frac{1}{n} \log \frac{P_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right)}{P_{2}\left(x_{1}, x_{2}, \ldots, x_{n}\right)} \leq D\left(P_{1} \| P_{2}\right)+\epsilon
$$

상대 엔트로피 전형 시퀀스의 집합을 상대 엔트로피 전형 집합 $A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)$라고 합니다.

상대 엔트로피 AEP의 결과로, 상대 엔트로피 전형 집합이 다음 속성을 만족함을 보일 수 있습니다.

# 정리 11.8.2

1. $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)$에 대해,

$$
\begin{aligned}
& P_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right) 2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)} \\
& \quad \leq P_{2}\left(x_{1}, x_{2}, \ldots, x_{n}\right) \\
& \quad \leq P_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right) 2^{-n\left(D\left(P_{1} \| P_{2}\right)-\epsilon\right)}
\end{aligned}
$$

2. $n$이 충분히 클 때, $P_{1}\left(A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)\right)>1-\epsilon$입니다.
3. $P_{2}\left(A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)\right)<2^{-n\left(D\left(P_{1} \| P_{2}\right)-\epsilon\right)}$입니다.
4. $n$이 충분히 클 때, $P_{2}\left(A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)\right)>(1-\epsilon) 2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)}$입니다.

증명: 증명은 정리 3.1.2의 증명과 같은 방식으로 진행되며, 계수 측도는 확률 측도 $P_{2}$로 대체됩니다. 속성 1의 증명은 상대 엔트로피의 정의에서 직접적으로 나옵니다.
<!-- Page 408 -->
일반적인 집합입니다. 두 번째 속성은 상대 엔트로피에 대한 AEP(정리 11.8.1)에서 비롯됩니다. 세 번째 속성을 증명하기 위해 다음과 같이 작성합니다.

$$
\begin{aligned}
& P_{2}\left(A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)\right)=\sum_{x^{n} \in A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)} P_{2}\left(x_{1}, x_{2}, \ldots, x_{n}\right) \\
& \quad \leq \sum_{x^{n} \in A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)} P_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right) 2^{-n\left(D\left(P_{1} \| P_{2}\right)-\epsilon\right)} \\
& \quad=2^{-n\left(D\left(P_{1} \| P_{2}\right)-\epsilon\right)} \sum_{x^{n} \in A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)} P_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right) \\
& \quad=2^{-n\left(D\left(P_{1} \| P_{2}\right)-\epsilon\right)} P_{1}\left(A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)\right) \\
& \quad \leq 2^{-n\left(D\left(P_{1} \| P_{2}\right)-\epsilon\right)}
\end{aligned}
$$

여기서 첫 번째 부등식은 속성 1에서 비롯되며, 두 번째 부등식은 $P_{1}$ 하에서 임의의 집합의 확률이 1보다 작다는 사실에서 비롯됩니다.

상대 엔트로피 일반 집합의 확률에 대한 하한을 증명하기 위해 확률에 대한 하한을 사용하여 유사한 논증을 사용합니다.

$$
\begin{aligned}
& P_{2}\left(A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)\right)=\sum_{x^{n} \in A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)} P_{2}\left(x_{1}, x_{2}, \ldots, x_{n}\right) \\
& \quad \geq \sum_{x^{n} \in A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)} P_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right) 2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)} \\
& \quad=2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)} \sum_{x^{n} \in A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)} P_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right) \\
& \quad=2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)} P_{1}\left(A_{\epsilon}^{(n)}\left(P_{1} \| P_{2}\right)\right) \\
& \quad \geq(1-\epsilon) 2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)}
\end{aligned}
$$

여기서 두 번째 부등식은 $A_{\epsilon}^{(n)}$ $\left(P_{1} \| P_{2}\right)$의 두 번째 속성에서 비롯됩니다.

3장에서 표준 AEP를 사용하여 높은 확률을 갖는 모든 집합이 일반 집합과 높은 교집합을 가지며 따라서 약 $2^{n H}$개의 요소를 갖는다는 것을 보여주었습니다. 이제 상대 엔트로피에 대한 해당 결과를 증명합니다.
<!-- Page 409 -->
## 11.8.1 보조정리

$B_{n} \subset \mathcal{X}^{n}$을 $x_{1}, x_{2}, \ldots, x_{n}$의 시퀀스 집합이라고 가정합니다. 이때 $P_{1}\left(B_{n}\right)>1-\epsilon$입니다. $P_{2}$를 $D\left(P_{1} \| P_{2}\right)<\infty$를 만족하는 다른 분포라고 가정합니다. 그러면 $P_{2}\left(B_{n}\right)>(1-2 \epsilon) 2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)}$입니다.

**증명:**

간단함을 위해 $A_{i}^{(n)}\left(P_{1} \| P_{2}\right)$를 $A_{n}$으로 표기하겠습니다. $P_{1}\left(B_{n}\right)>1-\epsilon$이고 $P\left(A_{n}\right)>1-\epsilon$ (정리 11.8.2)이므로, 합집합 사건의 경계에 의해 $P_{1}\left(A_{n}^{c} \cup B_{n}^{c}\right)<2 \epsilon$이거나 동등하게 $P_{1}\left(A_{n} \cap B_{n}\right)>1-2 \epsilon$입니다. 따라서,

$$
\begin{aligned}
P_{2}\left(B_{n}\right) & \geq P_{2}\left(A_{n} \cap B_{n}\right) \\
& =\sum_{x^{n} \in A_{n} \cap B_{n}} P_{2}\left(x^{n}\right) \\
& \geq \sum_{x^{n} \in A_{n} \cap B_{n}} P_{1}\left(x^{n}\right) 2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)} \\
& =2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)} \sum_{x^{n} \in A_{n} \cap B_{n}} P_{1}\left(x^{n}\right) \\
& =2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)} P_{1}\left(A_{n} \cap B_{n}\right) \\
& \geq 2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)}(1-2 \epsilon)
\end{aligned}
$$

여기서 두 번째 부등식은 상대 엔트로피의 전형적인 시퀀스(정리 11.8.2)의 속성에서 비롯되며, 마지막 부등식은 위에서 설명한 합집합 경계에서 비롯됩니다.

이제 두 가설 $P_{1}$ 대 $P_{2}$를 검정하는 문제를 고려합니다. 오류 확률 중 하나를 고정하고 다른 오류 확률을 최소화하려고 시도합니다. 상대 엔트로피가 오류 확률에서 가장 좋은 지수임을 보입니다.

## 11.8.3 정리 (체르노프-스타인 보조정리)

$X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim Q$라고 가정합니다. $D\left(P_{1} \| P_{2}\right)<\infty$일 때, $Q=P_{1}$와 $Q=P_{2}$의 두 대안 사이의 가설 검정을 고려합니다. $A_{n} \subseteq \mathcal{X}^{n}$을 가설 $H_{1}$에 대한 수락 영역이라고 합니다. 오류 확률을 다음과 같이 정의합니다.

$$
\alpha_{n}=P_{1}^{n}\left(A_{n}^{c}\right), \quad \beta_{n}=P_{2}^{n}\left(A_{n}\right)
$$

그리고 $0<\epsilon<\frac{1}{2}$에 대해 다음과 같이 정의합니다.

$$
\begin{gathered}
\beta_{n}^{\epsilon}=\min _{A_{n} \subseteq \mathcal{X}^{n}} \beta_{n} \\
\alpha_{n}<\epsilon
\end{gathered}
$$
<!-- Page 410 -->
그러면

$$
\lim _{n \rightarrow \infty} \frac{1}{n} \log \beta_{n}^{\epsilon}=-D\left(P_{1} \| P_{2}\right)
$$

증명: 이 정리를 두 부분으로 나누어 증명하겠습니다. 첫 번째 부분에서는 오류 확률 $\beta_{n}$이 $D\left(P_{1} \| P_{2}\right)$에 대해 지수적으로 0으로 수렴하는 집합 $A_{n}$의 수열을 제시합니다. 두 번째 부분에서는 다른 어떤 집합의 수열도 오류 확률에서 더 낮은 지수를 가질 수 없음을 보입니다.

첫 번째 부분에서는 집합 $A_{n}=A_{c}^{(n)}\left(P_{1} \| P_{2}\right)$를 선택합니다. 정리 11.8.2에서 증명했듯이, 이 집합의 수열은 충분히 큰 $n$에 대해 $P_{1}\left(A_{n}^{c}\right)<\epsilon$을 만족합니다. 또한,

$$
\lim _{n \rightarrow \infty} \frac{1}{n} \log P_{2}\left(A_{n}\right) \leq-\left(D\left(P_{1} \| P_{2}\right)-\epsilon\right)
$$

정리 11.8.2의 속성 3에 의해 얻어집니다. 따라서 상대 엔트로피 전형 집합은 보조 정리의 경계를 만족합니다.

다른 어떤 집합의 수열도 더 나아질 수 없음을 보이기 위해, $P_{1}\left(B_{n}\right)>1-\epsilon$을 만족하는 임의의 집합 $B_{n}$의 수열을 고려합니다. 보조 정리 11.8.1에 의해, 우리는 $P_{2}\left(B_{n}\right)>(1-2 \epsilon) 2^{-n\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)}$임을 알 수 있으며, 따라서

$$
\begin{aligned}
& \lim _{n \rightarrow \infty} \frac{1}{n} \log P_{2}\left(B_{n}\right)>-\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)+\lim _{n \rightarrow \infty} \frac{1}{n} \log (1-2 \epsilon) \\
& \quad=-\left(D\left(P_{1} \| P_{2}\right)+\epsilon\right)
\end{aligned}
$$

따라서 다른 어떤 집합의 수열도 $D\left(P_{1} \| P_{2}\right)$보다 더 나은 오류 확률 지수를 갖지 않습니다. 그러므로 집합 수열 $A_{n}=A_{c}^{(n)}\left(P_{1} \| P_{2}\right)$는 오류 확률의 지수 측면에서 점근적으로 최적입니다.

상대 엔트로피 전형 집합은 점근적으로 최적(즉, 최적의 점근적 속도를 달성)이지만, 고정된 가설 검정 문제에 대한 최적 집합은 아님을 유의하십시오. 오류 확률을 최소화하는 최적 집합은 Neyman-Pearson 보조 정리에 의해 주어진 것입니다.

# 11.9 CHERNOFF 정보

두 오류 확률을 별도로 취급하는 고전적인 설정에서 가설 검정 문제를 고려했습니다. Chernoff-Stein 보조 정리의 유도에서 $\alpha_{n} \leq \epsilon$으로 설정하고 $\beta_{n} \doteq 2^{-n D}$를 달성했습니다. 그러나 이 접근 방식은 대칭성이 부족합니다. 대신, 두 확률에 사전 확률을 할당하는 베이즈 접근 방식을 따를 수 있습니다.
<!-- Page 411 -->
가설입니다. 이 경우 개별 오류 확률의 가중 합으로 주어진 전체 오류 확률을 최소화하고자 합니다. 결과적인 오류 지수는 Chernoff 정보입니다.

설정은 다음과 같습니다: $X_{1}, X_{2}, \ldots, X_{n}$ i.i.d. $\sim Q$. 두 가지 가설이 있습니다: 사전 확률 $\pi_{1}$을 갖는 $Q=P_{1}$과 사전 확률 $\pi_{2}$를 갖는 $Q=P_{2}$입니다. 전체 오류 확률은 다음과 같습니다.

$$
P_{e}^{(n)}=\pi_{1} \alpha_{n}+\pi_{2} \beta_{n}
$$

다음과 같이 정의합니다.

$$
D^{*}=\lim _{n \rightarrow \infty}-\frac{1}{n} \log \min _{A_{n} \subseteq \mathcal{X}^{n}} P_{e}^{(n)}
$$

정리 11.9.1 (Chernoff) 베이즈 오류 확률에서 달성 가능한 최적의 지수는 $D^{*}$이며, 여기서

$$
D^{*}=D\left(P_{\lambda^{*}} \| P_{1}\right)=D\left(P_{\lambda^{*}} \| P_{2}\right)
$$

다음과 같습니다.

$$
P_{\lambda}=\frac{P_{1}^{\lambda}(x) P_{2}^{1-\lambda}(x)}{\sum_{a \in \mathcal{X}} P_{1}^{\lambda}(a) P_{2}^{1-\lambda}(a)}
$$

그리고 $\lambda^{*}$는 다음을 만족하는 $\lambda$의 값입니다.

$$
D\left(P_{\lambda^{*}} \| P_{1}\right)=D\left(P_{\lambda^{*}} \| P_{2}\right)
$$

증명: 증명의 기본 세부 사항은 섹션 11.8에서 제공되었습니다. 최적 검정은 가능도 비율 검정임을 보여주었으며, 이는 다음과 같은 형태로 간주될 수 있습니다.

$$
D\left(P_{X^{n}} \| P_{2}\right)-D\left(P_{X^{n}} \| P_{1}\right)>\frac{1}{n} \log T
$$

이 검정은 확률 단체(probability simplex)를 가설 1과 가설 2에 각각 해당하는 영역으로 나눕니다. 이는 그림 11.10에 설명되어 있습니다.

가설 1과 관련된 유형의 집합을 $A$라고 합시다. (11.200) 앞의 논의에서, $A^{c}$ 집합에서 $P_{1}$에 가장 가까운 점은 $A$의 경계에 있으며 (11.232)에 주어진 형태임을 알 수 있습니다. 그런 다음 섹션 11.8의 논의에서 $P_{\lambda}$가 분포임을 명확히 알 수 있습니다.
<!-- Page 412 -->

그림 11.10. 확률 단순체와 체르노프 정보.
$A$ 내에서 $P_{2}$에 가장 가까운 분포이며, $A^{c}$ 내에서 $P_{1}$에 가장 가까운 분포이기도 합니다. Sanov의 정리에 따라, 관련 오류 확률을 계산할 수 있습니다.

$$
\alpha_{n}=P_{1}^{n}\left(A^{c}\right) \doteq 2^{-n D\left(P_{\lambda} * \| P_{1}\right)}
$$

그리고

$$
\beta_{n}=P_{2}^{n}(A) \doteq 2^{-n D\left(P_{\lambda} * \| P_{2}\right)}
$$

베이즈 경우, 전체 오류 확률은 두 오류 확률의 가중 합입니다.

$$
P_{e} \doteq \pi_{1} 2^{-n D\left(P_{\lambda} \| P_{1}\right)}+\pi_{2} 2^{-n D\left(P_{\lambda} \| P_{2}\right)} \doteq 2^{-n \min \left\{D\left(P_{\lambda} \| P_{1}\right), D\left(P_{\lambda} \| P_{2}\right)\right\}}
$$

지수율은 최악의 지수에 의해 결정되기 때문입니다. $D\left(P_{\lambda} \| P_{1}\right)$은 $\lambda$와 함께 증가하고 $D\left(P_{\lambda} \| P_{2}\right)$는 $\lambda$와 함께 감소하므로, $\left\{D\left(P_{\lambda} \| P_{1}\right), D\left(P_{\lambda} \| P_{2}\right)\right\}$의 최솟값의 최대값은 두 값이 같을 때 달성됩니다. 이는 그림 11.11에 설명되어 있습니다. 따라서 $\lambda$를 다음과 같이 선택합니다.

$$
D\left(P_{\lambda} \| P_{1}\right)=D\left(P_{\lambda} \| P_{2}\right)
$$

따라서 $C\left(P_{1}, P_{2}\right)$는 오류 확률에 대해 달성 가능한 가장 높은 지수이며 체르노프 정보라고 불립니다.
<!-- Page 413 -->

그림 11.11. $\lambda$의 함수로서의 상대 엔트로피 $D\left(P_{\lambda} \| P_{1}\right)$ 및 $D\left(P_{\lambda} \| P_{2}\right)$.

정의 $D^{*}=D\left(P_{\lambda^{*}} \| P_{1}\right)=D\left(P_{\lambda^{*}} \| P_{2}\right)$는 Chernoff 정보의 표준 정의와 동등합니다.

$$
C\left(P_{1}, P_{2}\right) \triangleq-\min _{0 \leq \lambda \leq 1} \log \left(\sum_{x} P_{1}^{\lambda}(x) P_{2}^{1-\lambda}(x)\right)
$$

(11.231)과 (11.239)의 동등성을 독자에게 연습 문제로 남겨둡니다.

Chernoff 정보 경계의 일반적인 유도를 간략하게 설명합니다. 최대 사후 확률 결정 규칙은 베이즈 오차 확률을 최소화합니다. 최대 사후 확률 규칙에 대한 가설 $H_{1}$의 결정 영역 $A$는 다음과 같습니다.

$$
A=\left\{\mathbf{x}: \frac{\pi_{1} P_{1}(\mathbf{x})}{\pi_{2} P_{2}(\mathbf{x})}>1\right\}
$$

이는 사후 확률이 가설 $H_{1}$에 대한 것이 가설 $H_{2}$에 대한 것보다 큰 결과의 집합입니다. 이 규칙에 대한 오차 확률은 다음과 같습니다.

$$
\begin{aligned}
P_{e} & =\pi_{1} \alpha_{n}+\pi_{2} \beta_{n} \\
& =\sum_{A^{c}} \pi_{1} P_{1}+\sum_{A} \pi_{2} P_{2} \\
& =\sum \min \left\{\pi_{1} P_{1}, \pi_{2} P_{2}\right\}
\end{aligned}
$$
<!-- Page 414 -->
임의의 두 양수 $a$와 $b$에 대해 다음이 성립합니다.

$$
\min \{a, b\} \leq a^{\lambda} b^{1-\lambda} \quad \text { for all } 0 \leq \lambda \leq 1
$$

이를 사용하여 연쇄를 계속하면 다음과 같습니다.

$$
\begin{aligned}
P_{e} & =\sum \min \left\{\pi_{1} P_{1}, \pi_{2} P_{2}\right\} \\
& \leq \sum\left(\pi_{1} P_{1}\right)^{\lambda}\left(\pi_{2} P_{2}\right)^{1-\lambda} \\
& \leq \sum P_{1}^{\lambda} P_{2}^{1-\lambda}
\end{aligned}
$$

i.i.d. 관측값 시퀀스의 경우 $P_{k}(\mathbf{x})=\prod_{i=1}^{n} P_{k}\left(x_{i}\right)$이며,

$$
\begin{aligned}
P_{e}^{(n)} & \leq \sum \pi_{1}^{\lambda} \pi_{2}^{1-\lambda} \prod_{i} P_{1}^{\lambda}\left(x_{i}\right) P_{2}^{1-\lambda}\left(x_{i}\right) \\
& =\pi_{1}^{\lambda} \pi_{2}^{1-\lambda} \prod_{i} \sum P_{1}^{\lambda}\left(x_{i}\right) P_{2}^{1-\lambda}\left(x_{i}\right) \\
& \leq \prod_{x_{i}} \sum P_{1}^{\lambda} P_{2}^{1-\lambda} \\
& =\left(\sum_{x} P_{1}^{\lambda} P_{2}^{1-\lambda}\right)^{n}
\end{aligned}
$$

여기서 (11.250)은 $\pi_{1} \leq 1, \pi_{2} \leq 1$이므로 성립합니다. 따라서 다음이 성립합니다.

$$
\frac{1}{n} \log P_{e}^{(n)} \leq \log \sum P_{1}^{\lambda}(x) P_{2}^{1-\lambda}(x)
$$

이것이 모든 $\lambda$에 대해 참이므로, $0 \leq \lambda \leq 1$에 대한 최소값을 취하면 Chernoff 정보 바운드가 됩니다. 이는 지수가 $C\left(P_{1}, P_{2}\right)$보다 나을 수 없음을 증명합니다. 달성 가능성은 Theorem 11.9.1에서 따릅니다.

베이즈 오차 지수는 $\pi_{1}$과 $\pi_{2}$가 0이 아닌 한 실제 값에 의존하지 않는다는 점에 유의하십시오. 본질적으로 사전 확률의 효과는 큰 표본 크기에서는 사라집니다. 최적 결정 규칙은 최대 사후 확률을 갖는 가설을 선택하는 것이며, 이는 다음 테스트에 해당합니다.

$$
\frac{\pi_{1} P_{1}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{\pi_{2} P_{2}\left(X_{1}, X_{2}, \ldots, X_{n}\right)} \gtrsim 1
$$
<!-- Page 415 -->
$\log$을 취하고 $n$으로 나누면 이 검정은 다음과 같이 다시 작성할 수 있습니다.

$$
\frac{1}{n} \log \frac{\pi_{1}}{\pi_{2}}+\frac{1}{n} \sum_{i} \log \frac{P_{1}\left(X_{i}\right)}{P_{2}\left(X_{i}\right)} \gtrsim 0
$$

여기서 두 번째 항은 $P_{1}$ 또는 $P_{2}$가 실제 분포인 경우에 따라 $D\left(P_{1} \| P_{2}\right)$ 또는 $-D\left(P_{2} \| P_{1}\right)$으로 수렴합니다. 첫 번째 항은 0으로 수렴하며, 사전 분포의 효과는 사라집니다.

마지막으로, 대규모 편차 이론과 가설 검정에 대한 논의를 마무리하기 위해 조건부 극한 정리의 예를 고려합니다.

예제 11.9.1 메이저 리그 야구 선수들의 타율은 260이고 표준 편차는 15이며, 마이너 리그 야구 선수들의 타율은 240이고 표준 편차는 15라고 가정합니다. 임의로 선택된 리그의 선수 100명으로 구성된 그룹이 그룹 타율 250 이상을 기록하여 메이저 리거로 판단됩니다. 이제 우리가 틀렸다는 것을 알게 되었습니다. 이 선수들은 마이너 리거입니다. 이 100명의 선수들의 타율 분포에 대해 무엇을 말할 수 있습니까? 조건부 극한 정리를 사용하여 이 선수들의 타율 분포가 평균 250, 표준 편차 15를 가질 것임을 보여줄 수 있습니다. 이를 보기 위해 문제를 다음과 같이 추상화합니다.

평균이 다르고 분산이 같은 두 개의 가우시안 분포 $f_{1}=\mathcal{N}\left(1, \sigma^{2}\right)$와 $f_{2}=\mathcal{N}\left(-1, \sigma^{2}\right)$ 사이의 검정 예를 고려해 봅시다. 섹션 11.8에서 논의한 바와 같이, 이 경우의 우도비 검정은 표본 평균을 임계값과 비교하는 것과 동등합니다. 베이즈 검정은 " $\frac{1}{n} \sum_{i=1}^{n} X_{i}>0$이면 가설 $f=f_{1}$을 수락합니다."입니다. 이제 이 검정에서 제1종 오류(실제로 $f=f_{2}$인데 $f=f_{1}$이라고 말하는 경우)를 범한다고 가정해 봅시다. 오류를 범했다는 조건 하에 표본의 조건부 분포는 무엇입니까?

다양한 가능성을 추측할 수 있습니다.

- 표본은 두 정규 분포의 $\left(\frac{1}{2}, \frac{1}{2}\right)$ 혼합처럼 보일 것입니다. 이것이 타당해 보이지만, 틀렸습니다.
- 모든 $i$에 대해 $X_{i} \approx 0$입니다. 이것은 매우 가능성이 낮지만, 조건부로 $\bar{X}_{n}$이 0에 가깝다는 것은 가능합니다.
- 올바른 답은 조건부 극한 정리에 의해 제공됩니다. 실제 분포가 $f_{2}$이고 표본 유형이 집합 $A$에 속하는 경우, 조건부 분포는 $A$에서 $f_{2}$에 가장 가까운 분포인 $f^{*}$에 가깝습니다. 대칭에 의해, 이것은 (11.232)에서 $\lambda=\frac{1}{2}$에 해당합니다. 계산하면
<!-- Page 416 -->
분포를 얻으면 다음과 같습니다.

$$
\begin{aligned}
f^{*}(x) & =\frac{\left(\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-1)^{2}}{2 \sigma^{2}}}\right)^{\frac{1}{2}}\left(\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x+1)^{2}}{2 \sigma^{2}}}\right)^{\frac{1}{2}}}{\int\left(\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-1)^{2}}{2 \sigma^{2}}}\right)^{\frac{1}{2}}\left(\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x+1)^{2}}{2 \sigma^{2}}}\right)^{\frac{1}{2}} d x} \\
& =\frac{\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{\left(x^{2}+1\right)}{2 \sigma^{2}}}}{\int \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{\left(x^{2}+1\right)}{2 \sigma^{2}}} d x} \\
& =\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{x^{2}}{2 \sigma^{2}}} \\
& =\mathcal{N}\left(0, \sigma^{2}\right)
\end{aligned}
$$

조건부 분포가 평균이 0이고 원래 분포와 동일한 분산을 갖는 정규 분포라는 점은 흥미롭습니다. 이는 이상하지만 사실입니다. 만약 우리가 하나의 정규 모집단을 다른 모집단으로 착각한다면, 이 모집단의 "모양"은 여전히 동일한 분산과 다른 평균을 갖는 정규 분포처럼 보입니다. 명백히, 이 드문 사건은 기이하게 보이는 데이터에서 비롯되지 않습니다.

예제 11.9.2 (대규모 편차 이론과 축구) 점수가 획득한 야드 수와 직접적으로 관련되는 축구의 매우 단순한 버전을 고려해 보십시오. 코치가 달리기 또는 패스 전략 중에서 선택할 수 있다고 가정합니다. 각 전략에는 획득한 야드 수에 대한 분포가 연관되어 있습니다. 예를 들어, 일반적으로 달리기는

그림 11.12. 달리기 또는 패스 플레이에서 획득한 야드의 분포.
<!-- Page 417 -->
매우 높은 확률로 몇 야드를 얻게 되지만, 패스는 낮은 확률로 큰 이득을 가져옵니다. 분포의 예시는 그림 11.12에 나와 있습니다.

경기 초반에는 코치가 가장 큰 기대 이득을 약속하는 전략을 사용합니다. 이제 경기 종료 몇 분 전이고 한 팀이 큰 점수 차이로 앞서고 있다고 가정해 봅시다. (퍼스트 다운과 적응형 수비는 무시하겠습니다.) 따라서 뒤처진 팀은 매우 운이 좋아야만 이길 수 있습니다. 이기기 위해 운이 필요하다면, 운이 좋다고 가정하고 그에 따라 플레이하는 것이 좋습니다. 적절한 전략은 무엇입니까?

팀에 $n$번의 플레이가 남았고 $l$야드를 얻어야 한다고 가정합니다. 여기서 $l$은 각 플레이에서의 기대 이득의 $n$배보다 훨씬 큽니다. 팀이 $l$야드를 달성하는 데 성공할 확률은 지수적으로 작습니다. 따라서 이 사건의 확률을 계산하기 위해 대규모 편차 결과와 Sanov의 정리를 사용할 수 있습니다. 정확히 말하면, $Z_{i}$가 독립적인 확률 변수이고 $Z_{i}$가 선택된 전략에 해당하는 분포를 가질 때 $\sum_{i=1}^{n} Z_{i} \geq n \alpha$인 사건의 확률을 계산하고자 합니다.

이 상황은 그림 11.13에 나와 있습니다. $E$를 제약 조건에 해당하는 유형의 집합이라고 합시다.

$$
E=\left\{P: \sum_{a \in \mathcal{X}} P(a) a \geq \alpha\right\}
$$

만약 $P_{1}$이 항상 패스하는 것에 해당하는 분포라면, 승리 확률은 표본 유형이 $E$에 속할 확률이며, 이는 Sanov의 정리에 의해 $2^{-n D\left(P_{1}^{*} \mid P_{1}\right)}$입니다. 여기서 $P_{1}^{*}$은 $E$에서 $P_{1}$에 가장 가까운 분포입니다. 마찬가지로, 코치가 항상 러싱 게임을 사용한다면,

그림 11.13. 미식축구 경기의 확률 단순체.
<!-- Page 418 -->
승률은 $2^{-n D\left(P_{2}^{*} \| P_{2}\right)}$입니다. 만약 그가 전략의 혼합을 사용한다면 어떻게 될까요? 혼합 전략 $P_{\lambda}=\lambda P_{1}+(1-\lambda) P_{2}$을 사용했을 때의 승률인 $2^{-n D\left(P_{\lambda}^{*} \| P_{\lambda}\right)}$이 순수하게 패스하거나 순수하게 달리는 것보다 더 나을 수 있을까요? 다소 놀라운 대답은 그렇다는 것이며, 이는 예시를 통해 보여줄 수 있습니다. 이는 방어를 혼란스럽게 한다는 사실 외에 혼합 전략을 사용해야 하는 이유를 제공합니다.

이 절은 Chernoff에 의한 또 다른 부등식으로 마무리합니다. 이 부등식은 Markov 부등식의 특별한 버전입니다. 이 부등식은 Chernoff bound라고 불립니다.

정리 11.9.1 $Y$를 임의의 확률 변수라 하고 $\psi(s)$를 $Y$의 모멘트 생성 함수라고 합시다.

$$
\psi(s)=E e^{s Y}
$$

그러면 모든 $s \geq 0$에 대해,

$$
\operatorname{Pr}(Y \geq a) \leq e^{-s a} \psi(s)
$$

따라서

$$
\operatorname{Pr}(Y \geq a) \leq \min _{s \geq 0} e^{-s a} \psi(s)
$$

증명: 음이 아닌 확률 변수 $e^{s Y}$에 Markov 부등식을 적용합니다.

# 11.10 피셔 정보와 크라메르-라오 부등식

통계적 추정의 표준적인 문제는 해당 분포에서 추출된 데이터 샘플로부터 분포의 매개변수를 결정하는 것입니다. 예를 들어, $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim \mathcal{N}(\theta, 1)$에서 추출되었다고 가정합니다. $n$ 크기의 샘플로부터 $\theta$를 추정하고자 한다고 가정해 봅시다. $\theta$를 추정하기 위해 사용할 수 있는 데이터의 여러 함수가 있습니다. 예를 들어, 첫 번째 샘플 $X_{1}$을 사용할 수 있습니다. $X_{1}$의 기댓값은 $\theta$이지만, 더 많은 데이터를 사용하면 더 나은 결과를 얻을 수 있다는 것은 분명합니다. 우리는 $\theta$의 최적 추정량이 표본 평균 $\bar{X}_{n}=\frac{1}{n} \sum X_{i}$이라고 추측합니다. 실제로 $\bar{X}_{n}$이 최소 평균 제곱 오차 비편향 추정량임이 입증될 수 있습니다.

몇 가지 정의부터 시작하겠습니다. $\{f(x ; \theta)\}, \theta \in \Theta$가 매개변수 집합이라고 불리는 $\Theta$에 대한 밀도들의 색인된 가족을 나타낸다고 합시다. $f(x ; \theta) \geq 0, \int f(x ; \theta) d x=1$은 모든 $\theta \in \Theta$에 대해 성립합니다.

정의 $n$ 크기 샘플에 대한 $\theta$의 추정량은 함수 $T$ : $\lambda^{m} \rightarrow \Theta$입니다.
<!-- Page 419 -->
추정량은 모수의 값을 근사하기 위한 것입니다. 따라서 근사의 좋음을 어느 정도 파악하는 것이 바람직합니다. 추정량의 오차를 $T-\theta$라고 부르겠습니다. 오차는 확률 변수입니다.

정의 모수 $\theta$에 대한 추정량 $T\left(X_{1}, X_{2}, \ldots, X_{n}\right)$의 편향은 추정량 오차의 기댓값입니다 [즉, 편향은 $E_{\theta} T\left(x_{1}, x_{2}, \ldots, x_{n}\right)-\theta$입니다]. 아래 첨자 $\theta$는 기댓값이 밀도 $f(\cdot ; \theta)$에 대한 것임을 의미합니다. 추정량이 편향이 0이면 모든 $\theta \in \Theta$에 대해 불편향 추정량이라고 합니다 (즉, 추정량의 기댓값이 모수와 같습니다).

예제 11.10.1 $x \geq 0$인 지수 분포 $f(x)=(1 / \lambda)$ $e^{-x / \lambda}$에서 i.i.d.로 추출된 $X_{1}, X_{2}, \ldots, X_{n}$이 있다고 가정합니다. $\lambda$의 추정량에는 $X_{1}$과 $\bar{X}_{n}$이 포함됩니다. 두 추정량 모두 불편향입니다.

편향은 오차의 기댓값이며, 편향이 0이라는 사실이 오차가 높은 확률로 낮다는 것을 보장하지는 않습니다. 오차에 대한 손실 함수를 살펴볼 필요가 있습니다. 가장 일반적으로 선택되는 손실 함수는 오차의 제곱의 기댓값입니다. 좋은 추정량은 낮은 제곱 오차 기댓값을 가져야 하며, 표본 크기가 무한대로 갈 때 오차가 0에 가까워져야 합니다. 이는 다음 정의를 동기 부여합니다.

정의 $\theta$에 대한 추정량 $T\left(X_{1}, X_{2}, \ldots, X_{n}\right)$은 확률적으로 일치한다고 말합니다.
$T\left(X_{1}, X_{2}, \ldots, X_{n}\right) \rightarrow \theta$ as $n \rightarrow \infty$ in probability.
일치성은 바람직한 점근적 속성이지만, 작은 표본 크기에 대한 동작에도 관심이 있습니다. 그런 다음 평균 제곱 오차를 기준으로 추정량의 순위를 매길 수 있습니다.

정의 추정량 $T_{1}\left(X_{1}, X_{2}, \ldots, X_{n}\right)$은 모든 $\theta$에 대해 다른 추정량 $T_{2}\left(X_{1}, X_{2}, \ldots, X_{n}\right)$를 지배한다고 말합니다.
$$
E\left(T_{1}\left(X_{1}, X_{2}, \ldots, X_{n}\right)-\theta\right)^{2} \leq E\left(T_{2}\left(X_{1}, X_{2}, \ldots, X_{n}\right)-\theta\right)^{2}
$$
이는 자연스러운 질문을 제기합니다. 다른 모든 추정량을 지배하는 $\theta$의 최적 추정량이 존재합니까? 이 질문에 답하기 위해 모든 추정량의 평균 제곱 오차에 대한 Cramér-Rao 하한을 도출합니다. 먼저 분포 $f(x ; \theta)$의 점수 함수를 정의합니다. 그런 다음 Cauchy-Schwarz 부등식을 사용하여 모든 불편향 추정량의 분산에 대한 Cramér-Rao 하한을 증명합니다.
<!-- Page 420 -->
정의 점수 $V$는 다음과 같이 정의되는 확률 변수입니다.

$$
V=\frac{\partial}{\partial \theta} \ln f(X ; \theta)=\frac{\frac{\partial}{\partial \theta} f(X ; \theta)}{f(X ; \theta)}
$$

여기서 $X \sim f(x ; \theta)$입니다.
점수의 평균값은 다음과 같습니다.

$$
\begin{aligned}
E V & =\int \frac{\frac{\partial}{\partial \theta} f(x ; \theta)}{f(x ; \theta)} f(x ; \theta) d x \\
& =\int \frac{\partial}{\partial \theta} f(x ; \theta) d x \\
& =\frac{\partial}{\partial \theta} \int f(x ; \theta) d x \\
& =\frac{\partial}{\partial \theta} 1 \\
& =0
\end{aligned}
$$

따라서 $E V^{2}=\operatorname{var}(V)$입니다. 점수의 분산은 특별한 의미를 가집니다.

정의 피셔 정보량 $J(\theta)$는 점수의 분산입니다.

$$
J(\theta)=E_{\theta}\left[\frac{\partial}{\partial \theta} \ln f(X ; \theta)\right]^{2}
$$

$n$개의 확률 변수 $X_{1}, X_{2}, \ldots, X_{n}$가 i.i.d. $\sim f(x ; \theta)$로 추출된 표본을 고려하면 다음과 같습니다.

$$
f\left(x_{1}, x_{2}, \ldots, x_{n} ; \theta\right)=\prod_{i=1}^{n} f\left(x_{i} ; \theta\right)
$$

점수 함수는 개별 점수 함수의 합입니다.

$$
\begin{aligned}
V\left(X_{1}, X_{2}, \ldots, X_{n}\right) & =\frac{\partial}{\partial \theta} \ln f\left(X_{1}, X_{2}, \ldots, X_{n} ; \theta\right) \\
& =\sum_{i=1}^{n} \frac{\partial}{\partial \theta} \ln f\left(X_{i} ; \theta\right) \\
& =\sum_{i=1}^{n} V\left(X_{i}\right)
\end{aligned}
$$
<!-- Page 421 -->
$V\left(X_{i}\right)$는 평균이 0인 독립적이고 동일하게 분포된 확률변수입니다. 따라서 $n$-표본 Fisher 정보는 다음과 같습니다.

$$
\begin{aligned}
J_{n}(\theta) & =E_{\theta}\left[\frac{\partial}{\partial \theta} \ln f\left(X_{1}, X_{2}, \ldots, X_{n} ; \theta\right)\right]^{2} \\
& =E_{\theta} V^{2}\left(X_{1}, X_{2}, \ldots, X_{n}\right) \\
& =E_{\theta}\left(\sum_{i=1}^{n} V\left(X_{i}\right)\right)^{2} \\
& =\sum_{i=1}^{n} E_{\theta} V^{2}\left(X_{i}\right) \\
& =n J(\theta)
\end{aligned}
$$

결과적으로, $n$개의 i.i.d. 표본에 대한 Fisher 정보는 개별 Fisher 정보의 $n$배입니다. Fisher 정보의 중요성은 다음 정리에 나타나 있습니다.

정리 11.10.1 (Cramér-Rao 부등식) 모수 $\theta$의 모든 불편 추정량 $T(X)$의 평균 제곱 오차는 Fisher 정보의 역수보다 작지 않습니다.

$$
\operatorname{var}(T) \geq \frac{1}{J(\theta)}
$$

증명: $V$를 score function으로, $T$를 추정량으로 놓습니다. Cauchy-Schwarz 부등식에 의해 다음을 얻습니다.

$$
\left(E_{\theta}\left[\left(V-E_{\theta} V\right)\left(T-E_{\theta} T\right)\right]\right)^{2} \leq E_{\theta}\left(V-E_{\theta} V\right)^{2} E_{\theta}\left(T-E_{\theta} T\right)^{2}
$$

$T$는 불편 추정량이므로 모든 $\theta$에 대해 $E_{\theta} T=\theta$입니다. (11.269)에 의해 $E_{\theta} V=0이며, 따라서 $E_{\theta}\left(V-E_{\theta} V\right)\left(T-E_{\theta} T\right)=E_{\theta}(V T)$입니다. 또한 정의에 의해 $\operatorname{var}(V)=J(\theta)$입니다. 이러한 조건들을 (11.281)에 대입하면 다음과 같습니다.

$$
\left[E_{\theta}(V T)\right]^{2} \leq J(\theta) \operatorname{var}(T)
$$

이제,

$$
E_{\theta}(V T)=\int \frac{\frac{\partial}{\partial \theta} f(x ; \theta)}{f(x ; \theta)} T(x) f(x ; \theta) d x
$$
<!-- Page 422 -->
$$
\begin{aligned}
& =\int \frac{\partial}{\partial \theta} f(x ; \theta) T(x) d x \\
& =\frac{\partial}{\partial \theta} \int f(x ; \theta) T(x) d x \\
& =\frac{\partial}{\partial \theta} E_{\theta} T \\
& =\frac{\partial}{\partial \theta} \theta \\
& =1
\end{aligned}
$$

여기서 (11.285)의 미분과 적분의 교환은 적절하게 잘 정의된 $f(x ; \theta)$에 대한 유계 수렴 정리를 사용하여 정당화될 수 있으며, (11.287)은 추정량 $T$가 불편 추정량이라는 사실로부터 도출됩니다. 이를 (11.282)에 대입하면 다음과 같은 결과를 얻습니다.

$$
\operatorname{var}(T) \geq \frac{1}{J(\theta)}
$$

이는 불편 추정량에 대한 Cramér-Rao 부등식입니다.

본질적으로 동일한 논증을 통해, 임의의 추정량에 대해 다음과 같음을 보일 수 있습니다.

$$
E(T-\theta)^{2} \geq \frac{\left(1+b_{T}^{\prime}(\theta)\right)^{2}}{J(\theta)}+b_{T}^{2}(\theta)
$$

여기서 $b_{T}(\theta)=E_{\theta} T-\theta$이고 $b_{T}^{\prime}(\theta)$는 $\theta$에 대한 $b_{T}(\theta)$의 도함수입니다. 이에 대한 증명은 장의 마지막 문제로 남겨둡니다.

예제 11.10.2 $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim \mathcal{N}\left(\theta, \sigma^{2}\right)$이고 $\sigma^{2}$이 알려져 있다고 가정합니다. 여기서 $J(\theta)=n / \sigma^{2}$입니다. $T\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\bar{X}_{n}=\frac{1}{n} \sum X_{i}$라고 하면, $E_{\theta}\left(\bar{X}_{n}-\theta\right)^{2}=\sigma^{2} / n=1 / J(\theta)$입니다. 따라서 $\bar{X}_{n}$은 Cramér-Rao 하한을 달성하므로 $\theta$의 최소 분산 불편 추정량입니다.

Cramér-Rao 부등식은 모든 불편 추정량에 대한 분산의 하한을 제공합니다. 이 하한이 달성될 때, 해당 추정량을 효율적이라고 부릅니다.

정의 불편 추정량 $T$는 Cramér-Rao 하한과 등식으로 만나는 경우 [즉, $\operatorname{var}(T)=\frac{1}{J(\theta)}$인 경우] 효율적이라고 합니다.
<!-- Page 423 -->
피셔 정보는 따라서 데이터에 $\theta$에 대한 "정보"의 양을 측정하는 척도입니다. 이는 데이터로부터 $\theta$를 추정하는 데 있어 오차의 하한을 제공합니다. 그러나 이 하한을 충족하는 추정량이 존재하지 않을 수도 있습니다.

피셔 정보의 개념을 다변수 모수(multiparameter) 경우로 일반화할 수 있으며, 이 경우 각 요소가 다음과 같은 피셔 정보 행렬 $J(\theta)$를 정의합니다.

$$
J_{i j}(\theta)=\int f(x ; \theta) \frac{\partial}{\partial \theta_{i}} \ln f(x ; \theta) \frac{\partial}{\partial \theta_{j}} \ln f(x ; \theta) d x
$$

크라메르-라오 부등식(Cramér-Rao inequality)은 행렬 부등식으로 확장됩니다.

$$
\Sigma \geq J^{-1}(\theta)
$$

여기서 $\Sigma$는 모수 $\theta$에 대한 불편 추정량(unbiased estimators) 집합의 공분산 행렬이며, $\Sigma \geq J^{-1}(\theta)$는 $\Sigma-J^{-1}$의 차이가 비음 정정치 행렬(nonnegative definite matrix)이라는 의미입니다. 다변수 모수의 경우에 대한 증명의 세부 사항은 다루지 않겠습니다. 기본적인 아이디어는 유사합니다.

피셔 정보 $J(\theta)$와 이전에 정의된 엔트로피(entropy)와 같은 양 사이에 관계가 있습니까? 피셔 정보는 모수적 분포족(parametric distributions)에 대해 정의되는 반면, 엔트로피는 모든 분포에 대해 정의된다는 점에 유의하십시오. 그러나 우리는 위치 모수(location parameter) $\theta$를 사용하여 임의의 분포 $f(x)$를 모수화하고, 평행 이동(translation) 하에서의 밀도족 $f(x-\theta)$에 대한 피셔 정보를 정의할 수 있습니다. 우리는 17.8절에서 이 관계를 더 자세히 탐구할 것이며, 엔트로피가 전형 집합(typical set)의 부피와 관련이 있는 반면, 피셔 정보는 전형 집합의 표면적과 관련이 있음을 보여줄 것입니다. 피셔 정보와 상대 엔트로피(relative entropy)의 추가적인 관계는 문제에서 개발됩니다.

# 요약

## 기본 항등식

$$
\begin{aligned}
Q^{n}(\mathbf{x}) & =2^{-n\left(D\left(P_{\mathbf{x}} \| Q\right)+H\left(P_{\mathbf{x}}\right)\right)} \\
\left|\mathcal{P}_{n}\right| & \leq(n+1)^{[\mathcal{X}]} \\
|T(P)| & \doteq 2^{n H(P)} \\
Q^{n}(T(P)) & \doteq 2^{-n D(P\|Q)}
\end{aligned}
$$
<!-- Page 424 -->
# Universal data compression

$$
P_{e}^{(n)} \leq 2^{-n D\left(P_{R}^{*} \| Q\right)} \quad \text { for all } Q
$$

where

$$
D\left(P_{R}^{*} \| Q\right)=\min _{P: H(P) \geq R} D(P \| Q)
$$

## Large deviations (Sanov's theorem)

$$
\begin{aligned}
Q^{n}(E) & =Q^{n}\left(E \cap \mathcal{P}_{n}\right) \leq(n+1)^{|\mathcal{X}|} 2^{-n D\left(P^{*}| | Q\right)} \\
D\left(P^{*}| | Q\right) & =\min _{P \in E} D(P \| Q)
\end{aligned}
$$

If $E$ is the closure of its interior, then

$$
Q^{n}(E) \doteq 2^{-n D\left(P^{*}| | Q\right)}
$$

$\mathcal{L}_{1}$ bound on relative entropy

$$
D\left(P_{1} \| P_{2}\right) \geq \frac{1}{2 \ln 2}\left\|P_{1}-P_{2}\right\|_{1}^{2}
$$

Pythagorean theorem. If $E$ is a convex set of types, distribution $Q \notin$ $E$, and $P^{*}$ achieves $D\left(P^{*}| | Q\right)=\min _{P \in E} D(P \| Q)$, we have

$$
D(P \| Q) \geq D\left(P \| P^{*}\right)+D\left(P^{*}| | Q\right)
$$

for all $P \in E$.
Conditional limit theorem. If $X_{1}, X_{2}, \ldots, X_{n}$ i.i.d. $\sim Q$, then

$$
\operatorname{Pr}\left(X_{1}=a \mid P_{X^{n}} \in E\right) \rightarrow P^{*}(a) \quad \text { in probability, }
$$

where $P^{*}$ minimizes $D(P \| Q)$ over $P \in E$. In particular,

$$
\operatorname{Pr}\left\{X_{1}=a \left\lvert\, \frac{1}{n} \sum_{i=1}^{n} X_{i} \geq \alpha\right.\right\} \rightarrow \frac{Q(a) e^{\lambda a}}{\sum_{x} Q(x) e^{\lambda x}}
$$

Neyman-Pearson lemma. The optimum test between two densities $P_{1}$ and $P_{2}$ has a decision region of the form "accept $P=P_{1}$ if $\frac{P_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right)}{P_{2}\left(x_{1}, x_{2}, \ldots, x_{n}\right)}>T$."
<!-- Page 425 -->
체르노프-슈타인 보조정리. $\alpha_{n} \leq \epsilon$일 때 달성 가능한 최적의 오차 지수 $\beta_{n}^{\epsilon}$:

$$
\begin{aligned}
\beta_{n}^{\epsilon} & =\min _{\substack{A_{n} \subseteq \mathcal{X}^{n} \\
\alpha_{n}<\epsilon}} \beta_{n} \\
\lim _{n \rightarrow \infty} \frac{1}{n} \log \beta_{n}^{\epsilon} & =-D\left(P_{1} \| P_{2}\right)
\end{aligned}
$$

체르노프 정보. 베이즈 확률 오차에 대해 달성 가능한 최적의 지수는 다음과 같습니다.

$$
D^{*}=D\left(P_{\lambda^{*}} \| P_{1}\right)=D\left(P_{\lambda^{*}} \| P_{2}\right)
$$

여기서

$$
P_{\lambda}=\frac{P_{1}^{\lambda}(x) P_{2}^{1-\lambda}(x)}{\sum_{a \in \mathcal{X}} P_{1}^{\lambda}(a) P_{2}^{1-\lambda}(a)}
$$

이며, $\lambda=\lambda^{*}$는 다음을 만족하도록 선택됩니다.

$$
D\left(P_{\lambda} \| P_{1}\right)=D\left(P_{\lambda} \| P_{2}\right)
$$

# 피셔 정보

$$
J(\theta)=E_{\theta}\left[\frac{\partial}{\partial \theta} \ln f(x ; \theta)\right]^{2}
$$

크라메르-라오 부등식. 임의의 불편 추정량 $T$에 대해,

$$
E_{\theta}(T(X)-\theta)^{2}=\operatorname{var}(T) \geq \frac{1}{J(\theta)}
$$

## 문제

11.1 체르노프-슈타인 보조정리. 다음의 두 가설 검정을 고려하십시오.

$$
H_{1}: f=f_{1} \quad \text { 대 } \quad H_{2}: f=f_{2}
$$

$D\left(f_{1} \| f_{2}\right)$를 구하십시오. 만약
<!-- Page 426 -->
(a) $f_{i}(x)=N\left(0, \sigma_{i}^{2}\right), i=1,2$.
(b) $f_{i}(x)=\lambda_{i} e^{-\lambda_{i} x}, x \geq 0, i=1,2$.
(c) $f_{1}(x)$는 구간 $[0,1]$ 상의 균등 분포 밀도이고 $f_{2}(x)$는 $[a, a+1]$ 상의 균등 분포 밀도입니다. $0<a<1$이라고 가정합니다.
(d) $f_{1}$은 공정한 동전을 나타내고 $f_{2}$는 양면이 모두 앞면인 동전을 나타냅니다.
11.2 $D(P \| Q)$와 카이제곱 분포 간의 관계. $\chi^{2}$ 통계량이

$$
\chi^{2}=\Sigma_{x} \frac{(P(x)-Q(x))^{2}}{Q(x)}
$$

임을 보이고, 이것이 $D(P \|$ Q)를 $Q$에 대해 테일러 급수 전개했을 때 첫 번째 항의 (두 배)임을 보이십시오. 따라서 $D(P \| Q)=\frac{1}{2} \chi^{2}+\cdots$ 입니다. [힌트: $\frac{P}{Q}=1+\frac{P-Q}{Q}$로 쓰고 로그를 전개하십시오.]
11.3 범용 코드의 오차 지수. 속도 $R$의 범용 소스 코드는 오차 확률 $P_{e}^{(n)} \doteq e^{-n D\left(P^{*}\|Q\right)}$를 달성하며, 여기서 $Q$는 실제 분포이고 $P^{*}$는 $H(P) \geq R$을 만족하는 모든 $P$에 대해 $\min D(P \| Q)$를 달성합니다.
(a) $P^{*}$를 $Q$와 $R$로 표현하십시오.
(b) 이제 $X$가 이진 변수라고 가정합니다. 속도 $R$이 범용 소스 코드가 $P_{e}^{(n)} \rightarrow 0$을 달성하기에 충분한 소스 확률 $Q(x), x \in\{0,1\}$의 영역을 찾으십시오.
11.4 순차적 투영. $Q$를 $P_{1}$에 투영한 다음 투영된 값 $\hat{Q}$를 $P_{1} \bigcap P_{2}$에 투영하는 것이 $Q$를 직접 $P_{1} \bigcap P_{2}$에 투영하는 것과 같음을 보이려고 합니다. $\mathcal{P}_{1}$을 $\mathcal{X}$ 상의 확률 질량 함수 집합이라고 합시다. 이 집합은 다음을 만족합니다.

$$
\begin{aligned}
\sum_{x} p(x) & =1 \\
\sum_{x} p(x) h_{i}(x) & \geq \alpha_{i}, \quad i=1,2, \ldots, r
\end{aligned}
$$

$\mathcal{P}_{2}$를 $\mathcal{X}$ 상의 확률 질량 함수 집합이라고 합시다. 이 집합은 다음을 만족합니다.

$$
\begin{aligned}
\sum_{x} p(x) & =1 \\
\sum_{x} p(x) g_{j}(x) & \geq \beta_{j}, \quad j=1,2, \ldots, s
\end{aligned}
$$
<!-- Page 427 -->
$Q \notin P_{1} \bigcup P_{2}$라고 가정합니다. $P^{*}$는 모든 $P \in \mathcal{P}_{1}$에 대해 $D(P \| Q)$를 최소화합니다. $R^{*}$는 모든 $R \in \mathcal{P}_{1} \bigcap \mathcal{P}_{2}$에 대해 $D(R \| Q)$를 최소화합니다. $R^{*}$가 모든 $R \in P_{1} \bigcap P_{2}$에 대해 $D\left(R \| P^{*}\right)$를 최소화함을 논증하십시오.
11.5 계산. $\mathcal{X}=\{1,2, \ldots, m\}$라고 할 때, $\frac{1}{n} \sum_{i=1}^{n} g\left(x_{i}\right) \geq \alpha$를 만족하는 시퀀스 $x^{n} \in \mathcal{X}^{n}$의 개수가 $n$이 충분히 클 때, 지수에서 첫 번째 근사로 $2^{n H^{*}}$와 거의 같음을 보이십시오. 여기서

$$
H^{*}=\max _{P: \sum_{i=1}^{m} P(i) g(i) \geq \alpha} H(P)
$$

11.6 편향된 추정량이 더 좋을 수 있습니다. $\mathcal{N}\left(\mu, \sigma^{2}\right)$ 분포에서 i.i.d.로 추출된 $n$개의 데이터 샘플로부터 $\mu$와 $\sigma^{2}$를 추정하는 문제를 고려하십시오.
(a) $\bar{X}_{n}$이 $\mu$의 비편향 추정량임을 보이십시오.
(b) 추정량

$$
S_{n}^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}_{n}\right)^{2}
$$

이 $\sigma^{2}$의 편향된 추정량이고 추정량

$$
S_{n-1}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}_{n}\right)^{2}
$$

이 비편향 추정량임을 보이십시오.
(c) $S_{n}^{2}$가 $S_{n-1}^{2}$보다 낮은 평균 제곱 오차를 가짐을 보이십시오. 이는 편향된 추정량이 비편향된 추정량보다 "더 나을" 수 있다는 아이디어를 보여줍니다.
11.7 Fisher 정보와 상대 엔트로피. 모수적족 $\left\{p_{\theta}(x)\right\}$에 대해

$$
\lim _{\theta^{\prime} \rightarrow \theta} \frac{1}{\left(\theta-\theta^{\prime}\right)^{2}} D\left(p_{\theta} \| p_{\theta^{\prime}}\right)=\frac{1}{\ln 4} J(\theta)
$$

임을 보이십시오.
11.8 Fisher 정보의 예. $f_{\theta}(x), \theta \in \mathbf{R}$족에 대한 Fisher 정보 $J(\Theta)$는 다음과 같이 정의됩니다.

$$
J(\theta)=E_{\theta}\left(\frac{\partial f_{\theta}(X) / \partial \theta}{f_{\theta}(X)}\right)^{2}=\int \frac{\left(f_{\theta}^{\prime}\right)^{2}}{f_{\theta}}
$$

다음 족에 대한 Fisher 정보를 찾으십시오.
<!-- Page 428 -->
(a) $f_{\theta}(x)=N(0, \theta)=\frac{1}{\sqrt{2 \pi \theta}} e^{-\frac{x^{2}}{2 \theta}}$
(b) $f_{\theta}(x)=\theta e^{-\theta x}, x \geq 0$
(c) $\hat{\theta}(X)$가 (a)와 (b)에 대한 $\theta$의 불편 추정량일 때, $E_{\theta}(\hat{\theta}(X)-\theta)^{2}$에 대한 Cramèr-Rao 하한은 무엇입니까?
11.9 조건부 독립적인 두 번의 관측은 Fisher 정보를 두 배로 만듭니다. $g_{\theta}\left(x_{1}, x_{2}\right)=f_{\theta}\left(x_{1}\right) f_{\theta}\left(x_{2}\right)$라고 할 때, $J_{g}(\theta)=2 J_{f}(\theta)$임을 보이십시오.
11.10 결합 분포와 곱 분포. 주변 분포가 $Q(x)$와 $Q(y)$인 결합 분포 $Q(x, y)$를 고려하십시오. $E$는 $Q$에 대해 공동으로 전형적인(jointly typical) 유형들의 집합이라고 합시다:

$$
\begin{aligned}
E=\{P(x, y): & -\sum_{x, y} P(x, y) \log Q(x)-H(X)=0 \\
& -\sum_{x, y} P(x, y) \log Q(y)-H(Y)=0 \\
& -\sum_{x, y} P(x, y) \log Q(x, y) \\
& -H(X, Y)=0\}
\end{aligned}
$$

(a) 다른 분포 $Q_{0}(x, y)$가 $\mathcal{X} \times \mathcal{Y}$에 있다고 할 때, $E$에서 $Q_{0}$에 가장 가까운 분포 $P^{*}$가 다음과 같은 형태임을 논증하십시오.

$$
P^{*}(x, y)=Q_{0}(x, y) e^{\lambda_{0}+\lambda_{1} \log Q(x)+\lambda_{2} \log Q(y)+\lambda_{3} \log Q(x, y)}
$$

여기서 $\lambda_{0}, \lambda_{1}, \lambda_{2}, \lambda_{3}$는 제약 조건을 만족하도록 선택됩니다. 이 분포가 유일함을 논증하십시오.
(b) 이제 $Q_{0}(x, y)=Q(x) Q(y)$라고 합시다. $Q(x, y)$가 (11.322)의 형태이고 제약 조건을 만족함을 확인하십시오. 따라서 $P^{*}(x, y)=Q(x, y)$입니다 (즉, 곱 분포에 가장 가까운 $E$ 내의 분포는 결합 분포입니다).
11.11 편향 항을 포함한 Cramér-Rao 부등식. $X \sim f(x ; \theta)$이고 $T(X)$가 $\theta$의 추정량이라고 합시다. 추정량의 편향을 $b_{T}(\theta)=E_{\theta} T-\theta$라고 할 때, 다음을 보이십시오.

$$
E(T-\theta)^{2} \geq \frac{\left[1+b_{T}^{\prime}(\theta)\right]^{2}}{J(\theta)}+b_{T}^{2}(\theta)
$$
<!-- Page 429 -->
11.12 가설 검정. $X_{1}, X_{2}, \ldots, X_{n}$을 i.i.d. $\sim p(x)$라고 가정합니다. 가설 검정 $H_{1}: p=p_{1}$ 대 $H_{2}: p=p_{2}$를 고려합니다.

$$
p_{1}(x)= \begin{cases}\frac{1}{2}, & x=-1 \\ \frac{1}{4}, & x=0 \\ \frac{1}{4}, & x=1\end{cases}
$$

그리고

$$
p_{2}(x)= \begin{cases}\frac{1}{4}, & x=-1 \\ \frac{1}{4}, & x=0 \\ \frac{1}{2}, & x=1\end{cases}
$$

$\operatorname{Pr}\left\{\text { Decide } H_{1} \mid H_{2} \text { true }\right\} \leq \frac{1}{2}$를 제약 조건으로 하는 $H_{1}$ 대 $H_{2}$의 최적 가설 검정에서 $\operatorname{Pr}\left\{\text { Decide } H_{2} \mid H_{1} \text { true }\right\}$의 오차 지수를 찾으십시오.

11.13 사노프 정리. Bernoulli $(q)$ 확률 변수에 대한 사노프 정리의 간단한 버전을 증명하십시오.
시퀀스 $X_{1}, X_{2}, \ldots, X_{n}$에서 1의 비율을 다음과 같이 정의합니다.

$$
\bar{X}_{n}=\frac{1}{n} \sum_{i=1}^{n} X_{i}
$$

큰 수의 법칙에 따라 큰 $n$에 대해 $\bar{X}_{n}$이 $q$에 가까울 것으로 예상할 수 있습니다. 사노프 정리는 $p_{X^{n}}$이 $q$에서 멀리 떨어져 있을 확률을 다룹니다. 특히 구체적으로, $p>q>\frac{1}{2}$를 선택하면 사노프 정리는 다음과 같이 말합니다.

$$
\begin{aligned}
& -\frac{1}{n} \log \operatorname{Pr}\left\{\left(X_{1}, X_{2}, \ldots, X_{n}\right): \bar{X}_{n} \geq p\right\} \\
& \rightarrow p \log \frac{p}{q}+(1-p) \log \frac{1-p}{1-q} \\
& =D((p, 1-p) \|(q, 1-q))
\end{aligned}
$$

다음 단계들을 정당화하십시오.

- $\operatorname{Pr}\left\{\left(X_{1}, X_{2}, \ldots, X_{n}\right): \bar{X}_{n} \geq p\right\} \leq \sum_{i=\lfloor n p\rfloor}^{n}\binom{n}{i} q^{i}(1-q)^{n-i}$.
<!-- Page 430 -->
- $i=\lfloor n p\rfloor$에 해당하는 항이 마지막 방정식의 우변 합에서 가장 큰 항임을 논증하십시오.
- 이 항이 대략 $2^{-n D}$임을 보이십시오.
- 위 단계들을 사용하여 Sanov의 정리에서 확률에 대한 상한을 증명하십시오. 유사한 논증을 사용하여 하한을 증명하고 Sanov의 정리 증명을 완료하십시오.
11.14 Sanov. $X_{i}$는 i.i.d. $\sim N\left(0, \sigma^{2}\right)$라고 가정합니다.
(a) $\operatorname{Pr}\left\{\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} \geq \alpha^{2}\right\}$의 거동에서 지수를 찾으십시오. 이는 첫 번째 원리(정규 분포가 좋기 때문에)로부터 또는 Sanov의 정리를 사용하여 수행할 수 있습니다.
(b) $\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} \geq \alpha$라면 데이터는 어떻게 보입니까? 즉, $D(P \| Q)$를 최소화하는 $P^{*}$는 무엇입니까?
11.15 상태 개수 세기. 원자가 여섯 개의 상태 $X \in\left\{s_{1}, s_{2}, s_{3}, \ldots, s_{6}\right\}$ 중 어느 하나에나 동일한 확률로 존재한다고 가정합니다. 이 균일 분포에 따라 독립적으로 추출된 $n$개의 원자 $X_{1}, X_{2}, \ldots, X_{n}$를 관찰합니다. 상태 $s_{1}$의 발생 빈도가 상태 $s_{2}$의 발생 빈도의 두 배라고 관찰됩니다.
(a) 지수에서 일차적으로 이 사건을 관찰할 확률은 얼마입니까?
(b) $n$이 크다고 가정할 때, 이 관찰이 주어졌을 때 첫 번째 원자 $X_{1}$의 상태의 조건부 분포를 찾으십시오.
11.16 가설 검정. $\left\{X_{i}\right\}$가 i.i.d. $\sim p(x), x \in\{1,2, \ldots\}$라고 가정합니다. 두 가설 $H_{0}: p(x)=p_{0}(x)$ 대 $H_{1}: p(x)=p_{1}(x)$를 고려합니다. 여기서 $p_{0}(x)=\left(\frac{1}{2}\right)^{x}$이고 $p_{1}(x)=q p^{x-1}, x=1,2,3, \ldots$입니다.
(a) $D\left(p_{0} \| p_{1}\right)$를 찾으십시오.
(b) $\operatorname{Pr}\left\{H_{0}\right\}=\frac{1}{2}$라고 가정합니다. 데이터 $X_{1}, X_{2}, \ldots, X_{n} \sim p(x)$가 주어졌을 때 $H_{0}$ 대 $H_{1}$에 대한 최소 오류 확률 검정을 찾으십시오.
11.17 최대 우도 추정. $\left\{f_{\theta}(x)\right\}$가 모수 $\theta \epsilon \mathcal{R}$를 갖는 밀도 함수의 모수족이라고 가정합니다. $X_{1}, X_{2}, \ldots, X_{n}$는 i.i.d. $\sim f_{\theta}(x)$라고 가정합니다.

$$
l_{\theta}\left(x^{n}\right)=\ln \left(\prod_{i=1}^{n} f_{\theta}\left(x_{i}\right)\right)
$$

는 로그 우도 함수로 알려져 있습니다. $\theta_{0}$를 실제 모수 값이라고 가정합니다.
<!-- Page 431 -->
(a) 로그 우도 기댓값을 다음과 같이 정의합니다.

$$
E_{\theta_{0}} l_{\theta}\left(X^{n}\right)=\int\left(\ln \prod_{i=1}^{n} f_{\theta}\left(x_{i}\right)\right) \prod_{i=1}^{n} f_{\theta_{0}}\left(x_{i}\right) d x^{n}
$$

그리고 다음을 보여주십시오.

$$
E_{\theta_{0}}\left(l\left(X^{n}\right)\right)=\left(-h\left(f_{\theta_{0}}\right)-D\left(f_{\theta_{0}} \| f_{\theta}\right)\right) n
$$

(b) 로그 우도 기댓값의 최댓값이 $\theta=\theta_{0}$에 의해 달성됨을 보여주십시오.
11.18 큰 편차. $X_{1}, X_{2}, \ldots$는 기하 분포에 따라 추출된 i.i.d. 확률 변수입니다.

$$
\operatorname{Pr}\{X=k\}=p^{k-1}(1-p), \quad k=1,2, \ldots
$$

다음의 좋은 추정치를 (지수의 1차 항으로) 찾으십시오:
(a) $\operatorname{Pr}\left\{\frac{1}{n} \sum_{i=1}^{n} X_{i} \geq \alpha\right\}$.
(b) $\operatorname{Pr}\left\{X_{1}=k \left\lvert\, \frac{1}{n} \sum_{i=1}^{n} X_{i} \geq \alpha\right.\right\}$.
(c) $p=\frac{1}{2}, \alpha=4$에 대해 (a)와 (b)를 평가하십시오.
11.19 Fisher 정보에 대한 또 다른 표현. 부분 적분을 사용하여 다음을 보여주십시오.

$$
J(\theta)=-E \frac{\partial^{2} \ln f_{\theta}(x)}{\partial \theta^{2}}
$$

11.20 스털링 근사. 계승에 대한 스털링 근사의 약한 형태를 유도하십시오. 즉, 다음을 보여주십시오.

$$
\left(\frac{n}{e}\right)^{n} \leq n!\leq n\left(\frac{n}{e}\right)^{n}
$$

적분을 합으로 근사하여 사용하십시오. 다음 단계들을 정당화하십시오:

$$
\ln (n!)=\sum_{i=2}^{n-1} \ln (i)+\ln (n) \leq \int_{2}^{n-1} \ln x d x+\ln n=\cdots
$$
<!-- Page 432 -->
및

$$
\ln (n!)=\sum_{i=1}^{n} \ln (i) \geq \int_{0}^{n} \ln x d x=\cdots
$$

11.21 이항계수의 점근값. 문제 11.20의 간단한 근사를 사용하여 $0 \leq p \leq 1$이고 $k=\lfloor n p\rfloor$ (즉, $k$는 $n p$보다 작거나 같은 가장 큰 정수)이면 다음이 성립함을 보이십시오.

$$
\lim _{n \rightarrow \infty} \frac{1}{n} \log \binom{n}{k}=-p \log p-(1-p) \log (1-p)=H(p)
$$

이제 $p_{i}, i=1, \ldots, m$이 $m$개의 기호에 대한 확률 분포라고 가정합니다 (즉, $p_{i} \geq 0$이고 $\sum_{i} p_{i}=1$ ). 다음의 극한값은 무엇입니까?

$$
\begin{aligned}
& \frac{1}{n} \log \left(\begin{array}{c}
n \\
\left\lfloor n p_{1}\right\rfloor\left\lfloor n p_{2}\right\rfloor \ldots\left\lfloor n p_{m-1}\right\rfloor n-\sum_{j=0}^{m-1}\left\lfloor n p_{j}\right\rfloor
\end{array}\right) \\
& \quad=\frac{1}{n} \log \frac{n!}{\left\lfloor n p_{1}\right\rfloor!\left\lfloor n p_{2}\right\rfloor!\ldots\left\lfloor n p_{m-1}\right\rfloor!\left(n-\sum_{j=0}^{m-1}\left\lfloor n p_{j}\right\rfloor\right)!} ?
\end{aligned}
$$

11.22 실행 차분. $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim Q_{1}(x)$이고 $Y_{1}, Y_{2}, \ldots, Y_{n}$이 i.i.d. $\sim Q_{2}(y)$라고 가정합니다. $X^{n}$과 $Y^{n}$은 독립입니다. $\operatorname{Pr}\left\{\sum_{i=1}^{n} X_{i}-\sum_{i=1}^{n} Y_{i} \geq n t\right\}$에 대한 지수에서 첫 번째 항까지의 근사식을 찾으십시오. 다시 말하지만, 이 답은 매개변수 형태로 남겨둘 수 있습니다.
11.23 큰 가능도. $X_{1}, X_{2}, \ldots$가 i.i.d. $\sim Q(x), x \in\{1,2, \ldots, m\}$라고 가정합니다. $P(x)$는 다른 확률 질량 함수라고 가정합니다. 로그 가능도 비율을 형성합니다.

$$
\frac{1}{n} \log \frac{P^{n}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{Q^{n}\left(X_{1}, X_{2}, \ldots, X_{n}\right)}=\frac{1}{n} \sum_{i=1}^{n} \log \frac{P\left(X_{i}\right)}{Q\left(X_{i}\right)}
$$

시퀀스 $X^{n}$에 대해 특정 임계값을 초과할 확률을 묻습니다. 구체적으로, (지수에서 첫 번째 항까지) 다음을 찾으십시오.

$$
Q^{n}\left(\frac{1}{n} \log \frac{P\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{Q\left(X_{1}, X_{2}, \ldots, X_{n}\right)}>0\right)
$$

답변에 결정되지 않은 매개변수가 있을 수 있습니다.
<!-- Page 433 -->
11.24 혼합 분포에 대한 Fisher 정보량. 두 개의 확률 밀도 함수 $f_{1}(x)$와 $f_{0}(x)$가 주어졌다고 가정합니다. $Z$는 모르는 모수 $\theta$를 갖는 Bernoulli $(\theta)$ 분포를 따른다고 가정합니다. $Z=1$이면 $X \sim f_{1}(x)$이고, $Z=0$이면 $X \sim f_{0}(x)$라고 가정합니다.
(a) 관측된 $X$의 밀도 함수 $f_{\theta}(x)$를 구하십시오.
(b) Fisher 정보량 $J(\theta)$를 구하십시오.
(c) $\theta$의 불편 추정량의 평균 제곱 오차에 대한 Cramér-Rao 하한을 구하십시오.
(d) $\theta$의 불편 추정량을 제시할 수 있습니까?
11.25 기울어진 동전. $\left\{X_{i}\right\}$가 iid $Q$ 분포를 따르고,
$Q(k)=\operatorname{Pr}\left(X_{i}=k\right)=\binom{m}{k} q^{k}(1-q)^{m-k}$ for $k=0,1,2, \ldots, m$.
따라서 $X_{i}$들은 iid $\sim \operatorname{Binomial}(m, q)$ 분포를 따릅니다. $n \rightarrow \infty$일 때,

$$
\operatorname{Pr}\left(X_{1}=k \left\lvert\, \frac{1}{n} \sum_{i=1}^{n} X_{i} \geq \alpha\right.\right) \rightarrow P^{*}(k)
$$

여기서 $P^{*}$는 $\operatorname{Binomial}(m, \lambda)$ 분포를 따릅니다 (즉, $P^{*}(k)=\binom{m}{k} \lambda^{k}(1-\lambda)^{m-k}$ for some $\lambda \in[0,1]$). $\lambda$를 구하십시오.
11.26 조건부 극한 분포
(a) 다음의 정확한 값을 구하십시오.

$$
\operatorname{Pr}\left\{X_{1}=1 \left\lvert\, \frac{1}{n} \sum_{i=1}^{n} X_{i}=\frac{1}{4}\right.\right\}
$$

여기서 $X_{1}, X_{2}, \ldots$는 Bernoulli $\left(\frac{2}{3}\right)$ 분포를 따르고 $n$은 4의 배수입니다.
(b) 이제 $X_{i} \epsilon\{-1,0,1\}$이고 $X_{1}, X_{2} \ldots$는 $\{-1,0,+1\}$ 위에서 i.i.d. 균등 분포를 따른다고 가정합니다. 다음의 극한을 구하십시오.

$$
\operatorname{Pr}\left\{X_{1}=+1 \left\lvert\, \frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}=\frac{1}{2}\right.\right\}
$$

for $n=2 k, k \rightarrow \infty$.
<!-- Page 434 -->
11.27 변분 부등식. 양의 확률 변수 $X$에 대해 다음을 검증하십시오.

$$
\log E_{P}(X)=\sup _{Q}\left[E_{Q}(\log X)-D(Q \| P)\right]
$$

여기서 $E_{P}(X)=\sum_{x} x P(x)$이고 $D(Q \| P)=\sum_{x} Q(x) \log \frac{Q(x)}{P(x)}$이며, supremum은 모든 $Q(x) \geq 0, \quad \sum Q(x)=1$에 대해 취합니다. $J(Q)=E_{Q} \ln X-D(Q \| P)+$ $\lambda(\sum Q(x)-1)$을 극값으로 만드는 것으로 충분합니다.
11.28 타입 제약 조건
(a) 표본 분산 $\overline{X_{n}^{2}}-\left(\bar{X}_{n}\right)^{2} \leq \alpha$를 만족하는 타입 $P_{X^{n}}$에 대한 제약 조건을 찾으십시오. 여기서 $\overline{X_{n}^{2}}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}$이고 $\bar{X}_{n}=\frac{1}{n} \sum_{i=1}^{n} X_{i}$입니다.
(b) 확률 $Q^{n}\left(\overline{X_{n}^{2}}-\left(\bar{X}_{n}\right)^{2} \leq \alpha\right)$에서의 지수를 찾으십시오. 매개변수 형태로 답을 남겨둘 수 있습니다.
11.29 심플렉스 상의 균일 분포. 다음 방법 중 어떤 것이 심플렉스 $\left\{x \in R^{n}: x_{i} \geq 0, \quad \sum_{i=1}^{n} x_{i}=1\right\}$ 상의 균일 분포로부터 표본을 생성할까요?
(a) $Y_{i}$를 i.i.d. uniform $[0,1]$으로 하고 $X_{i}=Y_{i} / \sum_{j=1}^{n} Y_{j}$로 설정합니다.
(b) $Y_{i}$를 i.i.d. 지수 분포 $\sim \lambda e^{-\lambda y}, y \geq 0$으로 하고 $X_{i}=Y_{i} / \sum_{j=1}^{n} Y_{j}$로 설정합니다.
(c) (막대를 $n$개의 부분으로 나누기) $Y_{1}, Y_{2}, \ldots, Y_{n-1}$을 i.i.d. uniform $[0,1]$으로 하고, $X_{i}$를 $i$번째 구간의 길이로 설정합니다.

# 역사적 참고 사항

타입 방법은 강한 전형성(strong typicality)의 개념에서 발전했으며, 일부 아이디어는 Wolfowitz [566]가 채널 용량 정리를 증명하는 데 사용했습니다. 이 방법은 Csiszár와 Körner [149]에 의해 완전히 개발되었으며, 그들은 정보 이론의 주요 정리를 이 관점에서 도출했습니다. 섹션 11.1에 설명된 타입 방법은 Csiszár와 Körner의 발전을 따릅니다. 상대 엔트로피에 대한 $\mathcal{L}_{1}$ 하한은 Csiszár [138], Kullback [336], Kemperman [309]에 의해 이루어졌습니다. Sanov의 정리 [455]는 Csiszár [141]가 타입 방법을 사용하여 일반화했습니다.
<!-- Page 435 -->
# 최대 엔트로피

기체의 온도는 기체 내 분자들의 평균 운동 에너지에 해당합니다. 주어진 온도에서 기체 내 속도 분포에 대해 무엇이라고 말할 수 있을까요? 물리학에서 우리는 이 분포가 온도 제약 조건 하에서의 최대 엔트로피 분포, 즉 맥스웰-볼츠만 분포라는 것을 알고 있습니다. 최대 엔트로피 분포는 가장 많은 미시 상태(개별 기체 속도)를 갖는 거시 상태(경험적 분포로 인덱싱됨)에 해당합니다. 물리학에서 최대 엔트로피 방법을 사용할 때 암묵적으로 모든 미시 상태가 동일하게 확률적이라고 말하는 일종의 AEP가 내포되어 있습니다.

### 12.1 최대 엔트로피 분포

다음 문제를 고려해 보십시오: 모든 확률 밀도 함수 $f$에 대해 엔트로피 $h(f)$를 최대화하되, 다음 조건을 만족해야 합니다.

1. $f(x) \geq 0$, 지지 집합 $S$ 외부에서는 0이어야 합니다.
2. $\int_{S} f(x) d x=1$
3. $\int_{S} f(x) r_{i}(x) d x=\alpha_{i} \quad$ for $1 \leq i \leq m$.

따라서 $f$는 특정 모멘트 제약 조건 $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{m}$을 만족하는 지지 집합 $S$ 상의 밀도 함수입니다.

접근법 1 (미적분학) 미분 엔트로피 $h(f)$는 볼록 집합 상의 오목 함수입니다. 우리는 다음과 같은 함수를 구성합니다.

$$
J(f)=-\int f \ln f+\lambda_{0} \int f+\sum_{i=1}^{m} \lambda_{i} \int f r_{i}
$$

그리고 $f(x)$ (즉, $f$의 $x$번째 성분)에 대해 "미분"하여 다음을 얻습니다.

$$
\frac{\partial J}{\partial f(x)}=-\ln f(x)-1+\lambda_{0}+\sum_{i=1}^{m} \lambda_{i} r_{i}(x)
$$

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (c) 2006 John Wiley \& Sons, Inc.
<!-- Page 436 -->
이것을 0으로 설정하면 최대화하는 밀도의 형태를 얻습니다.

$$
f(x)=e^{\lambda_{0}-1+\sum_{i=1}^{m} \lambda_{i} r_{i}(x)}, \quad x \in S
$$

여기서 $\lambda_{0}, \lambda_{1}, \ldots, \lambda_{m}$은 $f$가 제약 조건을 만족하도록 선택됩니다.
미적분을 이용한 접근 방식은 엔트로피를 최대화하는 밀도의 형태만을 제시합니다. 이것이 실제로 최대값임을 증명하기 위해 두 번째 변형을 사용할 수 있습니다. 정보 부등식 $D(g \| f) \geq 0$을 사용하는 것이 더 간단합니다.

접근법 2 (정보 부등식) $g$가 (12.1)을 만족하고 $f^{*}$가 (12.4)의 형태라면, $0 \leq D(g \| f^{*})=-h(g)+h\left(f^{*}\right)$입니다. 따라서 제약 조건을 만족하는 모든 $g$에 대해 $h(g) \leq h\left(f^{*}\right)$입니다. 다음 정리에서 이를 증명합니다.

정리 12.1.1 (최대 엔트로피 분포) $f^{*}(x)=f_{\lambda}(x)$ $=e^{\lambda_{0}+\sum_{i=1}^{m} \lambda_{i} r_{i}(x)}, x \in S$라고 가정합니다. 여기서 $\lambda_{0}, \ldots, \lambda_{m}$은 $f^{*}$가 (12.1)을 만족하도록 선택됩니다. 그러면 $f^{*}$는 제약 조건 (12.1)을 만족하는 모든 확률 밀도 $f$에 대해 $h(f)$를 고유하게 최대화합니다.

증명: $g$가 제약 조건 (12.1)을 만족한다고 가정합니다. 그러면

$$
\begin{aligned}
h(g) & =-\int_{S} g \ln g \\
& =-\int_{S} g \ln \frac{g}{f^{*}} f^{*} \\
& =-D\left(g \| f^{*}\right)-\int_{S} g \ln f^{*} \\
& \stackrel{(a)}{\leq}-\int_{S} g \ln f^{*} \\
& \stackrel{(b)}{=}-\int_{S} g\left(\lambda_{0}+\sum \lambda_{i} r_{i}\right) \\
& \stackrel{(c)}{=}-\int_{S} f^{*}\left(\lambda_{0}+\sum \lambda_{i} r_{i}\right) \\
& =-\int_{S} f^{*} \ln f^{*} \\
& =h\left(f^{*}\right)
\end{aligned}
$$

여기서 (a)는 상대 엔트로피의 비음수성에서 비롯되고, (b)는 $f^{*}$의 정의에서 비롯되며, (c)는 $f^{*}$와 $g$ 모두 제약 조건을 만족한다는 사실에서 비롯됩니다. 등호는 (a)에서 다음의 경우에만 성립합니다.
<!-- Page 437 -->
$g(x)=f^{*}(x)$가 측도 0인 집합을 제외한 모든 $x$에 대해 성립한다고 가정하면, 유일성을 증명할 수 있습니다.

동일한 접근 방식은 이산 엔트로피와 다변수 분포에도 적용됩니다.

# 12.2 예제

예제 12.2.1 (온도 제약이 있는 일차원 기체) 제약 조건이 $E X=0$이고 $E X^{2}=\sigma^{2}$이라고 가정합니다. 그러면 최대화 분포의 형태는 다음과 같습니다.

$$
f(x)=e^{\lambda_{0}+\lambda_{1} x+\lambda_{2} x^{2}}
$$

적절한 상수를 찾기 위해, 이 분포가 정규 분포와 동일한 형태를 가진다는 것을 먼저 인식합니다. 따라서 제약 조건을 만족하고 엔트로피를 최대화하는 밀도는 $\mathcal{N}\left(0, \sigma^{2}\right)$ 분포입니다.

$$
f(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{x^{2}}{2 \sigma^{2}}}
$$

예제 12.2.2 (주사위, 제약 조건 없음) $S=\{1,2,3,4,5,6\}$이라고 가정합니다. 엔트로피를 최대화하는 분포는 균일 분포이며, $x \in S$에 대해 $p(x)=$ $\frac{1}{6}$입니다.
예제 12.2.3 (주사위, $E X=\sum i p_{i}=\alpha$ 포함) 이 중요한 예제는 볼츠만이 사용했습니다. $n$개의 주사위를 테이블에 던졌고, 보이는 총 점수가 $n \alpha$라는 정보를 받았다고 가정해 봅시다. 주사위 면 $i, i=1,2, \ldots, 6$이 나올 비율은 얼마입니까?

이 문제를 해결하는 한 가지 방법은 $n$개의 주사위가 $n_{i}$개의 면 $i$를 보이도록 떨어지는 경우의 수를 세는 것입니다. 이러한 경우의 수는 $\binom{n}{n_{1}, n_{2}, \ldots, n_{6}}$입니다. 이는 $\binom{n}{n_{1}, n_{2}, \ldots, n_{6}}$개의 마이크로스테이트에 해당하는 매크로스테이트이며, 각 마이크로스테이트의 확률은 $\frac{1}{6^{n}}$입니다. 가장 가능성 있는 매크로스테이트를 찾기 위해, 총 점수에 대한 관찰된 제약 조건 하에서 $\binom{n}{n_{1}, n_{2}, \ldots, n_{6}}$을 최대화하고자 합니다.

$$
\sum_{i=1}^{6} i n_{i}=n \alpha
$$

조잡한 스털링 근사치 $n!\approx\left(\frac{n}{e}\right)^{n}$을 사용하면 다음과 같이 계산됩니다.

$$
\binom{n}{n_{1}, n_{2}, \ldots, n_{6}} \approx \frac{\left(\frac{n}{e}\right)^{n}}{\prod_{i=1}^{6}\left(\frac{n_{i}}{e}\right)^{n_{i}}}
$$
<!-- Page 438 -->
$$
\begin{aligned}
& =\prod_{i=1}^{6}\left(\frac{n}{n_{i}}\right)^{n_{i}} \\
& =e^{n H\left(\frac{n_{1}}{n}, \frac{n_{2}}{n}, \ldots, \frac{n_{6}}{n}\right)}
\end{aligned}
$$

따라서, 제약 조건 (12.15) 하에서 $\binom{n}{n_{1}, n_{2}, \ldots, n_{6}}$을 최대화하는 것은 $\sum i p_{i}=\alpha$ 제약 조건 하에서 $H\left(p_{1}, p_{2}, \ldots, p_{6}\right)$을 최대화하는 것과 거의 동등합니다. 이 제약 조건 하에서 정리 12.1.1을 사용하면 최대 엔트로피 확률 질량 함수가 다음과 같음을 알 수 있습니다.

$$
p_{i}^{*}=\frac{e^{\lambda i}}{\sum_{i=1}^{6} e^{\lambda i}}
$$

여기서 $\lambda$는 $\sum i p_{i}^{*}=\alpha$를 만족하도록 선택됩니다. 따라서 가장 확률적인 거시 상태는 $\left(n p_{1}^{*}, n p_{2}^{*}, \ldots, n p_{6}^{*}\right)$이며, $n_{i}^{*}=n p_{i}^{*}$개의 주사위 면이 $i$를 나타낼 것으로 예상합니다.

제 11 장에서는 이러한 추론과 근사가 본질적으로 정확함을 보여줍니다. 사실, 최대 엔트로피 거시 상태가 가장 가능성이 높을 뿐만 아니라 거의 모든 확률을 포함한다는 것을 보여줍니다. 구체적으로, 유리수 $\alpha$에 대해,

$$
\operatorname{Pr}\left\{\left|\frac{N_{i}}{n}-p_{i}^{*}\right|<\epsilon, i=1,2, \ldots, 6\left|\sum_{i=1}^{n} X_{i}=n \alpha\right\} \rightarrow 1\right.
$$

$n \alpha$가 정수인 부분 수열을 따라 $n \rightarrow \infty$일 때.
예제 12.2.4 $S=[a, b]$이고 다른 제약 조건이 없는 경우. 그러면 최대 엔트로피 분포는 이 범위에 대한 균일 분포입니다.

예제 12.2.5 $S=[0, \infty)$이고 $E X=\mu$인 경우. 그러면 엔트로피를 최대화하는 분포는 다음과 같습니다.

$$
f(x)=\frac{1}{\mu} e^{-\frac{x}{\mu}}, \quad x \geq 0
$$

이 문제는 물리적 해석을 가지고 있습니다. 대기 중 분자의 높이 $X$의 분포를 고려하십시오. 분자의 평균 위치 에너지가 고정되어 있고, 기체는 $E(m g X)$가 고정되어 있다는 제약 조건 하에서 최대 엔트로피를 갖는 분포로 경향이 있습니다. 이것은 밀도 $f(x)=\lambda e^{-\lambda x}, x \geq 0$인 지수 분포입니다. 대기의 밀도는 실제로 이 분포를 따릅니다.
<!-- Page 439 -->
예제 12.2.6 $S=(-\infty, \infty)$이고 $E X=\mu$입니다. 여기서 최대 엔트로피는 무한대이며, 최대 엔트로피 분포는 존재하지 않습니다. (점점 더 큰 분산을 갖는 정규 분포를 고려하십시오.)

예제 12.2.7 $S=(-\infty, \infty), E X=\alpha_{1}$, 그리고 $E X^{2}=\alpha_{2}$입니다. 최대 엔트로피 분포는 $\mathcal{N}\left(\alpha_{1}, \alpha_{2}-\alpha_{1}^{2}\right)$입니다.

예제 12.2.8 $S=\mathcal{R}^{n}, E X_{i} X_{j}=K_{i j}, 1 \leq i, j \leq n$입니다. 이것은 다변량 예제이지만, 동일한 분석이 적용되며 최대 엔트로피 밀도는 다음과 같은 형태를 갖습니다.

$$
f(\mathbf{x})=e^{\lambda_{0}+\sum_{i, j} \lambda_{i j} x_{i} x_{j}}
$$

지수가 이차 형식이기 때문에, 밀도가 평균이 0인 다변량 정규 분포임은 명백합니다. 두 번째 모멘트 제약 조건을 만족해야 하므로, 공분산이 $K_{i j}$인 다변량 정규 분포여야 하며, 따라서 밀도는 다음과 같습니다.

$$
f(\mathbf{x})=\frac{1}{(\sqrt{2 \pi})^{n}|K|^{1 / 2}} e^{-\frac{1}{2} \mathbf{x}^{T} K^{-1} \mathbf{x}}
$$

이는 8장에서 유도된 바와 같이 다음과 같은 엔트로피를 갖습니다.

$$
h\left(\mathcal{N}_{n}(0, K)\right)=\frac{1}{2} \log (2 \pi e)^{n}|K|
$$

예제 12.2.9 예제 12.2.8과 동일한 제약 조건이 있지만, $E X_{i} X_{j}=K_{i j}$가 일부 제한된 $(i, j) \in A$ 집합에 대해서만 주어진다고 가정합니다. 예를 들어, $i=j \pm 2$인 경우에만 $K_{i j}$를 알 수 있습니다. 그러면 (12.22)와 (12.23)을 비교함으로써, $(i, j) \in A^{c}$인 경우 $\left(K^{-1}\right)_{i j}=0$임을 결론 내릴 수 있습니다 (즉, 공분산 행렬의 역행렬의 항목은 $(i, j)$가 제약 조건 집합 밖에 있을 때 0입니다).

# 12.3 비정상 최대 엔트로피 문제

제약 조건 하에서 최대 엔트로피 분포가 다음과 같은 형태임을 증명했습니다.

$$
\int_{S} h_{i}(x) f(x) d x=\alpha_{i}
$$

제약 조건 (12.25)을 만족하는 $\lambda_{0}, \lambda_{1}, \ldots, \lambda_{p}$가 존재한다면, 다음과 같은 형태를 갖습니다.

$$
f(x)=e^{\lambda_{0}+\sum \lambda_{i} h_{i}(x)}
$$
<!-- Page 440 -->
이제 제약 조건을 만족하도록 $\lambda_{i}$를 선택할 수 없는 까다로운 문제를 고려합니다. 그럼에도 불구하고 "최대" 엔트로피를 찾을 수 있습니다. 다음 문제를 고려합니다. 제약 조건 하에서 엔트로피를 최대화합니다.

$$
\begin{aligned}
\int_{-\infty}^{\infty} f(x) d x & =1 \\
\int_{-\infty}^{\infty} x f(x) d x & =\alpha_{1} \\
\int_{-\infty}^{\infty} x^{2} f(x) d x & =\alpha_{2} \\
\int_{-\infty}^{\infty} x^{3} f(x) d x & =\alpha_{3}
\end{aligned}
$$

여기서 최대 엔트로피 분포는 존재한다면 다음과 같은 형태를 가져야 합니다.

$$
f(x)=e^{\lambda_{0}+\lambda_{1} x+\lambda_{2} x^{2}+\lambda_{3} x^{3}}
$$

하지만 $\lambda_{3}$가 0이 아니면 $\int_{-\infty}^{\infty} f=\infty$이고 밀도를 정규화할 수 없습니다. 따라서 $\lambda_{3}$는 0이어야 합니다. 그러면 네 개의 방정식과 세 개의 변수만 남게 되므로 일반적으로 적절한 상수를 선택하는 것이 불가능합니다. 이 경우 방법이 실패한 것으로 보입니다.

겉보기 실패의 이유는 간단합니다. 엔트로피는 이러한 제약 조건 하에서 상한을 가지지만, 이를 달성하는 것은 불가능합니다. 첫 번째와 두 번째 모멘트 제약 조건만 있는 해당 문제를 고려하십시오. 이 경우 예제 12.2.1의 결과는 엔트로피 최대화 분포가 적절한 모멘트를 가진 정규 분포임을 보여줍니다. 추가적인 세 번째 모멘트 제약 조건으로 최대 엔트로피는 더 높아질 수 없습니다. 이 값을 달성하는 것이 가능합니까?

달성할 수는 없지만 임의로 가까이 다가갈 수 있습니다. 매우 높은 $x$ 값에 작은 "움직임"이 있는 정규 분포를 고려하십시오. 새로운 분포의 모멘트는 이전 분포의 모멘트와 거의 동일하며, 가장 큰 변화는 세 번째 모멘트입니다. 첫 번째 움직임으로 인한 변화를 상쇄하기 위해 새로운 움직임을 추가하여 첫 번째와 두 번째 모멘트를 원래 값으로 되돌릴 수 있습니다. 움직임의 위치를 선택함으로써 관련 정규 분포의 엔트로피를 크게 줄이지 않고 세 번째 모멘트의 임의의 값을 얻을 수 있습니다. 이 방법을 사용하여 최대 엔트로피 분포의 상한에 임의로 가까이 다가갈 수 있습니다. 따라서 다음과 같이 결론 내립니다.

$$
\sup h(f)=h\left(\mathcal{N}\left(0, \alpha_{2}-\alpha_{1}^{2}\right)\right)=\frac{1}{2} \ln 2 \pi e\left(\alpha_{2}-\alpha_{1}^{2}\right)
$$
<!-- Page 441 -->
이 예시는 최대 엔트로피가 $\epsilon$-달성 가능할 뿐임을 보여줍니다.

# 12.4 스펙트럼 추정

정상적인 평균이 0인 확률 과정 $\left\{X_{i}\right\}$가 주어졌을 때, 자기상관 함수를 다음과 같이 정의합니다.

$$
R(k)=E X_{i} X_{i+k}
$$

평균이 0인 과정의 자기상관 함수의 푸리에 변환은 전력 스펙트럼 밀도 $S(\lambda)$입니다:

$$
S(\lambda)=\sum_{m=-\infty}^{\infty} R(m) e^{-i m \lambda}, \quad-\pi<\lambda \leq \pi
$$

여기서 $i=\sqrt{-1}$입니다. 전력 스펙트럼 밀도는 과정의 구조를 나타내므로, 표본으로부터 추정하는 것이 유용합니다.

전력 스펙트럼을 추정하는 방법은 여러 가지가 있습니다. 가장 간단한 방법은 길이 $n$인 표본에 대해 표본 평균을 취하여 자기상관 함수를 추정하는 것입니다.

$$
\hat{R}(k)=\frac{1}{n-k} \sum_{i=1}^{n-k} X_{i} X_{i+k}
$$

표본 상관 함수 $\hat{R}(\cdot)$의 모든 값을 사용하여 스펙트럼을 계산하면, (12.34)에서 얻은 추정치는 $n$이 클 때 실제 전력 스펙트럼으로 수렴하지 않습니다. 따라서 이 방법, 즉 주기ogram 방법은 거의 사용되지 않습니다. 주기ogram 방법의 문제점 중 하나는 데이터로부터 얻은 자기상관 함수의 추정치가 서로 다른 정확도를 갖는다는 것입니다. 낮은 $k$ 값(지연이라고 함)에 대한 추정치는 많은 수의 표본에 기반하지만, 높은 $k$에 대한 추정치는 매우 적은 수의 표본에 기반합니다. 따라서 추정치는 낮은 $k$에서 더 정확합니다. 높은 지연 자기상관을 0으로 설정하여 낮은 $k$에서의 자기상관에만 의존하도록 방법을 수정할 수 있습니다. 그러나 이는 자기상관이 갑자기 0으로 바뀌기 때문에 일부 인공적인 현상을 발생시킵니다. 자기상관의 급격한 전환을 부드럽게 하기 위해 다양한 윈도잉 기법이 제안되었습니다. 그러나 윈도잉은 스펙트럼 해상도를 감소시키고 음의 전력 스펙트럼 추정치를 생성할 수 있습니다.

1960년대 후반, 지구물리학적 응용을 위한 스펙트럼 추정 문제를 연구하던 중 Burg는 대안적인 방법을 제안했습니다. 대신에
<!-- Page 442 -->
높은 지연 시간에서의 자기상관을 0으로 설정함으로써, 그는 데이터에 대한 가정이 가장 적은 값(즉, 프로세스의 entropy rate를 최대화하는 값)으로 설정했습니다. 이는 Jaynes [294]가 명확히 한 최대 엔트로피 원리와 일치합니다. Burg는 프로세스가 정상적이고 가우시안이라고 가정했으며, 상관 제약 조건 하에서 엔트로피를 최대화하는 프로세스가 적절한 차수의 자기회귀 가우시안 프로세스임을 발견했습니다. 데이터에 대한 근본적인 자기회귀 모델을 가정할 수 있는 일부 응용 분야에서는 이 방법이 모델의 매개변수를 결정하는 데 유용함이 입증되었습니다(예: 음성 신호의 선형 예측 코딩). 이 방법(최대 엔트로피 방법 또는 Burg 방법으로 알려짐)은 스펙트럼 밀도 추정에 널리 사용되는 방법입니다. Burg의 정리는 12.6절에서 증명합니다.

# 12.5 가우시안 프로세스의 엔트로피율

8장에서는 연속 확률 변수의 미분 엔트로피를 정의했습니다. 이제 실수값 확률 프로세스에 대한 엔트로피율의 정의를 확장할 수 있습니다.

정의 실수값 확률 프로세스 $\left\{X_{i}\right\}, X_{i} \in \mathcal{R}$의 미분 엔트로피율은 다음과 같이 정의됩니다.

$$
h(\mathcal{X})=\lim _{n \rightarrow \infty} \frac{h\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n}
$$

극한이 존재할 경우.
이산적인 경우와 마찬가지로, 정상 프로세스의 경우 극한이 존재하며 극한은 다음 두 표현식으로 주어진다는 것을 보여줄 수 있습니다.

$$
\begin{aligned}
h(\mathcal{X}) & =\lim _{n \rightarrow \infty} \frac{h\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n} \\
& =\lim _{n \rightarrow \infty} h\left(X_{n} \mid X_{n-1}, \ldots, X_{1}\right)
\end{aligned}
$$

정상 가우시안 확률 프로세스의 경우 다음과 같습니다.

$$
h\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\frac{1}{2} \log (2 \pi e)^{n}\left|K^{(n)}\right|
$$

여기서 공분산 행렬 $K^{(n)}$은 첫 번째 행에 $R(0), R(1), \ldots, R(n-1)$ 항목이 있는 Toeplitz 행렬입니다. 따라서 $K_{i j}^{(n)}=R(i-j)=E\left(X_{i}-E X_{i}\right)\left(X_{j}\right.$
<!-- Page 443 -->
$\left.-E X_{j}\right)$입니다. $n \rightarrow \infty$일 때, 공분산 행렬의 고유값의 밀도는 확률 과정의 스펙트럼인 극한값으로 수렴합니다. 실제로 Kolmogorov는 정상 가우시안 확률 과정의 엔트로피율이 다음과 같이 표현될 수 있음을 보였습니다.

$$
h(\mathcal{X})=\frac{1}{2} \log 2 \pi e+\frac{1}{4 \pi} \int_{-\pi}^{\pi} \log S(\lambda) d \lambda
$$

엔트로피율은 또한 $\lim _{n \rightarrow \infty} h\left(X_{n} \mid X^{n-1}\right)$입니다. 확률 과정이 가우시안이므로, 조건부 분포 또한 가우시안이며, 따라서 조건부 엔트로피는 $\frac{1}{2} \log 2 \pi e \sigma_{\infty}^{2}$입니다. 여기서 $\sigma_{\infty}^{2}$는 무한한 과거를 고려했을 때 $X_{n}$의 최적 추정치의 오차 분산입니다. 따라서,

$$
\sigma_{\infty}^{2}=\frac{1}{2 \pi e} 2^{2 \mathrm{~h}(\mathcal{X})}
$$

여기서 $h(\mathcal{X})$는 (12.40)에 의해 주어집니다. 그러므로, 엔트로피율은 무한한 과거를 고려했을 때 과정의 샘플에 대한 최적 추정치의 최소 평균 제곱 오차에 해당합니다.

# 12.6 버그의 최대 엔트로피 정리

정리 12.6.1 제약 조건을 만족하는 최대 엔트로피율 확률 과정 $\left\{X_{i}\right\}$는

$$
E X_{i} X_{i+k}=\alpha_{k}, \quad k=0,1, \ldots, p \quad \text { 모든 } i에 대해
$$

다음 형태의 p차 가우스-마르코프 과정입니다.

$$
X_{i}=-\sum_{k=1}^{p} a_{k} X_{i-k}+Z_{i}
$$

여기서 $Z_{i}$는 i.i.d. $\sim \mathcal{N}\left(0, \sigma^{2}\right)$이며, $a_{1}, a_{2}, \ldots, a_{p}, \sigma^{2}$는 (12.42)를 만족하도록 선택됩니다.

비고: $\left\{X_{i}\right\}$가 (a) 평균 0, (b) 가우시안, 또는 (c) 광대역 정상 과정이라고 가정하지 않습니다.

증명: $X_{1}, X_{2}, \ldots, X_{n}$을 제약 조건 (12.42)를 만족하는 임의의 확률 과정이라고 합시다. $Z_{1}, Z_{2}, \ldots, Z_{n}$을 $X_{1}, X_{2}, \ldots, X_{n}$과 동일한 공분산 행렬을 갖는 가우시안 과정이라고 합시다. 그러면 다변수 정규 분포는 모든 벡터 값 확률에 대해 엔트로피를 최대화하므로,
<!-- Page 444 -->
공분산 제약 조건 하에서 변수들은 다음과 같습니다.

$$
\begin{aligned}
h\left(X_{1}, X_{2}, \ldots, X_{n}\right) & \leq h\left(Z_{1}, Z_{2}, \ldots, Z_{n}\right) \\
& =h\left(Z_{1}, \ldots, Z_{p}\right)+\sum_{i=p+1}^{n} h\left(Z_{i} \mid Z_{i-1}, Z_{i-2}, \ldots, Z_{1}\right) \\
& \leq h\left(Z_{1}, \ldots, Z_{p}\right)+\sum_{i=p+1}^{n} h\left(Z_{i} \mid Z_{i-1}, Z_{i-2}, \ldots, Z_{i-p}\right)
\end{aligned}
$$

이는 연쇄 법칙과 조건화가 엔트로피를 감소시킨다는 사실에 의해 성립합니다. 이제 $Z_{1}^{\prime}, Z_{2}^{\prime}, \ldots, Z_{n}^{\prime}$을 $Z_{1}, Z_{2}, \ldots, Z_{n}$과 $p$차까지 동일한 분포를 갖는 $p$차 가우스-마르코프 과정으로 정의합니다. (이러한 과정의 존재는 증명 직후 Yule-Walker 방정식을 사용하여 검증될 것입니다.) 그러면 $h\left(Z_{i} \mid Z_{i-1}, \ldots, Z_{i-p}\right)$는 $p$차 분포에만 의존하므로, $h\left(Z_{i} \mid Z_{i-1}, \ldots, Z_{i-p}\right)=h\left(Z_{i}^{\prime} \mid Z_{i-1}^{\prime}, \ldots, Z_{i-p}^{\prime}\right)$이며, 부등식 연쇄를 계속하면 다음과 같이 얻습니다.

$$
\begin{aligned}
h\left(X_{1}, X_{2}, \ldots, X_{n}\right) & \leq h\left(Z_{1}, \ldots, Z_{p}\right)+\sum_{i=p+1}^{n} h\left(Z_{i} \mid Z_{i-1}, Z_{i-2}, \ldots, Z_{i-p}\right) \\
& =h\left(Z_{1}^{\prime}, \ldots, Z_{p}^{\prime}\right)+\sum_{i=p+1}^{n} h\left(Z_{i}^{\prime} \mid Z_{i-1}^{\prime}, Z_{i-2}^{\prime}, \ldots, Z_{i-p}^{\prime}\right) \\
& =h\left(Z_{1}^{\prime}, Z_{2}^{\prime}, \ldots, Z_{n}^{\prime}\right)
\end{aligned}
$$

여기서 마지막 등식은 $\left\{Z_{i}^{\prime}\right\}$의 $p$차 마르코프성에 의해 성립합니다. $n$으로 나누고 극한을 취하면 다음과 같이 얻습니다.

$$
\overline{\lim } \frac{1}{n} h\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq \lim \frac{1}{n} h\left(Z_{1}^{\prime}, Z_{2}^{\prime}, \ldots, Z_{n}^{\prime}\right)=h^{*}
$$

여기서

$$
h^{*}=\frac{1}{2} \log 2 \pi e \sigma^{2}
$$

이는 가우스-마르코프 과정의 엔트로피율입니다. 따라서 제약 조건을 만족하는 최대 엔트로피율 확률 과정은 제약 조건을 만족하는 $p$차 가우스-마르코프 과정입니다.

증명의 간략한 요약은 확률 과정의 유한 구간의 엔트로피는 다음과 같은 엔트로피에 의해 위에서 제한된다는 것입니다.
<!-- Page 445 -->
동일한 공분산 구조를 갖는 가우시안 랜덤 프로세스의 세그먼트입니다. 이 엔트로피는 주어진 공분산 제약 조건을 만족하는 최소 차수의 가우스-마르코프 프로세스의 엔트로피보다 상한이 정해져 있습니다. 이러한 프로세스는 존재하며 아래에 주어진 율-워커 방정식(Yule-Walker equations)을 통해 편리하게 특성화될 수 있습니다.
$a_{1}, \ldots, a_{p}$ 및 $\sigma^{2}$ 선택에 대한 참고 사항: 공분산 시퀀스 $R(0), R(1), \ldots, R(p)$가 주어졌을 때, 이러한 공분산을 갖는 $p$차 가우스-마르코프 프로세스가 존재합니까? (12.43) 형태의 프로세스가 주어졌을 때, 제약 조건을 만족하도록 $a_{k}$를 선택할 수 있습니까? (12.43)에 $X_{i-l}$을 곱하고 기댓값을 취하면, $R(k)=R(-k)$임을 인지하고 다음을 얻습니다.

$$
R(0)=-\sum_{k=1}^{p} a_{k} R(-k)+\sigma^{2}
$$

그리고

$$
R(l)=-\sum_{k=1}^{p} a_{k} R(l-k), \quad l=1,2, \ldots
$$

이 방정식들은 율-워커 방정식이라고 불립니다. $p+1$개의 미지수 $a_{1}, a_{2}, \ldots, a_{p}, \sigma^{2}$에 대해 $p+1$개의 방정식이 있습니다. 따라서 공분산으로부터 프로세스의 파라미터를 풀 수 있습니다.

레빈슨 알고리즘(Levinson algorithm) 및 더빈 알고리즘(Durbin algorithm) [433]과 같은 빠른 알고리즘은 이러한 방정식의 특수한 구조를 활용하여 공분산으로부터 계수 $a_{1}, a_{2}, \ldots, a_{p}$를 효율적으로 계산하도록 고안되었습니다. (일관된 표기를 위해 $a_{0}=1$로 설정합니다.) 율-워커 방정식은 $R(k)$로부터 $a_{k}$와 $\sigma^{2}$를 계산하기 위한 편리한 선형 방정식 세트를 제공할 뿐만 아니라, $p$보다 큰 지연에 대해 자기 상관 함수가 어떻게 동작하는지를 나타냅니다. 높은 지연에 대한 자기 상관 함수는 $p$보다 작은 지연에 대한 값의 확장입니다. 이 값들은 자기 상관 함수의 율-워커 확장이라고 불립니다. 최대 엔트로피 프로세스의 스펙트럼은 다음과 같이 보입니다.

$$
\begin{aligned}
S(\lambda) & =\sum_{m=-\infty}^{\infty} R(m) e^{-i m \lambda} \\
& =\frac{\sigma^{2}}{\left|1+\sum_{k=1}^{p} a_{k} e^{-i k \lambda}\right|^{2}}, \quad-\pi \leq \lambda \leq \pi
\end{aligned}
$$

이것은 $R(0), R(1), \ldots, R(p)$ 제약 조건 하에서의 최대 엔트로피 스펙트럼 밀도입니다.

그러나 $p$차 가우스-마르코프 프로세스의 경우, $a_{i}$를 계산하지 않고도 엔트로피율을 직접 계산할 수 있습니다. $K_{p}$를 다음과 같이 정의합니다.
<!-- Page 446 -->
이 프로세스에 해당하는 상관 행렬은 $R_{0}, R_{1}, \ldots, R_{p}$를 맨 위 행에 갖는 행렬입니다. 이 프로세스의 경우 엔트로피율은 다음과 같습니다.

$$
\begin{aligned}
h^{*}=h\left(X_{p} \mid X_{p-1}, \ldots, X_{0}\right) & =h\left(X_{0}, \ldots, X_{p}\right)-h\left(X_{0}, \ldots, X_{p-1}\right) \\
& =\frac{1}{2} \log (2 \pi e)^{p+1}\left|K_{p}\right|-\frac{1}{2} \log (2 \pi e)^{p}\left|K_{p-1}\right| \\
& =\frac{1}{2} \log (2 \pi e) \frac{\left|K_{p}\right|}{\left|K_{p-1}\right|}
\end{aligned}
$$

실제 문제에서는 일반적으로 샘플 시퀀스 $X_{1}, X_{2}, \ldots, X_{n}$가 주어지며, 이를 통해 상관 관계를 계산합니다. 중요한 질문은 다음과 같습니다. 몇 개의 상관 지연을 고려해야 합니까(즉, $p$의 최적값은 무엇입니까)? 논리적으로 타당한 방법은 데이터의 두 단계 설명에서 총 설명 길이를 최소화하는 $p$ 값을 선택하는 것입니다. 이 방법은 Rissanen [442, 447] 및 Barron [33]에 의해 제안되었으며 Kolmogorov 복잡성 개념과 밀접하게 관련되어 있습니다.

# 요약

최대 엔트로피 분포. 제약 조건을 만족하는 확률 밀도 함수 $f$가 있다고 가정합니다.

$$
\int_{S} f(x) r_{i}(x)=\alpha_{i} \quad \text { for } 1 \leq i \leq m
$$

$f^{*}(x)=f_{\lambda}(x)=e^{\lambda_{0}+\sum_{i=1}^{m} \lambda_{i} r_{i}(x)}, x \in S$라고 하고, $f^{*}$가 (12.59)를 만족하도록 $\lambda_{0}, \ldots, \lambda_{m}$를 선택합니다. 그러면 $f^{*}$는 이러한 제약 조건을 만족하는 모든 $f$에 대해 $h(f)$를 고유하게 최대화합니다.

최대 엔트로피 스펙트럼 밀도 추정. 상관 제약 조건 $R_{0}, R_{1}, \ldots, R_{p}$를 따르는 확률 프로세스의 엔트로피율은 이러한 제약 조건을 만족하는 $p$차 영평균 가우스-마르코프 프로세스에 의해 최대화됩니다. 최대 엔트로피율은 다음과 같습니다.

$$
h^{*}=\frac{1}{2} \log (2 \pi e) \frac{\left|K_{p}\right|}{\left|K_{p-1}\right|}
$$
<!-- Page 447 -->
최대 엔트로피 스펙트럼 밀도는 다음과 같습니다.

$$
S(\lambda)=\frac{\sigma^{2}}{\left|1+\sum_{k=1}^{p} a_{k} e^{-i k \lambda}\right|^{2}}
$$

# 문제

12.1 최대 엔트로피. $x \geq 0$에 대해 정의된 최대 엔트로피 밀도 $f$를 찾으십시오. 여기서 $E X=\alpha_{1}, E \ln X=\alpha_{2}$를 만족합니다. 즉, $\int x f(x) d x=\alpha_{1}, \int(\ln x) f(x) d x=$ $\alpha_{2}$를 제약 조건으로 하여 $-\int f \ln f$를 최대화하십시오. 여기서 적분은 $0 \leq x<\infty$에 대해 수행됩니다. 이 밀도는 어떤 종류의 분포입니까?
12.2 제약 조건 하에서 $D(P \| Q)$ 최소화. $P(x), x \in\{1,2, \ldots\}$의 확률 질량 함수 (모수 형태)를 찾고자 합니다. 여기서 $\sum P(x) g_{i}(x)=\alpha_{i}, i=1,2, \ldots$를 만족하는 모든 $P$에 대해 상대 엔트로피 $D(P \| Q)$를 최소화합니다.
(a) 라그랑주 승수법을 사용하여 다음을 추측하십시오.

$$
P^{*}(x)=Q(x) e^{\sum_{i=1}^{\infty} \lambda_{i} g_{i}(x)+\lambda_{0}}
$$

는 $\alpha_{i}$ 제약 조건을 만족하는 $\lambda_{i}$가 존재한다면 이 최소값을 달성합니다. 이는 제약 조건 하에서 최대 엔트로피 분포에 대한 정리를 일반화합니다.
(b) $P^{*}$가 $D(P \| Q)$를 최소화함을 검증하십시오.
12.3 최대 엔트로피 과정. 제약 조건 하에서 최대 엔트로피율 확률 과정 $\left\{X_{i}\right\}_{-\infty}^{\infty}$를 찾으십시오:
(a) $E X_{i}^{2}=1, \quad i=1,2, \ldots$
(b) $E X_{i}^{2}=1, E X_{i} X_{i+1}=\frac{1}{2}, \quad i=1,2, \ldots$
(c) (a) 및 (b)의 과정에 대한 최대 엔트로피 스펙트럼을 찾으십시오.
12.4 주변 분포를 갖는 최대 엔트로피. 다음 주변 분포를 갖는 최대 엔트로피 분포 $p(x, y)$는 무엇입니까?

| $x^{y}$ | 1 | 2 | 3 |  |
| :--: | :--: | :--: | :--: | :--: |
| 1 | $p_{11}$ | $p_{12}$ | $p_{13}$ | $\frac{1}{2}$ |
| 2 | $p_{21}$ | $p_{22}$ | $p_{23}$ | $\frac{1}{4}$ |
| 3 | $p_{31}$ | $p_{32}$ | $p_{33}$ | $\frac{1}{4}$ |
|  | $\frac{2}{3}$ | $\frac{1}{6}$ | $\frac{1}{6}$ |  |
<!-- Page 448 -->
(힌트: 더 일반적인 결과를 추측하고 검증하는 것이 좋을 수 있습니다.)
12.5 고정된 주변 확률 분포를 갖는 프로세스. 고정된 쌍별 주변 확률 분포 $f_{X_{1}, X_{2}}\left(x_{1}, x_{2}\right), f_{X_{2}, X_{3}}\left(x_{2}, x_{3}\right), \ldots$, $f_{X_{n-1}, X_{n}}\left(x_{n-1}, x_{n}\right)$를 갖는 모든 확률 밀도 함수의 집합을 고려하십시오. 이러한 주변 확률 분포를 갖는 최대 엔트로피 프로세스가 첫 번째 순서(시간에 따라 변할 수 있는) 마르코프 프로세스임을 보이십시오. 최대화하는 $f^{*}\left(x_{1}, x_{2}\right.$, $\left.\ldots, x_{n}\right)$를 식별하십시오.
12.6 모든 확률 밀도 함수는 최대 엔트로피 확률 밀도 함수입니다. 주어진 확률 밀도 함수 $f_{0}(x)$가 있다고 가정합니다. $r(x)$가 주어졌을 때, $\int f(x) r(x) d x=\alpha$를 만족하는 모든 $f$에 대해 $h(X)$를 최대화하는 확률 밀도 함수를 $g_{\alpha}(x)$라고 합시다. 이제 $r(x)=$ $\ln f_{0}(x)$라고 합시다. 적절한 선택 $\alpha=$ $\alpha_{0}$에 대해 $g_{\alpha}(x)=f_{0}(x)$임을 보이십시오. 따라서 $f_{0}(x)$는 $\int f \ln f_{0}=\alpha_{0}$ 제약 조건 하에서 최대 엔트로피 확률 밀도 함수입니다.
12.7 평균 제곱 오차. $\left\{X_{i}\right\}_{i=1}^{n}$가 $E X_{i} X_{i+k}=R_{k}, k=$ $0,1, \ldots, p$를 만족한다고 가정합니다. $X_{n}$에 대한 선형 예측을 고려하십시오. 즉,

$$
\hat{X}_{n}=\sum_{i=1}^{n-1} b_{i} X_{n-i}
$$

$n>p$라고 가정합니다. 다음을 구하십시오.

$$
\max _{f\left(x^{n}\right)} \min _{b} E\left(X_{n}-\hat{X}_{n}\right)^{2}
$$

여기서 최소값은 모든 선형 예측 $b$에 대해 계산되고 최대값은 $R_{0}, \ldots, R_{p}$를 만족하는 모든 확률 밀도 함수 $f$에 대해 계산됩니다.
12.8 최대 엔트로피 특성 함수. 제약 조건 $\Psi(u)=\int_{0}^{a} e^{i u x} f(x) d x$를 만족하는 최대 엔트로피 확률 밀도 함수 $f(x), 0 \leq x \leq a$를 찾습니다. 답은 매개변수 형태로만 제공하면 됩니다.
(a) 지정된 점 $u_{0}$에서 $\int_{0}^{a} f(x) \cos \left(u_{0} x\right) d x$ $=\alpha$를 만족하는 최대 엔트로피 $f$를 찾으십시오.
(b) 지정된 점 $u_{0}$에서 $\int_{0}^{a} f(x) \sin \left(u_{0} x\right) d x$ $=\beta$를 만족하는 최대 엔트로피 $f$를 찾으십시오.
(c) 지정된 점 $u_{0}$에서 특성 함수의 주어진 값 $\Psi\left(u_{0}\right)$를 갖는 최대 엔트로피 확률 밀도 함수 $f(x), 0 \leq x \leq a$를 찾으십시오.
(d) $a=\infty$인 경우 어떤 문제가 발생합니까?
<!-- Page 449 -->
12.9 최대 엔트로피 과정
(a) 모든 $i$에 대해 $\operatorname{Pr}\left\{X_{i}=X_{i+1}\right\}=\frac{1}{3}$을 만족하는 최대 엔트로피율 이진 확률 과정 $\left\{X_{i}\right\}_{i=-\infty}^{\infty}, X_{i} \in\{0,1\}$을 찾으십시오.
(b) 결과적인 엔트로피율은 무엇입니까?
12.10 합의 최대 엔트로피. $Y=X_{1}+X_{2}$라고 합시다. 제약 조건 $E X_{1}^{2}=P_{1}, E X_{2}^{2}$ $=P_{2}$ 하에서 $Y$의 최대 엔트로피 밀도를 찾으십시오.
(a) $X_{1}$과 $X_{2}$가 독립인 경우.
(b) $X_{1}$과 $X_{2}$가 종속일 수 있는 경우.
(c) (a) 부분을 증명하십시오.
12.11 최대 엔트로피 마르코프 연쇄. $\left\{X_{i}\right\}$를 $X_{i} \in\{1,2,3\}$인 정상 마르코프 연쇄라고 합시다. 모든 $n$에 대해 $I\left(X_{n} ; X_{n+2}\right)=0$이라고 합시다.
(a) 이 제약 조건을 만족하는 최대 엔트로피율 과정은 무엇입니까?
(b) $0 \leq \alpha \leq \log 3$인 주어진 값 $\alpha$에 대해 $I\left(X_{n} ; X_{n+2}\right)=\alpha$인 경우 어떻게 됩니까?
12.12 예측 오차에 대한 엔트로피 경계. $\left\{X_{n}\right\}$을 임의의 실수값 확률 과정이라고 합시다. $\hat{X}_{n+1}=E\left\{X_{n+1} \mid X^{n}\right\}$이라고 합시다. 따라서 조건부 평균 $\hat{X}_{n+1}$은 $n$-과거 $X^{n}$에 의존하는 확률 변수입니다. 여기서 $\hat{X}_{n+1}$은 과거가 주어졌을 때 $X_{n+1}$의 최소 평균 제곱 오차 예측입니다.
(a) 조건부 분산 $E\left\{E\left\{\left(X_{n+1}\right.\right.\right.$ $\left.\left.-\hat{X}_{n+1}\right)^{2} \mid X^{n}\right\}\}$에 대한 하한을 조건부 차분 엔트로피 $h\left(X_{n+1} \mid X^{n}\right)$의 항으로 찾으십시오.
(b) $\left\{X_{n}\right\}$이 가우시안 확률 과정일 때 등식이 달성됩니까?
12.13 최대 엔트로피율. 00이 시퀀스에 나타날 확률이 0인 기호 집합 $\{0,1\}$에 대한 최대 엔트로피율 확률 과정 $\left\{X_{i}\right\}$는 무엇입니까?
12.14 최대 엔트로피
(a) 두 조건 $E X^{8}=a, \quad E X^{16}=b$를 만족하는 매개변수 형태의 최대 엔트로피 밀도 $f(x)$는 무엇입니까?
(b) 조건 $E\left(X^{8}+X^{16}\right)=a+b$를 만족하는 최대 엔트로피 밀도는 무엇입니까?
(c) 어떤 엔트로피가 더 높습니까?
<!-- Page 450 -->
12.15 최대 엔트로피. 라플라스 변환 조건

$$
\int f(x) \mathrm{e}^{-x} d x=\alpha
$$

을 만족하는 최대 엔트로피 밀도 $f$의 매개변수 형태를 찾고, 매개변수에 대한 제약 조건을 제시하십시오.
12.16 최대 엔트로피 과정. $\left\{X_{i}\right\}, X_{i} \in \mathcal{R}$를 갖는 모든 확률 과정의 집합을 고려하십시오. 여기서

$$
R_{0}=E X_{i}^{2}=1, \quad R_{1}=E X_{i} X_{i+1}=\frac{1}{2}
$$

최대 엔트로피율을 찾으십시오.
12.17 이진 최대 엔트로피. $R_{0}=E X_{i}^{2}=1$ 및 $R_{1}=E X_{i} X_{i+1}=\frac{1}{2}$를 갖는 이진 과정 $\left\{X_{i}\right\}, X_{i} \in$ $\{-1,+1\}$을 고려하십시오.
(a) 이 제약 조건을 갖는 최대 엔트로피 과정을 찾으십시오.
(b) 엔트로피율은 얼마입니까?
(c) 이 제약 조건을 만족하는 베르누이 과정이 존재합니까?
12.18 최대 엔트로피. 에너지 제약 조건 $E\left(\frac{1}{2} m\|V\|^{2}+m g Z\right)=E_{0}$ 하에서 $h\left(Z, V_{x}, V_{y}, V_{z}\right)$를 최대화하십시오. 결과 분포가 다음을 산출함을 보이십시오.

$$
E \frac{1}{2} m\|V\|^{2}=\frac{3}{5} E_{0}, \quad E m g Z=\frac{2}{5} E_{0}
$$

따라서 에너지의 $\frac{2}{5}$는 장의 세기 $g$에 관계없이 퍼텐셜 장에 저장됩니다.
12.19 최대 엔트로피 이산 과정
(a) $\operatorname{Pr}\left\{X_{i}=X_{i+1}\right\}=\frac{1}{3}$을 모든 $i$에 대해 만족하는 최대 엔트로피율 이진 확률 과정 $\left\{X_{i}\right\}_{i=-\infty}^{\infty}, \quad X_{i} \in\{0,1\}$을 찾으십시오.
(b) 결과 엔트로피율은 얼마입니까?
12.20 합의 최대 엔트로피. $Y=X_{1}+X_{2}$라고 할 때, $E X_{1}^{2}=P_{1}, E X_{2}^{2}=P_{2}$ 제약 조건 하에서 $Y$의 최대 엔트로피를 찾으십시오.
(a) $X_{1}$과 $X_{2}$가 독립인 경우.
(b) $X_{1}$과 $X_{2}$가 종속일 수 있는 경우.
<!-- Page 451 -->
# 12.21 엔트로피율

(a) 최대 엔트로피율 확률 과정 $\left\{X_{i}\right\}$를 찾으십시오. 단, $E X_{i}^{2}=1, E X_{i} X_{i+2}=\alpha, i=1,2, \ldots$ 입니다. 주의하십시오.
(b) 최대 엔트로피율은 얼마입니까?
(c) 이 과정에 대한 $E X_{i} X_{i+1}$은 얼마입니까?
12.22 최소 기댓값
(a) 다음 세 가지 제약 조건을 만족하는 모든 확률 밀도 함수 $f(x)$에 대해 $E X$의 최소값을 찾으십시오:
(i) $x \leq 0$일 때 $f(x)=0$.
(ii) $\int_{-\infty}^{\infty} f(x) d x=1$.
(iii) $h(f)=h$.
(b) (i)이 다음으로 대체될 경우 동일한 문제를 푸십시오:
(i') $x \leq a$일 때 $f(x)=0$.

## 역사적 고찰

최대 엔트로피 원칙은 19세기 통계 역학에서 발생했으며, Jaynes [294]에 의해 더 넓은 맥락에서 사용이 주장되었습니다. 이는 Burg [80]에 의해 스펙트럼 추정에 적용되었습니다. Burg의 정리에 대한 정보 이론적 증명은 Choi와 Cover [98]에서 나왔습니다.
<!-- Page 452 -->
.
<!-- Page 453 -->
# 보편적 소스 코딩

여기서는 보편적 소스 코딩의 기초를 개발합니다. 최소 최대 후회 데이터 압축이 정의되고, 보편성의 기술 비용은 모든 소스 분포를 포함하는 상대 엔트로피 볼의 정보 반경임이 입증됩니다. 최소 최대 정리는 이 반경이 소스 분포에 의해 주어진 관련 채널의 채널 용량임을 보여줍니다. 산술 코딩은 즉석에서 학습된 소스 분포를 사용할 수 있게 합니다. 마지막으로, 개별 시퀀스 압축이 정의되고 일련의 Lempel-Ziv 파싱 알고리즘에 의해 달성됩니다.

제5장에서는 소스의 가장 짧은 표현을 찾는 문제를 소개하고, 엔트로피가 모든 고유 복호화 가능한 표현의 기대 길이의 근본적인 하한임을 보여주었습니다. 또한 소스의 확률 분포를 알고 있다면 Huffman 알고리즘을 사용하여 해당 분포에 대한 최적(최소 기대 길이) 코드를 구성할 수 있음을 보여주었습니다.

그러나 많은 실제 상황에서는 소스의 근본적인 확률 분포를 알 수 없으며 제5장의 방법을 직접 적용할 수 없습니다. 대신, 우리가 아는 것은 분포의 클래스뿐입니다. 가능한 접근 방식 중 하나는 모든 데이터를 볼 때까지 기다렸다가 데이터에서 분포를 추정하고, 이 분포를 사용하여 최상의 코드를 구성한 다음, 처음으로 돌아가서 이 코드를 사용하여 데이터를 압축하는 것입니다. 이 두 번의 통과 절차는 압축할 데이터의 양이 상당히 적은 일부 응용 프로그램에서 사용됩니다. 그러나 데이터에 대해 두 번의 통과를 수행하는 것이 불가능한 상황이 많이 있으며, 데이터의 확률 분포를 "학습"하고 이를 사용하여 들어오는 기호를 압축하는 단일 통과(또는 온라인) 알고리즘을 갖는 것이 바람직합니다. 우리는 분포 클래스 내의 모든 분포에 대해 잘 작동하는 이러한 알고리즘의 존재를 보여줍니다.

또 다른 경우에는 데이터의 근본적인 확률 분포가 없습니다. 우리가 제공받는 것은 결과의 개별 시퀀스뿐입니다. 예시

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 454 -->
text와 음악을 포함합니다. 그런 다음 "이 시퀀스를 얼마나 잘 압축할 수 있는가?"라는 질문을 할 수 있습니다. 알고리즘 클래스에 제한을 두지 않으면 의미 없는 답변을 얻게 됩니다. 특정 시퀀스를 하나의 비트로 압축하면서 다른 모든 시퀀스는 압축하지 않는 함수가 항상 존재합니다. 이 함수는 명백히 데이터에 "과적합"되었습니다. 그러나 베르누이 분포 또는 $k$차 마르코프 프로세스에 대한 최적의 단어 할당으로 달성 가능한 성능과 비교하면, 확률적 또는 평균 사례 분석 결과와 여러 면에서 유사한 더 흥미로운 답변을 얻게 됩니다. 개별 시퀀스의 압축 가능성에 대한 궁극적인 답변은 시퀀스의 Kolmogorov 복잡성이며, 이는 14장에서 논의합니다.

이 장에서는 코더가 표현의 평균 길이를 최소화하려고 시도하는 게임으로 소스 코딩 문제를 고려하고, 자연은 소스 시퀀스에 대한 분포를 선택한다고 가정합니다. 이 게임에는 전이 행렬의 행이 소스 시퀀스에 대한 가능한 분포인 채널 용량과 관련된 값이 있음을 보여줍니다. 그런 다음 시퀀스에 대한 알려진 또는 "추정된" 분포가 주어졌을 때 소스 시퀀스를 인코딩하는 알고리즘을 고려합니다. 특히, 5.9절의 Shannon-Fano-Elias 코드의 확장으로, 시퀀스 기호의 점진적인 인코딩 및 디코딩을 허용하는 산술 코딩을 설명합니다.

그런 다음 Ziv와 Lempel의 논문 [603, 604]을 기반으로 하는 Lempel-Ziv라는 적응형 사전 압축 알고리즘 클래스의 두 가지 기본 버전을 설명합니다. 이러한 알고리즘의 점근적 최적성에 대한 증명을 제공하며, 극한에서 모든 정상 정상 측지 소스에 대한 엔트로피율을 달성함을 보여줍니다. 16장에서는 보편성의 개념을 주식 시장 투자로 확장하고, 데이터 압축을 위한 보편적 방법과 유사한 온라인 포트폴리오 선택 절차를 설명합니다.

# 13.1 보편적 코드 및 채널 용량

모수 $\theta \in\{1,2, \ldots, m\}$가 알려지지 않은 분포 $\left\{p_{\theta}\right\}$에 따라 추출된 확률 변수 $X$가 있다고 가정합니다. 이 소스에 대한 효율적인 코드를 찾고자 합니다.

5장의 결과에 따르면, $\theta$를 알면 부호어 길이 $l(x)=\log \frac{1}{p_{\theta}(x)}$를 갖는 코드를 구성할 수 있으며, 이는 평균 부호어

<!-- Page 455 -->
길이가 엔트로피 $H_{\theta}(x)=-\sum_{x} p_{\theta}(x) \log p_{\theta}(x)$와 같으며, 이것이 우리가 할 수 있는 최선입니다. 이 절의 목적을 위해, 우리는 $l(x)$에 대한 정수 제약을 무시할 것이며, 정수 제약을 적용하면 기대 길이가 최대 1비트가 소요될 것임을 알고 있습니다. 따라서,

$$
\min _{l(x)} E_{p_{\theta}}[l(X)]=E_{p_{\theta}}\left[\log \frac{1}{p_{\theta}(X)}\right]=H\left(p_{\theta}\right)
$$

만약 우리가 실제 분포 $p_{\theta}$를 알지 못하지만 가능한 한 효율적으로 코딩하기를 원한다면 어떻게 될까요? 이 경우, 코드워드 길이 $l(x)$와 암시된 확률 $q(x)=2^{-l(x)}$를 가진 코드를 사용하여, 코드의 기대 길이와 기대 길이에 대한 하한 사이의 차이로 코드의 중복도를 정의합니다:

$$
\begin{aligned}
R\left(p_{\theta}, q\right) & =E_{p_{\theta}}[l(X)]-E_{p_{\theta}}\left[\log \frac{1}{p_{\theta}(X)}\right] \\
& =\sum_{x} p_{\theta}(x)\left(l(x)-\log \frac{1}{p(x)}\right) \\
& =\sum_{x} p_{\theta}(x)\left(\log \frac{1}{q(x)}-\log \frac{1}{p(x)}\right) \\
& =\sum_{x} p_{\theta}(x) \log \frac{p_{\theta}(x)}{q(x)} \\
& =D\left(p_{\theta} \| q\right)
\end{aligned}
$$

여기서 $q(x)=2^{-l(x)}$는 코드워드 길이 $l(x)$에 해당하는 분포입니다.

우리는 실제 분포 $p_{\theta}$에 관계없이 잘 작동하는 코드를 찾기를 원하며, 따라서 최소-최대 중복도를 다음과 같이 정의합니다:

$$
R^{*}=\min _{q} \max _{p_{\theta}} R\left(p_{\theta}, q\right)=\min _{q} \max _{p_{\theta}} D\left(p_{\theta} \| q\right)
$$

이 최소-최대 중복도는 분포 $p_{\theta}$를 포함하는 정보 볼의 "중심"에 있는 분포 $q$에 의해 달성됩니다. 즉, 모든 분포 $p_{\theta}$로부터의 최대 거리가 최소화되는 분포 $q$입니다 (그림 13.1).

상대 엔트로피에서 가능한 모든 $p_{\theta}$에 가능한 한 가까운 분포 $q$를 찾기 위해 다음 채널을 고려하십시오:
<!-- Page 456 -->

그림 13.1. 모든 $p_{\theta}$를 포함하는 최소 반경 정보 볼

$$
\theta \rightarrow\left[\begin{array}{c}
\ldots p_{1} \ldots \\
\ldots p_{2} \ldots \\
\vdots \\
\ldots p_{\theta} \ldots \\
\vdots \\
\ldots p_{m} \ldots
\end{array}\right] \rightarrow X
$$

이는 소스의 가능한 분포인 각 $p_{\theta}$가 전이 행렬의 행이 되는 채널 $\left\{\theta, p_{\theta}(x), \mathcal{X}\right\}$입니다. 우리는 최소 최대 중복도 $R^{*}$가 이 채널의 용량과 같다는 것을 보일 것이며, 해당 최적 코딩 분포는 용량 달성 입력 분포에 의해 유도된 이 채널의 출력 분포입니다. 이 채널의 용량은 다음과 같이 주어집니다.

$$
C=\max _{\pi(\theta)} I(\theta ; X)=\max _{\pi(\theta)} \sum_{\theta} \pi(\theta) p_{\theta}(x) \log \frac{p_{\theta}(x)}{q_{\pi}(x)}
$$

여기서

$$
q_{\pi}(x)=\sum_{\theta} \pi(\theta) p_{\theta}(x)
$$

$R^{*}$와 $C$의 동등성은 다음 정리에 의해 표현됩니다.
정리 13.1.1 (Gallager [229], Ryabko [450]) 행이 $p_{1}, p_{2}, \ldots, p_{m}$인 채널 $p(x \mid \theta)$의 용량은 다음과 같이 주어집니다.

$$
C=R^{*}=\min _{q} \max _{\theta} D\left(p_{\theta} \| q\right)
$$
<!-- Page 457 -->
(13.11)에서 최소값을 달성하는 분포 $q$는 용량-달성 입력 분포 $\pi^{*}(\theta)$에 의해 유도되는 출력 분포 $q^{*}(x)$입니다:

$$
q^{*}(x)=q_{\pi^{*}}(x)=\sum_{\theta} \pi^{*}(\theta) p_{\theta}(x)
$$

증명: $\theta \in\{1,2, \ldots, m\}$에 대한 입력 분포를 $\pi(\theta)$라고 하고, 유도된 출력 분포를 $q_{\pi}$라고 합시다:

$$
\left(q_{\pi}\right)_{j}=\sum_{i=1}^{m} \pi_{i} p_{i j}
$$

여기서 $p_{i j}=p_{\theta}(x)$는 $\theta=i, x=j$일 때입니다. 그러면 출력에 대한 모든 분포 $q$에 대해 다음이 성립합니다:

$$
\begin{aligned}
I_{\pi}(\theta ; X) & =\sum_{i, j} \pi_{i} p_{i j} \log \frac{p_{i j}}{\left(q_{\pi}\right)_{j}} \\
& =\sum_{i} \pi_{i} D\left(p_{i} \| q_{\pi}\right) \\
& =\sum_{i, j} \pi_{i} p_{i j} \log \frac{p_{i j}}{q_{j}} \frac{q_{j}}{\left(q_{\pi}\right)_{j}} \\
& =\sum_{i, j} \pi_{i} p_{i j} \log \frac{p_{i j}}{q_{j}}+\sum_{i, j} \pi_{i} p_{i j} \log \frac{q_{j}}{\left(q_{\pi}\right)_{j}} \\
& =\sum_{i, j} \pi_{i} p_{i j} \log \frac{p_{i j}}{q_{j}}+\sum_{j}\left(q_{\pi}\right)_{j} \log \frac{q_{j}}{\left(q_{\pi}\right)_{j}} \\
& =\sum_{i, j} \pi_{i} p_{i j} \log \frac{p_{i j}}{q_{j}}-D\left(q_{\pi} \| q\right) \\
& =\sum_{i} \pi_{i} D\left(p_{i} \| q\right)-D\left(q_{\pi} \| q\right) \\
& \leq \sum_{i} \pi_{i} D\left(p_{i} \| q\right)
\end{aligned}
$$

모든 $q$에 대해 등호는 $q=q_{\pi}$일 때만 성립합니다. 따라서 모든 $q$에 대해 다음이 성립합니다:

$$
\sum_{i} \pi_{i} D\left(p_{i} \| q\right) \geq \sum_{i} \pi_{i} D\left(p_{i} \| q_{\pi}\right)
$$
<!-- Page 458 -->
따라서

$$
I_{\pi}(\theta ; X)=\min _{q} \sum_{i} \pi_{i} D\left(p_{i} \| q\right)
$$

는 $q=q_{\pi}$일 때 달성됩니다. 따라서 전이 행렬의 모든 행에 대한 평균 거리를 최소화하는 출력 분포는 채널에 의해 유도되는 출력 분포입니다(정리 10.8.1).

채널 용량은 이제 다음과 같이 작성할 수 있습니다.

$$
\begin{aligned}
C & =\max _{\pi} I_{\pi}(\theta ; X) \\
& =\max _{\pi} \min _{q} \sum_{i} \pi_{i} D\left(p_{i} \| q\right)
\end{aligned}
$$

이제 게임 이론의 기본 정리를 적용할 수 있습니다. 이 정리는 연속 함수 $f(x, y), x \in \mathcal{X}, y \in \mathcal{Y}$에 대해 $f(x, y)$가 $x$에 대해 볼록이고 $y$에 대해 오목하며, $\mathcal{X}, \mathcal{Y}$가 컴팩트 볼록 집합이면 다음과 같습니다.

$$
\min _{x \in \mathcal{X}} \max _{y \in \mathcal{Y}} f(x, y)=\max _{y \in \mathcal{Y}} \min _{x \in \mathcal{X}} f(x, y)
$$

이 미니맥스 정리에 대한 증명은 [305, 392]에서 찾을 수 있습니다.
상대 엔트로피의 볼록성(정리 2.7.2)에 의해 $\sum_{i} \pi_{i} D\left(p_{i} \| q\right)$는 $q$에 대해 볼록이고 $\pi$에 대해 오목하므로,

$$
\begin{aligned}
C & =\max _{\pi} \min _{q} \sum_{i} \pi_{i} D\left(p_{i} \| q\right) \\
& =\min _{q} \max _{\pi} \sum_{i} \pi_{i} D\left(p_{i} \| q\right) \\
& =\min _{q} \max _{i} D\left(p_{i} \| q\right)
\end{aligned}
$$

여기서 마지막 등식은 (13.28)에서 $D\left(p_{i} \| q\right)$를 최대화하는 인덱스 $i$에 모든 가중치를 두어 최대값이 달성된다는 사실에서 비롯됩니다. 또한 $q^{*}=q_{\pi^{*}}$임을 알 수 있습니다. 이것으로 증명이 완료됩니다.

따라서 $\theta$에서 $X$로 가는 채널의 채널 용량은 소스 코딩에서의 미니맥스 기대 초과율입니다.

예제 13.1.1 $\mathcal{X}=\{1,2,3\}$이고 $\theta$가 두 개의 값 1과 2만 가지며 해당 분포가 $p_{1}=(1-\alpha$, $\alpha, 0)$ 및 $p_{2}=(0, \alpha, 1-\alpha)$인 경우를 고려해 보겠습니다. 분포가 $p_{1}$인지 $p_{2}$인지 알지 못한 채 $\mathcal{X}$의 기호 시퀀스를 인코딩하고 싶습니다. 위의 논증은 최악의 경우 최적 코드가 다음을 사용함을 나타냅니다.
<!-- Page 459 -->
두 분포 모두로부터 상대 엔트로피 거리가 최소인 분포에 해당하는 코드어 길이는 이 경우 두 분포의 중간값입니다. 이 분포 $q=\left\{\frac{1-\alpha}{2}, \alpha, \frac{1-\alpha}{2}\right\}$를 사용하면 다음과 같은 중복도를 얻습니다.

$$
D\left(p_{1} \| q\right)=D\left(p_{2} \| q\right)=(1-\alpha) \log \frac{1-\alpha}{(1-\alpha) / 2}+\alpha \log \frac{\alpha}{\alpha}+0=1-\alpha
$$

행이 $p_{1}$ 및 $p_{2}$와 같은 전이 행렬을 갖는 채널은 삭제 채널(7.1.5절)과 동등하며, 이 채널의 용량은 입력에 대한 균일 분포를 사용하여 $(1-\alpha)$로 쉽게 계산할 수 있습니다. 용량 달성 입력 분포에 해당하는 출력 분포는 $\left\{\frac{1-\alpha}{2}, \alpha, \frac{1-\alpha}{2}\right\}$와 같습니다 (즉, 위에서 설명한 분포 $q$와 동일합니다). 따라서 이 클래스의 소스에 대한 분포를 알지 못하면 $p_{1}$ 또는 $p_{2}$ 대신 $q$ 분포를 사용하여 코딩하며, 이상적인 엔트로피 한계보다 기호당 추가 비용으로 $1-\alpha$ 비트를 부담하게 됩니다.

# 13.2 이진 시퀀스에 대한 범용 코딩

이제 이진 시퀀스 $x^{n} \in\{0,1\}^{n}$의 인코딩에 대한 중요한 특수 사례를 고려합니다. $x_{1}, x_{2}, \ldots, x_{n}$에 대한 확률 분포에 대해 어떠한 가정도 하지 않습니다.

Wozencraft와 Reiffen [567]의 Lemma 17.5.1에서 증명된 $\binom{n}{k}$의 크기에 대한 바운드부터 시작하겠습니다. $k \neq 0$ 또는 $n$인 경우,

$$
\sqrt{\frac{n}{8 k(n-k)}} \leq\binom{ n}{k} 2^{-n H(k / n)} \leq \sqrt{\frac{n}{\pi k(n-k)}}
$$

먼저 시퀀스를 설명하기 위한 오프라인 알고리즘을 설명합니다. 시퀀스의 1의 개수를 세고, 전체 시퀀스를 본 후, 시퀀스에 대한 2단계 설명을 보냅니다. 첫 번째 단계는 시퀀스의 1의 개수입니다 [즉, $k=\sum_{i} x_{i}$ ( $\lceil\log (n+1)\rceil$ 비트 사용)]. 두 번째 단계는 1이 $k$개인 모든 시퀀스 중에서 이 시퀀스의 인덱스입니다 ( $\left\lceil\log \binom{n}{k}\right\rceil$ 비트 사용). 이 2단계 설명은 총 길이를 요구합니다.

$$
\begin{aligned}
l\left(x^{n}\right) & \leq \log (n+1)+\log \binom{n}{k}+2 \\
& \leq \log n+n H\left(\frac{k}{n}\right)-\frac{1}{2} \log n-\frac{1}{2} \log \left(\pi \frac{k}{n} \frac{(n-k)}{n}\right)+3
\end{aligned}
$$
<!-- Page 460 -->
$$
=n H\left(\frac{k}{n}\right)+\frac{1}{2} \log n-\frac{1}{2} \log \left(\pi \frac{k}{n} \frac{n-k}{n}\right)+3
$$

따라서, 시퀀스를 기술하는 비용은 베르누니 분포에 대한 Shannon 코드의 최적 비용보다 약 $\frac{1}{2} \log n$ 비트 더 높습니다. 마지막 항은 $k=0$ 또는 $k=n$에서 무한대가 되므로, 이 경우에 대한 바운드는 유용하지 않습니다 (실제 기술 길이는 $\log (n+1)$ 비트인 반면, $k=0$ 또는 $k=n$일 때 엔트로피 $H(k / n)=0$입니다).

이 계산 접근 방식은 압축기가 전체 시퀀스를 볼 때까지 기다려야 합니다. 이제 동일한 결과를 실시간으로 달성하는 혼합 분포를 사용하는 다른 접근 방식을 설명합니다. 코딩 분포 $q\left(x_{1}, x_{2}, \ldots, x_{n}\right)=2^{-l\left(x_{1}, x_{2}, \ldots, x_{n}\right)}$를 $x_{1}, x_{2}, \ldots, x_{n}$에 대한 모든 베르누니 $(\theta)$ 분포의 균일 혼합으로 선택합니다. 이 분포를 사용하는 코드의 성능을 분석하고 이러한 코드가 모든 입력 시퀀스에 대해 잘 수행됨을 보여줄 것입니다.

이 분포는 베르누니 분포의 매개변수인 $\theta$가 $[0,1]$ 구간에서 균일 분포에 따라 그려진다고 가정하여 구성합니다. $k$개의 1을 갖는 시퀀스 $x_{1}, x_{2}, \ldots, x_{n}$의 확률은 베르누니 $(\theta)$ 분포 하에서 $\theta^{k}(1-\theta)^{n-k}$입니다. 따라서 시퀀스의 혼합 확률은 다음과 같습니다.

$$
p\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\int_{0}^{1} \theta^{k}(1-\theta)^{n-k} d \theta \triangleq A(n, k)
$$

부분 적분을 사용하여 $u=(1-\theta)^{n-k}$ 및 $d v=\theta^{k} d \theta$로 설정하면 다음과 같습니다.

$$
\begin{aligned}
\int_{0}^{1} \theta^{k}(1-\theta)^{n-k} d \theta= & {\left[\frac{1}{k+1} \theta^{k+1}(1-\theta)^{n-k}\right]_{0}^{1} } \\
& +\frac{n-k}{k+1} \int_{0}^{1} \theta^{k+1}(1-\theta)^{n-k-1} d \theta
\end{aligned}
$$

또는

$$
A(n, k)=\frac{n-k}{k+1} A(n, k+1)
$$

이제 $A(n, n)=\int_{0}^{1} \theta^{n} d \theta=\frac{1}{n+1}$이며, 재귀로부터 쉽게 확인할 수 있습니다.

$$
p\left(x_{1}, x_{2}, \ldots, x_{n}\right)=A(n, k)=\frac{1}{n+1} \frac{1}{\binom{n}{k}}
$$
<!-- Page 461 -->
혼합 분포에 대한 코드워드 길이는 다음과 같습니다.

$$
\left\lceil\log \frac{1}{q\left(x^{n}\right)}\right\rceil \leq \log (n+1)+\log \binom{n}{k}+1
$$

이는 위에서 설명한 2단계 설명의 길이보다 1비트 이내입니다. 따라서 코드워드 길이에 대한 유사한 경계를 얻습니다.
$l\left(x_{1}, x_{2}, \ldots, x_{n}\right) \leq H\left(\frac{k}{n}\right)+\frac{1}{2} \log n-\frac{1}{2} \log \left(\pi \frac{k}{n} \frac{(n-k)}{n}\right)+2$
모든 시퀀스 $x_{1}, x_{2}, \ldots, x_{n}$에 대해. 이 혼합 분포는 소스가 실제로 Bernoulli $(k / n)$이라고 가정할 때 필요한 최적 코드 길이 $n H(k / n)$보다 $\frac{1}{2} \log n$ 비트 이내의 코드워드 길이를 달성합니다.

이 혼합 분포는 $x_{1}, x_{2}, \ldots, x_{n}$의 이전 기호가 주어졌을 때 다음 기호의 조건부 확률에 대한 좋은 표현을 제공합니다. $k_{i}$를 $x_{1}, x_{2}, \ldots, x_{n}$의 첫 $i$개 기호에서 1의 개수라고 합시다. (13.38)을 사용하면 다음과 같습니다.

$$
\begin{aligned}
q\left(x_{i+1}=1 \mid x^{i}\right) & =\frac{q\left(x^{i}, 1\right)}{q\left(x^{i}\right)} \\
& =\left(\frac{1}{i+2} \frac{1}{\binom{i+1}{k_{i}+1}}\right) /\left(\frac{1}{i+1} \frac{1}{\binom{i}{k_{i}}}\right) \\
& =\frac{1}{i+2} \frac{\left(k_{i}+1\right)!\left(n-k_{i}\right)!}{(i+1)!}(i+1) \frac{k_{i}!\left(i-k_{i}\right)!}{i!} \\
& =\frac{k_{i}+1}{i+2}
\end{aligned}
$$

이는 $\theta$에 대한 균일 사전 확률이 주어졌을 때 1의 베이지안 사후 확률이며, 다음 기호의 확률에 대한 라플라스 추정치라고 합니다. 이 사후 확률을 산술 코딩의 다음 기호 확률로 사용할 수 있으며, 유한 정밀도 산술을 사용하여 순차적으로 코드워드 길이 $\log \frac{1}{q\left(x^{n}\right)}$를 달성할 수 있습니다. 이는 절차의 길이가 시퀀스의 길이에 의존하지 않는다는 점에서 horizonfree 결과입니다.

균일 혼합 접근 방식 또는 2단계 접근 방식의 한 가지 문제는 $k=0$ 또는 $k=n$의 경우 경계가 적용되지 않는다는 것입니다. 추가 중복성에 대해 제공할 수 있는 유일한 균일 경계는 $\log n$이며, 이는 (11.40)의 경계를 사용하여 얻을 수 있습니다. 문제는 다음과 같습니다.
<!-- Page 462 -->
$k=0$ 또는 $k=n$인 시퀀스에 충분한 확률을 할당하지 않고 있습니다. 만약 $\theta$에 대한 균일 분포 대신, $\operatorname{Beta}\left(\frac{1}{2}, \frac{1}{2}\right)$ 분포라고도 불리는 Dirichlet $\left(\frac{1}{2}, \frac{1}{2}\right)$ 분포를 사용한다면, 시퀀스 $x_{1}, x_{2}, \ldots, x_{n}$의 확률은 다음과 같이 됩니다.

$$
q_{\frac{1}{2}}\left(x^{n}\right)=\int_{0}^{1} \theta^{k}(1-\theta)^{n-k} \frac{1}{\pi \sqrt{\theta(1-\theta)}} d \theta
$$

그리고 이것이 다음의 설명 길이를 달성한다는 것을 보일 수 있습니다.

$$
\log \frac{1}{q_{\frac{1}{2}\left(x^{n}\right)}} \leq H(k / n)+\frac{1}{2} \log n+\log \frac{\pi}{8}
$$

모든 $x^{n} \in\{0,1\}^{n}$에 대해, 이는 universal mixture code의 중복성에 대한 균일한 상한을 달성합니다. 균일 사전 확률의 경우와 마찬가지로, 이전 관측값들이 주어졌을 때 다음 기호의 조건부 분포를 다음과 같이 계산할 수 있습니다.

$$
q_{\frac{1}{2}}\left(x_{i+1}=1 \mid x^{i}\right)=\frac{k_{i}+\frac{1}{2}}{i+1}
$$

이는 산술 코딩과 함께 사용하여 시퀀스를 인코딩하기 위한 온라인 알고리즘을 제공할 수 있습니다. 우리는 16.7절에서 universal portfolio를 분석할 때 mixture 알고리즘의 성능을 더 자세히 분석할 것입니다.

# 13.3 산술 코딩

5장에서 설명된 Huffman 코딩 절차는 알려진 분포를 가진 랜덤 변수를 기호별로 인코딩해야 할 때 최적입니다. 그러나 Huffman 코드의 코드워드 길이가 정수로 제한된다는 사실 때문에, 인코딩 효율성에서 기호당 최대 1비트의 손실이 발생할 수 있었습니다. 입력 심볼 블록을 사용하면 이 손실을 완화할 수 있지만, 이 접근 방식의 복잡성은 블록 길이에 따라 지수적으로 증가합니다. 이제 이러한 비효율성 없이 인코딩하는 방법을 설명합니다. 산술 코딩에서는 기호를 나타내기 위해 비트 시퀀스를 사용하는 대신, 단위 구간의 부분 구간으로 이를 나타냅니다.

기호 시퀀스의 코드는 시퀀스에 더 많은 기호를 추가함에 따라 길이가 줄어드는 구간입니다. 이 속성은 코딩 방식이 점진적(시퀀스의 확장 코드의 원래 시퀀스의 코드로부터 간단히 계산될 수 있음)이고 코드워드 길이가 정수로 제한되지 않도록 합니다. 동기

<!-- Page 463 -->
산술 코딩은 섀넌-파노-엘리아스 코딩(5.9절)과 다음 보조정리에 기반합니다.

보조정리 13.3.1 $Y$를 연속 확률 분포 함수 $F(y)$를 갖는 확률 변수라고 합시다. $U=F(Y)$라고 하면 (즉, $U$는 분포 함수에 의해 정의된 $Y$의 함수입니다), $U$는 $[0,1]$에서 균등 분포를 갖습니다.

증명: $F(y) \in[0,1]$이므로 $U$의 범위는 $[0,1]$입니다. 또한, $u \in[0,1]$에 대해,

$$
\begin{aligned}
F_{U}(u) & =\operatorname{Pr}(U \leq u) \\
& =\operatorname{Pr}(F(Y) \leq u) \\
& =\operatorname{Pr}\left(Y \leq F^{-1}(u)\right) \\
& =F\left(F^{-1}(u)\right) \\
& =u
\end{aligned}
$$

이는 $U$가 $[0,1]$에서 균등 분포를 가짐을 증명합니다.
이제 유한 알파벳 $\mathcal{X}=0,1,2, \ldots, m$에서 무한한 확률 변수 시퀀스 $X_{1}, X_{2}, \ldots$를 고려해 봅시다. 이 알파벳에서 나온 임의의 시퀀스 $x_{1}, x_{2}, \ldots$에 대해, 시퀀스 앞에 0.을 붙여 0과 1 사이의 실수 (밑 $m+1$)로 간주할 수 있습니다. 실수 값 확률 변수 $X=0 . X_{1} X_{2} \ldots$를 정의하면, $X$는 다음과 같은 분포 함수를 갖습니다:

$$
\begin{aligned}
F_{X}(x) & =\operatorname{Pr}\left\{X \leq x=0 . x_{1} x_{2} \cdots\right\} \\
& =\operatorname{Pr}\left\{0 . X_{1} X_{2} \cdots \leq 0 . x_{1} x_{2} \cdots\right\} \\
& =\operatorname{Pr}\left\{X_{1}<x_{1}\right\}+\operatorname{Pr}\left\{X_{1}=x_{1}, X_{2}<x_{2}\right\}+\cdots
\end{aligned}
$$

이제 $U=F_{X}(X)=F_{X}\left(0 . X_{1} X_{2} \ldots\right)=0 . F_{1} F_{2} \ldots$라고 합시다. 만약 무한 시퀀스 $X^{\infty}$에 대한 분포가 원자(atom)를 갖지 않는다면, 위 보조정리에 의해 $U$는 $[0,1]$에서 균등 분포를 가지며, 따라서 $U$의 이진 전개에서 비트 $F_{1} F_{2} \ldots$는 베르누이($\frac{1}{2}$) 분포를 따릅니다 (즉, 독립적이며 $\{0,1\}$에서 균등 분포를 갖습니다). 이 비트들은 따라서 압축 불가능하며, 시퀀스 $0 . X_{1} X_{2} \ldots$의 압축된 표현을 형성합니다. 베르누이 또는 마르코프 모델의 경우, 다음 예시에서 보여주듯이 누적 분포 함수를 쉽게 계산할 수 있습니다.

예시 13.3.1 $X_{1}, X_{2}, \ldots, X_{n}$이 베르누이 $(p)$ 분포를 따른다고 합시다. 그러면 시퀀스 $x^{n}=110101$은 다음과 같이 매핑됩니다.
<!-- Page 464 -->
$$
\begin{aligned}
F\left(x^{n}\right)= & \operatorname{Pr}\left(X_{1}<1\right)+\operatorname{Pr}\left(X_{1}=1, X_{2}<1\right) \\
& +\operatorname{Pr}\left(X_{1}=1, X_{2}=1, X_{3}<0\right) \\
& +\operatorname{Pr}\left(X_{1}=1, X_{2}=1, X_{3}=0, X_{4}<1\right) \\
& +\operatorname{Pr}\left(X_{1}=1, X_{2}=1, X_{3}=0, X_{4}=1, X_{5}<0\right) \\
& +\operatorname{Pr}\left(X_{1}=1, X_{2}=1, X_{3}=0, X_{4}=1, X_{5}=0, X_{6}<1\right) \\
= & q+p q+p^{2} \cdot 0+p^{2} q \cdot q+p^{2} q p \cdot 0+p^{2} q p q q \\
= & q+p q+p^{2} q^{2}+p^{3} q^{3}
\end{aligned}
$$

각 항은 이전 항으로부터 쉽게 계산될 수 있음에 유의하십시오. 일반적으로 임의의 이진 프로세스 $\left\{X_{i}\right\}$에 대해,

$$
F\left(x^{n}\right)=\sum_{k=1}^{n} p\left(x^{k-1} 0\right) x_{k}
$$

따라서 확률 변환은 무한 소스 시퀀스에서 압축 불가능한 무한 이진 시퀀스로의 가역 매핑을 형성합니다. 이제 유한 시퀀스에 대한 압축을 고려합니다. $X_{1}, X_{2}, \ldots, X_{n}$을 길이가 $n$인 이진 확률 변수 시퀀스라고 하고, $x_{1}, x_{2}, \ldots, x_{n}$을 특정 결과라고 합시다. 이 시퀀스를 구간 $\left[0 . x_{1} x_{2} \ldots x_{n} 000 \ldots\right.$, $\left.0 . x_{1} x_{2} \ldots x_{n} 1111 \ldots\right)$ 또는 동등하게 $\left[0 . x_{1} x_{2} \ldots x_{n}, 0 . x_{1} x_{2} \ldots x_{n}+\right.$ $\left.\left(\frac{1}{2}\right)^{n}\right)$으로 나타내는 것으로 취급할 수 있습니다. 이것은 $0 . x_{1} x_{2} \cdots x_{n}$으로 시작하는 무한 시퀀스의 집합입니다. 확률 변환 하에서 이 구간은 다른 구간 $\left[F_{Y}\left(0 . x_{1} x_{2} \cdots x_{n}\right), F_{Y}\left(0 . x_{1} x_{2} \cdots x_{n}+\left(\frac{1}{2}\right)^{n}\right)\right)$으로 매핑되며, 이 구간의 길이는 $0 . x_{1} x_{2} \cdots x_{n}$으로 시작하는 모든 무한 시퀀스의 확률 합인 $P_{X}\left(x_{1}, x_{2}, \ldots, x_{n}\right)$과 같습니다. 확률 역변환 하에서 이 구간 내의 임의의 실수 $u$는 $x_{1}, x_{2}, \ldots, x_{n}$으로 시작하는 시퀀스로 매핑되므로, $u$와 $n$이 주어지면 $x_{1}, x_{2}, \ldots, x_{n}$을 재구성할 수 있습니다. 앞에서 설명한 Shannon-Fano-Elias 코딩 방식은 $\log \frac{1}{p\left(x_{1}, x_{2}, \ldots, x_{n}\right)}+2$ 비트 길이의 접두사 없는 코드를 구성할 수 있게 하므로, 이 길이로 시퀀스 $x_{1}, x_{2}, \ldots, x_{n}$을 인코딩하는 것이 가능합니다. $\log \frac{1}{p\left(x_{1}, \ldots, x_{n}\right)}$이 $x^{n}$에 대한 이상적인 코드워드 길이임을 유의하십시오.

위에서 설명한 누적 분포 함수를 사용하여 시퀀스를 인코딩하는 과정은 임의의 정확도를 계산에 가정합니다. 그러나 실제로는 모든 숫자를 유한 정밀도로 구현해야 하며, 그러한 구현을 설명합니다. 핵심은 다음을 고려하는 것입니다.
<!-- Page 465 -->
누적 분포 함수에 대한 무한 정밀도 점이 아니라 단위 구간 내의 구간입니다. 유한 길이의 기호 시퀀스는 단위 구간의 하위 구간에 해당한다고 말할 수 있습니다. 산술 코딩 알고리즘의 목표는 확률 변수 시퀀스를 $[0,1]$ 내의 하위 구간으로 표현하는 것입니다. 알고리즘이 더 많은 입력 기호를 관찰함에 따라 입력 시퀀스에 해당하는 하위 구간의 길이가 감소합니다. 구간의 상한과 하한이 가까워짐에 따라 처음 몇 비트에서 일치하기 시작합니다. 이것이 출력 시퀀스의 처음 몇 비트가 될 것입니다. 구간의 두 끝이 일치하는 즉시 해당 비트를 출력할 수 있습니다. 따라서 이러한 비트를 계산에서 제외하고 나머지 구간을 효과적으로 스케일링하여 전체 계산을 유한 정밀도로 수행할 수 있습니다. 여기서는 자세히 설명하지 않겠습니다. Bell et al. [41]에 알고리즘 및 성능 고려 사항에 대한 매우 좋은 설명이 있습니다.

예제 13.3.2 (삼진 입력 알파벳에 대한 산술 코딩) 확률이 각각 0.4, 0.4, 0.2인 삼진 알파벳 $\{A, B, C\}$를 가진 확률 변수 $X$를 고려하십시오. 인코딩할 시퀀스는 ACAA입니다. 따라서 $F_{l}(\cdot)=(0,0.4,0.8)$이고 $F_{h}(\cdot)=(0.4,0.8,1.0)$입니다. 처음에는 입력 시퀀스가 비어 있고 해당 구간은 $[0,1)$입니다. 첫 번째 입력 기호 후의 누적 분포 함수는 그림 13.2에 나와 있습니다. 첫 번째 기호 A 없이 스케일링하는 알고리즘의 구간은 다음과 같이 쉽게 계산할 수 있습니다.

그림 13.2. 첫 번째 기호 후의 누적 분포 함수.
<!-- Page 466 -->

그림 13.3. 두 번째 심볼 이후의 누적 분포 함수.
$[0,0.4)$; 두 번째 심볼 C 이후에는 $[0.32,0.4)$ (그림 13.3); 세 번째 심볼 A 이후에는 $[0.32,0.352)$; 네 번째 심볼 A 이후에는 $[0.32,0.3328)$ 입니다. 이 시퀀스의 확률은 0.0128이므로, Shannon-Fano-Elias 코딩을 사용하여 구간 시퀀스의 중간값 (0.3264, 즉 0.010100111 이진수)을 인코딩하기 위해 $\log (1 / 0.0128)+2$ (즉, 9 비트)를 사용할 것입니다.

요약하자면, 산술 코딩 절차는 임의의 길이 $n$과 확률 질량 함수 $q\left(x_{1} x_{2} \cdots x_{n}\right)$가 주어졌을 때, 시퀀스 $x_{1} x_{2} \cdots x_{n}$을 $\log \frac{1}{q\left(x_{1} x_{2} \cdots x_{n}\right)}+2$ 비트의 코드로 인코딩할 수 있게 합니다. 만약 소스가 i.i.d.이고 가정된 분포 $q$가 데이터의 실제 분포 $p$와 같다면, 이 절차는 블록의 평균 길이가 엔트로피보다 2 비트 이내가 되도록 달성합니다. 이것이 고정 블록 길이에 대해 반드시 최적은 아니지만 (분포에 대해 설계된 Huffman 코드는 더 낮은 평균 코드워드 길이를 가질 수 있습니다), 이 절차는 점진적이며 임의의 블록 길이에 사용될 수 있습니다.

# 13.4 렘펠-지브 코딩

섹션 13.3에서는 산술 코딩의 기본 아이디어와 알려지지 않은 분포로부터 시퀀스를 코딩하기 위한 최악의 경우 중복성에 대한 일부 결과를 논의했습니다. 이제 우리는 보편적으로 최적인 소스 코딩 기법의 인기 있는 부류에 대해 논의할 것입니다 (점근적

<!-- Page 467 -->
(stationary ergodic source에 대해 압축률이 소스의 엔트로피율에 접근하며) 구현이 간단합니다. 이 알고리즘 계열은 이 계열의 기초가 되는 두 가지 기본 알고리즘을 설명하는 두 개의 중요한 논문 [603, 604]의 저자인 Lempel과 Ziv의 이름을 따서 명명되었습니다. 이 알고리즘들은 적응형 사전 압축 알고리즘으로 설명될 수도 있습니다.

압축을 위해 사전을 사용하는 개념은 전신 발명으로 거슬러 올라갑니다. 당시에는 사용된 글자 수에 따라 요금이 부과되었으며, 많은 대기업들이 자주 사용되는 구문에 대한 코드북을 제작하여 전신 통신에 코드어를 사용했습니다. 또 다른 예는 인도에서 인기 있는 축하 전보의 개념입니다. "25:Merry Christmas" 및 "26:May Heaven's choicest blessings be showered on the newly married couple."와 같은 표준 축하 문구가 있습니다. 축하를 보내려는 사람은 숫자만 지정하면 되며, 이 숫자는 대상에서 실제 축하를 생성하는 데 사용됩니다.

적응형 사전 기반 방식의 아이디어는 Ziv와 Lempel이 1977년과 1978년에 논문을 쓸 때까지 탐구되지 않았습니다. 두 논문은 알고리즘의 두 가지 구별되는 버전을 설명합니다. 이 버전들을 LZ77 또는 슬라이딩 윈도우 Lempel-Ziv와 LZ78 또는 트리 구조 Lempel-Ziv라고 부릅니다. (각각 LZ1 및 LZ2라고도 합니다.)

먼저 두 경우의 기본 알고리즘을 설명하고 몇 가지 간단한 변형을 설명합니다. 나중에 최적성을 증명하고 실용적인 문제로 마무리합니다. Lempel-Ziv 알고리즘의 핵심 아이디어는 문자열을 구문으로 파싱하고 과거에 동일한 문자열이 발생한 위치를 가리키는 포인터로 구문을 대체하는 것입니다. 알고리즘 간의 차이점은 알고리즘이 허용하는 가능한 일치 위치(및 일치 길이) 집합의 차이에 기반합니다.

# 13.4.1 슬라이딩 윈도우 Lempel-Ziv 알고리즘

1977년 논문에서 설명된 알고리즘은 과거 기호의 윈도우 내에서 가장 긴 일치를 찾아 문자열을 인코딩하며, 윈도우 내 일치 위치와 일치 길이에 대한 포인터로 문자열을 나타냅니다. 이 기본 알고리즘에는 많은 변형이 있으며, Storer와 Szymanski [507]의 것을 설명합니다.

유한 알파벳에서 압축할 문자열 $x_{1}, x_{2}, \ldots$가 있다고 가정합니다. 문자열 $x_{1} x_{2} \cdots x_{n}$의 파싱 $S$는 문자열을 쉼표로 구분된 구문으로 나누는 것입니다. 윈도우의 길이를 $W$라고 할 때, 알고리즘은 다음과 같이 설명될 수 있습니다. 시간 $i-1$까지 문자열을 압축했다고 가정합니다. 그런 다음 다음 구문을 찾기 위해, $i-1-W \leq j \leq i-1$인 어떤 $j$에 대해 가장 큰 $k$를 찾습니다.
<!-- Page 468 -->
문자열의 길이가 $k$이고 $x_{j}$에서 시작하는 문자열은 $x_{i}$에서 시작하는 문자열과 동일합니다 (즉, 모든 $0 \leq l<k$에 대해 $x_{j+l}=x_{i+l}$ ). 그런 다음 다음 구문은 길이가 $k$ (즉, $x_{i} \ldots x_{i+k-1}$ )이며 $(P, L)$ 쌍으로 표현됩니다. 여기서 $P$는 일치의 시작 위치이고 $L$은 일치의 길이입니다. 창에서 일치를 찾지 못하면 다음 문자가 압축되지 않은 상태로 전송됩니다. 이 두 경우를 구별하기 위해 플래그 비트가 필요하므로 구문은 두 가지 유형입니다: $(F, P, L)$ 또는 $(F, C)$, 여기서 $C$는 압축되지 않은 문자입니다.

(포인터, 길이) 쌍의 대상이 창을 넘어 확장되어 새 구문과 겹칠 수 있다는 점에 유의하십시오. 이론적으로 이 일치는 임의로 길 수 있습니다. 그러나 실제로는 최대 구문 길이가 특정 매개변수보다 작은 것으로 제한됩니다.

예를 들어, $W=4$이고 문자열이 ABBABBABBBAABABA이며 초기 창이 비어 있으면 문자열은 다음과 같이 구문 분석됩니다: $\mathrm{A}, \mathrm{B}, \mathrm{B}, \mathrm{ABBABB}, \mathrm{BA}, \mathrm{A}, \mathrm{BA}, \mathrm{BA}$. 이는 "포인터" 시퀀스로 표현됩니다: $(0, \mathrm{~A}),(0, \mathrm{~B}),(1,1,1),(1,3,6),(1,4,2),(1,1,1),(1,3,2),(1,2,2)$. 여기서 플래그 비트는 일치가 없으면 0이고 일치가 있으면 1이며, 일치 위치는 창 끝에서 뒤로 측정됩니다. [예시에서는 창 내의 모든 일치를 $(P, L)$ 쌍을 사용하여 표현했지만, 짧은 일치를 압축되지 않은 문자로 표현하는 것이 더 효율적일 수 있습니다. 자세한 내용은 문제 13.8을 참조하십시오.]

이 알고리즘은 창 내의 문자열의 모든 부분 문자열과 모든 단일 문자로 구성된 사전(dictionary)을 사용하는 것으로 볼 수 있습니다. 알고리즘은 해당 일치에 대한 포인터를 보냅니다. 나중에 LZ77의 간단한 변형이 점근적으로 최적임을 보여줄 것입니다. gzip 및 pkzip과 같은 대부분의 실제 LZ77 구현도 이 버전의 LZ77을 기반으로 합니다.

# 13.4.2 트리 구조 Lempel-Ziv 알고리즘

1978년 논문에서 Ziv와 Lempel은 문자열을 구문으로 구문 분석하는 알고리즘을 설명했습니다. 여기서 각 구문은 이전에 보지 못한 가장 짧은 구문입니다. 이 알고리즘은 지금까지 본 구문의 형태로 사전을 구축하는 것으로 볼 수 있습니다. 이 알고리즘은 구현하기가 특히 간단하며 속도와 효율성으로 인해 컴퓨터에서 파일 압축을 위한 초기 표준 알고리즘 중 하나로 인기를 얻었습니다. 또한 고속 모뎀에서 데이터 압축에도 사용됩니다.

소스 시퀀스는 이전에 나타나지 않은 문자열로 순차적으로 구문 분석됩니다. 예를 들어, 문자열이 ABBABBABBBAABABAA ...이면 다음과 같이 구문 분석됩니다: $\mathrm{A}, \mathrm{B}, \mathrm{BA}, \mathrm{BB}, \mathrm{AB}, \mathrm{BBA}, \mathrm{ABA}, \mathrm{BAA} \ldots$. 각 쉼표 뒤에 입력 시퀀스를 따라 이전에 표시되지 않은 가장 짧은 문자열이 나올 때까지 살펴봅니다. 이것이 가장 짧은 문자열이므로,
<!-- Page 469 -->
모든 접두사는 이전에 발생해야 합니다. (따라서 이러한 구문의 트리를 구축할 수 있습니다.) 특히, 이 문자열의 마지막 비트를 제외한 모든 비트로 구성된 문자열은 이전에 발생해야 합니다. 이 구문은 접두사의 위치와 마지막 기호의 값을 제공하여 코딩합니다. 따라서 위의 문자열은 $(0, \mathrm{~A}),(0, \mathrm{~B}),(2, \mathrm{~A}),(2, \mathrm{~B}),(1, \mathrm{~B}),(4, \mathrm{~A}),(5, \mathrm{~A})$, $(3, \mathrm{~A}), \ldots$ 로 표현됩니다.

각 구문에서 압축되지 않은 문자를 보내는 것은 효율성 손실을 초래합니다. 현재 구문의 마지막 문자를 다음 구문의 일부로 간주함으로써 이를 해결할 수 있습니다. Welch [554]에 의한 이 변형은 Unix의 compress, 모뎀의 압축, GIF 형식의 이미지 파일과 같은 대부분의 실제 LZ78 구현의 기초입니다.

# 13.5 LEMPEL-ZIV 알고리즘의 최적성

### 13.5.1 슬라이딩 윈도우 Lempel-Ziv 알고리즘

Ziv와 Lempel [603]의 원본 논문에서 저자들은 기본 LZ77 알고리즘을 설명하고 해당 문자열에 작용하는 모든 유한 상태 압축기만큼 문자열을 압축한다고 증명했습니다. 그러나 이 알고리즘이 점근적 최적성(즉, 압축률이 에르고딕 소스의 엔트로피로 수렴하는 것)을 달성한다고 증명하지는 않았습니다. 이 결과는 Wyner와 Ziv [591]에 의해 증명되었습니다.

증명은 Kac의 간단한 보조 정리에 의존합니다. 특정 기호를 보기 위해 기다려야 하는 평균 시간은 기호 확률의 역수입니다. 따라서 우리는 윈도우 내에서 높은 확률의 문자열을 보고 이러한 문자열을 효율적으로 인코딩할 가능성이 높습니다. 윈도우 내에서 찾지 못하는 문자열은 확률이 낮으므로 점근적으로 달성된 압축에 영향을 미치지 않습니다.

실제 LZ77 버전의 최적성을 증명하는 대신, 실용적이지는 않지만 일부 기본 아이디어를 포착하는 다른 버전의 알고리즘에 대한 더 간단한 증명을 제시할 것입니다. 이 알고리즘은 송신자와 수신자 모두 문자열의 무한한 과거에 접근할 수 있다고 가정하고, 문자열의 길이를 $n$으로 하여 과거에 발생한 마지막 시간을 가리킴으로써 표현합니다.

우리는 $-\infty$에서 $\infty$까지의 시간에 대해 정의된 정상적이고 에르고딕적인 프로세스를 가정하고, 인코더와 디코더 모두 $\ldots, X_{-2}, X_{-1}$, 즉 시퀀스의 무한한 과거에 접근할 수 있다고 가정합니다. 그러면 $X_{0}, X_{1}, \ldots, X_{n-1}$ (길이 $n$의 블록)을 인코딩하기 위해 과거에 이 $n$개의 기호를 마지막으로 본 시간을 찾습니다. 다음을 정의합니다.

$$
\begin{aligned}
& R_{n}\left(X_{0}, X_{1}, \ldots, X_{n-1}\right)= \\
& \quad \max \left\{j<0:\left(X_{-j}, X_{-j+1} \ldots X_{-j+n-1}\right)=\left(X_{0}, \ldots, X_{n-1}\right)\right\}
\end{aligned}
$$
<!-- Page 470 -->
그러면 $X_{0}, \ldots, X_{n-1}$을 표현하기 위해 수신자에게 $R_{n}$만 보내면 되며, 수신자는 과거 $R_{n}$ 비트만큼 되돌아가 $X_{0}, \ldots, X_{n-1}$을 복구할 수 있습니다. 따라서 인코딩 비용은 $R_{n}$을 표현하는 비용입니다. 이 비용이 대략 $\log R_{n}$이고 점근적으로 $\frac{1}{n} E \log R_{n}$ $\rightarrow H(\mathcal{X})$임을 보임으로써 이 알고리즘의 점근적 최적성을 증명할 것입니다.

다음 보조정리가 필요합니다.
보조정리 13.5.1 정수 $k$에 대한 부호어 길이가 $\log k+2 \log \log k+O(1)$인 접두사 없는 코드가 존재합니다.

증명: $k \leq m$임을 안다면 $k$를 $\log m$ 비트로 인코딩할 수 있습니다. 그러나 $k$에 대한 상한이 없으므로 수신자에게 $k$의 인코딩 길이를 알려주어야 합니다 (즉, $\log k$를 지정해야 합니다). 정수 $k$에 대해 다음과 같은 인코딩을 고려하십시오: 먼저 $\lceil\log k\rceil$를 단항식으로 표현한 다음 $k$의 이진 표현을 따릅니다:

$$
C_{1}(k)=\underbrace{00 \cdots 0}_{\lceil\log k\rceil} 1 \underbrace{x x \cdots x}_{k \text { in binary }} .
$$

이 표현의 길이가 $2\lceil\log k\rceil+1 \leq$ $2 \log k+3$임을 쉽게 알 수 있습니다. 이는 $\log k$를 보내기 위해 매우 비효율적인 단항식 코드를 사용하기 때문에 우리가 찾는 길이보다 깁니다. 그러나 $C_{1}$을 사용하여 $\log k$를 표현한다면, 이 표현의 길이가 $\log k+2 \log \log k+4$보다 작다는 것을 쉽게 알 수 있으며, 이는 보조정리를 증명합니다. 유사한 방법이 정리 14.2.3 뒤의 논의에서 제시됩니다.

LZ77의 최적성 증명의 기초가 되는 핵심 결과는 확률적 과정이 정상적이고 에르고딕일 때 기호의 확률과 평균 재발 시간의 관계를 설명하는 Kac의 보조정리입니다. 예를 들어, $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. 과정이라면, $X_{1}=a$라는 사실이 주어졌을 때 기호 $a$를 다시 볼 때까지의 예상 대기 시간은 얼마인지 묻습니다. 이 경우 대기 시간은 매개변수 $p=p\left(X_{0}=a\right)$를 갖는 기하 분포를 따르며, 따라서 예상 대기 시간은 $1 / p\left(X_{0}=a\right)$입니다. 다소 놀라운 결과는 과정이 i.i.d.가 아니더라도 정상적이고 에르고딕인 경우에도 동일하다는 것입니다. 이에 대한 간단한 직관적인 이유는 길이가 $n$인 긴 표본에서 $a$를 약 $n p(a)$번 볼 것으로 예상되며, 이 발생 사이의 평균 거리는 $n /(n p(a))$ (즉, $1 / p(a)$)이기 때문입니다.

보조정리 13.5.2 (Kac) $\ldots, U_{2}, U_{1}, U_{0}, U_{1}, \ldots$가 가산 알파벳에 대한 정상적이고 에르고딕인 과정이라고 가정합니다. $p(u)>0$인 모든 $u$에 대해
<!-- Page 471 -->
그리고 $i=1,2, \ldots$에 대해,

$$
Q_{u}(i)=\operatorname{Pr}\left\{U_{-i}=u ; U_{j} \neq u \text { for }-i<j<0 \mid U_{0}=u\right\}
$$

[즉, $Q_{u}(i)$는 $U_{0}=u$가 주어졌을 때, 기호 $u$의 가장 최근 이전 발생이 $i$일 조건부 확률입니다]. 그러면

$$
E\left(R_{1}(U) \mid X_{0}=u\right)=\sum_{i} i Q_{u}(i)=\frac{1}{p(u)}
$$

따라서, 0에서부터 거꾸로 보았을 때 기호 $u$를 다시 볼 때까지의 조건부 기대 대기 시간은 $1 / p(u)$입니다.

기대 재발 시간(expected recurrence time)이

$$
E R_{1}(U)=\sum p(u) \frac{1}{p(u)}=m
$$

이며, 여기서 $m$은 알파벳 크기라는 재미있는 사실에 주목하십시오.
증명: $U_{0}=u$라고 합시다. $j=1,2, \ldots$ 및 $k=0,1,2, \ldots$에 대한 사건을 정의합니다:

$$
A_{j k}=\left\{U_{-j}=u, U_{l} \neq u,-j<l<k, U_{k}=u\right\}
$$

사건 $A_{j k}$는 0 이전 마지막으로 프로세스가 $u$와 같았던 시점이 $-j$이고, 0 이후 처음으로 프로세스가 $u$와 같아지는 시점이 $k$인 경우에 해당합니다. 이 사건들은 서로 배타적이며, 에르고딕성(ergodicity)에 의해 $\operatorname{Pr}\left\{\cup_{j, k} A_{j k}\right\}=1$입니다. 따라서,

$$
\begin{aligned}
1 & =\operatorname{Pr}\left\{\cup_{j, k} A_{j k}\right\} \\
& \stackrel{(a)}{=} \sum_{j=1}^{\infty} \sum_{k=0}^{\infty} \operatorname{Pr}\left\{A_{j k}\right\} \\
& =\sum_{j=1}^{\infty} \sum_{k=0}^{\infty} \operatorname{Pr}\left(U_{k}=u\right) \operatorname{Pr}\left\{U_{-j}=u, U_{l} \neq u,-j<l<k \mid U_{k}=u\right\} \\
& \stackrel{(b)}{=} \sum_{j=1}^{\infty} \sum_{k=0}^{\infty} \operatorname{Pr}\left(U_{k}=u\right) Q_{u}(j+k)
\end{aligned}
$$
<!-- Page 472 -->
$$
\begin{aligned}
& \stackrel{(\mathrm{C})}{=} \sum_{j=1}^{\infty} \sum_{k=0}^{\infty} \operatorname{Pr}\left(U_{0}=u\right) Q_{u}(j+k) \\
& =\operatorname{Pr}\left(U_{0}=u\right) \sum_{j=1}^{\infty} \sum_{k=0}^{\infty} Q_{u}(j+k) \\
& \stackrel{(\mathrm{d})}{=} \operatorname{Pr}\left(U_{0}=u\right) \sum_{i=1}^{\infty} i Q_{u}(i)
\end{aligned}
$$

여기서 (a)는 $A_{j k}$가 서로소라는 사실로부터, (b)는 $Q_{u}(\cdot)$의 정의로부터, (c)는 정상성으로부터, 그리고 (d)는 합에서 $j+k=i$를 만족하는 $(j, k)$ 쌍이 $i$개 존재한다는 사실로부터 도출됩니다. Kac의 보조정리는 이 방정식으로부터 직접적으로 도출됩니다.

계 Corollary
$\ldots, X_{-1}, X_{0}, X_{1}, \ldots$를 정상 에르고딕 과정(stationary ergodic process)이라고 하고, (13.60)에 정의된 바와 같이 뒤를 돌아보는 복귀 시간(recurrence time)을 $R_{n}\left(X_{0}, \ldots, X_{n-1}\right)$이라고 합시다. 그러면

$$
E\left[R_{n}\left(X_{0}, \ldots, X_{n-1}\right) \mid\left(X_{0}, \ldots, X_{n-1}\right)=x_{0}^{n-1}\right]=\frac{1}{p\left(x_{0}^{n-1}\right)}
$$

증명: 새로운 과정 $U_{i}=\left(X_{i}, X_{i+1}, \ldots, X_{i+n-1}\right)$를 정의합니다. $U$ 과정 또한 정상 에르고딕 과정이며, 따라서 Kac의 보조정리에 의해 $U_{0}=u$ 조건 하에서의 $U$의 평균 복귀 시간은 $1 / p(u)$입니다. 이를 $X$ 과정으로 변환하면 계 Corollary가 증명됩니다.

이제 우리는 복귀 시간을 이용한 Lempel-Ziv의 단순 버전의 압축률이 엔트로피에 접근함을 보여주는 주요 결과를 증명할 준비가 되었습니다. 이 알고리즘은 $X_{0}^{n-1}$을 $R_{n}\left(X_{0}^{n-1}\right)$을 기술함으로써 설명하며, 이는 보조정리 13.5.1에 의해 $\log R_{n}+2 \log \log R_{n}+4$ 비트로 수행될 수 있습니다. 이제 다음 정리를 증명합니다.

정리 13.5.1 위에서 설명한 단순 알고리즘에서 $X_{0}^{n-1}$의 설명 길이인 $L_{n}\left(X_{0}^{n-1}\right)=\log R_{n}+2 \log \log R_{n}+O(1)$에 대하여,

$$
\frac{1}{n} E L_{n}\left(X_{0}^{n-1}\right) \rightarrow H(\mathcal{X})
$$

$n \rightarrow \infty$일 때, 여기서 $H(\mathcal{X})$는 과정 $\left\{X_{i}\right\}$의 엔트로피율(entropy rate)입니다.
<!-- Page 473 -->
증명: $E L_{n}$에 대한 상한과 하한을 증명할 것입니다. 하한은 표준 소스 코딩 결과(즉, 모든 접두사 없는 코드에 대해 $E L_{n} \geq n H$)에서 직접적으로 나옵니다. 상한을 증명하기 위해 먼저 다음을 보여줍니다.

$$
\overline{\lim } \frac{1}{n} E \log R_{n} \leq H
$$

그리고 나중에 $L_{n}$에 대한 식의 다른 항들을 제한합니다. $E \log R_{n}$에 대한 상한을 증명하기 위해, $X_{0}^{n-1}$의 값에 조건부로 기대값을 확장한 다음 젠센의 부등식을 적용합니다. 따라서,

$$
\begin{aligned}
\frac{1}{n} E \log R_{n} & =\frac{1}{n} \sum_{x_{0}^{n-1}} p\left(x_{0}^{n-1}\right) E\left[\log R_{n}\left(X_{0}^{n-1}\right) \mid X_{0}^{n-1}=x_{0}^{n-1}\right] \\
& \leq \frac{1}{n} \sum_{x_{0}^{n-1}} p\left(x_{0}^{n-1}\right) \log E\left[R_{n}\left(X_{0}^{n-1}\right) \mid X_{0}^{n-1}=x_{0}^{n-1}\right] \\
& =\frac{1}{n} \sum_{x_{0}^{n-1}} p\left(x_{0}^{n-1}\right) \log \frac{1}{p\left(x_{0}^{n-1}\right)} \\
& =\frac{1}{n} H\left(X_{0}^{n-1}\right) \\
& \searrow H(\mathcal{X})
\end{aligned}
$$

$L_{n}$에 대한 식의 두 번째 항은 $\log \log R_{n}$이며, 다음을 보여주기를 원합니다.

$$
\frac{1}{n} E\left[\log \log R_{n}\left(X_{0}^{n-1}\right)\right] \rightarrow 0
$$

다시 젠센의 부등식을 사용합니다.

$$
\begin{aligned}
\frac{1}{n} E \log \log R_{n} & \leq \frac{1}{n} \log E\left[\log R_{n}\left(X_{0}^{n-1}\right)\right] \\
& \leq \frac{1}{n} \log H\left(X_{0}^{n-1}\right)
\end{aligned}
$$

여기서 마지막 부등식은 (13.79)에서 나옵니다. 임의의 $\epsilon>0$에 대해, 충분히 큰 $n$에 대해 $H\left(X_{0}^{n-1}\right)<n(H+\epsilon)$이므로, $\frac{1}{n} \log \log R_{n}<\frac{1}{n}$ $\log n+\frac{1}{n} \log (H+\epsilon) \rightarrow 0$입니다. 이것으로 정리의 증명이 완료됩니다.

따라서, 문자열을 과거에 마지막으로 본 시점을 인코딩하여 표현하는 압축 방식은 점근적으로 최적입니다. 물론 이 방식은 송신자와 수신자 모두가

<!-- Page 474 -->
무한한 과거의 시퀀스에 접근할 수 있습니다. 더 긴 문자열의 경우, 일치를 찾기 위해 과거로 더 멀리, 더 멀리 거슬러 올라가야 합니다. 예를 들어, 엔트로피율이 $\frac{1}{2}$이고 문자열 길이가 200비트라면, 일치를 찾기 위해 평균 $2^{100} \approx 10^{30}$비트만큼 과거로 거슬러 올라가야 합니다. 이는 실현 가능하지 않지만, 이 알고리즘은 과거를 일치시키는 것이 점근적으로 최적이라는 기본적인 아이디어를 보여줍니다. 유한한 창을 가진 실제 LZ77 버전의 최적성에 대한 증명은 유사한 아이디어에 기반합니다. 여기서는 세부 사항을 제시하지 않고, 원본 증명은 [591]을 참조하시기 바랍니다.

# 13.5.2 트리 구조 Lempel-Ziv 압축의 최적성

이제 Lempel-Ziv의 트리 구조 버전을 고려해 보겠습니다. 이 버전에서는 입력 시퀀스를 구문으로 분할하며, 각 구문은 지금까지 보지 못한 가장 짧은 문자열입니다. 이 알고리즘의 최적성에 대한 증명은 LZ77의 증명과는 매우 다른 성격을 가집니다. 증명의 핵심은 구문 수가 모두 다르다면 너무 많을 수 없다는 것을 보여주는 계산 논증이며, 임의의 기호 시퀀스의 확률은 시퀀스의 구문 분할에서 고유한 구문 수의 함수로 제한될 수 있습니다.

13.4.2절에 설명된 알고리즘은 문자열에 대한 두 번의 통과를 필요로 합니다. 첫 번째 통과에서는 문자열을 구문으로 분할하고 $c(n)$, 즉 분할된 문자열의 구문 수를 계산합니다. 그런 다음 이를 사용하여 알고리즘의 포인터에 할당할 비트 수 $[\log c(n)]$을 결정합니다. 두 번째 통과에서는 포인터를 계산하고 위에서 설명한 대로 코딩된 문자열을 생성합니다. 이 알고리즘은 단 한 번의 통과로 문자열을 처리하고 초기 포인터에 더 적은 비트를 사용하도록 수정할 수 있습니다. 이러한 수정은 알고리즘의 점근적 효율성에 영향을 미치지 않습니다. 일부 구현 세부 사항은 Welch [554]와 Bell 등 [41]에 의해 논의되었습니다.

슬라이딩 윈도우 버전의 Lempel-Ziv와 마찬가지로 이 알고리즘이 알 수 없는 에르고딕 소스에 대한 엔트로피율을 점근적으로 달성함을 보여줄 것입니다. 먼저 문자열의 구문 분할을 구문으로의 분해로 정의합니다.

정의 문자열 $x_{1} x_{2} \cdots x_{n}$의 구문 분할 $S$는 문자열을 쉼표로 구분된 구문으로 나누는 것입니다. 고유한 구문 분할은 두 개의 구문이 동일하지 않은 구문 분할입니다. 예를 들어, $0,111,1$은 01111의 고유한 구문 분할이지만, $0,11,11$은 고유하지 않은 구문 분할입니다.

위에 설명된 LZ78 알고리즘은 소스 시퀀스의 고유한 구문 분할을 제공합니다. 길이 $n$인 시퀀스의 LZ78 구문 분할에서 구문의 수를 $c(n)$으로 나타냅니다. 물론 $c(n)$은 시퀀스 $X^{n}$에 따라 달라집니다. 압축된 시퀀스(Lempel-Ziv 알고리즘 적용 후)
<!-- Page 475 -->
$c(n)$ 쌍의 숫자로 구성되며, 각 쌍은 구문의 이전 발생 위치에 대한 포인터와 구문의 마지막 비트로 구성됩니다. 각 포인터는 $\log c(n)$ 비트를 요구하므로 압축된 시퀀스의 총 길이는 $c(n)[\log c(n)+1]$ 비트입니다. 이제 우리는 $\frac{c(n)(\log c(n)+1)}{n} \rightarrow H(\mathcal{X})$가 정상적인 에르고딕 시퀀스 $X_{1}, X_{2}, \ldots, X_{n}$에 대해 성립함을 보일 것입니다. 우리의 증명은 Wyner와 Ziv [575]의 LZ78 코딩의 점근적 최적성에 대한 간단한 증명에 기반합니다.

증명의 세부 사항으로 넘어가기 전에 주요 아이디어의 개요를 제공합니다. 첫 번째 보조정리는 구별되는 구문 분석에서 구문의 수가 $n / \log n$보다 작다는 것을 보여줍니다. 증명의 주요 논증은 충분히 많은 구별되는 짧은 구문이 없다는 사실에 기반합니다. 이 경계는 LZ78 구문 분석뿐만 아니라 구문의 모든 구별되는 구문 분석에 대해 유효합니다.

두 번째 핵심 아이디어는 구별되는 구문의 수를 기반으로 시퀀스의 확률에 대한 경계입니다. 이를 설명하기 위해 네 가지 가능한 값 $\{A, B, C, D\}$를 각각 확률 $p_{A}, p_{B}, p_{C}, p_{D}$로 갖는 독립적이고 동일하게 분포된 확률 변수 $X_{1}, X_{2}, X_{3}, X_{4}$의 i.i.d. 시퀀스를 고려해 보겠습니다. 이제 시퀀스 $P(D, A, B, C)=p_{D} p_{A} p_{B} p_{C}$의 확률을 고려해 보겠습니다. $p_{A}+p_{B}+p_{C}+p_{D}=1$이므로, 확률이 같을 때 곱 $p_{D} p_{A} p_{B} p_{C}$이 최대화됩니다 (즉, 네 개의 구별되는 기호 시퀀스의 확률의 최대값은 $1 / 256$입니다). 반면에, 시퀀스 $A, B, A, B$를 고려하면, 이 시퀀스의 확률은 $p_{A}=p_{B}=\frac{1}{2}, p_{C}=p_{D}=0$일 때 최대화되며, $A, B, A, B$에 대한 최대 확률은 $\frac{1}{16}$입니다. $A, A, A, A$ 형태의 시퀀스는 확률 1을 가질 수 있습니다. 이 모든 예는 기본적인 점을 보여줍니다. 즉, 많은 수의 구별되는 기호(또는 구문)를 가진 시퀀스는 높은 확률을 가질 수 없습니다. Ziv의 부등식(보조정리 13.5.5)은 이 아이디어를 마르코프 사례로 확장한 것으로, 여기서 구별되는 기호는 소스 시퀀스의 구별되는 구문 분석의 구문입니다.

구문 분석 후 시퀀스의 설명 길이가 $c \log c$로 증가하므로, 매우 적은 수의 구별되는 구문을 가진 시퀀스는 효율적으로 압축될 수 있으며 높은 확률을 가질 수 있는 문자열에 해당합니다. 반면에, 많은 수의 구별되는 구문을 가진 문자열은 잘 압축되지 않습니다. 그러나 Ziv의 부등식에 따라 이러한 시퀀스의 확률이 너무 크지는 않을 수 있습니다. 따라서 Ziv의 부등식은 시퀀스의 확률의 로그와 구문 분석에서의 구문 수를 연결할 수 있게 해주며, 이는 트리 구조의 Lempel-Ziv 알고리즘이 점근적으로 최적임을 보여주는 데 최종적으로 사용됩니다.

먼저 정리 증명에 필요한 몇 가지 보조정리를 증명합니다. 첫 번째는 길이 $n$의 이진 시퀀스의 구별되는 구문 분석에서 가능한 구문 수에 대한 경계입니다.
<!-- Page 476 -->
13.5.3 보조정리 (Lempel 및 Ziv [604]) 이진 시퀀스 $X_{1}, X_{2}, \ldots, X_{n}$의 고유 파싱에서의 구문(phrase) 수 $c(n)$은 다음을 만족합니다.

$$
c(n) \leq \frac{n}{\left(1-\epsilon_{n}\right) \log n}
$$

여기서 $\epsilon_{n}=\min \left\{1, \frac{\log (\log n)+4}{\log n}\right\}$이며, $n \rightarrow \infty$일 때 $\epsilon_{n} \rightarrow 0$입니다.
증명:

$$
n_{k}=\sum_{j=1}^{k} j 2^{j}=(k-1) 2^{k+1}+2
$$

를 길이가 $k$ 이하인 모든 고유 문자열의 길이 합이라고 합시다. 길이 $n$인 시퀀스의 고유 파싱에서의 구문 수 $c$는 모든 구문이 가능한 한 짧을 때 최대화됩니다. 만약 $n=n_{k}$이면, 이는 모든 구문의 길이가 $\leq k$일 때 발생하며, 따라서

$$
c\left(n_{k}\right) \leq \sum_{j=1}^{k} 2^{j}=2^{k+1}-2<2^{k+1} \leq \frac{n_{k}}{k-1}
$$

만약 $n_{k} \leq n<n_{k+1}$이면, $n=n_{k}+\Delta$로 쓸 수 있으며, 여기서 $\Delta<(k+1) 2^{k+1}$입니다. 그러면 가장 짧은 구문으로의 파싱은 모든 구문의 길이가 $\leq k$이고, $\Delta /(k+1)$개의 구문은 길이가 $k+1$입니다. 따라서,

$$
c(n) \leq \frac{n_{k}}{k-1}+\frac{\Delta}{k+1} \leq \frac{n_{k}+\Delta}{k-1}=\frac{n}{k-1}
$$

이제 주어진 $n$에 대한 $k$의 크기를 제한합니다. $n_{k} \leq n<n_{k+1}$이라고 합시다. 그러면

$$
n \geq n_{k}=(k-1) 2^{k+1}+2 \geq 2^{k}
$$

따라서

$$
k \leq \log n
$$

또한,

$$
n \leq n_{k+1}=k 2^{k+2}+2 \leq(k+2) 2^{k+2} \leq(\log n+2) 2^{k+2}
$$

(13.89)에 의해, 따라서

$$
k+2 \geq \log \frac{n}{\log n+2}
$$
<!-- Page 477 -->
모든 $n \geq 4$에 대해,

$$
\begin{aligned}
k-1 & \geq \log n-\log (\log n+2)-3 \\
& =\left(1-\frac{\log (\log n+2)+3}{\log n}\right) \log n \\
& \geq\left(1-\frac{\log (2 \log n)+3}{\log n}\right) \log n \\
& =\left(1-\frac{\log (\log n)+4}{\log n}\right) \log n \\
& =\left(1-\epsilon_{n}\right) \log n
\end{aligned}
$$

여기서 $\epsilon_{n}=\min \left\{1, \frac{\log (\log n)+4}{\log n}\right\}$입니다. (13.96)과 (13.87)을 결합하면 보조정리를 얻습니다.

주정리의 증명에서 최대 엔트로피에 대한 간단한 결과가 필요합니다.

보조정리 13.5.4 음이 아닌 정수값 확률변수 $Z$의 평균이 $\mu$일 때, 엔트로피 $H(Z)$는 다음과 같이 제한됩니다.

$$
H(Z) \leq(\mu+1) \log (\mu+1)-\mu \log \mu
$$

증명: 이 보조정리는 음이 아닌 정수값 확률변수의 엔트로피를 평균 제약 조건 하에서 최대화하는 기하 분포의 결과를 보여주는 정리 12.1.1의 결과로부터 직접적으로 도출됩니다.

확률 질량 함수 $P\left(x_{1}, x_{2}, \ldots, x_{n}\right)$를 갖는 이진 정상 에르고딕 과정 $\left\{X_{i}\right\}_{i=-\infty}^{\infty}$를 고려하십시오. (에르고딕 과정은 16.8절에서 더 자세히 논의됩니다.) 고정된 정수 $k$에 대해, $P$의 $k$차 마르코프 근사치를 다음과 같이 정의합니다.

$$
Q_{k}\left(x_{-(k-1)}, \ldots, x_{0}, x_{1}, \ldots, x_{n}\right) \triangleq P\left(x_{-(k-1)}^{0}\right) \prod_{j=1}^{n} P\left(x_{j} \mid x_{j-k}^{j-1}\right)
$$

여기서 $x_{i}^{j} \triangleq\left(x_{i}, x_{i+1}, \ldots, x_{j}\right), i \leq j$이며, 초기 상태 $x_{-(k-1)}^{0}$는 $Q_{k}$의 명세의 일부가 될 것입니다. $P\left(X_{n} \mid X_{n-k}^{n-1}\right)$ 자체가 에르고딕이므로

<!-- Page 478 -->
처리 과정에서 다음이 있습니다.

$$
\begin{aligned}
-\frac{1}{n} \log Q_{k}\left(X_{1}, X_{2}, \ldots, X_{n} \mid X_{-(k-1)}^{0}\right) & =-\frac{1}{n} \sum_{j=1}^{n} \log P\left(X_{j} \mid X_{j-k}^{j-1}\right) \\
& \rightarrow-E \log P\left(X_{j} \mid X_{j-k}^{j-1}\right) \\
& =H\left(X_{j} \mid X_{j-k}^{j-1}\right)
\end{aligned}
$$

모든 $k$에 대해 $k$차 Markov 근사의 entropy rate로 LZ78 코드의 속도를 제한할 것입니다. Markov 근사의 entropy rate $H\left(X_{j} \mid X_{j-k}^{j-1}\right)$는 $k \rightarrow \infty$일 때 프로세스의 entropy rate로 수렴하며, 이것이 결과를 증명할 것입니다.

$X_{-(k-1)}^{n}=x_{-(k-1)}^{n}$이라고 가정하고, $x_{1}^{n}$이 $c$개의 서로 다른 구문 $y_{1}, y_{2}, \ldots, y_{c}$으로 파싱된다고 가정합니다. $v_{i}$를 $i$번째 구문의 시작 인덱스라고 정의합니다 (즉, $y_{i}=x_{v_{i}}^{v_{i+1}-1}$). 각 $i=1,2, \ldots, c$에 대해 $s_{i}=x_{v_{i}-k}^{v_{i}-1}$을 정의합니다. 따라서 $s_{i}$는 $y_{i}$ 앞에 오는 $x$의 $k$ 비트입니다. 물론 $s_{1}=x_{-(k-1)}^{0}$입니다.

$c_{l s}$를 길이가 $l$이고 이전 상태가 $s_{i}=s$인 구문의 수라고 정의합니다. 여기서 $l=1,2, \ldots$이고 $s \in \mathcal{X}^{k}$입니다. 그러면 다음이 성립합니다.

$$
\sum_{l, s} c_{l s}=c
$$

그리고

$$
\sum_{l, s} l c_{l s}=n
$$

이제 문자열의 파싱을 기반으로 문자열의 확률에 대한 놀라운 상한을 증명합니다.

정리 13.5.5 (Ziv의 부등식) 문자열 $x_{1} x_{2} \cdots x_{n}$의 모든 서로 다른 파싱 (특히 LZ78 파싱)에 대해 다음이 성립합니다.

$$
\log Q_{k}\left(x_{1}, x_{2}, \ldots, x_{n} \mid s_{1}\right) \leq-\sum_{l, s} c_{l s} \log c_{l s}
$$

우변은 $Q_{k}$에 의존하지 않는다는 점에 유의하십시오.
증명: 다음을 작성합니다.

$$
Q_{k}\left(x_{1}, x_{2}, \ldots, x_{n} \mid s_{1}\right)=Q_{k}\left(y_{1}, y_{2}, \ldots, y_{c} \mid s_{1}\right)
$$
<!-- Page 479 -->
$$
=\prod_{i=1}^{c} P\left(y_{i} \mid s_{i}\right)
$$

or

$$
\begin{aligned}
\log Q_{k}\left(x_{1}, x_{2}, \ldots, x_{n} \mid s_{1}\right) & =\sum_{i=1}^{c} \log P\left(y_{i} \mid s_{i}\right) \\
& =\sum_{l, s} \sum_{i:\left|y_{i}\right|=l, s_{i}=s} \log P\left(y_{i} \mid s_{i}\right) \\
& =\sum_{l, s} c_{l s} \sum_{i:\left|y_{i}\right|=l, s_{i}=s} \frac{1}{c_{l s}} \log P\left(y_{i} \mid s_{i}\right) \\
& \leq \sum_{l, s} c_{l s} \log \left(\sum_{i:\left|y_{i}\right|=l, s_{i}=s} \frac{1}{c_{l s}} P\left(y_{i} \mid s_{i}\right)\right)
\end{aligned}
$$

여기서 부등식은 Jensen의 부등식과 로그 함수의 오목성(concavity)으로부터 유도됩니다.

이제 $y_{i}$가 서로 다르므로, $\sum_{i:\left|y_{i}\right|=l, s_{i}=s} P\left(y_{i} \mid s_{i}\right) \leq 1$입니다. 따라서,

$$
\log Q_{k}\left(x_{1}, x_{2}, \ldots, x_{n} \mid s_{1}\right) \leq \sum_{l, s} c_{l s} \log \frac{1}{c_{l s}}
$$

이것으로 보조정리(lemma)가 증명됩니다.
이제 주정리(main theorem)를 증명할 수 있습니다.
정리 13.5.2 $\left\{X_{n}\right\}$을 엔트로피율 $H(\mathcal{X})$을 갖는 이진 정상 에르고딕 과정(binary stationary ergodic process)이라 하고, $c(n)$을 이 과정의 샘플 길이 $n$에 대한 고유 파싱(distinct parsing)에서의 구문(phrase)의 개수라고 합시다. 그러면

$$
\limsup _{n \rightarrow \infty} \frac{c(n) \log c(n)}{n} \leq H(\mathcal{X})
$$

입니다. 확률 1로.
증명: Ziv의 부등식으로 시작하며, 이를 다음과 같이 다시 작성합니다.

$$
\log Q_{k}\left(x_{1}, x_{2}, \ldots, x_{n} \mid s_{1}\right) \leq-\sum_{l, s} c_{l s} \log \frac{c_{l s} c}{c}
$$
<!-- Page 480 -->
$$
=-c \log c-c \sum_{l s} \frac{c_{l s}}{c} \log \frac{c_{l s}}{c}
$$

$\pi_{l s}=\frac{c_{l s}}{c}$로 표기하면,

$$
\sum_{l, s} \pi_{l s}=1, \quad \sum_{l, s} l \pi_{l s}=\frac{n}{c}
$$

(13.102)와 (13.103)으로부터 얻어집니다. 이제 다음과 같이 확률 변수 $U, V$를 정의합니다.

$$
\operatorname{Pr}(U=l, V=s)=\pi_{l s}
$$

따라서 $E U=\frac{n}{c}$이고,

$$
\log Q_{k}\left(x_{1}, x_{2}, \ldots, x_{n} \mid s_{1}\right) \leq c H(U, V)-c \log c
$$

또는

$$
-\frac{1}{n} \log Q_{k}\left(x_{1}, x_{2}, \ldots, x_{n} \mid s_{1}\right) \geq \frac{c}{n} \log c-\frac{c}{n} H(U, V)
$$

이제,

$$
H(U, V) \leq H(U)+H(V)
$$

이고 $H(V) \leq \log |\mathcal{X}|^{k}=k$입니다. Lemma 13.5.4에 의해,

$$
\begin{aligned}
H(U) & \leq(E U+1) \log (E U+1)-(E U) \log (E U) \\
& =\left(\frac{n}{c}+1\right) \log \left(\frac{n}{c}+1\right)-\frac{n}{c} \log \frac{n}{c} \\
& =\log \frac{n}{c}+\left(\frac{n}{c}+1\right) \log \left(\frac{c}{n}+1\right)
\end{aligned}
$$

따라서,

$$
\frac{c}{n} H(U, V) \leq \frac{c}{n} k+\frac{c}{n} \log \frac{n}{c}+o(1)
$$

주어진 $n$에 대해, $\frac{c}{n} \log \frac{n}{c}$의 최댓값은 $c$의 최댓값에서 달성됩니다 (단, $\frac{c}{n} \leq \frac{1}{e}$). 그러나 Lemma 13.5.3에 의해, $c \leq \frac{n}{\log n}(1+o(1))$입니다. 따라서,

$$
\frac{c}{n} \log \frac{n}{c} \leq O\left(\frac{\log \log n}{\log n}\right)
$$
<!-- Page 481 -->
그리고 따라서 $\frac{c}{n} H(U, V) \rightarrow 0$ 이고 $n \rightarrow \infty$ 입니다. 따라서,

$$
\frac{c(n) \log c(n)}{n} \leq-\frac{1}{n} \log Q_{k}\left(x_{1}, x_{2}, \ldots, x_{n} \mid s_{1}\right)+\epsilon_{k}(n)
$$

여기서 $\epsilon_{k}(n) \rightarrow 0$ 이고 $n \rightarrow \infty$ 입니다. 그러므로, 확률 1로,

$$
\begin{aligned}
\limsup _{n \rightarrow \infty} \frac{c(n) \log c(n)}{n} & \leq \lim _{n \rightarrow \infty}-\frac{1}{n} \log Q_{k}\left(X_{1}, X_{2}, \ldots, X_{n} \mid X_{-(k-1)}^{0}\right) \\
& =H\left(X_{0} \mid X_{-1}, \ldots, X_{-k}\right) \\
& \rightarrow H(\mathcal{X}) \quad \text { as } k \rightarrow \infty
\end{aligned}
$$

이제 LZ78 코딩이 점근적으로 최적임을 증명합니다.
정리 13.5.3 $\left\{X_{i}\right\}_{-\infty}^{\infty}$가 이진 정상 에르고딕 확률 과정이라고 가정합니다. $l\left(X_{1}, X_{2}, \ldots, X_{n}\right)$을 $X_{1}, X_{2}, \ldots, X_{n}$에 해당하는 LZ78 코드워드 길이라고 합니다. 그러면

$$
\limsup _{n \rightarrow \infty} \frac{1}{n} l\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq H(\mathcal{X}) \quad \text { with probability } l
$$

여기서 $H(\mathcal{X})$는 과정의 entropy rate입니다.
증명: $l\left(X_{1}, X_{2}, \ldots, X_{n}\right)=c(n)(\log c(n)+1)$임을 보였습니다. 여기서 $c(n)$은 문자열 $X_{1}, X_{2}, \ldots, X_{n}$의 LZ78 구문 분석에 있는 구문의 수입니다. 보조 정리 13.5.3에 의해, $\lim \sup c(n) / n=0$이며, 따라서 정리 13.5.2는 다음을 확립합니다.

$$
\begin{aligned}
\limsup \frac{l\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n} & =\limsup \left(\frac{c(n) \log c(n)}{n}+\frac{c(n)}{n}\right) \\
& \leq H(\mathcal{X}) \quad \text { with probability } 1
\end{aligned}
$$

따라서, 에르고딕 소스의 LZ78 인코딩의 소스 기호당 길이는 소스의 entropy rate보다 점근적으로 크지 않습니다. LZ78 최적성 증명에는 주목할 만한 흥미로운 특징이 있습니다. 고유한 구문의 수에 대한 경계와 Ziv의 부등식은 알고리즘에 사용된 증분 구문 분석뿐만 아니라 모든 고유한 문자열 구문 분석에 적용됩니다. 구문 분석 알고리즘의 변형을 통해 증명을 여러 방식으로 확장할 수 있습니다. 예를 들어, 컨텍스트 또는 상태를 사용하는 다중 트리를 사용할 수 있습니다.
<!-- Page 482 -->
dependent [218, 426]. Ziv의 부등식(정리 13.5.5)은 한쪽의 확률과 다른 쪽의 시퀀스 파싱의 순전히 결정론적 함수의 관계를 나타내므로 특히 흥미롭습니다.

Lempel-Ziv 코드는 보편 코드(즉, 소스의 분포에 의존하지 않는 코드)의 간단한 예입니다. 이 코드는 소스 분포를 알지 못해도 사용할 수 있으며, 소스의 엔트로피율과 동일한 점근적 압축을 달성할 수 있습니다.

# 요약

## 이상적인 단어 길이

$$
l^{*}(x)=\log \frac{1}{p(x)}
$$

## 평균 설명 길이

$$
E_{p} l^{*}(x)=H(p)
$$

추정된 확률 분포 $\hat{\boldsymbol{p}}(\boldsymbol{x})$. 만약 $\hat{l}(x)=\log \frac{1}{\hat{p}(x)}$라면,

$$
E_{p} \hat{l}(x)=H(p)+D(p \| \hat{p})
$$

## 평균 중복도

$$
R_{p}=E_{p} l(X)-H(p)
$$

최소-최대 중복도. $X \sim p_{\theta}(x), \theta \in \theta$에 대해,

$$
D^{*}=\min _{l} \max _{p} R_{p}=\min _{q} \max _{\theta} D\left(p_{\theta} \| q\right)
$$

최소-최대 정리. $D^{*}=C$, 여기서 $C$는 채널 $\left\{\theta, p_{\theta}(x), \mathcal{X}\right\}$의 용량입니다.

베르누이 시퀀스. $X^{n} \sim$ Bernoulli $(\theta)$에 대해, 중복도는 다음과 같습니다.

$$
D_{n}^{*}=\min _{q} \max _{\theta} D\left(p_{\theta}\left(x^{n}\right) \| q\left(x^{n}\right)\right) \approx \frac{1}{2} \log n+o(\log n)
$$

산술 코딩. $F\left(x^{n}\right)$의 $n H$ 비트는 $x^{n}$의 약 $n$ 비트를 나타냅니다.
<!-- Page 483 -->
Lempel-Ziv 코딩 (재발 시간 코딩). $X^{n}$이라는 $n$개의 심볼 블록을 마지막으로 본 시점을 $R_{n}\left(X^{n}\right)$이라고 할 때, $\frac{1}{n} \log R_{n} \rightarrow H(\mathcal{X})$이며, 재발 시간을 기술하여 인코딩하는 것은 점근적으로 최적입니다.

Lempel-Ziv 코딩 (시퀀스 파싱). 시퀀스를 이전에 보지 못한 가장 짧은 구문으로 파싱하고 (예: 011011101은 $0,1,10,11,101, \ldots$로 파싱됨) 파싱된 시퀀스의 설명 길이를 $l\left(x^{n}\right)$이라고 하면, 모든 정상적인 에르고딕 프로세스 $\left\{X_{i}\right\}$에 대해 확률 1로 다음이 성립합니다.

$$
\limsup \frac{1}{n} l\left(X^{n}\right) \leq H(\mathcal{X}) \quad \text { with probability } 1
$$

# 문제

13.1 미니맥스 후회 데이터 압축 및 채널 용량. 먼저 두 소스 분포에 대한 보편 데이터 압축을 고려합니다. 알파벳을 $V=\{1, e, 0\}$이라고 하고, $p_{1}(v)$는 $v=1$에 $1-\alpha$의 질량을, $v=e$에 $\alpha$의 질량을 할당합니다. $p_{2}(v)$는 0에 $1-\alpha$의 질량을, $v=e$에 $\alpha$의 질량을 할당합니다. 우리는 $l(v)=\log \frac{1}{p(v)}$에 따라 $V$에 단어 길이를 할당하며, 이는 영리하게 선택된 확률 질량 함수 $p(v)$에 대한 이상적인 코드워드 길이입니다. (참 분포에 대한) 초과 설명 길이의 최악 경우는 다음과 같습니다.

$$
\max _{i}\left(E_{p_{i}} \log \frac{1}{p(V)}-E_{p_{i}} \log \frac{1}{p_{i}(V)}\right)=\max _{i} D\left(p_{i} \| p\right)
$$

따라서 미니맥스 후회는 $D^{*}=\min _{p} \max _{i} D\left(p_{i} \| p\right)$입니다.
(a) $D^{*}$를 찾으십시오.
(b) $D^{*}$를 달성하는 $p(v)$를 찾으십시오.
(c) $D^{*}$를 이진 삭제 채널의 용량과 비교하고 논평하십시오.

$$
\left[\begin{array}{ccc}
1-\alpha & \alpha & 0 \\
0 & \alpha & 1-\alpha
\end{array}\right]
$$
<!-- Page 484 -->
13.2 범용 데이터 압축. $\mathcal{X}$ 상의 세 가지 가능한 소스 분포를 고려하십시오.
$P_{a}=(0.7,0.2,0.1), \quad P_{b}=(0.1,0.7,0.2), \quad P_{c}=(0.2,0.1,0.7)$.
(a) 압축의 최소 증분 비용을 찾으십시오.

$$
D^{*}=\min _{P} \max _{\theta} D\left(P_{\theta} \| P\right)
$$

관련 질량 함수 $P=\left(p_{1}, p_{2}, p_{3}\right)$ 및 이상적인 코드어 길이 $l_{i}=\log \left(1 / p_{i}\right)$.
(b) 행이 $P_{a}, P_{b}, P_{c}$인 채널 행렬의 채널 용량은 얼마입니까?
13.3 산술 코딩. 전환 행렬을 갖는 정상 이진 마르코프 연쇄 $\left\{X_{i}\right\}_{i=0}^{\infty}$를 고려하십시오.

$$
p_{i j}=\left[\begin{array}{ll}
\frac{3}{4} & \frac{1}{4} \\
\frac{1}{4} & \frac{3}{4}
\end{array}\right]
$$

$X^{\infty}=$ $1010111 \ldots$일 때 $F\left(X^{\infty}\right)=0 . F_{1} F_{2} \ldots$의 첫 3비트를 계산하십시오. 이것은 $X^{\infty}$의 몇 비트를 지정합니까?
13.4 산술 코딩. $X_{i}$는 이진 정상 마르코프이며 전환 행렬은 $\left[\begin{array}{ll}\frac{1}{3} & \frac{2}{3} \\ \frac{2}{3} & \frac{1}{3}\end{array}\right]$입니다.
(a) $F(01110)=\operatorname{Pr}\left\{. X_{1} X_{2} X_{3} X_{4} X_{5}<.01110\right\}$을 찾으십시오.
(b) $X=01110$이 어떻게 계속될지 알 수 없을 때 $. F_{1} F_{2} \ldots$의 몇 비트를 확실하게 알 수 있습니까?
13.5 Lempel-Ziv. 00000011010100000110101의 LZ78 파싱 및 인코딩을 제공하십시오.
13.6 상수 시퀀스 압축. 상수 시퀀스 $x^{n}=11111 \ldots$가 주어졌습니다.
(a) 이 시퀀스에 대한 LZ78 파싱을 제공하십시오.
(b) 이 시퀀스에 대한 기호당 인코딩 비트 수가 $n \rightarrow \infty$일 때 0으로 수렴함을 논증하십시오.
13.7 Lempel-Ziv 코딩의 또 다른 이상적인 버전. LZ의 이상적인 버전이 최적임이 입증되었습니다. 인코더와 디코더 모두 프로세스에 의해 생성된 "무한 과거"인 $\ldots, X_{-1}, X_{0}$에 접근할 수 있으며, 인코더는 디코더에게 과거의 위치 $R_{n}$를 알려줌으로써 문자열 ( $X_{1}$, $X_{2}, \ldots, X_{n}$ )을 설명합니다.
<!-- Page 485 -->
해당 문자열의 첫 번째 재발생까지의 길이를 나타냅니다. 이는 대략 $\log R_{n}+$ $2 \log \log R_{n}$ 비트를 사용합니다. 이제 다음 변형을 고려해 보십시오. $R_{n}$을 설명하는 대신, 인코더는 $R_{n-1}$과 마지막 기호 $X_{n}$을 설명합니다. 이 두 가지로부터 디코더는 문자열 $\left(X_{1}, X_{2}, \ldots, X_{n}\right)$을 재구성할 수 있습니다.
(a) 이 경우 $\left(X_{1}, X_{2}, \ldots, X_{n}\right)$을 인코딩하는 데 사용되는 기호당 비트 수는 얼마입니까?
(b) 텍스트에 주어진 증명을 수정하여 이 버전 또한 점근적으로 최적임을 보이십시오. 즉, 기호당 평균 비트 수가 엔트로피율로 수렴함을 보이십시오.
13.8 LZ77의 포인터 길이. Storer와 Szymanski [507]의 LZ77 버전에서, Section 13.4.1에 설명된 짧은 일치는 $(F, P, L)$ (플래그, 포인터, 길이) 또는 $(F, C)$ (플래그, 문자)로 표현될 수 있습니다. 윈도우 길이가 $W$이고 최대 일치 길이가 $M$이라고 가정합니다.
(a) $P$를 표현하는 데 몇 비트가 필요합니까? $L$을 표현하는 데는 몇 비트가 필요합니까?
(b) 문자 표현인 $C$가 8비트라고 가정합니다. $P$와 $L$의 표현이 8비트보다 길다면, 단일 문자 일치를 사전 내 일치로 표현하는 것보다 압축되지 않은 문자로 표현하는 것이 더 나을 것입니다. $W$와 $M$의 함수로, 압축되지 않은 문자로 표현하는 것보다 일치로 표현하는 것이 더 짧은 일치는 무엇입니까?
(c) $W=4096$이고 $M=256$일 때, 압축되지 않은 문자로 표현하는 것보다 일치로 표현하는 것이 더 짧은 일치는 무엇입니까?
13.9 Lempel-Ziv 78.
(a) 시퀀스 $0,00,001,00000011010111$의 Lempel-Ziv 파싱을 계속하십시오.
(b) LZ 파싱에서 구문의 수가 가능한 한 빠르게 증가하는 시퀀스를 제공하십시오.
(c) LZ 파싱에서 구문의 수가 가능한 한 느리게 증가하는 시퀀스를 제공하십시오.
13.10 고정 데이터베이스 Lempel-Ziv의 두 가지 버전. 소스 $(\mathcal{A}, P)$를 고려하십시오. 단순화를 위해 알파벳이 유한하고 $|\mathcal{A}|=$ $A<\infty$이며 기호들이 i.i.d. $\sim P$라고 가정합니다. 고정 데이터베이스 $\mathcal{D}$가 주어지고 디코더에게 공개됩니다. 인코더는 대상 시퀀스 $x_{1}^{n}$을 길이 $l$의 블록으로 파싱하고, 이후 마지막 등장에 대한 이진 설명을 제공하여 인코딩합니다.
<!-- Page 486 -->
데이터베이스에서 일치하는 항목을 찾지 못하면 전체 블록이 압축되지 않은 상태로 전송되며, 이는 $l \log A$ 비트를 요구합니다. 디코더에게 일치 위치를 설명하는 것인지 아니면 시퀀스 자체를 설명하는 것인지를 알려주기 위해 플래그가 사용됩니다. (a)와 (b)는 (c)에서 고정 데이터베이스 LZ의 최적성을 보여주는 데 필요한 예비 지식을 제공합니다.

(a) $x_{l}$을 0에서 시작하는 길이 $l$의 $\delta$-typical 시퀀스라고 하고, $R_{l}\left(x^{l}\right)$을 무한 과거 $\ldots, X_{-2}, X_{-1}$에서의 해당 반복 인덱스라고 할 때, 다음을 증명하십시오.

$$
E\left[R_{l}\left(X^{l}\right) \mid X^{l}=x^{l}\right] \leq 2^{l(H+\delta)}
$$

여기서 $H$는 소스의 엔트로피율입니다.

(b) 임의의 $\epsilon>0$에 대해, $\operatorname{Pr}\left(R_{l}\left(X^{l}\right)>2^{l(H+\epsilon)}\right) \rightarrow 0$ as $l \rightarrow \infty$임을 증명하십시오. (힌트: 확률을 문자열 $x^{l}$에 대한 조건부 확률로 확장하고, typical한 부분과 non-typical한 부분으로 나누십시오. Markov의 부등식과 AEP가 유용할 것입니다.)

(c) 다음 두 가지 고정 데이터베이스를 고려하십시오: (i) $\mathcal{D}_{1}$은 모든 $\delta$-typical $l$-벡터를 취하여 형성됩니다. (ii) $\mathcal{D}_{2}$는 무한 과거의 가장 최근 $\bar{L}=2^{l(H+\delta)}$개의 심볼 (즉, $X_{-\bar{L}}, \ldots, X_{-1}$)을 취하여 형성됩니다. 위에서 설명한 알고리즘이 점근적으로 최적임을 논증하십시오. 즉, 데이터베이스 $\mathcal{D}_{1}$ 또는 $\mathcal{D}_{2}$와 함께 사용될 때 심볼당 평균 비트 수가 엔트로피율로 수렴함을 보이십시오.

13.11 Tunstall 코딩. 일반적인 소스 코딩 설정은 유한 알파벳의 심볼 (또는 심볼 블록)을 가변 길이 문자열에 매핑합니다. 이러한 코드의 예는 허프만 코드이며, 이는 심볼 집합을 접두사 없는 코드워드 집합으로 매핑하는 최적 (최소 기대 길이) 매핑입니다. 이제 가변 길이 코드를 고정 길이 코드로 변환하는 이중 문제, 즉 가변 길이 소스 심볼 시퀀스를 고정 길이 이진 (또는 $D$-진수) 표현으로 매핑하는 문제를 고려합니다. i.i.d. 확률 변수 시퀀스 $X_{1}, X_{2}, \ldots, X_{n}, X_{i} \sim p(x), x \in \mathcal{X}$ $=\{0,1, \ldots, m-1\}$에 대한 가변 길이-고정 길이 코드는 접두사 없는 구문 집합 $A_{D} \subset$ $\mathcal{X}^{*}$으로 정의되며, 여기서 $\mathcal{X}^{*}$는 $\mathcal{X}$의 심볼로 이루어진 유한 길이 문자열의 집합이고, $\left|A_{D}\right|=D$입니다. 임의의 시퀀스 $X_{1}, X_{2}, \ldots, X_{n}$이 주어지면, 문자열은 $A_{D}$의 구문으로 파싱되고 ( $A_{D}$의 접두사 없는 속성 때문에 고유함), $D$-진수 알파벳의 심볼 시퀀스로 표현됩니다. 이 코딩 방식의 효율성을 다음과 같이 정의합니다.

$$
R\left(A_{D}\right)=\frac{\log D}{E L\left(A_{D}\right)}
$$
<!-- Page 487 -->
$E L\left(A_{D}\right)$는 $A_{D}$의 구문에서 기대되는 길이를 나타냅니다.
(a) $R\left(A_{D}\right) \geq H(X)$임을 증명하십시오.
(b) $A_{D}$를 구성하는 과정은 $A_{D}$의 구문을 잎으로 하는 $m$-진 트리를 구성하는 과정으로 간주될 수 있습니다. $D=1+k(m-1)$이 어떤 정수 $k \geq 1$에 대해 성립한다고 가정합니다. Tunstall의 다음 알고리즘을 고려하십시오:
(i) 확률 $p_{0}$, $p_{1}, \ldots, p_{m-1}$을 가진 $A=\{0,1, \ldots, m-1\}$로 시작합니다. 이는 깊이 1의 완전한 $m$-진 트리에 해당합니다.
(ii) 가장 높은 확률을 가진 노드를 확장합니다. 예를 들어, $p_{0}$가 가장 높은 확률을 가진 노드라면, 새로운 집합은 $A=\{00,01, \ldots, 0(m-1), 1, \ldots,(m-1)\}$입니다.
(iii) 필요한 값에 도달할 때까지 단계 2를 반복합니다.
Tunstall 알고리즘이 주어진 $D$에 대해 최상의 $R\left(A_{D}\right)$를 갖는 변수를 고정 코드로 구성하는 의미에서 최적임을 보여주십시오 [즉, 주어진 $D$에 대해 $E L\left(A_{D}\right)$의 가장 큰 값].
(c) $R\left(A_{D}^{*}\right)<H(X)+1$이 되는 $D$가 존재함을 보여주십시오.

# 역사적 고찰

알 수 없는 분포를 가진 소스를 인코딩하는 문제는 Fitingof [211]와 Davisson [159]에 의해 분석되었으며, 이들은 보편적 코딩 절차가 점근적으로 최적인 소스 클래스가 존재함을 보여주었습니다. 보편적 코드의 평균 중복도와 채널 용량 간의 관계는 Gallager [229]와 Ryabko [450]에 의해 확립되었습니다. 우리의 증명은 Csiszár의 증명을 따릅니다. 이 결과는 Merhav와 Feder [387]에 의해 "대부분의" 소스 클래스에 대해 채널 용량이 중복도의 하한이라는 것을 보여주기 위해 확장되었으며, 이는 Rissanen [444, 448]이 매개변수 경우에 대해 얻은 결과를 확장한 것입니다.

산술 코딩 절차는 Elias (미발표)에 의해 개발된 Shannon-Fano 코드에 뿌리를 두고 있으며, 이는 Jelinek [297]에 의해 분석되었습니다. 본문에서 설명된 접두사 없는 코드 구성 절차는 Gilbert와 Moore [249]에 의해 개발되었습니다. 산술 코딩 자체는 Rissanen [441]과 Pasco [414]에 의해 개발되었으며, Langdon과 Rissanen [343]에 의해 일반화되었습니다. Cover [120]의 열거 방법도 참조하십시오. 산술 코딩에 대한 튜토리얼 소개는 Langdon [342]와 Witten 등 [564]에서 찾을 수 있습니다. Willems 등 [560, 561]에 의한 컨텍스트 트리 가중 알고리즘과 결합된 산술 코딩은 Rissanen을 달성합니다.
<!-- Page 488 -->
lower bound [444]이며, 따라서 파라미터가 알려지지 않은 트리 소스에 대한 entropy로의 최적 수렴율을 가집니다.

Lempel-Ziv 알고리즘 계열은 Lempel과 Ziv의 기념비적인 논문 [603, 604]에서 처음으로 설명되었습니다. 원래 결과는 이론적으로 흥미로웠지만, 알고리즘을 구현하는 사람들은 Welch [554]에 의한 알고리즘의 간단하고 효율적인 버전이 출판될 때까지 주목하지 않았습니다. 그 이후로 알고리즘의 여러 버전이 설명되었으며, 그중 다수는 특허를 받았습니다. 이 알고리즘의 버전은 현재 이미지 압축을 위한 GIF 파일과 모뎀 압축을 위한 CCITT 표준을 포함한 많은 압축 제품에 사용됩니다. Lempel-Ziv (LZ77)의 슬라이딩 윈도우 버전의 최적성은 Wyner와 Ziv [575]에 의해 입증되었습니다. LZ78 [426]의 최적성 증명에 대한 확장은 LZ78의 중복도가 $1 / \log (n)$의 순서이며, 이는 $\log (n) / n$의 하한과 대조됩니다. 따라서 LZ78은 모든 정상 ergodic 소스에 대해 점근적으로 최적이지만, 유한 상태 마르코프 소스에 대한 하한에 비해 entropy율로 매우 느리게 수렴합니다. 그러나 모든 ergodic 소스 계열에 대해, Shields [492]와 Shields 및 Weiss [494]의 예에서 보듯이, 범용 코드의 중복도에 대한 하한은 존재하지 않습니다. 블록을 정렬하고 Burrows와 Wheeler [81]에 의한 간단한 런-랭스 인코딩을 사용하는 손실 없는 블록 압축 알고리즘은 Effros 외 [181]에 의해 분석되었습니다. 예측을 위한 범용 방법은 Feder, Merhav 및 Gutman [204, 386, 388]에서 논의됩니다.
<!-- Page 489 -->
# KOLMOGOROV COMPLEXITY

위대한 수학자 Kolmogorov는 1965년에 객체의 고유한 기술적 복잡성에 대한 정의를 통해 수학, 복잡성, 정보 이론 분야에서의 평생 연구를 집대성했습니다. 지금까지의 논의에서 객체 $X$는 확률 질량 함수 $p(x)$에 따라 추출된 확률 변수였습니다. 만약 $X$가 확률 변수라면, 사건 $X=x$의 기술적 복잡성은 $\log \frac{1}{p(x)}$라는 의미가 있습니다. 왜냐하면 $\left\lceil\log \frac{1}{p(x)}\right\rceil$는 Shannon 코드에 의해 $x$를 기술하는 데 필요한 비트 수이기 때문입니다. 이러한 객체의 기술적 복잡성은 확률 분포에 의존한다는 점을 즉시 알 수 있습니다.

Kolmogorov는 더 나아갔습니다. 그는 객체의 알고리즘적(기술적) 복잡성을 객체를 기술하는 가장 짧은 이진 컴퓨터 프로그램의 길이로 정의했습니다. (명백히, 가장 일반적인 형태의 데이터 압축 해제기인 컴퓨터는 유한한 계산 후 이 기술을 사용하여 기술된 객체를 보여줄 것입니다.) 따라서 객체의 Kolmogorov 복잡성은 확률 분포를 사용하지 않습니다. Kolmogorov는 복잡성 정의가 본질적으로 컴퓨터 독립적이라는 중요한 관찰을 했습니다. 확률 변수의 가장 짧은 이진 컴퓨터 기술의 기대 길이가 엔트로피와 거의 같다는 것은 놀라운 사실입니다. 따라서 가장 짧은 컴퓨터 기술은 모든 확률 분포에 대해 보편적으로 좋은 보편 코드 역할을 합니다. 이러한 의미에서 알고리즘 복잡성은 엔트로피의 개념적 선구자입니다.

아마도 이 장의 역할을 이해하는 좋은 관점은 Kolmogorov 복잡성을 사고방식으로 고려하는 것일 수 있습니다. 실제로는 최소 프로그램을 찾는 데 무한한 시간이 걸릴 수 있으므로 최소 컴퓨터 프로그램을 실제로 사용하지는 않습니다. 하지만 실제로는 매우 짧은, 반드시 최소는 아닌 프로그램을 사용할 수 있습니다. 그리고 이러한 짧은 프로그램을 찾는 아이디어는 보편 코드, 귀납적 추론의 좋은 기반, 오컴의 면도날("가장 단순한 설명이 최고다")의 형식화, 그리고 물리학, 컴퓨터 과학, 통신 이론의 근본적인 이해로 이어집니다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 490 -->
Formalizing the notion of Kolmogorov complexity에 앞서, 세 가지 문자열을 예시로 들어보겠습니다.

1. 01010101010101010101010101010101010101010101010101010101010101
2. 0110101000001001111001100110011111110011101111001100100100001000
3. 1101111001110101111101101111101110101101111000101110010100111011

이 각각의 시퀀스에 대한 가장 짧은 이진 컴퓨터 프로그램은 무엇입니까? 첫 번째 시퀀스는 분명히 간단합니다. 01이 32번 반복됩니다. 두 번째 시퀀스는 무작위로 보이며 대부분의 무작위성 테스트를 통과하지만, 사실 $\sqrt{2}-1$의 이진 확장의 초기 부분입니다. 다시 말하지만, 이것은 간단한 시퀀스입니다. 세 번째 시퀀스는 다시 무작위로 보이지만, 1의 비율이 $\frac{1}{2}$에 가깝지 않다는 점을 제외하면 그렇습니다. 이 시퀀스가 그 외에는 무작위라고 가정하겠습니다. 시퀀스에 포함된 1의 개수 $k$를 설명하고, 이 개수를 가진 시퀀스들의 사전순 정렬에서 해당 시퀀스의 인덱스를 제공함으로써, 대략 $\log n+n H\left(\frac{k}{n}\right)$ 비트로 시퀀스를 설명할 수 있음이 밝혀졌습니다. 이것 역시 시퀀스 자체의 $n$ 비트보다 상당히 적은 양입니다. 다시 한번, 이 시퀀스는 무작위처럼 보이지만 간단하다고 결론 내릴 수 있습니다. 그러나 이 경우, 상수 길이 프로그램을 가진 다른 두 시퀀스만큼 간단하지는 않습니다. 사실, 그 복잡성은 $n$에 비례합니다. 마지막으로, 순수한 동전 던지기로 생성된 진정한 무작위 시퀀스를 상상할 수 있습니다. 이러한 시퀀스는 $2^{n}$개이며 모두 동일하게 확률적입니다. 이러한 무작위 시퀀스는 압축될 수 없다는 것(즉, "다음과 같이 출력: 0101100111010...0"이라고 말하는 것보다 더 나은 프로그램이 없다는 것)은 매우 가능성이 높습니다. 그 이유는 짧은 프로그램이 충분히 많지 않기 때문입니다. 따라서 진정한 무작위 이진 시퀀스의 기술적 복잡성은 시퀀스 자체의 길이와 같습니다.

이것들이 기본적인 아이디어입니다. 이 내재적 복잡성이라는 개념이 컴퓨터 독립적이라는 것(즉, 가장 짧은 프로그램의 길이가 컴퓨터에 따라 달라지지 않는다는 것)을 보여주는 것이 남아 있습니다. 처음에는 이것이 말이 안 되는 것처럼 보일 수 있습니다. 그러나 상수만큼의 차이까지는 사실임이 밝혀졌습니다. 그리고 높은 복잡성을 가진 긴 시퀀스의 경우, 이 상수(한 컴퓨터가 다른 컴퓨터를 모방할 수 있도록 하는 사전 프로그램의 길이)는 무시할 수 있습니다.

# 14.1 계산 모델

알고리즘 복잡성의 개념을 형식화하기 위해, 먼저 컴퓨터에 대한 허용 가능한 모델을 논의합니다. 가장 사소한 컴퓨터를 제외한 모든 컴퓨터는 다른 컴퓨터의 동작을 모방할 수 있다는 의미에서 보편적입니다.
<!-- Page 491 -->
잠시 동안 특정 표준적인 보편 컴퓨터, 즉 개념적으로 가장 단순한 보편 컴퓨터인 보편 튜링 기계에 대해 다루겠습니다.

1936년에 튜링은 살아있는 뇌의 생각이 무생물 부품의 모음으로도 똑같이 잘 유지될 수 있는지에 대한 질문에 몰두했습니다. 요컨대, 기계가 생각할 수 있을까요? 그는 인간의 계산 과정을 분석하여 그러한 컴퓨터에 대한 몇 가지 제약을 가정했습니다. 분명히 인간은 생각하고, 쓰고, 더 생각하고, 쓰고, 이런 식으로 계속합니다. 컴퓨터를 유한한 기호 집합에서 작동하는 유한 상태 기계로 간주하십시오. (유한한 공간에서는 무한한 기호 집합의 기호를 구별할 수 없습니다.) 이진 프로그램이 작성된 프로그램 테이프가 이 유한 상태 기계에 왼쪽에서 오른쪽으로 공급됩니다. 각 시간 단위마다 기계는 프로그램 테이프를 검사하고, 작업 테이프에 일부 기호를 쓰고, 전환 테이블에 따라 상태를 변경하고, 추가 프로그램을 요청합니다. 이러한 기계의 작동은 유한한 전환 목록으로 설명될 수 있습니다. 튜링은 이 기계가 인간의 계산 능력을 모방할 수 있다고 주장했습니다.

튜링의 작업 이후, 모든 새로운 계산 시스템이 튜링 기계로 축소될 수 있으며, 그 반대도 마찬가지임이 밝혀졌습니다. 특히, CPU, 메모리 및 입출력 장치를 갖춘 친숙한 디지털 컴퓨터는 튜링 기계에 의해 시뮬레이션될 수 있으며 튜링 기계를 시뮬레이션할 수 있었습니다. 이는 처치로 하여금 현재 처치의 논리로 알려진 것을 진술하게 했으며, 이는 모든 (충분히 복잡한) 계산 모델이 동일한 함수족을 계산할 수 있다는 의미에서 동등하다는 것을 명시합니다. 그들이 계산할 수 있는 함수 클래스는 효과적으로 계산 가능한 함수에 대한 우리의 직관적인 개념과 일치합니다. 즉, 원하는 계산 결과에 유한한 수의 기계적으로 지정된 계산 단계를 거쳐 도달할 수 있는 유한한 처방 또는 프로그램이 있는 함수입니다.

이 장 전체에서 우리는 그림 14.1에 묘사된 컴퓨터를 염두에 둘 것입니다. 계산의 각 단계에서 컴퓨터는 입력 테이프에서 기호를 읽고, 상태 전환 테이블에 따라 상태를 변경하고, 작업 테이프 또는 출력 테이프에 무언가를 쓸 수 있으며,

그림 14.1. 튜링 기계.
<!-- Page 492 -->
프로그램 읽기 헤드를 프로그램 읽기 테이프의 다음 셀로 이동시킵니다. 이 기계는 오른쪽에서 왼쪽으로만 프로그램을 읽으며, 되돌아가지 않으므로 프로그램은 접두사 없는 집합을 형성합니다. 정지하는 계산으로 이어지는 어떤 프로그램도 다른 그러한 프로그램의 접두사가 될 수 없습니다. 접두사 없는 프로그램으로의 제한은 즉시 Kolmogorov 복잡성에 대한 정보 이론과 형식적으로 유사한 이론으로 이어집니다.

튜링 기계를 유한 길이의 이진 문자열 집합에서 유한 또는 무한 길이의 이진 문자열 집합으로의 맵으로 볼 수 있습니다. 어떤 경우에는 계산이 정지하지 않으며, 이러한 경우에는 함수의 값이 정의되지 않은 것으로 간주됩니다. 튜링 기계로 계산 가능한 함수 $f:\{0,1\}^{*} \rightarrow$ $\{0,1\}^{*} \cup\{0,1\}^{\infty}$의 집합을 부분 재귀 함수 집합이라고 합니다.

# 14.2 KOLMOGOROV 복잡성: 정의 및 예시

유한 길이의 이진 문자열을 $x$라고 하고 범용 컴퓨터를 $\mathcal{U}$라고 합시다. $l(x)$는 문자열 $x$의 길이를 나타냅니다. $\mathcal{U}(p)$는 프로그램 $p$가 주어졌을 때 컴퓨터 $\mathcal{U}$의 출력을 나타냅니다.

문자열 $x$의 Kolmogorov (또는 알고리즘) 복잡성을 $x$의 최소 설명 길이로 정의합니다.

정의 범용 컴퓨터 $\mathcal{U}$에 대한 문자열 $x$의 Kolmogorov 복잡성 $K_{\mathcal{U}}(x)$는 다음과 같이 정의됩니다.

$$
K_{\mathcal{U}}(x)=\min _{p: \mathcal{U}(p)=x} l(p)
$$

$x$를 출력하고 정지하는 모든 프로그램에 대한 최소 길이입니다. 따라서 $K_{\mathcal{U}}(x)$는 컴퓨터 $\mathcal{U}$에 의해 해석되는 모든 설명에 대한 $x$의 가장 짧은 설명 길이입니다.

Kolmogorov 복잡성을 생각하는 유용한 기법은 다음과 같습니다. 한 사람이 다른 사람에게 시퀀스를 명확하게 유한한 시간 내에 해당 시퀀스의 계산으로 이어지도록 설명할 수 있다면, 해당 통신에 포함된 비트 수는 Kolmogorov 복잡성에 대한 상한선입니다. 예를 들어, "e의 제곱근의 첫 번째 $1,239,875,981,825,931$ 비트를 출력하십시오."라고 말할 수 있습니다. 문자당 8비트(ASCII)를 허용하면, 위에서 언급한 명확한 73개의 기호 프로그램은 이 거대한 숫자의 Kolmogorov 복잡성이 $(8)(73)=584$ 비트보다 크지 않음을 보여줍니다. 이 길이의 대부분의 숫자(1000조 비트 이상)는 Kolmogorov 복잡성을 가집니다.
<!-- Page 493 -->
약 $1,239,875,981,825,931$ 비트입니다. $e$의 제곱근을 계산하는 간단한 algorithm이 존재한다는 사실이 기술적 복잡성의 절약을 제공합니다.

위의 정의에서 $x$의 길이에 대해 언급하지 않았습니다. 컴퓨터가 이미 $x$의 길이를 알고 있다고 가정하면, $l(x)$를 아는 조건부 Kolmogorov complexity를 다음과 같이 정의할 수 있습니다.

$$
K_{\mathcal{U}}(x \mid l(x))=\min _{p: \mathcal{U}(p, l(x))=x} l(p)
$$

이는 컴퓨터 $\mathcal{U}$에 $x$의 길이가 제공될 경우 가장 짧은 설명 길이입니다.

$K_{\mathcal{U}}(x \mid y)$는 일반적으로 $K_{\mathcal{U}}\left(x \mid y, y^{*}\right)$로 정의된다는 점에 유의해야 합니다. 여기서 $y^{*}$는 $y$에 대한 가장 짧은 프로그램입니다. 이는 특정 약간의 비대칭성을 피하기 위한 것이지만, 여기서는 이 정의를 사용하지 않을 것입니다.

먼저 Kolmogorov complexity의 기본적인 속성 중 일부를 증명한 다음 다양한 예제를 고려할 것입니다.

정리 14.2.1 (Kolmogorov complexity의 보편성) $\mathcal{U}$가 universal computer이면, 다른 모든 computer $\mathcal{A}$에 대해 상수 $c_{\mathcal{A}}$가 존재하여 모든 문자열 $x \in\{0,1\}^{*}$에 대해 다음이 성립합니다.

$$
K_{\mathcal{U}}(x) \leq K_{\mathcal{A}}(x)+c_{\mathcal{A}}
$$

상수 $c_{\mathcal{A}}$는 $x$에 의존하지 않습니다.
증명: computer $\mathcal{A}$를 위한 프로그램 $p_{\mathcal{A}}$가 $x$를 출력한다고 가정합니다. 즉, $\mathcal{A}\left(p_{\mathcal{A}}\right)=x$입니다. 이 프로그램 앞에 computer $\mathcal{U}$에게 computer $\mathcal{A}$를 시뮬레이션하는 방법을 알려주는 시뮬레이션 프로그램 $s_{\mathcal{A}}$를 붙일 수 있습니다. 그러면 computer $\mathcal{U}$는 $\mathcal{A}$에 대한 프로그램의 지시를 해석하고, 해당 계산을 수행하며, $x$를 출력합니다. $\mathcal{U}$에 대한 프로그램은 $p=$ $s_{\mathcal{A}} p_{\mathcal{A}}$이며, 그 길이는 다음과 같습니다.

$$
l(p)=l\left(s_{\mathcal{A}}\right)+l\left(p_{\mathcal{A}}\right)=c_{\mathcal{A}}+l\left(p_{\mathcal{A}}\right)
$$

여기서 $c_{\mathcal{A}}$는 시뮬레이션 프로그램의 길이입니다. 따라서 모든 문자열 $x$에 대해 다음이 성립합니다.

$$
K_{\mathcal{U}}(x)=\min _{p: \mathcal{U}(p)=x} l(p) \leq \min _{p: \mathcal{A}(p)=x}\left(l(p)+c_{\mathcal{A}}\right)=K_{\mathcal{A}}(x)+c_{\mathcal{A}}
$$

정리의 상수 $c_{\mathcal{A}}$는 매우 클 수 있습니다. 예를 들어, $\mathcal{A}$는 많은 함수가 시스템에 내장된 대형 computer일 수 있습니다.
<!-- Page 494 -->
컴퓨터 $\mathcal{U}$는 간단한 마이크로프로세서일 수 있습니다. 시뮬레이션 프로그램은 이 모든 함수의 구현 세부 사항, 사실상 대형 컴퓨터에서 사용 가능한 모든 소프트웨어를 포함할 것입니다. 중요한 점은 이 시뮬레이션 프로그램의 길이가 압축될 문자열 $x$의 길이에 독립적이라는 것입니다. 충분히 긴 $x$에 대해 이 시뮬레이션 프로그램의 길이는 무시할 수 있으며, 상수에 대해 이야기하지 않고도 Kolmogorov 복잡성에 대해 논의할 수 있습니다.

$\mathcal{A}$와 $\mathcal{U}$가 모두 universal하다면, 우리는 다음과 같은 관계를 가집니다.

$$
\left|K_{\mathcal{U}}(x)-K_{\mathcal{A}}(x)\right|<c
$$

모든 $x$에 대해. 따라서, 우리는 모든 추가 정의에서 $\mathcal{U}$에 대한 언급을 생략할 것입니다. 우리는 명시되지 않은 컴퓨터 $\mathcal{U}$가 고정된 universal 컴퓨터라고 가정할 것입니다.

정리 14.2.2 (조건부 복잡성은 시퀀스의 길이보다 작습니다)

$$
K(x \mid l(x)) \leq l(x)+c
$$

증명: $x$를 출력하는 프로그램은 다음과 같습니다.
다음 $l$-비트 시퀀스를 출력하십시오: $x_{1} x_{2} \ldots x_{l(x)}$.
$l$은 주어졌기 때문에 $l$을 설명하는 데 비트가 필요하지 않다는 점에 유의하십시오. 프로그램은 $l(x)$가 제공되고 프로그램의 끝이 따라서 명확하게 정의되므로 자체적으로 구분됩니다. 이 프로그램의 길이는 $l(x)+c$입니다.

문자열 길이에 대한 지식 없이, 우리는 추가적인 중지 기호가 필요하거나 다음 정리의 증명에서 설명된 것과 같은 자체 구두점 방식을 사용할 수 있습니다.

정리 14.2.3 (Kolmogorov 복잡성에 대한 상한)

$$
K(x) \leq K(x \mid l(x))+2 \log l(x)+c
$$

증명: 컴퓨터가 $l(x)$를 모르는 경우, 정리 14.2.2의 방법은 적용되지 않습니다. 우리는 컴퓨터에게 비트 시퀀스가 시퀀스를 설명하는 끝에 도달했음을 알릴 수 있는 방법이 있어야 합니다. 우리는 "쉼표"로 01 시퀀스를 사용하는 간단하지만 비효율적인 방법을 설명합니다.

$l(x)=n$이라고 가정합니다. $l(x)$를 설명하기 위해, $n$의 이진 확장의 각 비트를 두 번 반복하십시오. 그런 다음 컴퓨터가 $n$의 설명 끝에 도달했음을 알 수 있도록 01로 설명을 끝내십시오.
<!-- Page 495 -->
예를 들어, 숫자 5 (이진수 101)는 11001101로 설명될 것입니다. 이 설명에는 $2\lceil\log n\rceil+2$ 비트가 필요합니다. 따라서 $l(x)$의 이진 표현을 포함해도 프로그램 길이에 $2 \log l(x)+c$ 비트 이상이 추가되지 않으므로, 정리의 상한이 존재합니다.

$n$을 설명하는 더 효율적인 방법은 재귀적으로 설명하는 것입니다. 먼저 $n$의 이진 표현에 있는 비트 수 $(\log n)$를 지정한 다음 $n$의 실제 비트를 지정합니다. $n$의 이진 표현 길이인 $\log n$을 지정하기 위해 비효율적인 방법 $(2 \log \log n)$ 또는 효율적인 방법 $(\log \log n+\cdots)$을 사용할 수 있습니다. 각 단계에서 효율적인 방법을 사용하여 지정할 작은 숫자가 나올 때까지 사용하면, 마지막 양수 항까지 합을 계속하는 $\log n+\log \log n+\log \log \log n+\cdots$ 비트로 $n$을 설명할 수 있습니다. 이 반복 로그의 합은 때때로 $\log ^{*} n$으로 표기됩니다. 따라서 정리 14.2.3은 다음과 같이 개선될 수 있습니다.

$$
K(x) \leq K(x \mid l(x))+\log ^{*} l(x)+c
$$

이제 낮은 복잡도를 가진 시퀀스가 매우 적다는 것을 증명합니다.
정리 14.2.4 (Kolmogorov 복잡도에 대한 하한). 복잡도 $K(x)<k$를 갖는 문자열 $x$의 개수는 다음과 같습니다.

$$
\left|\{x \in\{0,1\}^{*}: K(x)<k\}\right|<2^{k}
$$

증명: 짧은 프로그램이 많지 않습니다. 길이가 $<k$인 모든 프로그램을 나열하면 다음과 같습니다.

$$
\underbrace{\Lambda}_{1}, \underbrace{0,1}_{2}, \underbrace{00,01,10,11}_{4}, \ldots, \underbrace{\ldots, 11 \ldots 1}_{2^{k-1}}
$$

이러한 프로그램의 총 개수는 다음과 같습니다.

$$
1+2+4+\cdots+2^{k-1}=2^{k}-1<2^{k}
$$

각 프로그램은 하나의 가능한 출력 시퀀스만 생성할 수 있으므로, 복잡도가 $<k$인 시퀀스의 개수는 $2^{k}$보다 적습니다.

혼란을 피하고 이 장의 나머지 부분에서 설명을 용이하게 하기 위해 이진 엔트로피 함수에 대한 특별한 표기법을 도입해야 합니다.

$$
H_{0}(p)=-p \log p-(1-p) \log (1-p)
$$
<!-- Page 496 -->
따라서 $H_{0}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}\right)$라고 쓸 때, 이는 $\bar{X}_{n}$의 엔트로피가 아니라 $-\bar{X}_{n} \log \bar{X}_{n}-(1-$ $\left.\bar{X}_{n}\right) \log \left(1-\bar{X}_{n}\right)$를 의미합니다. 혼동의 여지가 없을 때는 단순히 $H(p)$를 $H_{0}(p)$에 대해 쓸 것입니다.

이제 다양한 Kolmogorov complexity의 예시를 살펴보겠습니다. 복잡성은 컴퓨터에 따라 달라지지만, 상수만큼만 더해집니다. 구체적으로, 영어로 된 모호하지 않은 명령(숫자는 이진 표기법으로 주어짐)을 받을 수 있는 컴퓨터를 고려합니다. 우리는 다음 부등식을 사용할 것입니다.

$$
\sqrt{\frac{n}{8 k(n-k)}} 2^{n H(k / n)} \leq\binom{ n}{k} \leq \sqrt{\frac{n}{\pi k(n-k)}} 2^{n H(k / n)}, \quad k \neq 0, n
$$

이는 Lemma 17.5.1에서 증명됩니다.
예시 14.2.1 (n개의 0으로 이루어진 시퀀스) 컴퓨터가 n을 알고 있다고 가정하면, 이 문자열을 출력하는 짧은 프로그램은 다음과 같습니다.

지정된 수만큼의 0을 출력하십시오.
이 프로그램의 길이는 상수 비트 수입니다. 이 프로그램 길이는 n에 의존하지 않습니다. 따라서 이 시퀀스의 Kolmogorov complexity는 c이며,

$$
K(000 \ldots 0 \mid n)=c \quad \text { for all } n
$$

예시 14.2.2 ($\pi$의 Kolmogorov complexity) $\pi$의 처음 n 비트는 간단한 급수 표현을 사용하여 계산할 수 있습니다. 컴퓨터가 이미 n을 알고 있다면 이 프로그램은 작은 상수 길이를 가집니다. 따라서,

$$
K\left(\pi_{1} \pi_{2} \cdots \pi_{n} \mid n\right)=c
$$

예시 14.2.3 (고담 날씨) 컴퓨터가 n일 동안 고담의 날씨를 출력하도록 하려는 경우를 가정해 봅시다. 여기서 $x_{i}=1$은 i일에 비가 오는 것을 나타내는 전체 시퀀스 $x=x_{1} x_{2} \cdots x_{n}$를 포함하는 프로그램을 작성할 수 있습니다. 그러나 날씨는 상당히 의존적이므로 이는 비효율적입니다. 의존성을 고려하여 시퀀스에 대한 다양한 코딩 방식을 고안할 수 있습니다. 간단한 방법 중 하나는 시퀀스를 근사하기 위해 마르코프 모델을 찾고(경험적 전이 확률 사용) 이 확률 분포에 대한 Shannon 코드를 사용하여 시퀀스를 코딩하는 것입니다. 경험적 마르코프 전이를 $O(\log n)$ 비트로 설명하고, p가

<!-- Page 497 -->
확률적 마르코프 연쇄를 가정합니다. 날씨의 엔트로피가 하루에 $\frac{1}{5}$ 비트라고 가정하면, $n$일 동안의 날씨를 약 $n/5$ 비트로 설명할 수 있으며, 따라서

$$
K(\text { Gotham weather }|n) \approx \frac{n}{5}+O(\log n)+c
$$

예제 14.2.4 (01010101...01 형태의 반복되는 시퀀스) 짧은 프로그램으로 충분합니다. 단순히 지정된 수의 01 쌍을 출력하면 됩니다. 따라서,

$$
K(010101010 \ldots 01 \mid n)=c
$$

예제 14.2.5 (프랙탈) 프랙탈은 만델브로 집합의 일부이며 간단한 컴퓨터 프로그램으로 생성됩니다. 복소 평면의 다른 점 $c$에 대해, $|z|$가 특정 임계값을 넘는 데 필요한 반복 횟수를 계산합니다. $z_{n+1}=z_{n}^{2}+c$ (단, $z_{0}=0$에서 시작)의 맵에 대한 반복 횟수를 계산합니다. 그런 다음 점 $c$는 필요한 반복 횟수에 따라 색칠됩니다. 따라서 프랙탈은 매우 복잡해 보이지만 본질적으로 매우 간단한 객체의 예입니다. 이 객체의 콜모고로프 복잡성은 본질적으로 0입니다.

예제 14.2.6 (모나리자) 그림에 있는 많은 구조와 종속성을 활용할 수 있습니다. 기존에 쉽게 설명할 수 있는 이미지 압축 알고리즘을 사용하여 이미지를 약 3배 정도 압축할 수 있습니다. 따라서 모나리자 이미지의 픽셀 수를 $n$이라고 하면,

$$
K(\text { Mona Lisa }|n) \leq \frac{n}{3}+c
$$

예제 14.2.7 (정수 n) 컴퓨터가 정수의 이진 표현에서 비트 수를 알고 있다면, 해당 비트의 값만 제공하면 됩니다. 이 프로그램의 길이는 $c+\log n$이 될 것입니다.

일반적으로 컴퓨터는 정수의 이진 표현 길이를 알지 못합니다. 따라서 설명이 언제 끝나는지 어떤 식으로든 컴퓨터에 알려야 합니다. (14.9)를 유도하는 데 사용된 정수 설명 방법을 사용하면, 정수의 콜모고로프 복잡성은 다음과 같이 제한됨을 알 수 있습니다.

$$
K(n) \leq \log ^{*} n+c
$$

예제 14.2.8 (1이 k개인 n개의 비트 시퀀스) 1이 k개인 n개의 비트 시퀀스를 압축할 수 있습니까?

첫 번째 추측은 '아니오'입니다. 왜냐하면 정확하게 복제해야 하는 비트 시리즈가 있기 때문입니다. 하지만 다음 프로그램을 고려해 보십시오.
<!-- Page 498 -->
$k$개의 1을 포함하는 모든 시퀀스를 사전순으로 생성하십시오.
이 시퀀스들 중에서 $i$번째 시퀀스를 출력하십시오.
이 프로그램은 요구되는 시퀀스를 출력할 것입니다. 프로그램의 유일한 변수는 $k$ (범위 $\{0,1, \ldots, n\}$)와 $i$ (조건부 범위 $\left.\left\{1,2, \ldots,\binom{n}{k}\right\}\right)$입니다. 이 프로그램의 총 길이는

$$
\begin{aligned}
l(p) & =c+\underbrace{\log n}_{\text {k를 표현하기 위한 것}}+\underbrace{\log\binom{n}{k}}_{\text {i를 표현하기 위한 것}} \\
& \leq c^{\prime}+\log n+n H\left(\frac{k}{n}\right)-\frac{1}{2} \log n
\end{aligned}
$$

$p=k / n$이고 $q=1-p$이며 $k \neq 0$이고 $k \neq n$일 때 $\binom{n}{k} \leq \frac{1}{\sqrt{\pi n p q}} 2^{n H_{0}(p)}$에 의해 (14.14)를 사용했기 때문입니다. 우리는 $k$를 표현하기 위해 $\log n$ 비트를 사용했습니다. 따라서, $\sum_{i=1}^{n} x_{i}=k$이면,

$$
K\left(x_{1}, x_{2}, \ldots, x_{n} \mid n\right) \leq n H_{0}\left(\frac{k}{n}\right)+\frac{1}{2} \log n+c
$$

예제 14.2.8을 다음 정리로 요약할 수 있습니다.
정리 14.2.5 이진 문자열 $x$의 Kolmogorov 복잡도는 다음과 같이 제한됩니다.

$$
K\left(x_{1} x_{2} \cdots x_{n} \mid n\right) \leq n H_{0}\left(\frac{1}{n} \sum_{i=1}^{n} x_{i}\right)+\frac{1}{2} \log n+c
$$

증명: 예제 14.2.8에 설명된 프로그램을 사용하십시오.
비고: 압축하려는 데이터가 $x \in\{0,1\}^{*}$이고 프로그램 $p$를 압축된 데이터로 간주합니다. 데이터 압축에 성공한 경우는 $l(p)<l(x)$이거나

$$
K(x)<l(x)
$$

일반적으로, 시퀀스 $x$의 길이 $l(x)$가 작을 때, Kolmogorov 복잡도에 대한 표현식에 나타나는 상수들이 $l(x)$에 의한 기여를 압도할 것입니다. 따라서, 이 이론은 주로 $l(x)$가 매우 클 때 유용합니다. 이러한 경우 $l(x)$에 의존하지 않는 항들을 안전하게 무시할 수 있습니다.
<!-- Page 499 -->
# 14.3 KOLMOGOROV 복잡도와 엔트로피

이제 확률 변수 시퀀스의 Kolmogorov 복잡도와 엔트로피 간의 관계를 고려합니다. 일반적으로 확률적 시퀀스의 Kolmogorov 복잡도 기대값이 Shannon 엔트로피에 근접함을 보입니다. 먼저, 프로그램 길이가 Kraft 부등식을 만족함을 증명합니다.

정리 14.3.1 임의의 컴퓨터 $\mathcal{U}$에 대해,

$$
\sum_{p: \mathcal{U}(p) \text { halts }} 2^{-l(p)} \leq 1
$$

증명: 컴퓨터가 어떤 프로그램에 대해 실행을 멈추면, 더 이상 입력을 찾지 않습니다. 따라서 이 프로그램을 접두사로 가지는 다른 실행 중인 프로그램은 존재할 수 없습니다. 그러므로 실행 중인 프로그램들은 접두사 없는 집합을 형성하며, 그 길이들은 Kraft 부등식(정리 5.2.1)을 만족합니다.

이제 유한 알파벳을 가진 i.i.d. 프로세스에 대해 $\frac{1}{n} E K\left(X^{n} \mid n\right) \approx H(X)$임을 보입니다.

정리 14.3.1 (Kolmogorov 복잡도와 엔트로피의 관계) 확률 과정 $\left\{X_{i}\right\}$가 확률 질량 함수 $f(x), x \in \mathcal{X}$에 따라 i.i.d.로 추출된다고 가정합니다. 여기서 $\mathcal{X}$는 유한 알파벳입니다. $f\left(x^{n}\right)=\prod_{i=1}^{n} f\left(x_{i}\right)$라고 할 때, 상수 $c$가 존재하여 모든 $n$에 대해 다음이 성립합니다.

$$
H(X) \leq \frac{1}{n} \sum_{x^{n}} f\left(x^{n}\right) K\left(x^{n} \mid n\right) \leq H(X)+\frac{(|\mathcal{X}|-1) \log n}{n}+\frac{c}{n}
$$

결과적으로,

$$
E \frac{1}{n} K\left(X^{n} \mid n\right) \rightarrow H(X)
$$

증명: 하한을 고려합니다. 허용된 프로그램들은 접두사 속성을 만족하므로, 그 길이들은 Kraft 부등식을 만족합니다. 우리는 각 $x^{n}$에 대해 $\mathcal{U}(p, n)=x^{n}$을 만족하는 가장 짧은 프로그램 $p$의 길이를 할당합니다. 이 가장 짧은 프로그램들도 Kraft 부등식을 만족합니다. 우리는 소스 코딩 이론으로부터 기대되는 코드워드 길이가 엔트로피보다 커야 함을 알고 있습니다. 따라서,

$$
\sum_{x^{n}} f\left(x^{n}\right) K\left(x^{n} \mid n\right) \geq H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=n H(X)
$$
<!-- Page 500 -->
먼저 $\mathcal{X}$가 이진(binary)인 경우($X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim$ Bernoulli $(\theta)$인 경우)의 상한을 증명합니다. Theorem 14.2.5의 방법을 사용하여 이진 문자열의 복잡도를 다음과 같이 제한할 수 있습니다.

$$
K\left(x_{1} x_{2} \ldots x_{n} \mid n\right) \leq n H_{0}\left(\frac{1}{n} \sum_{i=1}^{n} x_{i}\right)+\frac{1}{2} \log n+c
$$

따라서,

$$
\begin{aligned}
E K\left(X_{1} X_{2} \ldots X_{n} \mid n\right) & \leq n E H_{0}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}\right)+\frac{1}{2} \log n+c \\
& \stackrel{(\mathrm{a})}{\leq} n H_{0}\left(\frac{1}{n} \sum_{i=1}^{n} E X_{i}\right)+\frac{1}{2} \log n+c \\
& =n H_{0}(\theta)+\frac{1}{2} \log n+c
\end{aligned}
$$

여기서 (a)는 Jensen의 부등식과 엔트로피의 오목성(concavity)에 의해 성립합니다. 따라서 이진 프로세스에 대한 정리의 상한을 증명했습니다.

비이진 유한 알파벳의 경우에도 동일한 기법을 사용할 수 있습니다. 먼저 Section 11.1에 정의된 대로 알파벳 기호의 발생 빈도(empirical frequency)를 나타내는 타입(type)을 $(|\mathcal{X}|-1) \log n$ 비트로 설명합니다 (마지막 기호의 빈도는 나머지 기호의 빈도로부터 계산할 수 있습니다). 그런 다음 동일한 타입을 가진 모든 시퀀스 중에서 해당 시퀀스의 인덱스를 설명합니다. Chapter 11에서 보듯이 타입 클래스는 $2^{n H\left(P_{x^{n}}\right)}$개 미만의 요소를 가지며 (여기서 $P_{x^{n}}$은 시퀀스 $x^{n}$의 타입입니다), 따라서 문자열 $x^{n}$의 이단계 설명(two-stage description)의 길이는 다음과 같습니다.

$$
K\left(x^{n} \mid n\right) \leq n H\left(P_{x^{n}}\right)+(|\mathcal{X}|-1) \log n+c
$$

다시 한번, 이진 경우와 마찬가지로 기댓값을 취하고 Jensen의 부등식을 적용하면 다음과 같은 결과를 얻습니다.

$$
E K\left(X^{n} \mid n\right) \leq n H(X)+(|\mathcal{X}|-1) \log n+c
$$

이를 $n$으로 나누면 정리의 상한을 얻게 됩니다.
<!-- Page 501 -->
시퀀스 길이에 대한 조건부 제거는 간단합니다. 유사한 논증을 통해 다음을 보일 수 있습니다.

$$
H(X) \leq \frac{1}{n} \sum_{x^{n}} f\left(x^{n}\right) K\left(x^{n}\right) \leq H(X)+\frac{(|\lambda|+1) \log n}{n}+\frac{c}{n}
$$

모든 $n$에 대해. 하한은 $K\left(x^{n}\right)$이 소스에 대한 접두사 없는 코드(prefix-free code)라는 사실에서 비롯되며, 상한은 $K\left(x^{n}\right) \leq K\left(x^{n} \mid n\right)+2 \log n+c$라는 사실에서 도출될 수 있습니다. 따라서,

$$
E \frac{1}{n} K\left(X^{n}\right) \rightarrow H(X)
$$

이며, 컴퓨터에 의해 달성되는 압축률은 엔트로피 한계로 수렴합니다.

# 14.4 정수의 KOLMOGOROV 복잡성

섹션 14.3에서는 이진 문자열의 Kolmogorov 복잡성을 해당 문자열을 출력하는 범용 컴퓨터(universal computer)에 대한 가장 짧은 프로그램의 길이로 정의했습니다. 이 정의를 확장하여 정수의 Kolmogorov 복잡성을 해당 이진 문자열의 Kolmogorov 복잡성으로 정의할 수 있습니다.

정의 정수 $n$의 Kolmogorov 복잡성은 다음과 같이 정의됩니다.

$$
K(n)=\min _{p: \mathcal{U}(p)=n} l(p)
$$

정수의 Kolmogorov 복잡성의 속성은 비트 문자열의 Kolmogorov 복잡성의 속성과 매우 유사합니다. 다음 속성들은 문자열에 대한 해당 속성들의 직접적인 결과입니다.

정리 14.4.1 범용 컴퓨터 $\mathcal{A}$와 $\mathcal{U}$에 대해,

$$
K_{\mathcal{U}}(n) \leq K_{\mathcal{A}}(n)+c_{\mathcal{A}}
$$

또한, 모든 숫자는 이진 표현으로 지정될 수 있으므로 다음 정리를 얻습니다.

## 정리 14.4.2

$$
K(n) \leq \log ^{*} n+c
$$
<!-- Page 502 -->
정리 14.4.3 $K(n)>\log n$을 만족하는 정수 $n$은 무수히 많다.

증명: 보조정리 14.3.1로부터 다음을 안다.

$$
\sum_{n} 2^{-K(n)} \leq 1
$$

그리고

$$
\sum_{n} 2^{-\log n}=\sum_{n} \frac{1}{n}=\infty
$$

그러나 만약 모든 $n>n_{0}$에 대해 $K(n)<\log n$이라면,

$$
\sum_{n=n_{0}}^{\infty} 2^{-K(n)}>\sum_{n=n_{0}}^{\infty} 2^{-\log n}=\infty
$$

이는 모순이다.

# 14.5 알고리즘적으로 무작위적이고 압축 불가능한 수열

14.2절의 예시들로부터, $\pi$의 처음 백만 비트와 같이 설명하기 쉬운 긴 수열들이 있다는 것은 명확하다. 마찬가지로, 다음과 같이 설명하기 쉬운 큰 정수들도 존재한다.

$$
2^{2^{22^{2^{2}}}}
$$

또는 $(100!)!$.
이제 우리는 일부 수열은 간단하지만, 대부분의 수열은 간단한 설명을 가지지 않는다는 것을 보인다. 유사하게, 대부분의 정수는 간단하지 않다. 따라서, 우리가 무작위로 수열을 뽑는다면, 복잡한 수열을 뽑을 가능성이 높다. 다음 정리는 $k$ 비트 이상 압축될 수 있는 수열의 확률이 $2^{-k}$를 넘지 않음을 보여준다.

정리 14.5.1 $X_{1}, X_{2}, \ldots, X_{n}$이 베르누이 $\left(\frac{1}{2}\right)$ 과정에 따라 뽑혔다고 하자. 그러면

$$
P\left(K\left(X_{1} X_{2} \ldots X_{n} \mid n\right)<n-k\right)<2^{-k}
$$
<!-- Page 503 -->
# 증명:

$$
\begin{aligned}
& P\left(K\left(X_{1} X_{2} \ldots X_{n} \mid n\right)<n-k\right) \\
& \quad=\sum_{x_{1} x_{2} \ldots x_{n}: K\left(x_{1} x_{2} \ldots x_{n} \mid n\right)<n-k} p\left(x_{1}, x_{2}, \ldots, x_{n}\right) \\
& \quad=\sum_{x_{1} x_{2} \ldots x_{n}: K\left(x_{1} x_{2} \ldots x_{n} \mid n\right)<n-k} 2^{-n} \\
& \quad=\left|\left\{x_{1} x_{2} \ldots x_{n}: K\left(x_{1} x_{2} \ldots x_{n} \mid n\right)<n-k\right\}\right| 2^{-n} \\
& \quad<2^{n-k} 2^{-n} \quad(\text{정리 14.2.4에 의해}) \\
& \quad=2^{-k} \text {. }
\end{aligned}
$$

따라서 대부분의 시퀀스는 그 길이와 가까운 복잡도를 가집니다. 예를 들어, 길이가 $n$인 시퀀스 중 복잡도가 $n-5$보다 작은 시퀀스의 비율은 $1/32$보다 작습니다. 이는 다음 정의를 뒷받침합니다:

정의 시퀀스 $x_{1}, x_{2}, \ldots, x_{n}$은 다음과 같을 때 알고리즘적으로 무작위라고 합니다.

$$
K\left(x_{1} x_{2} \ldots x_{n} \mid n\right) \geq n
$$

각 $n$에 대해 다음을 만족하는 최소한 하나의 시퀀스 $x^{n}$이 존재한다는 계산적 논증에 주목하십시오.

$$
K\left(x^{n} \mid n\right) \geq n
$$

정의 무한 문자열 $x$를 압축 불가능하다고 부릅니다.

$$
\lim _{n \rightarrow \infty} \frac{K\left(x_{1} x_{2} x_{3} \cdots x_{n} \mid n\right)}{n}=1
$$

정리 14.5.2 (압축 불가능한 시퀀스에 대한 큰 수의 법칙) 문자열 $x_{1} x_{2} \ldots$이 압축 불가능하다면, 다음과 같은 의미에서 큰 수의 법칙을 만족합니다.

$$
\frac{1}{n} \sum_{i=1}^{n} x_{i} \rightarrow \frac{1}{2}
$$

따라서 압축 불가능한 문자열의 0과 1의 비율은 거의 같습니다.
<!-- Page 504 -->
증명: $\theta_{n}=\frac{1}{n} \sum_{i=1}^{n} x_{i}$를 $x_{1} x_{2} \ldots x_{n}$에서 1의 비율로 나타냅니다. 그러면 예제 14.2의 방법을 사용하여 $x^{n}$을 출력하기 위한 길이 $n H_{0}\left(\theta_{n}\right)+2 \log \left(n \theta_{n}\right)+c$의 프로그램을 작성할 수 있습니다. 따라서,

$$
\frac{K\left(x^{n} \mid n\right)}{n}<H_{0}\left(\theta_{n}\right)+2 \frac{\log n}{n}+\frac{c^{\prime}}{n}
$$

압축 불가능성 가정에 의해, 충분히 큰 $n$에 대해 하한도 다음과 같이 얻을 수 있습니다.

$$
1-\epsilon \leq \frac{K\left(x^{n} \mid n\right)}{n} \leq H_{0}\left(\theta_{n}\right)+2 \frac{\log n}{n}+\frac{c^{\prime}}{n}
$$

따라서,

$$
H_{0}\left(\theta_{n}\right)>1-\frac{2 \log n+c^{\prime}}{n}-\epsilon
$$

$H_{0}(p)$ 그래프(그림 14.2)를 살펴보면 큰 $n$에 대해 $\theta_{n}$이 $\frac{1}{2}$에 가깝다는 것을 알 수 있습니다. 구체적으로, 위의 부등식은 다음을 의미합니다.

$$
\theta_{n} \in\left(\frac{1}{2}-\delta_{n}, \frac{1}{2}+\delta_{n}\right)
$$

그림 14.2. $H_{0}(p)$ 대 $p$.
<!-- Page 505 -->
여기서 $\delta_{n}$은 다음과 같이 선택됩니다.

$$
H_{0}\left(\frac{1}{2}-\delta_{n}\right)=1-\frac{2 \log n+c_{n}+c^{\prime}}{n}
$$

이는 $\delta_{n} \rightarrow 0$이 $n \rightarrow \infty$일 때 성립함을 의미합니다. 따라서 $\frac{1}{n} \sum x_{i} \rightarrow \frac{1}{2}$이 $n \rightarrow \infty$일 때 성립합니다.
이제 우리는 압축 불가능한(incompressible) 시퀀스가 0과 1의 비율이 거의 같다는 점에서 무작위적으로 보인다는 것을 증명했습니다. 일반적으로, 시퀀스가 압축 불가능하다면 무작위성에 대한 모든 계산 가능한 통계적 테스트를 만족한다는 것을 보일 수 있습니다. (그렇지 않다면, $x$가 실패하는 테스트를 식별하는 것은 $x$의 기술적 복잡성(descriptive complexity)을 줄여 모순을 야기할 것입니다.) 이러한 의미에서, 무작위성에 대한 알고리즘적 테스트는 궁극적인 테스트이며, 무작위성에 대한 다른 모든 계산 가능한 테스트를 포함합니다.

이제 우리는 Bernoulli $(\theta)$ 시퀀스의 Kolmogorov 복잡성에 대한 관련 대수의 법칙을 증명합니다. Bernoulli $(\theta)$ 프로세스에 따라 i.i.d.로 추출된 이진 난수 변수의 시퀀스의 Kolmogorov 복잡성은 entropy $H_{0}(\theta)$에 가깝습니다. Theorem 14.3.1에서 우리는 무작위 Bernoulli 시퀀스의 Kolmogorov 복잡성의 기댓값이 entropy로 수렴한다는 것을 증명했습니다 [즉, $E \frac{1}{n} K\left(X_{1} X_{2} \ldots X_{n} \mid n\right) \rightarrow H_{0}(\theta)]$. 이제 우리는 기댓값을 제거합니다.

Theorem 14.5.3 $X_{1}, X_{2}, \ldots, X_{n}$이 i.i.d. $\sim$ Bernoulli $(\theta)$로 추출되었다고 가정합니다. 그러면

$$
\frac{1}{n} K\left(X_{1} X_{2} \ldots X_{n} \mid n\right) \rightarrow H_{0}(\theta) \quad \text { 확률적으로 }
$$

증명: $X_{1}, X_{2}, \ldots, X_{n}$에서 1의 비율인 $\bar{X}_{n}=\frac{1}{n} \sum X_{i}$라고 합시다. 그러면 (14.23)에서 설명된 방법을 사용하면 다음과 같습니다.

$$
K\left(X_{1} X_{2} \ldots X_{n} \mid n\right) \leq n H_{0}\left(\bar{X}_{n}\right)+2 \log n+c
$$

그리고 대수의 약한 법칙(weak law of large numbers)에 의해, $\bar{X}_{n} \rightarrow \theta$가 확률적으로 성립하므로, 우리는 다음과 같습니다.

$$
\operatorname{Pr}\left\{\frac{1}{n} K\left(X_{1} X_{2} \ldots X_{n} \mid n\right)-H_{0}(\theta) \geq \epsilon\right\} \rightarrow 0
$$

반대로, entropy보다 상당히 낮은 복잡성을 가진 시퀀스의 수를 제한할 수 있습니다. AEP(Asymptotic Equipartition Property)로부터, 우리는 시퀀스의 집합을 일반적인 집합(typical set)과 일반적이지 않은 집합(nontypical set)으로 나눌 수 있습니다. 거기에는
<!-- Page 506 -->
최소 $(1-\epsilon) 2^{n\left(H_{0}(\theta)-\epsilon\right)}$ 개의 시퀀스가 일반 집합에 속합니다. 이 일반 시퀀스 중 최대 $2^{n\left(H_{0}(\theta)-c\right)}$ 개는 $n\left(H_{0}(\theta)-c\right)$ 보다 낮은 복잡도를 가질 수 있습니다. 무작위 시퀀스의 복잡도가 $n\left(H_{0}(\theta)-c\right)$ 보다 낮을 확률은 다음과 같습니다.

$$
\begin{aligned}
& \operatorname{Pr}\left(K\left(X^{n} \mid n\right)<n\left(H_{0}(\theta)-c\right)\right) \\
& \quad \leq \operatorname{Pr}\left(X^{n} \notin A_{\epsilon}^{(n)}\right)+\operatorname{Pr}\left(X^{n} \in A_{\epsilon}^{(n)}, K\left(X^{n} \mid n\right)<n\left(H_{0}(\theta)-c\right)\right) \\
& \quad \leq \epsilon+\sum_{x^{n} \in A_{\epsilon}^{(n)}, K\left(x^{n} \mid n\right)<n\left(H_{0}(\theta)-c\right)} p\left(x^{n}\right) \\
& \quad \leq \epsilon+\sum_{x^{n} \in A_{\epsilon}^{(n)}, K\left(x^{n} \mid n\right)<n\left(H_{0}(\theta)-c\right)} 2^{-n\left(H_{0}(\theta)-\epsilon\right)} \\
& \quad \leq \epsilon+2^{n\left(H_{0}(\theta)-c\right)} 2^{-n\left(H_{0}(\theta)-\epsilon\right)} \\
& \quad=\epsilon+2^{-n(c-\epsilon)}
\end{aligned}
$$

이는 $\epsilon, n, c$를 적절하게 선택하면 임의로 작게 만들 수 있습니다. 따라서 높은 확률로 무작위 시퀀스의 Kolmogorov 복잡도는 entropy에 가깝고, 다음과 같은 관계를 얻습니다.

$$
\frac{K\left(X_{1}, X_{2}, \ldots, X_{n} \mid n\right)}{n} \rightarrow H_{0}(\theta) \quad \text { in probability }
$$

# 14.6 보편 확률

컴퓨터에 무작위 프로그램이 입력된다고 가정해 봅시다. 키보드 앞에 앉아 무작위로 키를 누르는 원숭이를 상상해 보십시오. 동등하게, 공정한 동전 던지기 결과를 보편 튜링 기계에 입력하는 것과 같습니다. 어느 경우든 대부분의 문자열은 컴퓨터에 의미가 없을 것입니다. 사람이 터미널에 앉아 무작위로 키를 누르면, 아마도 오류 메시지를 받게 될 것입니다 (즉, 컴퓨터는 null 문자열을 출력하고 중단합니다). 그러나 특정 확률로 의미 있는 것을 입력하게 될 것입니다. 그러면 컴퓨터는 의미 있는 것을 출력할 것입니다. 이 출력 시퀀스는 무작위로 보일까요?

이전 논의에서 길이 $n$인 대부분의 시퀀스는 복잡도가 $n$에 가깝다는 것을 명확히 알 수 있습니다. 입력 프로그램 $p$의 확률이 $2^{-l(p)}$이므로, 더 짧은 프로그램이 더 긴 프로그램보다 훨씬 더 확률적입니다. 그리고 더 짧은 프로그램이 긴 문자열을 생성할 때, 무작위 문자열을 생성하는 것이 아니라 단순히 구조가 설명되는 문자열을 생성합니다.

출력 문자열에 대한 확률 분포는 균일 분포와는 거리가 멉니다. 컴퓨터 유도 분포 하에서는 간단한 문자열이 더 가능성이 높습니다.
<!-- Page 507 -->
같은 길이의 복잡한 문자열보다. 이는 다음과 같이 문자열에 대한 보편 확률 분포를 정의하도록 동기를 부여합니다.

정의 문자열 $x$의 보편 확률은 다음과 같습니다.

$$
P_{\mathcal{U}}(x)=\sum_{p: \mathcal{U}(p)=x} 2^{-l(p)}=\operatorname{Pr}(\mathcal{U}(p)=x)
$$

이는 공정한 동전 던지기 $p_{1}, p_{2}, \ldots$의 시퀀스로 무작위로 선택된 프로그램이 문자열 $x$를 출력할 확률입니다.

이 확률은 여러 면에서 보편적입니다. 우리는 이를 자연에서 그러한 문자열을 관찰할 확률로 간주할 수 있습니다. 암묵적인 믿음은 간단한 문자열이 복잡한 문자열보다 더 가능성이 높다는 것입니다. 예를 들어, 물리학의 법칙을 설명하고 싶다면, 법칙을 설명하는 가장 간단한 문자열을 가장 가능성 있는 것으로 간주할 수 있습니다. 오컴의 면도날로 알려진 이 원칙은 수세기 동안 과학 연구를 안내하는 일반적인 원칙이었습니다. 관찰된 데이터와 일치하는 여러 설명이 있다면 가장 간단한 것을 선택하십시오. 우리의 틀에서 오컴의 면도날은 주어진 문자열을 생성하는 가장 짧은 프로그램을 선택하는 것과 동등합니다.

이 확률 질량 함수는 다음 정리에 의해 보편적이라고 불립니다.

정리 14.6.1 모든 컴퓨터 $\mathcal{A}$에 대해,

$$
P_{\mathcal{U}}(x) \geq c_{\mathcal{A}}^{\prime} P_{\mathcal{A}}(x)
$$

모든 문자열 $x \in\{0,1\}^{*}$에 대해, 여기서 상수 $c_{\mathcal{A}}^{\prime}$는 $\mathcal{U}$와 $\mathcal{A}$에만 의존합니다.

증명: 섹션 14.2의 논의에서, $\mathcal{A}$에 대한 $x$를 출력하는 모든 프로그램 $p^{\prime}$에 대해, 시뮬레이션 프로그램을 접두사로 붙여 생성된 $l\left(p^{\prime}\right)+c_{\mathcal{A}}$보다 길지 않은 길이를 가진 $\mathcal{U}$에 대한 프로그램 $p$가 존재함을 기억합니다. 따라서,

$$
P_{\mathcal{U}}(x)=\sum_{p: \mathcal{U}(p)=x} 2^{-l(p)} \geq \sum_{p^{\prime}: \mathcal{A}\left(p^{\prime}\right)=x} 2^{-l\left(p^{\prime}\right)-c_{\mathcal{A}}}=c_{\mathcal{A}}^{\prime} P_{\mathcal{A}}(x)
$$

계산 가능한 확률 질량 함수에 따라 그려진 모든 시퀀스는 무작위 입력에 대한 컴퓨터 $\mathcal{A}$에 의해 생성된 것으로 간주될 수 있습니다 (확률 역 변환을 무작위 입력에 적용하여). 따라서 보편 확률 분포는 모든 계산 가능한 확률 분포의 혼합을 포함합니다.
<!-- Page 508 -->
주석 (유계 가능도 비율). 특히, 정리 14.6.1은 $X$가 $P_{\mathcal{U}}$에 따라 추출되었는지 대 $P_{\mathcal{A}}$에 따라 추출되었는지에 대한 가설의 가능도 비율 검정이 유계 가능도 비율을 가질 것임을 보장합니다. 만약 $\mathcal{U}$와 $A$가 보편적이라면, 모든 $x$에 대해 $P_{\mathcal{U}}(x) / P_{\mathcal{A}}(x)$는 0과 무한대에서 멀리 떨어져 유계됩니다. 이는 다른 단순 가설 검정 문제(예: Bernoulli($\theta_{1}$) 대 Bernoulli($\theta_{2}$))와 대조적입니다. 이러한 문제에서는 표본 크기가 무한대로 갈 때 가능도 비율이 0 또는 $\infty$로 갑니다. 명백히, 모든 계산 가능한 분포의 혼합인 $P_{\mathcal{U}}$는 어떤 계산 가능한 확률 분포에 따라 추출된 데이터의 실제 분포로서 완전히 기각될 수 없습니다. 그런 의미에서 우리는 우주가 컴퓨터 앞에서 타이핑하는 원숭이의 결과라는 가능성을 기각할 수 없습니다. 그러나 우리는 우주가 무작위라는 가설(컴퓨터 없는 원숭이)을 기각할 수 있습니다.

14.11절에서 우리는 다음을 증명합니다.

$$
P_{\mathcal{U}}(x) \approx 2^{-K(x)}
$$

이는 $K(x)$와 $\log \frac{1}{P_{\mathcal{U}}(x)}$가 보편 알고리즘 복잡도 척도로서 동등한 지위를 가짐을 보여줍니다. 이는 특히 흥미로운데, 왜냐하면 $\log \frac{1}{P_{\mathcal{U}}(x)}$는 보편 확률 분포 $P_{\mathcal{U}}(x)$에 대한 이상적인 코드워드 길이(Shannon 코드워드 길이)이기 때문입니다.

이 절을 타자기 앞의 원숭이 대 컴퓨터 키보드 앞의 원숭이 예시로 마무리합니다. 원숭이가 타자기에서 무작위로 타이핑할 때, 셰익스피어의 모든 작품을 타이핑할 확률(텍스트 길이가 1백만 비트라고 가정)은 $2^{-1,000,000}$입니다. 그러나 원숭이가 컴퓨터 터미널에 앉아 있다면, 셰익스피어를 타이핑할 확률은 이제 $2^{-K(\text{Shakespeare})} \approx 2^{-250,000}$이 됩니다. 이는 극도로 작지만, 원숭이가 단순한 타자기에 앉아 있을 때보다 여전히 지수적으로 더 가능성이 높습니다.

이 예시는 컴퓨터에 대한 무작위 입력이 타자기에 대한 무작위 입력보다 "흥미로운" 출력을 생성할 가능성이 훨씬 높다는 것을 나타냅니다. 우리는 모두 컴퓨터가 지능 증폭기라는 것을 알고 있습니다. 명백히, 그것은 무의미함에서도 의미를 창조합니다.

# 14.7 정지 문제와 콜모고로프 복잡도의 비계산성

다음의 역설적인 문장을 고려하십시오:
이 문장은 거짓입니다.
이 역설은 때때로 두 문장 형태로 제시됩니다:
<!-- Page 509 -->
다음 문장은 거짓입니다.
이전 문장은 참입니다.
이러한 역설들은 에피메니데스의 거짓말쟁이 역설의 변형이며, 자기 참조와 관련된 함정을 보여줍니다. 1931년에 괴델은 이 자기 참조의 아이디어를 사용하여 흥미로운 수학 시스템은 완전하지 않다는 것을 보여주었습니다. 즉, 시스템 내에는 참이지만 시스템 내에서는 증명할 수 없는 명제들이 존재합니다. 이를 달성하기 위해 그는 정리와 증명을 정수로 번역하고 위와 같은 형태의 명제를 구성했으며, 따라서 이 명제는 참 또는 거짓으로 증명될 수 없습니다.

컴퓨터 과학의 정지 문제(halting problem)는 괴델의 불완전성 정리와 매우 밀접하게 관련되어 있습니다. 본질적으로, 이는 어떤 계산 모델에 대해서도 프로그램이 정지할지 아니면 멈추지 않을지(영원히 계속될지)를 결정하는 일반적인 알고리즘은 존재하지 않는다고 말합니다. 이는 특정 프로그램에 대한 명제가 아님을 유의하십시오. 명백히, 정지하거나 영원히 계속되는 것을 쉽게 보여줄 수 있는 많은 프로그램들이 있습니다. 정지 문제는 모든 프로그램에 대해 이 질문에 답할 수 없다고 말합니다. 그 이유는 다시 자기 참조의 아이디어입니다.

실용적인 사람에게 정지 문제는 즉각적인 중요성을 갖지 않을 수 있지만, 컴퓨터로 할 수 있는 것(무한한 메모리와 시간을 고려할 때)과 전혀 할 수 없는 것(정수론의 모든 참 명제를 증명하는 것과 같은) 사이의 경계선으로서 큰 이론적 중요성을 갖습니다. 괴델의 불완전성 정리는 20세기 가장 중요한 수학적 결과 중 하나이며, 그 결과는 여전히 탐구되고 있습니다. 정지 문제는 괴델의 불완전성 정리에 대한 필수적인 예입니다.

정지 문제에 대한 알고리즘의 부재의 결과 중 하나는 콜모고로프 복잡도(Kolmogorov complexity)의 비계산성입니다. 일반적으로 가장 짧은 프로그램을 찾는 유일한 방법은 모든 짧은 프로그램을 시도해보고 어떤 프로그램이 작업을 수행할 수 있는지 확인하는 것입니다. 그러나 언제든지 짧은 프로그램 중 일부는 아직 정지하지 않았을 수 있으며, 그것들이 정지할지 여부와 무엇을 출력할지 여부를 알 수 있는 효과적인(유한한 기계적인) 방법은 없습니다. 따라서 주어진 문자열을 출력하는 가장 짧은 프로그램을 찾는 효과적인 방법은 없습니다.

콜모고로프 복잡도의 비계산성은 베리 역설(Berry paradox)의 예입니다. 베리 역설은 10단어 미만으로 명명할 수 없는 가장 짧은 숫자를 묻습니다. $1,101,121$과 같은 숫자는 정의하는 표현 자체가 10단어 미만이므로 해답이 될 수 없습니다. 이는 명명 가능한(nameable) 및 기술 가능한(describable)이라는 용어의 문제를 보여줍니다. 이 용어들은 엄격한 의미 없이 사용하기에는 너무 강력합니다. 만약 우리가 "컴퓨터에 출력하기 위해 설명될 수 있는"이라는 의미로 스스로를 제한한다면, 가장 작은 설명할 수 없는 숫자를 말함으로써 베리 역설을 해결할 수 있습니다.
<!-- Page 510 -->
존재하지만 계산할 수 없습니다. 이 "설명"은 숫자를 계산하기 위한 프로그램이 아닙니다. E. F. Beckenbach은 숫자를 지루하거나 흥미로운 것으로 분류하는 데 비슷한 문제가 있음을 지적했습니다. 가장 작은 지루한 숫자는 흥미로워야 합니다.

이 장의 시작 부분에서 언급했듯이, 실무자가 주어진 문자열에 대한 가장 짧은 컴퓨터 프로그램을 찾을 것이라고는 실제로 예상하지 않습니다. 가장 짧은 프로그램은 계산할 수 없지만, 주어진 문자열을 생성하는 프로그램이 점점 더 많이 발견됨에 따라 Kolmogorov 복잡성에 대한 상한 추정치는 실제 Kolmogorov 복잡성으로 수렴합니다. (물론 문제는 가장 짧은 프로그램을 찾았지만 더 짧은 프로그램이 존재하지 않는다는 것을 결코 알지 못할 수도 있다는 것입니다.) Kolmogorov 복잡성은 계산할 수 없지만, 무작위성과 추론에 대한 질문을 고려할 수 있는 프레임워크를 제공합니다.

# $14.8 \Omega$

이 섹션에서는 Chaitin의 신비롭고 마법 같은 숫자 $\Omega$를 소개하며, 이는 매우 흥미로운 속성을 가지고 있습니다.

## 정의

$$
\Omega=\sum_{p: \mathcal{U}(p) \text { halts }} 2^{-l(p)}
$$

$\Omega=\operatorname{Pr}(\mathcal{U}(p)$ halts)는 주어진 범용 컴퓨터에 입력이 베르누이 $\left(\frac{1}{2}\right)$ 프로세스에 따라 그려진 이진 문자열일 때 컴퓨터가 멈출 확률임을 주목하십시오.

멈추는 프로그램은 접두사 없는(prefix-free) 것이므로, 그 길이들은 Kraft 부등식을 만족하며, 따라서 위의 합은 항상 0과 1 사이입니다. $\Omega_{n}=. \omega_{1} \omega_{2} \cdots \omega_{n}$을 $\Omega$의 처음 $n$ 비트라고 합시다. $\Omega$의 속성은 다음과 같습니다.

1. $\Omega$는 계산 불가능합니다. 임의의 프로그램이 멈추는지 여부를 확인하는 효과적인(유한하고 기계적인) 방법이 없으므로(정지 문제), $\Omega$를 계산하는 효과적인 방법도 없습니다.
2. $\Omega$는 "현자의 돌"입니다. $n$ 비트 정확도로 $\Omega$를 알면 $n$ 비트 미만으로 작성할 수 있는 증명 가능하거나 유한하게 반증 가능한 모든 수학적 정리의 진실을 결정할 수 있습니다. 실제로 이는 $\Omega$의 $n$ 비트를 알면 $n$ 비트 정리를 결정하는 효과적인 절차가 있다는 것을 의미합니다. 이 절차는 임의로 길지만 유한한 시간이 걸릴 수 있습니다. 물론 $\Omega$를 알지 못하면 진실 또는 거짓을 확인할 수 없습니다.
<!-- Page 511 -->
모든 정리를 효과적인 절차로 증명할 수 있습니다(괴델의 불완전성 정리).
$\Omega$의 $n$ 비트를 사용하는 절차의 기본 아이디어는 간단합니다. 우리는 중단되는 프로그램이 기여하는 질량 $2^{-l(p)}$의 합이 우리가 받은 $\Omega$의 잘린 버전인 $\Omega_{n}=0 . \omega_{1} \omega_{2} \cdots \omega_{n}$와 같거나 초과할 때까지 모든 프로그램을 실행합니다. 그러면,

$$
\Omega-\Omega_{n}<2^{-n}
$$

이므로, 중단되는 프로그램에 대한 $2^{-l(p)}$의 모든 추가적인 기여의 합 또한 $2^{-n}$보다 작다는 것을 압니다. 이는 아직 중단되지 않은 길이가 $\leq n$인 프로그램은 결코 중단되지 않을 것임을 의미하며, 이는 길이가 $\leq n$인 모든 프로그램의 중단 또는 비중단 여부를 결정할 수 있게 합니다.
증명을 완료하기 위해, 컴퓨터가 모든 가능한 프로그램을 "병렬로" 실행하여 중단되는 모든 프로그램이 결국 중단됨을 발견할 수 있음을 보여야 합니다. 먼저, null 프로그램 $\Lambda$부터 시작하여 가능한 모든 프로그램을 나열합니다:

$$
\Lambda, 0,1,00,01,10,11,000,001,010,011, \ldots
$$

그런 다음 첫 번째 사이클에서는 컴퓨터가 $\Lambda$의 한 클럭 사이클을 실행하도록 합니다. 다음 사이클에서는 컴퓨터가 $\Lambda$의 두 클럭 사이클과 프로그램 0의 두 클럭 사이클을 실행하도록 합니다. 세 번째 사이클에서는 처음 세 프로그램 각각의 세 클럭 사이클을 실행하도록 합니다. 이런 식으로 컴퓨터는 결국 모든 가능한 프로그램을 실행하고 점점 더 긴 시간 동안 실행하게 되므로, 어떤 프로그램이라도 중단된다면 결국 중단됨을 발견하게 될 것입니다. 컴퓨터는 어떤 프로그램이 실행되고 있는지, 그리고 사이클 번호를 추적하여 중단되는 모든 프로그램의 목록을 생성할 수 있습니다. 따라서 우리는 $n$ 비트 미만의 어떤 프로그램이 중단될지 여부를 궁극적으로 알게 될 것입니다. 이는 컴퓨터가 정리의 증명이나, 정리가 $n$ 비트 미만으로 표현될 수 있다면 정리에 대한 반례를 찾을 수 있게 합니다. $\Omega$의 지식은 이전에 증명할 수 없었던 정리를 증명 가능한 정리로 바꿉니다. 여기서 $\Omega$는 오라클 역할을 합니다.

$\Omega$는 이 점에서 마법처럼 보이지만, 동일한 정보를 담고 있는 다른 숫자들도 있습니다. 예를 들어, 프로그램 목록을 가져와서 $i$번째 비트가 프로그램 $i$가 중단되는지 여부를 나타내는 실수로 구성한다면, 이 숫자 또한 수학에서 유한하게 반증 가능한 질문을 결정하는 데 사용될 수 있습니다. 이 숫자는 정보 내용 면에서 매우 희석되어 있는데, 왜냐하면 약 $2^{n}$이 필요하기 때문입니다.
<!-- Page 512 -->
이 지시 함수의 비트들을 사용하여 $n$ 비트 프로그램이 정지하는지 여부를 결정할 수 있습니다. $2^{n}$ 비트를 사용하면 $n$보다 짧은 길이의 모든 프로그램이 정지하는지 여부를 계산 없이 즉시 알 수 있습니다. 그러나 $\Omega$는 알고리즘적으로 무작위적이고 압축 불가능하므로 이 정보의 가장 간결한 표현입니다.

$\Omega$를 사용하여 해결할 수 있는 질문은 무엇입니까? 정수론에서 흥미로운 문제의 상당수는 반례를 찾는 것으로 설명될 수 있습니다. 예를 들어, 정수 $x, y, z, n$을 검색하고 페르마의 마지막 정리에 대한 반례를 찾을 경우에만 정지하는 프로그램을 작성하는 것은 간단합니다. 페르마의 마지막 정리는 다음과 같이 말합니다.

$$
x^{n}+y^{n}=z^{n}
$$

는 $n \geq 3$인 정수에서 해가 없습니다. 또 다른 예는 모든 짝수가 두 소수의 합이라고 말하는 골드바흐의 추측입니다. 우리의 프로그램은 2부터 시작하는 모든 짝수를 검색하고, 그보다 작은 모든 소수를 확인하고, 두 소수의 합으로 분해하는 것을 찾을 것입니다. 그러한 분해가 없는 짝수를 발견하면 정지할 것입니다. 이 프로그램이 정지하는지 여부를 아는 것은 골드바흐의 추측의 진실성을 아는 것과 동등합니다.

또한 모든 증명을 검색하고 필요한 정리에 대한 증명을 찾을 경우에만 정지하는 프로그램을 설계할 수도 있습니다. 이 프로그램은 정리가 유한한 증명을 가지고 있다면 결국 정지할 것입니다. 따라서 $\Omega$의 $n$ 비트를 알면 유한한 증명을 가지거나 유한하게 반증될 수 있으며 $n$ 비트 미만으로 표현될 수 있는 모든 정리의 참 또는 거짓을 찾을 수 있습니다.
3. $\Omega$는 알고리즘적으로 무작위적입니다.

정리 14.8.1 $\Omega$는 상수보다 더 많이 압축될 수 없습니다. 즉, 상수가 존재하여 다음과 같습니다.

$$
K\left(\omega_{1} \omega_{2} \ldots \omega_{n}\right) \geq n-c \quad \text { 모든 } n에 대해 }
$$

증명: $\Omega$의 $n$ 비트를 받으면 길이가 $\leq n$인 모든 프로그램이 정지하는지 여부를 결정할 수 있다는 것을 알고 있습니다. $K\left(\omega_{1} \omega_{2} \cdots\right.$ $\omega_{n}$ ) 비트를 사용하여 $\Omega$의 $n$ 비트를 계산한 다음, 해당 출력과 함께 정지하는 길이가 $\leq n$인 모든 프로그램 목록을 생성할 수 있습니다. 이 목록에 없는 첫 번째 문자열 $x_{0}$을 찾습니다. 그러면 문자열 $x_{0}$은 $K\left(x_{0}\right)>n$인 가장 짧은 문자열입니다.
<!-- Page 513 -->
이 프로그램이 $x_{0}$를 출력하는 복잡도는 $K\left(\Omega_{n}\right)+c$이며, 이는 $x_{0}$에 대한 가장 짧은 프로그램보다 길어야 합니다. 따라서,

$$
K\left(\Omega_{n}\right)+c \geq K\left(x_{0}\right)>n
$$

모든 $n$에 대해 성립합니다. 그러므로 $K\left(\omega_{1} \omega_{2} \cdots \omega_{n}\right)>n-c$이며, $\Omega$는 상수 이상으로 압축될 수 없습니다.

# 14.9 보편적 도박

도박꾼이 순차적으로 $x \in\{0,1\}^{*}$ 시퀀스에 대해 도박을 한다고 가정해 봅시다. 그는 시퀀스의 출처에 대해 전혀 알지 못합니다. 그는 각 비트에 대해 공정한 배당률(2 대 1)을 받습니다. 그는 어떻게 도박을 해야 할까요? 만약 그가 문자열의 분포를 알고 있다면, 제6장에서 보여진 것처럼 최적의 성장률 속성 때문에 비례 베팅을 사용할 수 있습니다. 만약 그가 문자열이 자연적으로 발생했다고 믿는다면, 복잡한 문자열보다 단순한 문자열이 더 가능성이 높다고 직관적으로 생각할 수 있습니다. 따라서, 비례 베팅의 아이디어를 확장한다면, 그는 문자열의 보편적 확률에 따라 베팅할 수 있습니다. 참고로, 도박꾼이 사전에 문자열 $x$를 알고 있다면, 다음 기호에 대해 매번 자신의 모든 재산을 걸어 $2^{l(x)}$ 배만큼 부를 늘릴 수 있습니다. 베팅 방식 $b(x), \sum b(x)=1$과 관련된 재산 $S(x)$는 다음과 같이 주어집니다.

$$
S(x)=2^{l(x)} b(x)
$$

도박꾼이 문자열 $x$에 대해 $b(x)=2^{-K(x)}$로 베팅한다고 가정해 봅시다. 이 베팅 전략은 보편적 도박이라고 불릴 수 있습니다. 우리는 베팅의 합이 다음과 같음을 주목합니다.

$$
\sum_{x} b(x)=\sum_{x} 2^{-K(x)} \leq \sum_{p: p \text { halts }} 2^{-l(p)}=\Omega \leq 1
$$

그리고 그는 모든 돈을 사용하지 않을 것입니다. 단순화를 위해, 그는 나머지는 버린다고 가정합시다. 예를 들어, 시퀀스 $x=0110$에 대한 베팅 $b(0110)$으로 인한 재산의 양은 $2^{l(x)} b(x)=2^{4} b(0110)$에 $x$를 확장하는 시퀀스에 대한 모든 베팅 $b(0110 \ldots)$에서 얻은 금액이 더해진 것입니다.

그러면 다음과 같은 정리가 있습니다.
정리 14.9.1 보편적 도박을 사용하여 도박꾼이 시퀀스에서 달성하는 재산의 로그에 시퀀스의 복잡도를 더한 값은 시퀀스의 길이보다 작지 않거나,

$$
\log S(x)+K(x) \geq l(x)
$$
<!-- Page 514 -->
주석 이 내용은 6장에서 다룬 도박 보존 정리 $W^{*}+H=\log m$의 대응되는 내용입니다.

증명: 증명은 보편 도박 방식 $b(x)=2^{-K(x)}$으로부터 직접적으로 도출됩니다. 왜냐하면

$$
S(x)=\sum_{x^{\prime} \supseteq x} 2^{l(x)} b\left(x^{\prime}\right) \geq 2^{l(x)} 2^{-K(x)}
$$

여기서 $x^{\prime} \supseteq x$는 $x$가 $x^{\prime}$의 접두어임을 의미합니다. 로그를 취하면 정리가 확립됩니다.

이 결과는 여러 방식으로 이해될 수 있습니다. 유한한 Kolmogorov complexity를 갖는 무한 시퀀스 $x$에 대해,

$$
S\left(x_{1} x_{2} \cdots x_{l}\right) \geq 2^{l-K(x)}=2^{l-c}
$$

모든 $l$에 대해 성립합니다. $2^{l}$은 공정한 확률로 $l$번의 도박에서 얻을 수 있는 최대 금액이므로, 이 방식은 시퀀스를 미리 알고 있는 방식과 점근적으로 동일한 성능을 보입니다. 예를 들어, $x=\pi_{1} \pi_{2} \cdots \pi_{n} \cdots$가 $\pi$의 전개에서 나오는 숫자라면, 시간 $n$에서의 자산은 모든 $n$에 대해 $S_{n}=S\left(x^{n}\right) \geq 2^{n-c}$가 될 것입니다.

만약 문자열이 매개변수 $p$를 갖는 베르누이 과정에 의해 실제로 생성된다면,

$$
S\left(X_{1} \ldots X_{n}\right) \geq 2^{n-n H_{0}\left(\bar{X}_{n}\right)-2 \log n-c} \approx 2^{n\left(1-H_{0}(p)-2 \frac{\log n}{n}-\frac{c}{n}\right)}
$$

이는 6장에서와 같이 도박꾼이 사전에 분포를 알고 있을 때 달성되는 속도와 첫 번째 근사치에서 동일합니다.

예시를 통해 우리는 무작위 시퀀스에 대한 보편 도박 방식이 실제 분포에 대한 사전 지식을 사용하는 방식과 점근적으로 동일한 성능을 보인다는 것을 알 수 있습니다.

# 14.10 오컴의 면도날

과학 연구의 여러 분야에서 관찰된 데이터에 대한 다양한 설명 중에서 선택하는 것이 중요합니다. 설명을 선택한 후, 우리는 추론된 법칙에서 발생하는 예측에 신뢰 수준을 할당하고자 합니다. 예를 들어, 라플라스는 기록된 역사상 매일 해가 떴다는 사실을 고려하여 내일 해가 다시 뜰 확률을 고려했습니다. 라플라스의 해결책은 해가 뜨는 것을 알려지지 않은 매개변수 $\theta$를 갖는 베르누이 $(\theta)$ 과정으로 가정하는 것이었습니다. 그는 $\theta$가 단위 구간에서 균등하게 분포한다고 가정했습니다. 이를 사용하여
<!-- Page 515 -->
관측된 데이터를 바탕으로, 그는 내일 해가 다시 뜰 확률을 계산했고 그 결과는 다음과 같았습니다.

$$
\begin{aligned}
& P\left(X_{n+1}=1 \mid X_{n}=1, X_{n-1}=1, \ldots, X_{1}=1\right) \\
& \quad=\frac{P\left(X_{n+1}=1, X_{n}=1, X_{n-1}=1, \ldots, X_{1}=1\right)}{P\left(X_{n}=1, X_{n-1}=1, \ldots, X_{1}=1\right)} \\
& \quad=\frac{\int_{0}^{1} \theta^{n+1} d \theta}{\int_{0}^{1} \theta^{n} d \theta} \\
& \quad=\frac{n+1}{n+2}
\end{aligned}
$$

이는 1일부터 n일까지 해가 떴다는 조건 하에 n+1일째에 해가 뜰 확률로 제시되었습니다.

Kolmogorov 복잡성과 보편 확률의 아이디어를 사용하여 이 문제에 대한 대안적인 접근 방식을 제공할 수 있습니다. 보편 확률 하에서, 지금까지 시퀀스에서 1이 n개 관측된 후 다음에 1을 볼 확률을 계산해 보겠습니다. 다음 기호가 1일 조건부 확률은 초기 세그먼트 $1^{n}$을 가지고 다음 비트가 1인 모든 시퀀스의 확률과 초기 세그먼트 $1^{n}$을 가진 모든 시퀀스의 확률의 비율입니다. 가장 간단한 프로그램이 대부분의 확률을 차지하므로, 다음에 1이 올 확률을 "영원히 1을 출력하라"는 프로그램의 확률로 근사할 수 있습니다. 따라서,

$$
\sum_{y} p\left(1^{n} 1 y\right) \approx p\left(1^{\infty}\right)=c>0
$$

다음에 0이 올 확률을 추정하는 것은 더 어렵습니다. $1^{n} 0 \ldots$을 출력하는 모든 프로그램은 n에 대한 설명을 제공하므로, 그 길이는 적어도 $K(n)$이어야 합니다. 이는 대부분의 n에 대해 약 $\log n+O(\log \log n)$이며, 따라서 이차 항을 무시하면 다음과 같습니다.

$$
\sum_{y} p\left(1^{n} 0 y\right) \approx p\left(1^{n} 0\right) \approx 2^{-\log n} \approx \frac{1}{n}
$$

따라서 다음에 0을 관측할 조건부 확률은 다음과 같습니다.

$$
p\left(0 \mid 1^{n}\right)=\frac{p\left(1^{n} 0\right)}{p\left(1^{n} 0\right)+p\left(1^{\infty}\right)} \approx \frac{1}{c n+1}
$$

이는 Laplace가 도출한 결과 $p\left(0 \mid 1^{n}\right)=1 /(n+1)$과 유사합니다.
<!-- Page 516 -->
이러한 유형의 논증은 과학 연구를 지배하는 일반적인 원칙인 오컴의 면도날의 특수한 경우로, 복잡성에 따라 가능한 설명을 가중합니다. 윌리엄 오브 오컴은 "필요 없이 복수성을 두지 말라"고 말했습니다: 설명은 필요 이상으로 복수화되어서는 안 됩니다 [516]. 결국 우리는 관찰된 데이터와 일치하는 가장 간단한 설명을 선택합니다. 예를 들어, 수성의 근일점 세차 운동을 설명하기 위해 중력 법칙에 $c / r^{3}$의 보정 계수를 받아들이는 것보다 일반 상대성 이론을 받아들이는 것이 더 쉽습니다. 왜냐하면 일반 이론은 "패치된" 뉴턴 이론보다 적은 가정으로 더 많은 것을 설명하기 때문입니다.

# 14.11 콜모고로프 복잡성과 보편 확률

이제 콜모고로프 복잡성과 보편 확률 간의 동등성을 증명합니다. 기본적인 정의를 반복하면서 시작하겠습니다.

$$
\begin{aligned}
K(x) & =\min _{p: \mathcal{U}(p)=x} l(p) \\
P_{\mathcal{U}}(x) & =\sum_{p: \mathcal{U}(p)=x} 2^{-l(p)}
\end{aligned}
$$

정리 14.11.1 ($K(x)$와 $\log \frac{1}{P_{\mathcal{U}}(x)}$의 동등성.) $x$에 독립적인 상수 $c$가 존재하여, 모든 문자열 $x$에 대해 다음이 성립합니다.

$$
2^{-K(x)} \leq P_{\mathcal{U}}(x) \leq c 2^{-K(x)}
$$

따라서 문자열 $x$의 보편 확률은 본질적으로 콜모고로프 복잡성에 의해 결정됩니다.

비고 이는 $K(x)$와 $\log \frac{1}{P_{\mathcal{U}}(x)}$가 보편 복잡성 척도로서 동등한 지위를 갖는다는 것을 의미합니다. 왜냐하면

$$
K(x)-c^{\prime} \leq \log \frac{1}{P_{\mathcal{U}}(x)} \leq K(x)
$$

두 개의 다른 컴퓨터 $K_{\mathcal{U}}$와 $K_{\mathcal{U}^{\prime}}$에 대해 정의된 복잡성이 $\mid K_{\mathcal{U}}(x)-$ $K_{\mathcal{U}^{\prime}}(x) \mid$가 유계이면 본질적으로 동등한 복잡성 척도임을 기억하십시오. 정리 14.11.1은 $K_{\mathcal{U}}(x)$와 $\log \frac{1}{P_{\mathcal{U}}(x)}$가 본질적으로 동등한 복잡성 척도임을 보여줍니다.

콜모고로프 복잡성에서 $K(x)$와 $\log \frac{1}{P_{\mathcal{U}}(x)}$의 관계와 정보 이론에서 $H(X)$와 $\log \frac{1}{p(x)}$의 관계 사이의 놀라운 유사성에 주목하십시오. 이상적인 섀넌 코드 길이 할당
<!-- Page 517 -->
$l(x)=\log \frac{1}{p(x)}$ 는 평균 설명 길이 $H(X)$ 를 달성하며, Kolmogorov 복잡성 이론에서는 이상적인 설명 길이 $\log \frac{1}{P_{\mathcal{U}}(x)}$ 가 $K(x)$ 와 거의 같습니다. 따라서 $\log \frac{1}{p(x)}$ 는 알고리즘적 및 확률적 설정 모두에서 $x$ 의 설명 복잡성에 대한 자연스러운 개념입니다.

(14.90)의 상한은 정의로부터 명백하지만, 하한은 증명하기가 더 어렵습니다. 이 결과는 $x$ 를 출력하는 무한한 수의 프로그램이 존재하기 때문에 매우 놀랍습니다. 어떤 프로그램으로부터든 관련 없는 명령어를 패딩하여 더 긴 프로그램을 생성할 수 있습니다. 이 정리는 그러한 프로그램이 무한히 많음에도 불구하고, 보편 확률은 본질적으로 가장 큰 항인 $2^{-K(x)}$ 에 의해 결정된다는 것을 증명합니다. $P_{\mathcal{U}}(x)$ 가 크면 $K(x)$ 는 작고, 그 반대도 마찬가지입니다.

그러나 상한을 덜 놀랍게 만드는 또 다른 관점이 있습니다. 문자열에 대한 계산 가능한 확률 질량 함수 $p(x)$ 를 고려하십시오. 이 질량 함수를 사용하여 소스에 대한 Shannon-Fano 코드(5.9절)를 구성한 다음 해당 코드워드에 해당하는 각 문자열을 설명할 수 있으며, 이는 $\log \frac{1}{p(x)}$ 의 길이를 갖습니다. 따라서 계산 가능한 분포에 대해 문자열의 설명이 $\log \frac{1}{p(x)}+c$ 비트 이하가 되도록 구성할 수 있으며, 이는 Kolmogorov 복잡성 $K(x)$ 에 대한 상한입니다. $P_{\mathcal{U}}(x)$ 가 계산 가능한 확률 질량 함수가 아니더라도, 아래에 설명된 다소 복잡한 트리 구성 절차를 사용하여 문제를 해결할 수 있습니다.

증명: (정리 14.11.1). 첫 번째 부등식은 간단합니다. $p^{*}$ 를 $x$ 에 대한 가장 짧은 프로그램이라고 합시다. 그러면

$$
P_{\mathcal{U}}(x)=\sum_{p: \mathcal{U}(p)=x} 2^{-l(p)} \geq 2^{-l\left(p^{*}\right)}=2^{-K(x)}
$$

우리가 보여주고자 했던 바와 같습니다.
두 번째 부등식을 다음과 같이 다시 쓸 수 있습니다.

$$
K(x) \leq \log \frac{1}{P_{\mathcal{U}}(x)}+c
$$

증명에서의 우리의 목표는 $P_{\mathcal{U}}(x)$ 가 높은 문자열을 설명하는 짧은 프로그램을 찾는 것입니다. 명백한 아이디어는 $P_{\mathcal{U}}(x)$ 를 기반으로 한 일종의 Huffman 코딩이지만, $P_{\mathcal{U}}(x)$ 는 효과적으로 계산할 수 없으므로 Huffman 코딩을 사용하는 절차는 컴퓨터에서 구현할 수 없습니다. 마찬가지로 Shannon-Fano 코드를 사용하는 과정도 구현할 수 없습니다. 그러나 Shannon-Fano 코드 트리를 가지고 있다면, 재구성할 수 있습니다.
<!-- Page 518 -->
문자열을 트리에서 해당 노드를 찾아 검색합니다. 이것이 다음 트리 구성 절차의 기초입니다.

$P_{\mathcal{U}}(x)$의 계산 불가능성 문제를 극복하기 위해 수정된 접근 방식을 사용하여 코드를 직접 구성하려고 합니다. Huffman 코딩과 달리 이 접근 방식은 최소 기대 코드어 길이 측면에서 최적이 아닙니다. 그러나 각 $x$에 대한 코드어의 길이가 $\log \frac{1}{P_{\mathcal{U}}(x)}$에 상수만큼 가까운 코드를 도출하기에 충분합니다.

증명의 세부 사항을 살펴보기 전에 접근 방식을 개략적으로 설명하겠습니다. 높은 확률을 가진 문자열이 낮은 깊이를 갖도록 코드 트리를 구성하려고 합니다. 문자열의 확률을 계산할 수 없으므로 사전에 트리에서의 문자열 깊이를 알 수 없습니다. 대신, $P_{\mathcal{U}}(x)$에 대한 추정치가 개선됨에 따라 $x$를 루트에 더 가까운 노드에 할당하면서 $x$를 트리의 노드에 순차적으로 할당합니다. 컴퓨터가 트리를 재현하고 문자열 $x$에 해당하는 가장 낮은 깊이 노드를 사용하여 문자열을 재구성할 수 있기를 바랍니다.

이제 프로그램과 해당 출력의 집합 $\{(p, x)\}$를 고려합니다. 이 쌍을 트리에 할당하려고 합니다. 그러나 즉시 문제에 직면합니다. 주어진 문자열에 대해 무한한 수의 프로그램이 있으며 낮은 깊이의 노드가 충분하지 않습니다. 그러나 우리가 보여줄 것처럼 프로그램-출력 쌍 목록을 다듬으면 트리에 할당할 수 있는 더 관리하기 쉬운 목록을 정의할 수 있습니다. 다음으로, $\log \frac{1}{P_{\mathcal{U}}(x)}$의 길이를 갖는 $x$에 대한 프로그램의 존재를 보여줍니다.

트리 구성 절차: 범용 컴퓨터 $\mathcal{U}$에 대해 섹션 14.8에 설명된 기술을 사용하여 모든 프로그램을 시뮬레이션합니다. 모든 이진 프로그램을 나열합니다.

$$
\Lambda, 0,1,00,01,10,11,000,001,010,011, \ldots
$$

그런 다음 첫 번째 단계에서 컴퓨터가 $\Lambda$의 클럭 사이클 하나를 실행하도록 합니다. 다음 단계에서는 컴퓨터가 $\Lambda$의 두 클럭 사이클과 프로그램 0의 두 클럭 사이클을 실행하도록 합니다. 세 번째 단계에서는 컴퓨터가 처음 세 프로그램 각각의 세 클럭 사이클을 실행하도록 합니다. 이 방식으로 컴퓨터는 결국 모든 가능한 프로그램을 실행하고 점점 더 오래 실행하게 되므로, 어떤 프로그램이라도 중단되면 결국 중단되는 것으로 발견될 것입니다. 이 방법을 사용하여 중단되는 모든 프로그램의 목록을 중단되는 순서대로 해당 출력과 함께 생성합니다. 각 프로그램과 해당 출력 $(p_{k}, x_{k})$에 대해 $n_{k}$를 계산하며, 이는 $P_{\mathcal{U}}(x)$의 현재 추정치에 해당하도록 선택됩니다. 구체적으로,

$$
n_{k}=\left\lceil\log \frac{1}{\hat{P}_{\mathcal{U}}\left(x_{k}\right)}\right\rceil
$$
<!-- Page 519 -->
여기

$$
\hat{P}_{\mathcal{U}}\left(x_{k}\right)=\sum_{\left(p_{i}, x_{i}\right): x_{i}=x_{k}, i \leq k} 2^{-l\left(p_{i}\right)}
$$

$x_{k}=x$인 시간의 부분 수열에서 $\hat{P}_{\mathcal{U}}\left(x_{k}\right) \uparrow P_{\mathcal{U}}(x)$임을 주목하십시오. 이제 트리를 구성할 준비가 되었습니다. 중단되는 프로그램의 삼중항 리스트 $\left(p_{k}, x_{k}, n_{k}\right)$에 추가함에 따라, 일부를 이진 트리의 노드에 매핑합니다. 구성의 목적을 위해, 특정 $x_{k}$에 해당하는 모든 $n_{i}$가 고유하도록 보장해야 합니다. 이를 보장하기 위해 이전 삼중항과 동일한 $x$ 및 $n$을 가진 삼중항을 리스트에서 제거합니다. 이렇게 하면 주어진 $x$에 해당하는 트리의 각 레벨에 최대 하나의 노드만 존재하게 됩니다.

새로운 리스트를 $\left\{\left(p_{i}^{\prime}, x_{i}^{\prime}, n_{i}^{\prime}\right): i=1,2,3, \ldots\right\}$라고 합시다. 선별된 리스트에서 삼중항 $\left(p_{k}^{\prime}, x_{k}^{\prime}, n_{k}^{\prime}\right)$을 레벨 $n_{k}^{\prime}+1$의 첫 번째 사용 가능한 노드에 할당합니다. 노드가 할당되는 즉시, 해당 노드의 모든 자손은 할당에 사용할 수 없게 됩니다. (이는 할당을 접두사-자유로 유지합니다.)

예시를 통해 이를 설명하겠습니다.

$$
\begin{aligned}
& \left(p_{1}, x_{1}, n_{1}\right)=(10111,1110,5), n_{1}=5 \text { 는 } P_{\mathcal{U}}\left(x_{1}\right) \geq 2^{-l\left(p_{1}\right)}=2^{-5} \text { 이기 때문입니다. } \\
& \left(p_{2}, x_{2}, n_{2}\right)=(11,10,2), \quad n_{2}=2 \text { 는 } P_{\mathcal{U}}\left(x_{2}\right) \geq 2^{-l\left(p_{2}\right)}=2^{-2} \text { 이기 때문입니다. } \\
& \left(p_{3}, x_{3}, n_{3}\right)=(0,1110,1), \quad n_{3}=1 \text { 는 } P_{\mathcal{U}}\left(x_{3}\right) \geq 2^{-l\left(p_{3}\right)}+2^{-l\left(p_{1}\right)} \\
& =2^{-5}+2^{-1} \\
& \geq 2^{-1} \text { 이기 때문입니다. } \\
& \left(p_{4}, x_{4}, n_{4}\right)=(1010,1111,4), \quad n_{4}=4 \text { 는 } P_{\mathcal{U}}\left(x_{4}\right) \geq 2^{-l\left(p_{4}\right)}=2^{-4} \text { 이기 때문입니다. } \\
& \left(p_{5}, x_{5}, n_{5}\right)=(101101,1110,1), n_{5}=1 \text { 는 } P_{\mathcal{U}}\left(x_{5}\right) \geq 2^{-1}+2^{-5}+2^{-5} \\
& \geq 2^{-1} \text { 이기 때문입니다. } \\
& \left(p_{6}, x_{6}, n_{6}\right)=(100,1,3), \quad n_{6}=3 \text { 는 } P_{\mathcal{U}}\left(x_{6}\right) \geq 2^{-l\left(p_{6}\right)}=2^{-3} \text { 이기 때문입니다. }
\end{aligned}
$$

문자열 $x=(1110)$이 리스트의 1, 3, 5번째 위치에 나타나지만, $n_{3}=n_{5}$임을 주목하십시오. 확률 추정치 $\hat{P}_{\mathcal{U}}(1110)$은 $\left(p_{5}, x_{5}, n_{5}\right)$가 잘림을 통과할 만큼 충분히 증가하지 않았습니다. 따라서 선별된 리스트는 다음과 같습니다.

$$
\begin{aligned}
& \left(p_{1}^{\prime}, x_{1}^{\prime}, n_{1}^{\prime}\right)=(10111,1110,5) \\
& \left(p_{2}^{\prime}, x_{2}^{\prime}, n_{2}^{\prime}\right)=(11,10,2) \\
& \left(p_{3}^{\prime}, x_{3}^{\prime}, n_{3}^{\prime}\right)=(0,1110,1) \\
& \left(p_{4}^{\prime}, x_{4}^{\prime}, n_{4}^{\prime}\right)=(1010,1111,4) \\
& \left(p_{5}^{\prime}, x_{5}^{\prime}, n_{5}^{\prime}\right)=(100,1,3)
\end{aligned}
$$

선별된 리스트를 트리의 노드에 할당하는 것은 그림 14.3에 설명되어 있습니다.
<!-- Page 520 -->

그림 14.3. 노드 할당.

예시에서는 삼중항을 할당할 수 있는 $n_{k}+1$ 레벨의 노드를 찾을 수 있었습니다. 이제 할당을 완료할 수 있을 만큼 항상 충분한 노드가 있음을 증명하겠습니다. Kraft 부등식이 만족되는 경우에만 삼중항을 노드에 할당할 수 있습니다.

이제 프라임을 제거하고 (14.97)에 설명된 선별된 목록만 다루겠습니다. Kraft 부등식의 무한 합으로 시작하여 출력 문자열에 따라 분할합니다.

$$
\sum_{k=1}^{\infty} 2^{-\left(n_{k}+1\right)}=\sum_{x \in\{0,1\}^{*}} \sum_{k: x_{k}=x} 2^{-\left(n_{k}+1\right)}
$$

그런 다음 내부 합을 다음과 같이 작성합니다.

$$
\sum_{k: x_{k}=x} 2^{-\left(n_{k}+1\right)}=2^{-1} \sum_{k: x_{k}=x} 2^{-n_{k}}
$$
<!-- Page 521 -->
$$
\begin{aligned}
& \leq 2^{-1}\left(2^{\left\lfloor\log P_{U}(x)\right\rfloor}+2^{\left\lfloor\log P_{U}(x)\right\rfloor-1}+2^{\left\lfloor\log P_{U}(x)\right\rfloor-2}+\cdots\right) \\
& =2^{-1} 2^{\left\lfloor\log P_{U}(x)\right\rfloor}\left(1+\frac{1}{2}+\frac{1}{4}+\cdots\right) \\
& =2^{-1} 2^{\left\lfloor\log P_{U}(x)\right\rfloor} 2 \\
& \leq P_{U}(x)
\end{aligned}
$$

여기서 (14.100)은 각 레벨마다 특정 $x$를 출력하는 노드가 최대 하나만 존재하기 때문에 참입니다. 더 정확하게 말하면, 특정 출력 문자열 $x$에 대한 winnowed 리스트의 $n_{k}$들은 모두 다른 정수입니다. 따라서,

$$
\sum_{k} 2^{-\left(n_{k}+1\right)} \leq \sum_{x} \sum_{k: x_{k}=x} 2^{-\left(n_{k}+1\right)} \leq \sum_{x} P_{U}(x) \leq 1
$$

이고, 우리는 삼중항으로 레이블이 지정된 트리를 구성할 수 있습니다.
위에 구성된 트리가 주어진다면, $x$를 출력하는 가장 낮은 깊이의 노드까지의 경로를 통해 주어진 $x$를 쉽게 식별할 수 있습니다. 이 노드를 $\tilde{p}$라고 부릅니다. (구성상, $l(\tilde{p}) \leq \log \frac{1}{P_{U}(x)}+2$입니다.) 이 트리를 프로그램을 사용하여 $x$를 출력하는 데 사용하려면, $\tilde{p}$를 지정하고 컴퓨터에게 모든 프로그램의 앞서 설명한 시뮬레이션을 실행하도록 요청합니다. 그러면 컴퓨터는 위에서 설명한 대로 트리를 구성하고 특정 노드 $\tilde{p}$가 할당될 때까지 기다립니다. 컴퓨터는 송신자와 동일한 구성을 실행하므로, 결국 노드 $\tilde{p}$가 할당될 것입니다. 이때 컴퓨터는 중단하고 해당 노드에 할당된 $x$를 출력합니다.

이것은 컴퓨터가 $x$를 재구성하는 효과적인 (유한하고 기계적인) 절차입니다. 그러나 $x$에 해당하는 가장 낮은 깊이의 노드를 찾는 효과적인 절차는 없습니다. 우리가 증명한 것은 $x$에 해당하는 노드가 $\left\lceil\log \frac{1}{P_{U}(x)}\right\rceil+1$ 레벨에 있는 (무한한) 트리가 존재한다는 것뿐입니다. 하지만 이것으로 우리의 목적을 달성할 수 있습니다.

예시를 참조하면, $x=1110$의 설명은 노드 ( $p_{3}, x_{3}, n_{3}$ ) (즉, 01)까지의 경로이며, $x=1111$의 설명은 경로 00001입니다. 문자열 1110을 설명하려면, 노드 01이 할당될 때까지 (시뮬레이션) 트리 구성을 수행하도록 컴퓨터에 요청합니다. 그런 다음 노드 01 (즉, $p_{3}$)에 해당하는 프로그램을 실행하도록 컴퓨터에 요청합니다. 이 프로그램의 출력은 원하는 문자열인 $x=1110$입니다.

$x$를 재구성하는 프로그램의 길이는 본질적으로 가장 낮은 깊이의 노드 $\tilde{p}$의 위치 설명의 길이와 같습니다.
<!-- Page 522 -->
트리에서 $x$로 가는 경로입니다. $x$에 대한 이 프로그램의 길이는 $l(\tilde{p})+c$이며, 여기서

$$
l(\tilde{p}) \leq\left\lceil\log \frac{1}{P_{\mathcal{U}}(x)}\right\rceil+1
$$

이므로 $x$의 복잡도는 다음을 만족합니다.

$$
K(x) \leq\left\lceil\log \frac{1}{P_{\mathcal{U}}(x)}\right\rceil+c
$$

# 14.12 콜모고로프 충분 통계량

베르누이 $(\theta)$ 프로세스에서 샘플 시퀀스가 주어졌다고 가정해 봅시다. 이 시퀀스에서 무작위성으로부터의 규칙성 또는 편차는 무엇입니까? 이 질문에 답하는 한 가지 방법은 콜모고로프 복잡도 $K\left(x^{n} \mid n\right)$를 찾는 것이며, 이는 대략 $n H_{0}(\theta)+\log n+c$임을 발견합니다. $\theta \neq \frac{1}{2}$의 경우, 이는 $n$보다 훨씬 작으므로 $x^{n}$는 구조를 가지며 무작위로 추출된 베르누이 $\left(\frac{1}{2}\right)$가 아님을 결론 내립니다. 그러나 구조는 무엇입니까? 구조를 찾기 위한 첫 번째 시도는 $x^{n}$에 대한 최단 프로그램 $p^{*}$를 조사하는 것입니다. 그러나 $p^{*}$의 최단 설명은 $p^{*}$ 자체의 길이와 거의 같습니다. 그렇지 않으면 $x^{n}$의 설명을 더 압축할 수 있으며, 이는 $p^{*}$의 최소성에 위배됩니다. 따라서 이 시도는 소용이 없습니다.

$p^{*}$가 $x^{n}$을 설명하는 방식에 대한 조사를 통해 좋은 접근 방식에 대한 힌트를 얻을 수 있습니다. "이 시퀀스에는 $k$개의 1이 있으며, 이러한 시퀀스 중 $i$번째입니다"라는 프로그램은 베르누이 $(\theta)$ 시퀀스에 대해 우선적으로 최적입니다. 이는 이단 설명이며, 시퀀스의 모든 구조는 첫 번째 단계에서 포착됩니다. 또한 $x^{n}$은 설명의 첫 번째 단계가 주어졌을 때 복잡성이 최대입니다. 첫 번째 단계인 $k$의 설명은 $\log (n+1)$ 비트를 요구하며 집합 $S=\left\{x \in\{0,1\}^{n}: \sum x_{i}=k\right\}$를 정의합니다. 두 번째 단계는 $\log |S|=\log \binom{n}{k} \approx n H_{0}\left(\bar{x}_{n}\right) \approx n H_{0}(\theta)$ 비트를 요구하며 $x^{n}$에 대해 특별한 것은 아무것도 밝히지 않습니다.

일반 시퀀스의 경우 $x^{n}$을 포함하는 간단한 집합 $S$를 찾아 이 프로세스를 모방합니다. 그런 다음 $\log |S|$ 비트를 사용하여 $S$에서 $x^{n}$에 대한 무차별 대입 설명을 따릅니다. 0개 이상의 $k$ 비트로 설명할 수 있는 $x^{n}$을 포함하는 가장 작은 집합의 정의로 시작합니다.

정의 이진 문자열 $x \in\{0,1\}^{n}$의 콜모고로프 구조 함수 $K_{k}\left(x^{n} \mid n\right)$는 다음과 같이 정의됩니다.

$$
\begin{gathered}
K_{k}\left(x^{n} \mid n\right)=\min _{p: l(p) \leq k} \log |S| . \\
\mathcal{U}(p, n)=S \\
x^{n} \in S \subseteq\{0,1\}^{n}
\end{gathered}
$$
<!-- Page 523 -->
집합 $S$는 $k$ 비트로 기술될 수 있는 가장 작은 집합이며, $x^{n}$을 포함합니다. $\mathcal{U}(p, n)=S$라는 것은 범용 컴퓨터 $\mathcal{U}$에서 프로그램 $p$를 데이터 $n$과 함께 실행하면 집합 $S$의 지시 함수를 출력한다는 것을 의미합니다.

정의 작은 상수 $c$가 주어졌을 때, $k^{*}$를 다음을 만족하는 가장 작은 $k$라고 정의합니다.

$$
K_{k}\left(x^{n} \mid n\right)+k \leq K\left(x^{n} \mid n\right)+c
$$

해당하는 집합을 $S^{* *}$라고 하고, $S^{* *}$의 지시 함수를 출력하는 프로그램을 $p^{* *}$라고 합시다. 그러면 $p^{* *}$를 $x^{n}$에 대한 Kolmogorov 최소 충분 통계량이라고 부르겠습니다.

집합 $S^{*}$를 기술하는 프로그램 $p^{*}$를 고려해 봅시다. 이 프로그램은 다음을 만족합니다.

$$
K_{k}\left(x^{n} \mid n\right)+k=K\left(x^{n} \mid n\right)
$$

모든 프로그램 $p^{*}$는 "충분 통계량"입니다. 왜냐하면 $x^{n}$의 복잡도는 $S^{*}$가 주어졌을 때 최대가 되기 때문입니다. 그러나 최소 충분 통계량은 가장 짧은 "충분 통계량"입니다.

위 정의에서의 등호는 컴퓨터 $U$에 따라 달라지는 큰 상수를 제외한 것입니다. 그러면 $k^{*}$는 $x^{n}$의 이단계 기술이 $x^{n}$의 최상의 일단계 기술만큼 좋을 때의 가장 작은 $k$에 해당합니다. 기술의 두 번째 단계는 단순히 집합 $S^{* *}$ 내에서 $x^{n}$의 인덱스를 제공합니다. 이는 $x^{n}$이 집합 $S^{* *}$가 주어졌을 때 조건부로 최대 복잡도를 가질 경우 $K_{k}\left(x^{n} \mid n\right)$ 비트를 사용합니다. 따라서 집합 $S^{* *}$는 $x^{n}$ 내의 모든 구조를 포착합니다. $S^{* *}$ 내에서 $x^{n}$의 나머지 기술은 본질적으로 문자열 내의 무작위성에 대한 기술입니다. 따라서 $S^{* *}$ 또는 $p^{* *}$를 $x^{n}$에 대한 Kolmogorov 충분 통계량이라고 부릅니다.

이는 수학 통계학에서의 충분 통계량 정의와 유사합니다. 통계량 $T$는 모수 $\theta$에 대해 충분하다고 말합니다. 만약 표본의 분포가 충분 통계량이 주어졌을 때 모수와 독립이라면 그렇습니다. 즉,

$$
\theta \rightarrow T(X) \rightarrow X
$$

가 순서대로 Markov 연쇄를 형성합니다. Kolmogorov 충분 통계량의 경우, 프로그램 $p^{* *}$는 문자열 $x^{n}$의 "구조"에 대해 충분합니다. $x^{n}$의 나머지 기술은 본질적으로 $x^{n}$의 "구조"와 독립적입니다. 특히, $x^{n}$은 $S^{* *}$가 주어졌을 때 최대 복잡도를 가집니다.

구조 함수의 일반적인 그래프는 그림 14.4에 나와 있습니다. $k=0$일 때, 기술될 수 있는 유일한 집합은 전체 집합 $\{0,1\}^{n}$입니다.
<!-- Page 524 -->

그림 14.4. Kolmogorov 충분 통계량.
따라서 해당 $\log$ 집합 크기는 $n$입니다. $k$를 증가시키면 집합 크기가 빠르게 감소하다가

$$
k+K_{k}\left(x^{n} \mid n\right) \approx K\left(x^{n} \mid n\right)
$$

이후, $k$의 추가 비트마다 집합이 절반으로 줄어들고, $k=K\left(x^{n} \mid n\right)$까지 기울기 -1인 선을 따라 진행합니다. $k \geq K\left(x^{n} \mid n\right)$의 경우, $x^{n}$을 포함하는 설명 가능한 가장 작은 집합은 단일 원소 $\left\{x^{n}\right\}$이므로 $K_{k}\left(x^{n} \mid n\right)=0$입니다.

이제 몇 가지 예시를 통해 개념을 설명하겠습니다.

1. 베르누이 $(\theta)$ 시퀀스. 미지의 매개변수 $\theta$를 갖는 베르누이 시퀀스의 길이 $n$ 샘플을 고려하십시오. 예시 14.2에서 논의한 바와 같이, 첫 번째 단계에서 $k$를 설명하고 ( $\log n$ 비트 사용) 두 번째 단계에서 $k$개의 1을 갖는 모든 시퀀스 내에서 시퀀스를 설명하는 ( $\log \binom{n}{k}$ 비트 사용) 이단계 설명을 사용하여 $n H\left(\frac{k}{n}\right)+\frac{1}{2} \log n$ 비트로 이 시퀀스를 설명할 수 있습니다. 그러나 더 짧은 첫 번째 단계 설명을 사용할 수도 있습니다. $k$를 정확하게 설명하는 대신, $k$의 범위를 구간으로 나누고 $\frac{1}{2} \log n$ 비트를 사용하여 $\sqrt{\frac{k}{n} \frac{n-k}{n}} \sqrt{n}$의 정확도로만 $k$를 설명합니다. 그런 다음 실제

<!-- Page 525 -->

그림 14.5. 베르누이 수열에 대한 콜모고로프 충분 통계량.
$k$와 같은 빈에 속하는 모든 수열 중에서 해당 수열. $l$개의 1을 갖는 모든 수열의 집합 크기, $l \in k \pm \sqrt{\frac{k}{n} \frac{n-k}{n}} \sqrt{n}$은 스털링 공식에 의해 $n H\binom{k}{n}+o(n)$이며, 따라서 총 설명 길이는 여전히 $n H\binom{k}{n}+\frac{1}{2} \log n+o(n)$이지만, 콜모고로프 충분 통계량의 설명 길이는 $k^{*} \approx \frac{1}{n} \log n$입니다.
2. 마르코프 연쇄에서 샘플링. 이전 예와 같은 맥락에서, 첫 번째 순서 이진 마르코프 연쇄에서 샘플링을 고려하십시오. 이 경우에도 $p^{**}$는 수열의 마르코프 유형(수열에서 00, 01, 10, 11의 발생 횟수)을 설명하는 것에 해당하며, 이는 수열의 모든 구조를 전달합니다. 설명의 나머지 부분은 이 마르코프 유형의 모든 수열 집합에서 해당 수열의 인덱스가 됩니다. 따라서 이 경우 $k^{*} \approx 2\left(\frac{1}{2} \log n\right)=\log n$이며, 이는 조건부 결합 유형의 두 요소를 적절한 정확도로 설명하는 것에 해당합니다. (조건부 결합 유형의 다른 요소는 이 두 요소에서 결정될 수 있습니다.)
3. 모나리자. 흰색 배경에 회색 원으로 구성된 이미지를 고려하십시오. 원은 균일한 회색이 아니라 매개변수 $\theta$를 갖는 베르누이 분포를 따릅니다. 이는 그림 14.6에 설명되어 있습니다. 이 경우, 최상의 2단계 설명은 먼저 크기와 위치를 설명하는 것입니다.

<!-- Page 526 -->

그림 14.6. 모나리자.
원과 그 평균 회색 수준을 파악한 다음, 동일한 회색 수준을 가진 모든 원 중에서 해당 원의 인덱스를 설명합니다. $n$ 픽셀 이미지($\sqrt{n}$ x $\sqrt{n}$ 크기)를 가정할 때, 약 $n+1$개의 가능한 회색 수준이 있으며, 약 $(\sqrt{n})^{3}$개의 구별 가능한 원이 존재합니다. 따라서 이 경우 $k^{*} \approx \frac{5}{2} \log n$입니다.

# 14.13 최소 설명 길이 원리

오컴의 면도날 원리의 자연스러운 확장은 우리가 알려지지 않은 분포에서 추출된 데이터를 설명해야 할 때 발생합니다. $X_{1}, X_{2}, \ldots, X_{n}$이 확률 질량 함수 $p(x)$에 따라 i.i.d.로 추출되었다고 가정합니다. 우리는 $p(x)$를 알지 못하지만 $p(x) \in \mathcal{P}$이며, $\mathcal{P}$는 확률 질량 함수의 집합이라는 것을 압니다. 주어진 데이터를 바탕으로 데이터에 가장 잘 맞는 $\mathcal{P}$ 내의 확률 질량 함수를 추정할 수 있습니다. 간단한 $\mathcal{P}$의 경우(예: $\mathcal{P}$가 유한 개의 분포만 가지는 경우), 문제는 간단하며 최대 가능도 절차[즉, $\hat{p}\left(X_{1}, X_{2}, \ldots, X_{n}\right)$를 최대화하는 $\hat{p} \in \mathcal{P}$를 찾음]가 잘 작동합니다. 그러나 $\mathcal{P}$가 충분히 풍부한 경우, 데이터 과적합 문제가 발생합니다. 예를 들어, $X_{1}, X_{2}, \ldots, X_{n}$이 연속 확률 변수이고 $\mathcal{P}$가 모든 확률 분포의 집합이라면, $X_{1}, X_{2}, \ldots, X_{n}$이 주어졌을 때의 최대 가능도 추정치는 각 관측값에 가중치 $\frac{1}{n}$의 단일 질량점을 두는 분포입니다. 명백히 이 추정치는 실제 관측 데이터에 너무 밀접하게 연결되어 있으며, 근본적인 분포의 어떤 구조도 포착하지 못합니다.

이 문제를 해결하기 위해 다양한 방법이 적용되었습니다. 가장 간단한 경우, 데이터는 특정 모수 분포(예: 정규 분포)에서 추출된 것으로 가정되며, 분포의 모수는 데이터로부터 추정됩니다. 이 방법을 검증하기 위해, 데이터가 정규 분포처럼 "보이는지" 확인하기 위해 테스트를 거쳐야 하며, 데이터가 테스트를 통과하면 이 데이터 설명을 사용할 수 있습니다. 더 일반적인 절차는 최대 가능도 추정치를 가져와 평활화하여 평활한 밀도를 얻는 것입니다. 충분한 데이터와 적절한 평활도를 사용하면

<!-- Page 527 -->
원래의 밀도를 잘 추정하는 것이 가능합니다. 이 과정을 커널 밀도 추정이라고 합니다.

그러나 Kolmogorov complexity (또는 Kolmogorov sufficient statistic) 이론은 다른 절차를 제안합니다. 즉, 다음을 최소화하는 $p \in \mathcal{P}$를 찾습니다.

$$
L_{p}\left(X_{1}, X_{2}, \ldots, X_{n}\right)=K(p)+\log \frac{1}{p\left(X_{1}, X_{2}, \ldots, X_{n}\right)}
$$

이것은 데이터를 두 단계로 설명하는 길이입니다. 첫 번째 단계에서는 분포 $p$를 설명하고, 분포가 주어지면 Shannon code를 구성하고 $\log \frac{1}{p\left(X_{1}, X_{2}, \ldots, X_{n}\right)}$ 비트를 사용하여 데이터를 설명합니다. 이 절차는 최소 설명 길이 (MDL) 원칙이라고 불리는 것의 특수한 경우입니다. 즉, 데이터와 모델 선택이 주어졌을 때, 모델의 설명과 데이터의 조건부 설명이 가능한 한 짧도록 모델을 선택합니다.

# 요약

정의. 문자열 $x$의 Kolmogorov complexity $K(x)$는 다음과 같습니다.

$$
\begin{aligned}
K(x) & =\min _{p: \mathcal{U}(p)=x} l(p) \\
K(x \mid l(x)) & =\min _{p: \mathcal{U}(p, l(x))=x} l(p)
\end{aligned}
$$

Kolmogorov complexity의 보편성. 다른 모든 컴퓨터 $\mathcal{A}$에 대해 다음을 만족하는 보편 컴퓨터 $\mathcal{U}$가 존재합니다.

$$
K_{\mathcal{U}}(x) \leq K_{\mathcal{A}}(x)+c_{\mathcal{A}}
$$

모든 문자열 $x$에 대해 상수 $c_{\mathcal{A}}$는 $x$에 의존하지 않습니다. 만약 $\mathcal{U}$와 $\mathcal{A}$가 보편적이라면, 모든 $x$에 대해 $\left|K_{\mathcal{U}}(x)-K_{\mathcal{A}}(x)\right| \leq c$입니다.

## Kolmogorov complexity의 상한

$$
\begin{gathered}
K(x \mid l(x)) \leq l(x)+c \\
K(x) \leq K(x \mid l(x))+2 \log l(x)+c
\end{gathered}
$$
<!-- Page 528 -->
Kolmogorov complexity와 entropy. $H$의 entropy를 갖는 i.i.d. 정수값 확률 변수 $X_{1}, X_{2}, \ldots$가 주어지면, 모든 $n$에 대해 상수 $c$가 존재하여 다음과 같습니다.

$$
H \leq \frac{1}{n} E K\left(X^{n} \mid n\right) \leq H+|\mathcal{X}| \frac{\log n}{n}+\frac{c}{n}
$$

Kolmogorov complexity의 하한. 복잡도 $K(x)<k$를 갖는 문자열 $x$는 $2^{k}$개보다 많지 않습니다. 베르누이 $\left(\frac{1}{2}\right)$ 프로세스에 따라 $X_{1}, X_{2}, \ldots, X_{n}$이 추출된다면,

$$
\operatorname{Pr}\left(K\left(X_{1} X_{2} \ldots X_{n} \mid n\right) \leq n-k\right) \leq 2^{-k}
$$

정의 문자열 $x$는 $K\left(x_{1} x_{2} \ldots x_{n} \mid n\right) / n \rightarrow 1$이면 압축 불가능하다고 말합니다.

# 압축 불가능한 문자열에 대한 큰 수의 법칙

$$
\frac{K\left(x_{1}, x_{2}, \ldots, x_{n}\right)}{n} \rightarrow 1 \Rightarrow \frac{1}{n} \sum_{i=1}^{n} x_{i} \rightarrow \frac{1}{2}
$$

정의 문자열 $x$의 보편 확률은 다음과 같습니다.

$$
P_{\mathcal{U}}(x)=\sum_{p: \mathcal{U}(p)=x} 2^{-l(p)}=\operatorname{Pr}(\mathcal{U}(p)=x)
$$

$P_{\mathcal{U}}(x)$의 보편성. 모든 컴퓨터 $\mathcal{A}$에 대해, 상수 $c_{\mathcal{A}}$는 $\mathcal{U}$와 $\mathcal{A}$에만 의존하며, 모든 문자열 $x \in\{0,1\}^{*}$에 대해 다음이 성립합니다.

$$
P_{\mathcal{U}}(x) \geq c_{\mathcal{A}} P_{\mathcal{A}}(x)
$$

정의 $\Omega=\sum_{p: \mathcal{U}(p)}$ halts $2^{-l(p)}=\operatorname{Pr}(\mathcal{U}(p)$ halts)는 컴퓨터에 대한 입력 $p$가 베르누이 $\left(\frac{1}{2}\right)$ 프로세스에 따라 추출된 이진 문자열일 때 컴퓨터가 중단될 확률입니다.

## $\Omega$의 속성

1. $\Omega$는 계산 불가능합니다.
2. $\Omega$는 "철학자의 돌"입니다.
3. $\Omega$는 알고리즘적으로 무작위적입니다(압축 불가능합니다).
<!-- Page 529 -->
$\mathbf{K}(\mathbf{x})$와 $\log \left(\frac{1}{\mathbf{P}_{\mathcal{U}}(\mathbf{x})}\right)$의 동등성. 모든 문자열 $x$에 대해 다음을 만족하는 상수 $c$가 존재합니다.

$$
\left|\log \frac{1}{P_{\mathcal{U}}(x)}-K(x)\right| \leq c
$$

따라서, 문자열 $x$의 보편 확률은 본질적으로 그 Kolmogorov 복잡성에 의해 결정됩니다.

정의 이진 문자열 $x^{n} \in\{0,1\}^{n}$의 Kolmogorov 구조 함수 $K_{k}\left(x^{n} \mid n\right)$는 다음과 같이 정의됩니다.

$$
\begin{gathered}
K_{k}\left(x^{n} \mid n\right)=\min _{p: l(p) \leq k} \log |S| \\
\mathcal{U}(p, n)=S \\
x \in S
\end{gathered}
$$

정의 $K_{k^{*}}\left(x^{n} \mid n\right)+k^{*}=K\left(x^{n} \mid n\right)$를 만족하는 가장 작은 $k$를 $k^{*}$라고 합시다.

해당 집합을 $S^{* *}$라고 하고, $S^{* *}$의 지시 함수를 출력하는 프로그램를 $p^{* *}$라고 합시다. 그러면 $p^{* *}$는 $x$의 Kolmogorov 최소 충분 통계량입니다.

# 문제

14.1 두 시퀀스의 Kolmogorov 복잡성. $x, y \in\{0,1\}^{*}$라고 합시다. $K(x, y) \leq K(x)+K(y)+c$임을 논증하십시오.
14.2 합의 복잡성
(a) $K(n) \leq \log n+2 \log \log n+c$임을 논증하십시오.
(b) $K\left(n_{1}+n_{2}\right) \leq K\left(n_{1}\right)+K\left(n_{2}\right)+c$임을 논증하십시오.
(c) $n_{1}$과 $n_{2}$는 복잡하지만 합은 상대적으로 간단한 예시를 제시하십시오.
14.3 이미지. 0과 1로 이루어진 $n \times n$ 배열 $x$를 고려하십시오. 따라서 $x$는 $n^{2}$ 비트를 가집니다.
<!-- Page 530 -->

다음의 경우 Kolmogorov 복잡도 $K(x \mid n)$ (첫 번째 근사치)를 구하십시오:
(a) $x$가 수평선인 경우.
(b) $x$가 정사각형인 경우.
(c) $x$가 수직선 또는 수평선인 두 선의 합집합인 경우.
14.4 컴퓨터는 entropy를 감소시키는가? 보편 컴퓨터에 무작위 프로그램 $P$를 입력합니다. 해당 출력의 entropy는 얼마입니까? 구체적으로, $P$가 Bernoulli $\left(\frac{1}{2}\right)$ 시퀀스일 때 $X=\mathcal{U}(P)$라고 합시다. 여기서 이진 시퀀스 $X$는 정의되지 않거나 $\{0,1\}^{*}$에 속합니다. $H(X)$를 $X$의 Shannon entropy라고 합시다. $H(X)=\infty$임을 논증하십시오. 따라서 컴퓨터는 무의미한 것을 의미 있는 것으로 바꾸지만, 출력 entropy는 여전히 무한합니다.
14.5 컴퓨터 앞의 원숭이. 컴퓨터에 무작위 프로그램이 입력된다고 가정합니다. 컴퓨터가 다음 시퀀스를 출력할 확률을 대략적으로 추정하십시오:
(a) $0^{n}$ 다음에 임의의 시퀀스.
(b) $\pi_{1} \pi_{2} \ldots \pi_{n}$ 다음에 임의의 시퀀스, 여기서 $\pi_{i}$는 $\pi$의 전개에서 $i$번째 비트입니다.
(c) $0^{n} 1$ 다음에 임의의 시퀀스.
(d) $\omega_{1} \omega_{2} \ldots \omega_{n}$ 다음에 임의의 시퀀스.
(e) 사색 정리의 증명.
14.6 Kolmogorov 복잡도와 삼진 프로그램. 보편 컴퓨터 $\mathcal{U}$의 입력 프로그램이 $\{0,1,2\}^{*}$ (삼진 입력)의 시퀀스라고 가정합니다. 또한 $\mathcal{U}$가 삼진 출력을 출력한다고 가정합니다. $K(x \mid l(x))=\min _{U(p, l(x))=x} l(p)$라고 할 때 다음을 보여주십시오:
(a) $K\left(x^{n} \mid n\right) \leq n+c$.
(b) $\left|x^{n} \in\{0,1\}^{*}: K\left(x^{n} \mid n\right)<k\right|<3^{k}$.
14.7 큰 수의 법칙. 문제 14.14.6과 같이 삼진 입력 및 출력을 사용하여, 시퀀스 $x$가 알고리즘적으로 무작위인 경우 [즉, $K(x \mid l(x)) \approx l(x)$이면], $x$에서 0, 1, 2의 비율이 각각 $\frac{1}{3}$에 가까워야 함을 보여주는 논증을 개략적으로 설명하십시오. Stirling 근사 $n!\approx(n / e)^{n}$을 사용하는 것이 도움이 될 수 있습니다.
<!-- Page 531 -->
14.8 이미지 복잡도. $n \times n$ 격자의 두 이진 부분집합 A와 B를 고려하십시오 (예:

다음의 일반적인 상한 및 하한을 $K(A \mid n)$ 및 $K(B \mid n)$으로 찾으십시오:
(a) $K\left(A^{c} \mid n\right)$.
(b) $K(A \cup B \mid n)$.
(c) $K(A \cap B \mid n)$.
14.9 랜덤 프로그램. 랜덤 프로그램(심볼은 심볼 집합에 대해 i.i.d. 균일 분포)이 가장 가까운 사용 가능한 컴퓨터에 공급된다고 가정합니다. 놀랍게도 $1 / \sqrt{2}$의 이진 확장의 처음 $n$ 비트가 출력됩니다. 다음 출력 비트가 $1 / \sqrt{2}$의 확장에서 해당 비트와 일치할 확률은 대략 얼마라고 말할 수 있습니까?
14.10 얼굴-꽃병 착시

(a) 격자 중앙을 통과하는 수직 축에 대해 거울 이미지 대칭을 가지며 수평 선분으로 구성된 $m \times m$ 격자의 패턴 복잡도에 대한 상한은 무엇입니까?
(b) 위에서 설명한 패턴과 한 셀에서 다른 패턴은 무엇입니까?
14.11 Kolmogorov 복잡도. $n$이 매우 크고 알려져 있다고 가정합니다. 모든 직사각형은 프레임에 평행하다고 가정합니다.
<!-- Page 532 -->
(a) $n \times n$ 격자 상의 두 직사각형의 합집합의 (최대) Kolmogorov 복잡도는 얼마입니까?

(b) 직사각형이 한 꼭짓점에서 교차하는 경우는 어떻습니까?

(c) 직사각형이 동일한 (알려지지 않은) 모양을 가지는 경우는 어떻습니까?
(d) 직사각형이 동일한 (알려지지 않은) 면적을 가지는 경우는 어떻습니까?
(e) 두 직사각형 합집합의 최소 Kolmogorov 복잡도는 얼마입니까? 즉, 가장 간단한 합집합은 무엇입니까?
(f) $n \times n$ 격자 상의 모든 이미지 (반드시 직사각형일 필요는 없음)에 대한 (최대) Kolmogorov 복잡도는 얼마입니까?
14.12 암호화된 텍스트. 영어 텍스트 $x^{n}$이 치환 암호에 의해 $y^{n}$으로 암호화된다고 가정합니다: 알파벳 27개 문자 (공백 문자를 포함한 A-Z) 각각을 자신에게 1대1로 재할당합니다. 텍스트 $x^{n}$의 Kolmogorov 복잡도가 $K\left(x^{n}\right)=\frac{n}{4}$이라고 가정합니다. (이는 영어 텍스트에 대해 적절한 값입니다. 우리는 이제 27진법 프로그래밍 언어를 가정합니다. 즉, 길이 n인 특정 영어 텍스트 문자열을 출력하는 27진법 프로그래밍 언어를 사용한 가장 짧은 프로그램의 길이는 약 n/4입니다.)
(a) 암호화 맵의 Kolmogorov 복잡도는 얼마입니까?
<!-- Page 533 -->
(b) 암호화된 텍스트 $y^{n}$의 Kolmogorov 복잡도를 추정하십시오.
(c) $y^{n}$을 디코딩할 수 있을 것으로 예상하려면 $n$이 얼마나 커야 합니까?
14.13 Kolmogorov 복잡도. 정수 $n$에 대한 Kolmogorov 복잡도 $K(n)$을 고려하십시오. 특정 정수 $n_{1}$이 낮은 Kolmogorov 복잡도 $K\left(n_{1}\right)$을 갖는다면, 정수 $n_{1}+k$에 대한 Kolmogorov 복잡도 $K\left(n_{1}+k\right)$는 $K\left(n_{1}\right)$에서 얼마나 변할 수 있습니까?
14.14 큰 수의 복잡도. $A(n)$을 길이가 $n$ 비트 이하인 종료 프로그램 $p$가 존재하여 $x$를 출력하는 양의 정수 $x$의 집합이라고 정의합니다. $B(n)$을 $A(n)$의 여집합이라고 정의합니다 (즉, $B(n)$은 길이가 $n$ 비트 이하인 프로그램이 $x$를 출력하지 않는 정수 $x$의 집합입니다). $M(n)$을 $A(n)$에 있는 최대 정수라고 정의하고, $S(n)$을 $B(n)$에 있는 최소 정수라고 정의합니다. $K(M(n))$ (근사치)은 얼마입니까? $K(S(n))$ (근사치)은 얼마입니까? 둘 중 어느 것이 더 큽니까 ($M(n)$ 또는 $S(n)$)? $M(n)$에 대한 합리적인 하한선과 $S(n)$에 대한 합리적인 상한선을 제시하십시오.

# 역사적 고찰

Kolmogorov 복잡도의 초기 아이디어는 Kolmogorov [321, 322], Solomonoff [504], Chaitin [89]에 의해 독립적으로 거의 동시에 제시되었습니다. 이러한 아이디어는 Kolmogorov의 학생들인 Martin-Löf [374] (알고리즘적 무작위 시퀀스 및 무작위성에 대한 알고리즘적 테스트의 개념을 정의함)와 Levin 및 Zvonkin [353] (보편 확률과 복잡성과의 관계를 탐구함)에 의해 더욱 발전되었습니다. Chaitin [90]-[92]의 일련의 논문은 알고리즘 복잡성과 수학적 증명 간의 관계를 발전시켰습니다. C. P. Schnorr는 [466]-[468]에서 보편적인 무작위성의 개념을 연구하고 이를 도박과 연관시켰습니다.

Kolmogorov 구조 함수의 개념은 1973년 Tallin 회의에서의 강연에서 Kolmogorov에 의해 정의되었으나, 이러한 결과는 출판되지 않았습니다. V'yugin은 [549]에서 $K_{k}\left(x^{n} \mid n\right)=n-k, k<K\left(x^{n} \mid n\right)$의 의미에서 구조를 임의로 느리게 드러내는 매우 이상한 시퀀스 $x^{n}$이 존재함을 보여줍니다. Zurek [606]-[608]은 Maxwell의 악마와 열역학 제2법칙의 근본적인 질문을 Kolmogorov 복잡성의 물리적 결과를 확립함으로써 다룹니다.
<!-- Page 534 -->
Rissanen의 최소 기술 길이(MDL) 원칙은 Kolmogorov 충분 통계량과 매우 유사한 정신을 가지고 있습니다. Rissanen [445, 446]은 데이터에 대한 높은 가능성을 제공하는 저복잡도 모델을 찾습니다. Barron과 Cover [32]는 밀도 최소화 $K(f)+\log \frac{1}{\prod f\left(X_{i}\right)}$가 일관된 밀도 추정을 제공한다고 주장합니다.

다양한 복잡도 측정에 대한 비기술적인 소개는 Pagels [412]의 생각을 자극하는 책에서 찾을 수 있습니다. 이 분야의 작업에 대한 추가 참조는 정보 이론 및 알고리즘 복잡성에 대한 Kolmogorov의 기여에 관한 Cover 외 [114]의 논문에서 찾을 수 있습니다. 알고리즘 및 오토마타 분석에 대한 이론 적용을 포함하여 이 분야에 대한 포괄적인 소개는 Li와 Vitanyi [354]의 책에서 찾을 수 있습니다. 추가적인 내용은 Chaitin [86, 93]의 책에서 찾을 수 있습니다.
<!-- Page 535 -->
# 네트워크 정보 이론

많은 송신자와 수신자를 가진 시스템은 통신 문제에 간섭, 협력, 피드백과 같은 새로운 요소들을 포함합니다. 이것들은 네트워크 정보 이론의 영역입니다. 일반적인 문제는 쉽게 설명할 수 있습니다. 많은 송신자와 수신자, 그리고 네트워크의 간섭과 잡음의 영향을 설명하는 채널 전이 행렬이 주어졌을 때, 소스들이 채널을 통해 전송될 수 있는지 여부를 결정하는 것입니다. 이 문제는 분산 소스 코딩(데이터 압축)뿐만 아니라 분산 통신(네트워크의 용량 영역 찾기)을 포함합니다. 이 일반적인 문제는 아직 해결되지 않았으므로, 이 장에서는 다양한 특수한 경우를 고려합니다.

대규모 통신 네트워크의 예로는 컴퓨터 네트워크, 위성 네트워크, 전화 시스템 등이 있습니다. 단일 컴퓨터 내부에서도 서로 통신하는 다양한 구성 요소가 있습니다. 네트워크 정보 이론의 완전한 이론은 통신 및 컴퓨터 네트워크 설계에 광범위한 영향을 미칠 것입니다.

그림 15.1에 표시된 것처럼 $m$개의 스테이션이 공통 채널을 통해 공통 위성과 통신하기를 원한다고 가정해 봅시다. 이것은 다중 접속 채널로 알려져 있습니다. 다양한 송신자들이 수신자에게 정보를 보내기 위해 서로 어떻게 협력합니까? 어떤 통신 속도를 동시에 달성할 수 있습니까? 송신자 간의 간섭은 총 통신 속도에 어떤 제한을 둡니까? 이것은 가장 잘 이해된 다중 사용자 채널이며, 위의 질문들에 만족스러운 답이 있습니다.

대조적으로, 네트워크를 반대로 하여 그림 15.2와 같이 하나의 TV 방송국이 $m$개의 TV 수신자에게 정보를 보내는 것을 고려할 수 있습니다. 송신자는 서로 다른 수신자를 위한 정보를 공통 신호로 어떻게 인코딩합니까? 서로 다른 수신자에게 정보를 보낼 수 있는 속도는 얼마입니까? 이 채널에 대한 답은 특수한 경우에만 알려져 있습니다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 536 -->

그림 15.1. 다중 접속 채널.

그림 15.2. 방송 채널.

이 외에도 릴레이 채널(하나의 송신자와 하나의 수신자가 있지만, 송신자와 수신자 간의 통신을 용이하게 하는 하나 이상의 중간 송수신 쌍이 릴레이 역할을 하는 채널), 간섭 채널(두 송신자와 두 수신자가 있으며 상호 간섭이 있는 채널), 양방향 채널(두 송수신 쌍이 서로 정보를 주고받는 채널) 등이 있습니다. 이러한 모든 채널에 대해, 달성 가능한 통신 속도와 적절한 코딩 전략에 대한 질문에 대한 답은 일부만 가지고 있습니다.

이러한 모든 채널은 그림 15.3에 나타난 바와 같이 서로 통신하려는 $m$개의 노드로 구성된 일반 통신 네트워크의 특수한 경우로 간주될 수 있습니다. 각 시간 순간에, $i$번째 노드는 자신이 보내고자 하는 메시지에 따라 달라지는 기호 $x_{i}$를 보냅니다.
<!-- Page 537 -->

그림 15.3. 통신 네트워크.
그리고 노드에서 과거에 수신된 기호들을 기반으로 합니다. 기호 $\left(x_{1}, x_{2}, \ldots, x_{m}\right)$의 동시 전송은 조건부 확률 분포 $p\left(y^{(1)}, y^{(2)}, \ldots, y^{(m)}\left|x^{(1)}, x^{(2)}, \ldots, x^{(m)}\right)\right.$에 따라 추출된 랜덤 수신 기호 $\left(Y_{1}, Y_{2}, \ldots, Y_{m}\right)$를 생성합니다. 여기서 $p(\cdot \mid \cdot)$는 네트워크에 존재하는 잡음과 간섭의 영향을 나타냅니다. 만약 $p(\cdot \mid \cdot)$가 0과 1의 값만 가진다면, 네트워크는 결정론적입니다.

네트워크의 일부 노드에는 다른 노드로 통신되어야 하는 확률적 데이터 소스가 연관되어 있습니다. 만약 소스들이 독립적이라면, 노드에서 보내는 메시지들도 독립적입니다. 그러나 완전한 일반성을 위해, 우리는 소스들이 종속적일 수 있도록 허용해야 합니다. 종속성을 활용하여 전송되는 정보의 양을 줄이는 방법은 무엇입니까? 소스의 확률 분포와 채널 전이 함수가 주어졌을 때, 이 소스들을 채널을 통해 전송하고 적절한 왜곡으로 목적지에서 소스를 복구할 수 있습니까?

우리는 네트워크 통신의 다양한 특수한 경우를 고려합니다. 우리는 채널이 잡음이 없고 간섭이 없는 경우의 소스 코딩 문제를 고려합니다. 이러한 경우, 문제는 각 소스와 연관된 속도 집합을 찾아 요구되는 소스들을 낮은 오류 확률(또는 적절한 왜곡)로 목적지에서 디코딩할 수 있도록 하는 것으로 축소됩니다. 분산 소스 코딩의 가장 간단한 경우는 Slepian-Wolf 소스 코딩 문제입니다. 이 문제에서는 두 개의 소스가 별도로 인코딩되어야 하지만, 공통 노드에서 함께 디코딩되어야 합니다. 우리는 두 소스 중 하나만 목적지에서 복구되어야 하는 경우에 대한 이 이론의 확장도 고려합니다.

네트워크에서의 흐름 이론은 회로 이론 및 파이프에서의 물 흐름과 같은 영역에서 만족스러운 답을 제공합니다. 예를 들어, 그림 15.4에 표시된 단일 소스 단일 싱크 파이프 네트워크에서, $A$에서 $B$로의 최대 흐름은 Ford-Fulkerson 정리를 통해 쉽게 계산될 수 있습니다. 엣지들이 표시된 대로 용량 $C_{i}$를 가진다고 가정합니다. 명백히,

<!-- Page 538 -->

그림 15.4. 수도관 네트워크.
모든 컷셋을 가로지르는 최대 유량은 컷 엣지의 용량 합보다 클 수 없습니다. 따라서 컷셋을 가로지르는 최대 유량을 최소화하면 네트워크 용량의 상한을 얻을 수 있습니다. Ford-Fulkerson 정리 [214]는 이 용량이 달성될 수 있음을 보여줍니다.

네트워크에서의 정보 흐름 이론은 파이프에서의 물 흐름 이론과 같은 간단한 답을 가지고 있지 않습니다. 모든 컷셋을 가로지르는 정보 흐름 속도에 대한 상한을 증명하지만, 이러한 상한은 일반적으로 달성할 수 없습니다. 그러나 릴레이 채널 및 캐스케이드 채널과 같은 일부 문제는 간단한 최대 유량-최소 컷 해석을 허용한다는 점은 만족스럽습니다. 일반 이론을 찾는 데 있어 또 다른 미묘한 문제는 섹션 15.10에서 간략하게 다루는 소스-채널 분리 정리의 부재입니다. 분산 소스 코딩과 네트워크 채널 코딩을 결합한 완전한 이론은 아직 먼 목표입니다.

다음 섹션에서는 네트워크 정보 이론의 기본 채널 중 일부에 대한 가우시안 예제를 고려합니다. 물리적으로 동기가 부여된 가우시안 채널은 구체적이고 쉽게 해석할 수 있는 답변에 적합합니다. 나중에 우리는 다중 사용자 정보 이론의 정리를 증명하는 데 사용하는 공동 전형성에 대한 몇 가지 기본 결과를 증명합니다. 그런 다음 여러 문제를 자세히 살펴봅니다. 다중 접속 채널, 상관 소스 코딩(Slepian-Wolf 데이터 압축), 방송 채널, 릴레이 채널, 측면 정보가 있는 랜덤 변수 코딩, 측면 정보가 있는 속도 왜곡 문제. 마지막으로 네트워크에서의 정보 흐름에 대한 일반 이론을 소개합니다. 이 분야에는 많은 미해결 문제가 있으며, 아직 포괄적인 정보 네트워크 이론은 존재하지 않습니다. 그러한 이론이 발견되더라도 구현하기에는 너무 복잡할 수 있습니다. 그러나 이 이론은 통신 설계자에게 최적성에 얼마나 가까운지 알려주고 통신 속도를 개선할 수 있는 몇 가지 방법을 제안할 수 있습니다.
<!-- Page 539 -->
# 15.1 가우시안 다중 사용자 채널

가우시안 다중 사용자 채널은 네트워크 정보 이론의 중요한 특징들을 보여줍니다. 9장에서 가우시안 채널에 대해 얻은 직관은 이 섹션을 유용한 소개로 만드는 데 도움이 될 것입니다. 여기서는 가우시안 다중 접속, 방송, 릴레이 및 양방향 채널의 용량 영역을 설정하는 핵심 아이디어를 증명 없이 제시할 것입니다. 이러한 정리들의 이산 메모리 없는(discrete memoryless) 대응에 대한 코딩 정리의 증명은 이 장의 후반부 섹션에서 제공됩니다.

입력 전력 $P$와 잡음 분산 $N$을 갖는 기본 이산 시간 백색 가우시안 잡음 채널은 다음과 같이 모델링됩니다.

$$
Y_{i}=X_{i}+Z_{i}, \quad i=1,2, \ldots
$$

여기서 $Z_{i}$는 평균 0과 분산 $N$을 갖는 i.i.d. 가우시안 확률 변수입니다. 신호 $\mathbf{X}=\left(X_{1}, X_{2}, \ldots, X_{n}\right)$는 전력 제약 조건을 갖습니다.

$$
\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} \leq P
$$

$E X^{2} \leq P$를 만족하는 모든 확률 변수 $X$에 대해 $I(X ; Y)$를 최대화하여 얻어지는 Shannon 용량 $C$는 (9장) 다음과 같이 주어집니다.

$$
C=\frac{1}{2} \log \left(1+\frac{P}{N}\right) \quad \text { 전송당 비트. }
$$

이 장에서는 이산 시간 메모리 없는 채널에 대한 우리의 주의를 제한합니다. 결과는 연속 시간 가우시안 채널로 확장될 수 있습니다.

### 15.1.1 단일 사용자 가우시안 채널

먼저 9장에서 연구한 단일 사용자 가우시안 채널을 검토합니다. 여기서 $Y=X+Z$입니다. 속도 $R<\frac{1}{2} \log \left(1+\frac{P}{N}\right)$를 선택합니다. 전력 $P$를 갖는 좋은 $\left(2^{n R}, n\right)$ 코드북을 고정합니다. 위에서 생성된 코드북에서 인덱스 $w$를 $2^{n R}$ 집합에서 선택합니다. $w$번째 코드워드 $\mathbf{X}(w)$를 전송합니다. 수신기는 $\mathbf{Y}=\mathbf{X}(w)+\mathbf{Z}$를 관찰하고, 관찰된 벡터 $\mathbf{Y}$에 가장 가까운 코드워드의 인덱스 $\hat{w}$를 찾습니다. $n$이 충분히 크면 오류 확률 $\operatorname{Pr}(w \neq \hat{w})$은 임의로 작아질 것입니다. 결합된 типи성(joint typicality)의 정의에서 볼 수 있듯이, 이 최소 거리 디코딩 방식은 본질적으로 수신 벡터 $\mathbf{Y}$와 결합된 типи성을 갖는 코드북 내의 코드워드를 찾는 것과 동등합니다.
<!-- Page 540 -->
# 15.1.2 $m$명의 사용자를 갖는 가우시안 다중 접속 채널

각각 전력 $P$를 갖는 $m$개의 송신기를 고려합니다.

$$
Y=\sum_{i=1}^{m} X_{i}+Z
$$

라고 합시다.

$$
C\left(\frac{P}{N}\right)=\frac{1}{2} \log \left(1+\frac{P}{N}\right)
$$

는 신호 대 잡음비 $P / N$를 갖는 단일 사용자 가우시안 채널의 용량을 나타냅니다. 가우시안 채널에 대한 달성 가능한 속도 영역은 다음 방정식에 주어진 간단한 형태를 취합니다.

$$
\begin{aligned}
R_{i} & <C\left(\frac{P}{N}\right) \\
R_{i}+R_{j} & <C\left(\frac{2 P}{N}\right) \\
R_{i}+R_{j}+R_{k} & <C\left(\frac{3 P}{N}\right) \\
& \vdots \\
\sum_{i=1}^{m} R_{i} & <C\left(\frac{m P}{N}\right)
\end{aligned}
$$

모든 속도가 동일할 때 마지막 부등식이 다른 부등식을 지배한다는 점에 유의하십시오.

여기서는 $i$번째 코드북이 전력 $P$의 $2^{n R_{i}}$개의 코드를 갖는 $m$개의 코드북이 필요합니다. 전송은 간단합니다. 독립적인 송신기 각각은 자신의 코드북에서 임의의 코드를 선택합니다. 사용자는 이러한 벡터를 동시에 전송합니다. 수신기는 가우시안 잡음 $\mathbf{Z}$와 함께 더해진 이러한 코드를 수신합니다.

최적 디코딩은 유클리드 거리에서 벡터 합이 $\mathbf{Y}$에 가장 가까운 $m$개의 코드, 즉 각 코드북에서 하나씩을 찾는 것으로 구성됩니다. $\left(R_{1}, R_{2}, \ldots, R_{m}\right)$가 위에 주어진 용량 영역에 있다면, 오류 확률은 $n$이 무한대로 갈 때 0으로 수렴합니다.
<!-- Page 541 -->
Remarks 이 문제에서 사용자의 비율 합 $C(m P / N)$이 $m$과 함께 무한대로 증가하는 것을 보는 것은 흥미롭습니다. 따라서, $N$의 잡음 환경에서 $P$의 파워를 가진 $m$명의 축하객이 모인 칵테일 파티에서, 듣는 사람은 사람이 무한대로 늘어남에 따라 무한한 양의 정보를 받게 됩니다. 물론 지상 통신에서 위성으로의 통신에도 유사한 결론이 적용됩니다. 명백히, 송신자 수 $m \rightarrow \infty$가 증가함에 따라 간섭이 증가하더라도 수신된 총 정보는 제한되지 않습니다.

또한 최적의 전송 방식이 시분할 다중화를 포함하지 않는다는 점도 주목할 만합니다. 사실, 각 송신자는 항상 모든 대역폭을 사용합니다.

# 15.1.3 가우시안 방송 채널

여기서는 파워 $P$를 가진 송신자와 두 개의 원거리 수신자를 가정합니다. 한 수신자는 가우시안 잡음 파워 $N_{1}$을 가지고, 다른 수신자는 가우시안 잡음 파워 $N_{2}$를 가집니다. 일반성을 잃지 않고 $N_{1}<N_{2}$라고 가정합니다. 따라서 수신기 $Y_{1}$은 수신기 $Y_{2}$보다 잡음이 적습니다. 채널 모델은 $Y_{1}=X+Z_{1}$이고 $Y_{2}=X+Z_{2}$이며, 여기서 $Z_{1}$과 $Z_{2}$는 각각 분산 $N_{1}$과 $N_{2}$를 가진 임의로 상관된 가우시안 확률 변수입니다. 송신자는 각각 수신기 $Y_{1}$과 $Y_{2}$로 독립적인 메시지를 비율 $R_{1}$과 $R_{2}$로 보내고자 합니다.

다행히도 모든 스칼라 가우시안 방송 채널은 섹션 15.6.2에서 논의된 열화된 방송 채널 클래스에 속합니다. 이 작업을 전문화하면 가우시안 방송 채널의 용량 영역이 다음과 같음을 알 수 있습니다.

$$
\begin{aligned}
& R_{1}<C\left(\frac{\alpha P}{N_{1}}\right) \\
& R_{2}<C\left(\frac{(1-\alpha) P}{\alpha P+N_{2}}\right)
\end{aligned}
$$

여기서 $\alpha$는 송신자가 원하는 대로 비율 $R_{1}$을 비율 $R_{2}$로 교환하기 위해 임의로 선택될 수 있습니다 $(0 \leq \alpha \leq 1)$.

메시지를 인코딩하기 위해 송신자는 두 개의 코드북을 생성합니다. 하나는 비율 $R_{1}$에 대해 파워 $\alpha P$를 가지고, 다른 하나는 비율 $R_{2}$에 대해 파워 $\bar{\alpha} P$를 가집니다. 여기서 $R_{1}$과 $R_{2}$는 위의 용량 영역에 있습니다. 그런 다음 $Y_{1}$과 $Y_{2}$로 각각 인덱스 $w_{1} \in\left\{1,2, \ldots, 2^{n R_{1}}\right\}$와 $w_{2} \in\left\{1,2, \ldots, 2^{n R_{2}}\right\}$를 보내기 위해, 송신자는 첫 번째 코드북에서 코드워드 $\mathbf{X}\left(w_{1}\right)$와 두 번째 코드북에서 코드워드 $\mathbf{X}\left(w_{2}\right)$를 가져와 합을 계산합니다. 그는 채널을 통해 합을 보냅니다.
<!-- Page 542 -->
수신자는 이제 메시지를 디코딩해야 합니다. 먼저 나쁜 수신자 $Y_{2}$를 고려하십시오. 그는 단순히 두 번째 코드북을 살펴보고 수신된 벡터 $\mathbf{Y}_{2}$에 가장 가까운 코드를 찾습니다. 그의 유효 신호 대 잡음비는 $\bar{\alpha} P /\left(\alpha P+N_{2}\right)$입니다. 왜냐하면 $Y_{1}$의 메시지는 $Y_{2}$에게 잡음으로 작용하기 때문입니다. (이는 증명될 수 있습니다.)

좋은 수신자 $Y_{1}$은 먼저 $Y_{2}$의 코드를 디코딩합니다. 그는 더 낮은 잡음 $N_{1}$ 때문에 이를 수행할 수 있습니다. 그는 이 코드를 $\hat{\mathbf{X}}_{2}$를 $\mathbf{Y}_{1}$에서 뺍니다. 그런 다음 그는 첫 번째 코드북에서 $\mathbf{Y}_{1}-\hat{\mathbf{X}}_{2}$에 가장 가까운 코드를 찾습니다. 결과적인 오류 확률은 원하는 만큼 낮게 만들 수 있습니다.

열화된 브로드캐스트 채널에 대한 최적 인코딩의 좋은 이점은 더 나은 수신자 $Y_{1}$이 자신에게 의도된 메시지 외에도 수신자 $Y_{2}$에게 의도된 메시지를 항상 알고 있다는 것입니다.

# 15.1.4 가우시안 릴레이 채널

릴레이 채널의 경우, 송신자 $X$와 최종 의도된 수신자 $Y$가 있습니다. 또한 수신자를 돕기 위한 릴레이 채널도 존재합니다. 가우시안 릴레이 채널(15.7절의 그림 15.31)은 다음과 같이 주어집니다.

$$
\begin{aligned}
Y_{1} & =X+Z_{1} \\
Y & =X+Z_{1}+X_{1}+Z_{2}
\end{aligned}
$$

여기서 $Z_{1}$과 $Z_{2}$는 각각 분산 $N_{1}$과 $N_{2}$를 갖는 독립적인 평균 제로 가우시안 랜덤 변수입니다. 릴레이에 의해 허용되는 인코딩은 인과적 시퀀스입니다.

$$
X_{1 i}=f_{i}\left(Y_{11}, Y_{12}, \ldots, Y_{1 i-1}\right)
$$

송신자 $X$는 전력 $P$를 가지고 송신자 $X_{1}$은 전력 $P_{1}$을 가집니다. 용량은 다음과 같습니다.

$$
C=\max _{0 \leq \alpha \leq 1} \min \left\{C\left(\frac{P+P_{1}+2 \sqrt{\bar{\alpha} P P_{1}}}{N_{1}+N_{2}}\right), C\left(\frac{\alpha P}{N_{1}}\right)\right\}
$$

여기서 $\bar{\alpha}=1-\alpha$입니다. 만약

$$
\frac{P_{1}}{N_{2}} \geq \frac{P}{N_{1}}
$$

이면 $C=C\left(P / N_{1}\right)$임을 알 수 있으며, 이는 $\alpha=1$로 달성됩니다. 채널은 릴레이 이후 잡음이 없는 것처럼 보이며, $X$에서 릴레이까지의 용량 $C\left(P / N_{1}\right)$을 달성할 수 있습니다. 따라서 릴레이가 없는 $C\left(P /\left(N_{1}+N_{2}\right)\right)$의 속도는 릴레이의 존재로 인해 $C\left(P / N_{1}\right)$으로 증가합니다. 큰 경우
<!-- Page 543 -->
$N_{2}$이고 $P_{1} / N_{2} \geq P / N_{1}$일 때, 전송률의 증가는 $C\left(P /\left(N_{1}+N_{2}\right)\right) \approx 0$에서 $C\left(P / N_{1}\right)$까지임을 알 수 있습니다.

$R_{1}<C\left(\alpha P / N_{1}\right)$이라고 합시다. 두 개의 코드북이 필요합니다. 첫 번째 코드북은 전력 $\alpha P$를 갖는 $2^{n R_{1}}$개의 단어를 가지고 있습니다. 두 번째 코드북은 전력 $\bar{\alpha} P$를 갖는 $2^{n R_{0}}$개의 코드를 가지고 있습니다. 릴레이의 협력을 위한 기회를 만들기 위해 이 코드북들의 코드를 순차적으로 사용할 것입니다. 첫 번째 코드북의 코드를 전송하는 것으로 시작합니다. 릴레이는 $R_{1}<C\left(\alpha P / N_{1}\right)$이므로 이 코드의 인덱스를 알고 있지만, 수신자는 $2^{n\left(R_{1}-C\left(\alpha P /\left(N_{1}+N_{2}\right)\right)\right)}$ 크기의 가능한 코드 목록을 가지고 있습니다. 이 목록 계산은 목록 코드에 대한 결과를 포함합니다.

다음 블록에서 송신자와 릴레이는 이전에 수신자의 목록에 있던 코드에 대한 수신자의 불확실성을 해결하기 위해 협력하기를 원합니다. 불행히도, 수신된 신호 $Y$를 모르기 때문에 이 목록이 무엇인지 확신할 수 없습니다. 따라서 첫 번째 코드북을 $2^{n R_{0}}$개의 셀로 무작위로 분할하며 각 셀에는 동일한 수의 코드가 포함됩니다. 릴레이, 수신자, 송신자는 이 분할에 동의합니다. 릴레이와 송신자는 첫 번째 코드북의 코드가 속한 분할 셀을 찾고 해당 인덱스를 갖는 두 번째 코드북의 코드를 협력하여 전송합니다. 즉, $X$와 $X_{1}$은 동일한 지정된 코드를 전송합니다. 릴레이는 물론 이 코드를 그의 전력 제약 조건 $P_{1}$을 충족하도록 스케일링해야 합니다. 이제 그들은 코드를 동시에 전송합니다. 여기서 주목해야 할 중요한 점은 릴레이와 송신자가 보낸 협력 정보가 일관되게 전송된다는 것입니다. 따라서 수신자 $Y$가 보는 합의 전력은 $\left(\sqrt{\bar{\alpha} P}+\sqrt{P_{1}}\right)^{2}$입니다.

그러나 이것이 송신자가 두 번째 블록에서 하는 모든 것은 아닙니다. 그는 또한 첫 번째 코드북에서 새로운 코드를 선택하고, 이를 두 번째 코드북의 협력 코드에 "종이상으로" 추가하여 채널을 통해 합을 전송합니다.

최종 수신자 $Y$의 두 번째 블록에서의 수신은 두 번째 코드북에서 가장 가까운 코드를 찾아 두 번째 코드북에서 협력 인덱스를 찾는 것을 포함합니다. 그는 수신된 시퀀스에서 코드를 빼고, 두 번째 블록에서 전송되었을 수 있는 첫 번째 코드북의 모든 코드에 해당하는 $2^{n R_{0}}$ 크기의 인덱스 목록을 계산합니다.

이제 최종 수신자가 첫 번째 블록에서 전송된 첫 번째 코드북의 코드를 완성할 차례입니다. 그는 첫 번째 블록에서 전송되었을 수 있는 가능한 코드 목록을 가져와 두 번째 블록의 협력 릴레이 전송에서 학습한 분할 셀과 교차시킵니다. 비율과 전력이 선택되어 하나만 있을 확률이 매우 높습니다.
<!-- Page 544 -->
교차점의 코드워드입니다. 이것은 $Y$가 첫 번째 블록에서 전송된 정보에 대해 추측한 것입니다.

이제 정상 상태에 도달했습니다. 각 새 블록에서 송신기와 릴레이는 이전 블록의 목록 불확실성을 해결하기 위해 협력합니다. 또한 송신기는 두 번째 코드북에서의 전송에 첫 번째 코드북의 일부 새로운 정보를 중첩하여 합을 전송합니다. 수신기는 항상 한 블록 뒤처지지만, 충분히 많은 블록의 경우 이는 수신율의 전반적인 속도에 영향을 미치지 않습니다.

# 15.1.5 가우시안 간섭 채널

간섭 채널은 두 명의 송신기와 두 명의 수신기를 가집니다. 송신기 1은 수신기 1에게 정보를 보내기를 원합니다. 그는 수신기 2가 무엇을 받고 이해하는지는 신경 쓰지 않습니다. 마찬가지로 송신기 2와 수신기 2도 그렇습니다. 각 채널은 다른 채널과 간섭합니다. 이 채널은 그림 15.5에 설명되어 있습니다. 각 송신기에 대해 의도된 수신기가 하나뿐이므로 브로드캐스트 채널이라고 하기는 어렵고, 각 수신기는 해당 송신기가 보내는 것에만 관심이 있으므로 다중 접속 채널이라고 하기도 어렵습니다. 대칭 간섭의 경우 다음과 같습니다.

$$
\begin{aligned}
& Y_{1}=X_{1}+a X_{2}+Z_{1} \\
& Y_{2}=X_{2}+a X_{1}+Z_{2}
\end{aligned}
$$

여기서 $Z_{1}, Z_{2}$는 독립적인 $\mathcal{N}(0, N)$ 확률 변수입니다. 이 채널은 가우시안 경우에도 일반적으로 해결되지 않았습니다. 하지만 놀랍게도 높은 간섭의 경우, 이 채널의 용량 영역이 전혀 간섭이 없는 경우와 동일하다는 것을 보여줄 수 있습니다.

이를 달성하기 위해 각각 전력 $P$와 속도 $C(P / N)$를 가진 두 개의 코드북을 생성합니다. 각 송신기는 자신의 책에서 독립적으로 단어를 선택하고

그림 15.5. 가우시안 간섭 채널.
<!-- Page 545 -->
전송합니다. 이제 간섭 $a$가 $C\left(a^{2} P /(P+N)\right)>C(P / N)$를 만족하면, 첫 번째 송신자는 두 번째 송신자의 인덱스를 완벽하게 이해합니다. 그는 수신된 신호에 가장 가까운 코드워드를 찾는 일반적인 기법을 사용하여 이를 파악합니다. 이 신호를 찾으면 자신의 수신 파형에서 이를 뺍니다. 이제 그와 송신자 사이에 깨끗한 채널이 형성됩니다. 그런 다음 송신자의 코드북을 검색하여 가장 가까운 코드워드를 찾아 해당 코드워드가 전송된 것이라고 선언합니다.

# 15.1.6 가우시안 양방향 채널

양방향 채널은 간섭 채널과 매우 유사하며, 그림 15.6과 같이 송신자 1이 수신자 2에 연결되고 송신자 2가 수신자 1에 연결된다는 추가적인 조건이 있습니다. 따라서 송신자 1은 수신자 2의 이전 수신 심볼 정보를 사용하여 다음에 무엇을 보낼지 결정할 수 있습니다. 이 채널은 네트워크 정보 이론의 또 다른 근본적인 측면, 즉 피드백을 도입합니다. 피드백은 송신자들이 서로의 메시지에 대해 각자가 가지고 있는 부분 정보를 사용하여 서로 협력할 수 있도록 합니다.

양방향 채널의 용량 영역은 일반적으로 알려져 있지 않습니다. 이 채널은 Shannon [486]에 의해 처음 고려되었으며, 그는 영역에 대한 상한과 하한을 도출했습니다(문제 15.15 참조). 가우시안 채널의 경우, 이 두 경계는 일치하며 용량 영역은 알려져 있습니다. 실제로 가우시안 양방향 채널은 두 개의 독립적인 채널로 분해됩니다.

$P_{1}$과 $P_{2}$를 각각 송신자 1과 2의 전력이라고 하고, $N_{1}$과 $N_{2}$를 두 채널의 잡음 분산이라고 합시다. 그러면 간섭 채널에 대해 설명된 기법을 사용하여 $R_{1}<C\left(P_{1} / N_{1}\right)$ 및 $R_{2}<C\left(P_{2} / N_{2}\right)$의 속도를 달성할 수 있습니다. 이 경우, 속도 $R_{1}$ 및 $R_{2}$의 두 코드북을 생성합니다. 송신자 1은 첫 번째 코드북에서 코드워드를 전송합니다. 수신자 2는 송신자가 보낸 코드워드의 합을 수신합니다.

그림 15.6. 양방향 채널.
<!-- Page 546 -->
두 송신기와 약간의 잡음이 있습니다. 그는 단순히 송신기 2의 부호어를 빼내면 송신기 1로부터 깨끗한 채널(분산 $N_{1}$의 잡음만 포함)을 얻게 됩니다. 따라서 양방향 가우시안 채널은 두 개의 독립적인 가우시안 채널로 분해됩니다. 그러나 일반적인 양방향 채널의 경우는 그렇지 않습니다. 일반적으로 두 송신기 간에는 상충 관계가 존재하므로 두 송신기가 동시에 최적 속도로 전송할 수 없습니다.

# 15.2 공동으로 전형적인 시퀀스

다중 사용자 가우시안 채널을 고려하여 네트워크의 용량 결과를 미리 살펴보았습니다. 이 섹션에서는 7장에서 증명된 공동 AEP를 네트워크 정보 이론의 정리를 증명하는 데 사용할 형태로 확장하여 보다 상세한 분석을 시작합니다. 공동 AEP를 통해 이 장에서 고려된 다양한 코딩 방식에 대한 공동으로 전형적인 디코딩의 오류 확률을 계산할 수 있습니다.

$\left(X_{1}, X_{2}, \ldots, X_{k}\right)$를 고정된 결합 분포 $p\left(x^{(1)}, x^{(2)}, \ldots, x^{(k)}\right),\left(x^{(1)}, x^{(2)}\right.$, $\left.\ldots, x^{(k)}\right) \in \mathcal{X}_{1} \times \mathcal{X}_{2} \times \cdots \times \mathcal{X}_{k}$를 갖는 유한한 이산 확률 변수들의 모음이라고 합시다. $S$를 이 확률 변수들의 순서 있는 부분 집합이라고 하고 $S$의 독립적인 복사본 $n$개를 고려합니다. 따라서,

$$
\operatorname{Pr}\{S=s\}=\prod_{i=1}^{n} \operatorname{Pr}\left\{S_{i}=s_{i}\right\}, \quad s \in \mathcal{S}^{n}
$$

예를 들어, $S=\left(X_{j}, X_{l}\right)$이면,

$$
\begin{aligned}
\operatorname{Pr}\{S=s\} & =\operatorname{Pr}\left\{\left(\mathbf{X}_{j}, \mathbf{X}_{l}\right)=\left(\mathbf{x}_{j}, \mathbf{x}_{l}\right)\right\} \\
& =\prod_{i=1}^{n} p\left(x_{i j}, x_{i l}\right)
\end{aligned}
$$

명확하게 하기 위해 때때로 $S$에 대해 $X(S)$를 사용할 것입니다. 대수의 법칙에 따라, 임의의 확률 변수 부분 집합 $S$에 대해,

$$
-\frac{1}{n} \log p\left(S_{1}, S_{2}, \ldots, S_{n}\right)=-\frac{1}{n} \sum_{i=1}^{n} \log p\left(S_{i}\right) \rightarrow H(S)
$$

여기서 수렴은 모든 $2^{k}$ 부분 집합 $S \subseteq\left\{X^{(1)}, X^{(2)}, \ldots, X^{(k)}\right\}$에 대해 확률 1로 발생합니다.
<!-- Page 547 -->
정의 $ \epsilon $-typical $n$-sequence $\left(\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{k}\right)$의 집합 $A_{\epsilon}^{(n)}$은 다음과 같이 정의됩니다.

$$
\begin{aligned}
& A_{\epsilon}^{(n)}\left(X^{(1)}, X^{(2)}, \ldots, X^{(k)}\right) \\
& =A_{\epsilon}^{(n)} \\
& =\left\{\left(\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{k}\right):\left|-\frac{1}{n} \log p(\mathbf{s})-H(S)\right|<\epsilon, \forall S \subseteq\left\{X^{(1)}, X^{(2)}, \ldots\right.\right. \\
& \left.\left.\quad X^{(k)}\right\}\right\}
\end{aligned}
$$

$A_{\epsilon}^{(n)}(S)$는 $S$의 좌표로 제한된 $A_{\epsilon}^{(n)}$을 나타냅니다. 따라서 $S=\left(X_{1}, X_{2}\right)$이면 다음과 같습니다.

$$
\begin{aligned}
A_{\epsilon}^{(n)}\left(X_{1}, X_{2}\right)= & \left\{\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right):\right. \\
& \left|-\frac{1}{n} \log p\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)-H\left(X_{1}, X_{2}\right)\right|<\epsilon \\
& \left|-\frac{1}{n} \log p\left(\mathbf{x}_{1}\right)-H\left(X_{1}\right)\right|<\epsilon \\
& \left|-\frac{1}{n} \log p\left(\mathbf{x}_{2}\right)-H\left(X_{2}\right)\right|<\epsilon\}
\end{aligned}
$$

정의 $a_{n} \doteq 2^{n(b \pm \epsilon)}$ 표기는 충분히 큰 $n$에 대해 다음과 같음을 의미합니다.

$$
\left|\frac{1}{n} \log a_{n}-b\right|<\epsilon
$$

정리 15.2.1 모든 $ \epsilon>0 $에 대해, 충분히 큰 $n$에 대해,

1. $P\left(A_{\epsilon}^{(n)}(S)\right) \geq 1-\epsilon, \quad \forall S \subseteq\left\{X^{(1)}, X^{(2)}, \ldots, X^{(k)}\right\}$.
2. $ \mathbf{s} \in A_{\epsilon}^{(n)}(S) \Longrightarrow p(\mathbf{s}) \doteq 2^{n(H(S) \pm \epsilon)}$.
3. $\left|A_{\epsilon}^{(n)}(S)\right| \doteq 2^{n(H(S) \pm 2 \epsilon)}$.
<!-- Page 548 -->
4. $S_{1}, S_{2} \subseteq\left\{X^{(1)}, X^{(2)}, \ldots, X^{(k)}\right\}$라고 가정합니다. 만약 $\left(\mathbf{s}_{1}, \mathbf{s}_{2}\right) \in A_{\epsilon}^{(n)}\left(S_{1}, S_{2}\right)$이면,

$$
p\left(\mathbf{s}_{1} \mid \mathbf{s}_{2}\right) \doteq 2^{-n\left(H\left(S_{1} \mid S_{2}\right) \pm 2 \epsilon\right)}
$$

# 증명

1. 이는 $A_{\epsilon}^{(n)}(S)$의 정의에 있는 확률 변수에 대한 대수의 법칙으로부터 도출됩니다.
2. 이는 $A_{\epsilon}^{(n)}(S)$의 정의로부터 직접 도출됩니다.
3. 이는 다음으로부터 도출됩니다.

$$
\begin{aligned}
1 & \geq \sum_{\mathbf{s} \in A_{\epsilon}^{(n)}(S)} p(\mathbf{s}) \\
& \geq \sum_{\mathbf{s} \in A_{\epsilon}^{(n)}(S)} 2^{-n(H(S)+\epsilon)} \\
& =\left|A_{\epsilon}^{(n)}(S)\right| 2^{-n(H(S)+\epsilon)}
\end{aligned}
$$

$n$이 충분히 크다면, 다음과 같이 주장할 수 있습니다.

$$
\begin{aligned}
1-\epsilon & \leq \sum_{\mathbf{s} \in A_{\epsilon}^{(n)}(S)} p(\mathbf{s}) \\
& \leq \sum_{\mathbf{s} \in A_{\epsilon}^{(n)}(S)} 2^{-n(H(S)-\epsilon)} \\
& =\left|A_{\epsilon}^{(n)}(S)\right| 2^{-n(H(S)-\epsilon)}
\end{aligned}
$$

(15.33)과 (15.36)을 결합하면, $n$이 충분히 크다면 $\left|A_{\epsilon}^{(n)}(S)\right| \doteq 2^{n(H(S) \pm 2 \epsilon)}$임을 알 수 있습니다.
4. $\left(\mathbf{s}_{1}, \mathbf{s}_{2}\right) \in A_{\epsilon}^{(n)}\left(S_{1}, S_{2}\right)$에 대해, $p\left(\mathbf{s}_{1}\right) \doteq 2^{-n\left(H\left(S_{1}\right) \pm \epsilon\right)}$이고 $p\left(\mathbf{s}_{1}, \mathbf{s}_{2}\right) \doteq 2^{-n\left(H\left(S_{1}, S_{2}\right) \pm \epsilon\right)}$입니다. 따라서,

$$
p\left(\mathbf{s}_{2} \mid \mathbf{s}_{1}\right)=\frac{p\left(\mathbf{s}_{1}, \mathbf{s}_{2}\right)}{p\left(\mathbf{s}_{1}\right)} \doteq 2^{-n\left(H\left(S_{2} \mid S_{1}\right) \pm 2 \epsilon\right)}
$$

다음 정리는 주어진 типичный последовательность에 대한 조건부 типичный последовательность의 수를 제한합니다.
<!-- Page 549 -->
정리 15.2.2 $S_{1}, S_{2}$를 $X^{(1)}, X^{(2)}, \ldots, X^{(k)}$의 두 부분집합이라고 합시다. 임의의 $\epsilon>0$에 대하여, $A_{\epsilon}^{(n)}\left(S_{1} \mid \mathbf{s}_{2}\right)$를 특정 $\mathbf{s}_{2}$ 시퀀스와 공동으로 $\epsilon$ 전형적인 $\mathbf{s}_{1}$ 시퀀스의 집합으로 정의합니다. 만약 $\mathbf{s}_{2} \in A_{\epsilon}^{(n)}\left(S_{2}\right)$이면, 충분히 큰 $n$에 대하여 다음이 성립합니다.

$$
\left|A_{\epsilon}^{(n)}\left(S_{1} \mid \mathbf{s}_{2}\right)\right| \leq 2^{n\left(H\left(S_{1} \mid S_{2}\right)+2 \epsilon\right)}
$$

그리고

$$
(1-\epsilon) 2^{n\left(H\left(S_{1} \mid S_{2}\right)-2 \epsilon\right)} \leq \sum_{\mathbf{s}_{2}} p\left(\mathbf{s}_{2}\right)\left|A_{\epsilon}^{(n)}\left(S_{1} \mid \mathbf{s}_{2}\right)\right|
$$

증명: 정리 15.2.1의 3항과 마찬가지로, 다음이 성립합니다.

$$
\begin{aligned}
1 & \geq \sum_{\mathbf{s}_{1} \in A_{\epsilon}^{(n)}\left(S_{1} \mid \mathbf{s}_{2}\right)} p\left(\mathbf{s}_{1} \mid \mathbf{s}_{2}\right) \\
& \geq \sum_{\mathbf{s}_{1} \in A_{\epsilon}^{(n)}\left(S_{1} \mid \mathbf{s}_{2}\right)} 2^{-n\left(H\left(S_{1} \mid S_{2}\right)+2 \epsilon\right)} \\
& =\left|A_{\epsilon}^{(n)}\left(\mathbf{S}_{1} \mid \mathbf{s}_{2}\right)\right| 2^{-n\left(H\left(S_{1} \mid S_{2}\right)+2 \epsilon\right)}
\end{aligned}
$$

만약 $n$이 충분히 크다면, (15.27)로부터 다음과 같이 추론할 수 있습니다.

$$
\begin{aligned}
1-\epsilon & \leq \sum_{\mathbf{s}_{2}} p\left(\mathbf{s}_{2}\right) \sum_{\mathbf{s}_{1} \in A_{\epsilon}^{(n)}\left(S_{1} \mid \mathbf{s}_{2}\right)} p\left(\mathbf{s}_{1} \mid \mathbf{s}_{2}\right) \\
& \leq \sum_{\mathbf{s}_{2}} p\left(\mathbf{s}_{2}\right) \sum_{\mathbf{s}_{1} \in A_{\epsilon}^{(n)}\left(S_{1} \mid \mathbf{s}_{2}\right)} 2^{-n\left(H\left(S_{1} \mid S_{2}\right)-2 \epsilon\right)} \\
& =\sum_{\mathbf{s}_{2}} p\left(\mathbf{s}_{2}\right)\left|A_{\epsilon}^{(n)}\left(S_{1} \mid \mathbf{s}_{2}\right)\right| 2^{-n\left(H\left(S_{1} \mid S_{2}\right)-2 \epsilon\right)}
\end{aligned}
$$

디코딩 오류 확률을 계산하기 위해, 조건부 독립 시퀀스가 공동으로 전형적인 확률을 알아야 합니다. $S_{1}, S_{2}$, 그리고 $S_{3}$를 $\left\{X^{(1)}, X^{(2)}, \ldots, X^{(k)}\right\}$의 세 부분집합이라고 합시다. 만약 $S_{1}^{\prime}$와 $S_{2}^{\prime}$가 $S_{3}^{\prime}$가 주어졌을 때 조건부 독립이고, 그 외에는 $\left(S_{1}, S_{2}, S_{3}\right)$의 쌍별 주변 확률과 동일하다면, 다음과 같은 공동 전형성 확률을 가집니다.
<!-- Page 550 -->
정리 15.2.3 $A_{\epsilon}^{(n)}$를 확률 질량 함수 $p\left(s_{1}, s_{2}, s_{3}\right)$에 대한 전형 집합이라고 할 때,

$$
P\left(\mathbf{S}_{1}^{\prime}=\mathbf{s}_{1}, \mathbf{S}_{2}^{\prime}=\mathbf{s}_{2}, \mathbf{S}_{3}^{\prime}=\mathbf{s}_{3}\right)=\prod_{i=1}^{n} p\left(s_{1 i} \mid s_{3 i}\right) p\left(s_{2 i} \mid s_{3 i}\right) p\left(s_{3 i}\right)
$$

그러면

$$
P\left\{\left(\mathbf{S}_{1}^{\prime}, \mathbf{S}_{2}^{\prime}, \mathbf{S}_{3}^{\prime}\right) \in A_{\epsilon}^{(n)}\right\} \doteq 2^{n\left(I\left(S_{1} ; S_{2} \mid S_{3}\right) \pm 6 \epsilon\right)}
$$

증명: 상하한을 별도로 계산하는 것을 피하기 위해 (15.26)의 $\doteq$ 표기법을 사용합니다. 우리는 다음과 같이 합니다.

$$
\begin{aligned}
& P\left\{\left(\mathbf{S}_{1}^{\prime}, \mathbf{S}_{2}^{\prime}, \mathbf{S}_{3}^{\prime}\right) \in A_{\epsilon}^{(n)}\right\} \\
& =\sum_{\left(\mathbf{s}_{1}, \mathbf{s}_{2}, \mathbf{s}_{3}\right) \in A_{\epsilon}^{(n)}} p\left(\mathbf{s}_{3}\right) p\left(\mathbf{s}_{1} \mid \mathbf{s}_{3}\right) p\left(\mathbf{s}_{2} \mid \mathbf{s}_{3}\right) \\
& \doteq\left|A_{\epsilon}^{(n)}\left(S_{1}, S_{2}, S_{3}\right)\right| 2^{-n\left(H\left(S_{3}\right) \pm \epsilon\right)} 2^{-n\left(H\left(S_{1} \mid S_{3}\right) \pm 2 \epsilon\right)} 2^{-n\left(H\left(S_{2} \mid S_{3}\right) \pm 2 \epsilon\right)} \\
& \doteq 2^{n\left(H\left(S_{1}, S_{2}, S_{3}\right) \pm \epsilon\right)} 2^{-n\left(H\left(S_{3}\right) \pm \epsilon\right)} 2^{-n\left(H\left(S_{1} \mid S_{3}\right) \pm 2 \epsilon\right)} 2^{-n\left(H\left(S_{2} \mid S_{3}\right) \pm 2 \epsilon\right)} \\
& \doteq 2^{-n\left(I\left(S_{1} ; S_{2} \mid S_{3}\right) \pm 6 \epsilon\right)}
\end{aligned}
$$

이 정리를 본 장의 다양한 달성 가능성 증명을 위해 $S_{1}, S_{2}$, 및 $S_{3}$의 특정 선택에 대해 전문화할 것입니다.

# 15.3 다중 접속 채널

처음으로 자세히 살펴볼 채널은 다중 접속 채널로, 두 명(또는 그 이상)의 송신자가 공통 수신기로 정보를 보냅니다. 채널은 그림 15.7에 나와 있습니다. 이 채널의 일반적인 예는 여러 독립적인 지상국이 있는 위성 수신기 또는 기지국과 통신하는 휴대폰 세트입니다. 송신자는 수신기 잡음뿐만 아니라 서로 간의 간섭에도 대처해야 함을 알 수 있습니다.

정의 이산 무기억 다중 접속 채널은 세 개의 알파벳 $\mathcal{X}_{1}, \mathcal{X}_{2}$, 및 $\mathcal{Y}$와 확률 전이 행렬 $p\left(y \mid x_{1}, x_{2}\right)$로 구성됩니다.
<!-- Page 551 -->

그림 15.7. 다중 접속 채널.

정의 $\mathrm{A}\left(\left(2^{n R_{1}}, 2^{n R_{2}}\right), n\right)$ 코드는 다중 접속 채널에 대해 두 개의 정수 집합 $\mathcal{W}_{1}=\left\{1,2, \ldots, 2^{n R_{1}}\right\}$ 및 $\mathcal{W}_{2}=\{1,2, \ldots$, $\left.2^{n R_{2}}\right\}$ (메시지 집합이라고 함), 두 개의 인코딩 함수,

$$
X_{1}: \mathcal{W}_{1} \rightarrow \mathcal{X}_{1}^{n}
$$

및

$$
X_{2}: \mathcal{W}_{2} \rightarrow \mathcal{X}_{2}^{n}
$$

그리고 디코딩 함수,

$$
g: \mathcal{Y}^{n} \rightarrow \mathcal{W}_{1} \times \mathcal{W}_{2}
$$

로 구성됩니다. 이 채널에는 두 명의 송신자와 한 명의 수신자가 있습니다. 송신자 1은 $\left\{1,2, \ldots, 2^{n R_{1}}\right\}$ 집합에서 균등하게 인덱스 $W_{1}$을 선택하고 해당 코드워드를 채널로 전송합니다. 송신자 2도 마찬가지입니다. 메시지가 곱 집합 $\mathcal{W}_{1} \times \mathcal{W}_{2}$에 대해 균등하게 분포한다고 가정하면 (즉, 메시지가 독립적이고 동일하게 가능성이 있음), $\left(\left(2^{n R_{1}}, 2^{n R_{2}}\right), n\right)$ 코드에 대한 평균 오류 확률을 다음과 같이 정의합니다.

$$
P_{e}^{(n)}=\frac{1}{2^{n\left(R_{1}+R_{2}\right)}} \sum_{\left(w_{1}, w_{2}\right) \in \mathcal{W}_{1} \times \mathcal{W}_{2}} \operatorname{Pr}\left\{g\left(Y^{n}\right) \neq\left(w_{1}, w_{2}\right) \mid\left(w_{1}, w_{2}\right) \text { sent }\right\}
$$

정의 속도 쌍 $\left(R_{1}, R_{2}\right)$는 $P_{e}^{(n)} \rightarrow 0$인 $\left(\left(2^{n R_{1}}, 2^{n R_{2}}\right), n\right)$ 코드의 시퀀스가 존재하면 다중 접속 채널에 대해 달성 가능하다고 말합니다.
<!-- Page 552 -->
정의 다중 접속 채널의 용량 영역은 달성 가능한 $\left(R_{1}, R_{2}\right)$ 비율 쌍의 폐포입니다.

다중 접속 채널의 용량 영역의 예시는 그림 15.8에 나와 있습니다. 먼저 용량 영역을 정리의 형태로 명시합니다.

정리 15.3.1 (다중 접속 채널 용량) 다중 접속 채널 $\left(\mathcal{X}_{1} \times \mathcal{X}_{2}, p\left(y \mid x_{1}, x_{2}\right), \mathcal{Y}\right)$의 용량은 다음을 만족하는 모든 $\left(R_{1}, R_{2}\right)$의 볼록 껍질의 폐포입니다.

$$
\begin{aligned}
R_{1} & <I\left(X_{1} ; Y \mid X_{2}\right) \\
R_{2} & <I\left(X_{2} ; Y \mid X_{1}\right) \\
R_{1}+R_{2} & <I\left(X_{1}, X_{2} ; Y\right)
\end{aligned}
$$

$\mathcal{X}_{1} \times \mathcal{X}_{2}$ 상의 일부 곱 분포 $p_{1}\left(x_{1}\right) p_{2}\left(x_{2}\right)$에 대해.
이것이 다중 접속 채널의 용량 영역임을 증명하기 전에 몇 가지 다중 접속 채널의 예를 고려해 보겠습니다.

예시 15.3.1 (독립 이진 대칭 채널) 그림 15.9와 같이 첫 번째 송신자와 두 번째 송신자에 대해 각각 독립적인 두 개의 이진 대칭 채널이 있다고 가정합니다. 이 경우, 제7장의 결과로부터 첫 번째 채널에서 $1-H\left(p_{1}\right)$의 비율로, 두 번째 채널에서 $1-H\left(p_{2}\right)$의 비율로 전송할 수 있다는 것은 분명합니다.

그림 15.8. 다중 접속 채널의 용량 영역.
<!-- Page 553 -->

그림 15.9. 독립적인 이진 대칭 채널.

채널들이 독립적이므로 송신자 간에는 간섭이 없습니다. 이 경우의 용량 영역은 그림 15.10에 나타나 있습니다.

예제 15.3.2 (이진 곱셈 채널) 이진 입력과 출력을 갖는 다중 접속 채널을 고려해 봅시다.

$$
Y=X_{1} X_{2}
$$

이러한 채널을 이진 곱셈 채널이라고 합니다. $X_{2}=1$로 설정하면 송신자 1로부터 수신자에게 초당 1비트의 속도로 전송할 수 있음을 쉽게 알 수 있습니다. 마찬가지로 $X_{1}=1$로 설정하면 $R_{2}=1$을 달성할 수 있습니다. 출력은 이진이므로 송신자 1과 송신자 2의 결합된 속도 $R_{1}+R_{2}$는 1비트를 초과할 수 없습니다. 시분할을 통해 $R_{1}+R_{2}=1$을 만족하는 속도의 모든 조합을 달성할 수 있습니다. 따라서 용량 영역은 그림 15.11에 나타난 것과 같습니다.

예제 15.3.3 (이진 삭제 다중 접속 채널) 이 다중 접속 채널은 이진 입력 $\mathcal{X}_{1}=\mathcal{X}_{2}=\{0,1\}$과 삼진 출력 $Y=X_{1}+X_{2}$를 갖습니다. $Y=0$ 또는 $Y=2$가 수신되면 $(X_{1}, X_{2})$에 모호성이 없지만, $Y=1$은 $(0,1)$ 또는 $(1,0)$에서 발생할 수 있습니다.
<!-- Page 554 -->

그림 15.10. 독립적인 BSC의 용량 영역.

그림 15.11. 이진 곱셈 채널의 용량 영역.

이제 축상의 달성 가능한 속도를 살펴보겠습니다. $X_{2}=0$으로 설정하면 송신자 1로부터 전송당 1비트의 속도로 전송할 수 있습니다. 마찬가지로 $X_{1}=0$으로 설정하면 $R_{2}=1$의 속도로 전송할 수 있습니다. 이것이 용량 영역의 두 극단점을 제공합니다. 더 나은 결과를 얻을 수 있을까요? $R_{1}=1$이라고 가정하면 $X_{1}$의 코드워드는 가능한 모든 이진 시퀀스를 포함해야 합니다. $X_{1}$은 베르누이 $\left(\frac{1}{2}\right)$ 프로세스처럼 보일 것입니다. 이것은 $X_{2}$로부터의 전송에 노이즈처럼 작용합니다. $X_{2}$의 경우 채널은 그림 15.12의 채널처럼 보입니다. 이것은 7장의 이진 삭제 채널입니다. 결과를 다시 생각해보면,
<!-- Page 555 -->

그림 15.12. 이진 삭제 다중 접속 채널의 사용자 2에 대한 등가 단일 사용자 채널.

그림 15.13. 이진 삭제 다중 접속 채널의 용량 영역.
이 채널의 용량은 전송당 $\frac{1}{2}$ 비트입니다. 따라서 송신자 1에 대해 최대 속도 1로 전송할 때, 송신자 2로부터 추가로 $\frac{1}{2}$ 비트를 전송할 수 있습니다. 나중에 용량 영역을 도출한 후, 이러한 속도가 달성할 수 있는 최상의 속도임을 확인할 수 있습니다. 이진 삭제 채널의 용량 영역은 그림 15.13에 나와 있습니다.
<!-- Page 556 -->
# 15.3.1 다중 접속 채널의 용량 영역 달성 가능성

이제 정리 15.3.1의 비율 영역 달성 가능성을 증명합니다. 역정리 증명은 다음 섹션으로 넘기겠습니다. 달성 가능성 증명은 단일 사용자 채널 증명과 매우 유사합니다. 따라서 단일 사용자 경우와 다른 점만 강조하겠습니다. 먼저 고정된 곱 확률 분포 $p\left(x_{1}\right) p\left(x_{2}\right)$에 대해 (15.58)을 만족하는 비율 쌍의 달성 가능성을 증명합니다. 섹션 15.3.3에서는 이를 확장하여 (15.58)의 볼록 껍질 내 모든 점이 달성 가능함을 증명합니다.

증명: (정리 15.3.1의 달성 가능성). $p\left(x_{1}, x_{2}\right)=p_{1}\left(x_{1}\right) p_{2}\left(x_{2}\right)$를 고정합니다.
코드북 생성: 길이 $n$의 독립적인 $2^{n R_{1}}$개의 코드워드 $\mathbf{X}_{1}(i)$, $i \in\left\{1,2, \ldots, 2^{n R_{1}}\right\}$를 생성합니다. 각 요소는 i.i.d. $\sim \prod_{i=1}^{n} p_{1}\left(x_{1 i}\right)$로 생성합니다. 마찬가지로, 각 요소는 i.i.d. $\sim \prod_{i=1}^{n} p_{2}\left(x_{2 i}\right)$로 생성하는 독립적인 $2^{n R_{2}}$개의 코드워드 $\mathbf{X}_{2}(j)$, $j \in\left\{1,2, \ldots, 2^{n R_{2}}\right\}$를 생성합니다. 이 코드워드들은 송신자와 수신자에게 공개되는 코드북을 형성합니다.

인코딩: 인덱스 $i$를 전송하기 위해 송신자 1은 코드워드 $\mathbf{X}_{1}(i)$를 전송합니다. 마찬가지로, $j$를 전송하기 위해 송신자 2는 $\mathbf{X}_{2}(j)$를 전송합니다.

디코딩: $A_{\epsilon}^{(n)}$을 전형적인 $(\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{y})$ 시퀀스의 집합이라고 합시다. 수신자 $Y^{n}$은 다음과 같이 $(i, j)$ 쌍을 선택합니다.

$$
\left(\mathbf{x}_{1}(i), \mathbf{x}_{2}(j), \mathbf{y}\right) \in A_{\epsilon}^{(n)}
$$

이러한 쌍 $(i, j)$가 존재하고 유일한 경우에만 선택하며, 그렇지 않으면 오류가 선언됩니다.

오류 확률 분석: 랜덤 코드 구성의 대칭성으로 인해, 조건부 오류 확률은 어떤 인덱스 쌍이 전송되었는지에 따라 달라지지 않습니다. 따라서 조건부 오류 확률은 무조건부 오류 확률과 동일합니다. 그러므로 일반성을 잃지 않고 $(i, j)=(1,1)$이 전송되었다고 가정할 수 있습니다.

올바른 코드워드가 수신 시퀀스와 전형적이지 않거나, 잘못된 코드워드 쌍이 수신 시퀀스와 전형적인 경우 오류가 발생합니다. 다음과 같이 이벤트를 정의합니다.

$$
E_{i j}=\left\{\left(\mathbf{X}_{1}(i), \mathbf{X}_{2}(j), \mathbf{Y}\right) \in A_{\epsilon}^{(n)}\right\}
$$
<!-- Page 557 -->
그러면 사건들의 합집합에 의해,

$$
\begin{aligned}
P_{e}^{(n)}= & P\left(E_{11}^{c} \bigcup \cup_{(i, j) \neq(1,1)} E_{i j}\right) \\
\leq & P\left(E_{11}^{c}\right)+\sum_{i \neq 1, j=1} P\left(E_{i 1}\right)+\sum_{i=1, j \neq 1} P\left(E_{1 j}\right) \\
& +\sum_{i \neq 1, j \neq 1} P\left(E_{i j}\right)
\end{aligned}
$$

여기서 $P$는 $(1,1)$이 전송되었다는 조건 하에서의 조건부 확률입니다. AEP로부터 $P\left(E_{11}^{c}\right) \rightarrow 0$입니다. 정리 15.2.1과 15.2.3에 의해, $i \neq 1$에 대해 다음이 성립합니다.

$$
\begin{aligned}
P\left(E_{i 1}\right) & =P\left(\left(\mathbf{X}_{1}(i), \mathbf{X}_{2}(1), \mathbf{Y}\right) \in A_{e}^{(n)}\right) \\
& =\sum_{\left(\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{y}\right) \in A_{e}^{(n)}} p\left(\mathbf{x}_{1}\right) p\left(\mathbf{x}_{2}, \mathbf{y}\right) \\
& \leq\left|A_{\epsilon}^{(n)}\right| 2^{-n\left(H\left(X_{1}\right)-\epsilon\right)} 2^{-n\left(H\left(X_{2}, Y\right)-\epsilon\right)} \\
& \leq 2^{-n\left(H\left(X_{1}\right)+H\left(X_{2}, Y\right)-H\left(X_{1}, X_{2}, Y\right)-3 \epsilon\right)} \\
& =2^{-n\left(I\left(X_{1} ; X_{2}, Y\right)-3 \epsilon\right)} \\
& =2^{-n\left(I\left(X_{1} ; Y \mid X_{2}\right)-3 \epsilon\right)}
\end{aligned}
$$

여기서 (15.68)과 (15.69)의 동등성은 $X_{1}$과 $X_{2}$의 독립성과 그로 인한 $I\left(X_{1} ; X_{2}, Y\right)=I\left(X_{1} ; X_{2}\right)+$ $I\left(X_{1} ; Y \mid X_{2}\right)=I\left(X_{1} ; Y \mid X_{2}\right)$로부터 따릅니다. 마찬가지로, $j \neq 1$에 대해,

$$
P\left(E_{1 j}\right) \leq 2^{-n\left(I\left(X_{2} ; Y \mid X_{1}\right)-3 \epsilon\right)}
$$

그리고 $i \neq 1, j \neq 1$에 대해,

$$
P\left(E_{i j}\right) \leq 2^{-n\left(I\left(X_{1}, X_{2} ; Y\right)-4 \epsilon\right)}
$$

따라서 다음이 성립합니다.

$$
\begin{aligned}
P_{e}^{(n)} \leq P\left(E_{11}^{c}\right)+ & 2^{n R_{1}} 2^{-n\left(I\left(X_{1} ; Y \mid X_{2}\right)-3 \epsilon\right)}+2^{n R_{2}} 2^{-n\left(I\left(X_{2} ; Y \mid X_{1}\right)-3 \epsilon\right)} \\
& +2^{n\left(R_{1}+R_{2}\right)} 2^{-n\left(I\left(X_{1}, X_{2} ; Y\right)-4 \epsilon\right)}
\end{aligned}
$$

$\epsilon>0$은 임의적이므로, 정리의 조건은 각 항이 $n \rightarrow \infty$일 때 0으로 수렴함을 의미합니다. 따라서, 조건부 오류 확률은
<!-- Page 558 -->
특정 코드워드가 전송될 때, 정리의 조건이 충족되면 0으로 수렴합니다. 위에서 제시된 상한은 평균 오류 확률이 임의의 코드북 선택에 대해 평균화되었을 때 임의로 작아질 수 있음을 보여줍니다. 이는 대칭성에 의해 개별 코드워드에 대한 확률과 동일합니다. 따라서 임의로 작은 오류 확률을 갖는 최소한 하나의 코드 $C^{*}$가 존재합니다.

이것으로 고정된 입력 분포에 대해 (15.58)에 있는 영역의 달성 가능성에 대한 증명이 완료됩니다. 나중에 15.3.3절에서 시분할 방식을 통해 볼록 껍질 내의 모든 $\left(R_{1}, R_{2}\right)$를 달성할 수 있음을 보여줌으로써 정리의 순방향 부분에 대한 증명을 완료할 것입니다.

# 15.3.2 다중 접속 채널의 용량 영역에 대한 논평

이제 우리는 다중 접속 채널의 용량 영역의 달성 가능성을 증명했습니다. 이는 다음을 만족하는 점 $\left(R_{1}, R_{2}\right)$들의 집합의 볼록 껍질의 폐포입니다.

$$
\begin{aligned}
R_{1} & <I\left(X_{1} ; Y \mid X_{2}\right) \\
R_{2} & <I\left(X_{2} ; Y \mid X_{1}\right) \\
R_{1}+R_{2} & <I\left(X_{1}, X_{2} ; Y\right)
\end{aligned}
$$

이는 $\mathcal{X}_{1} \times \mathcal{X}_{2}$에 대한 특정 분포 $p_{1}\left(x_{1}\right) p_{2}\left(x_{2}\right)$에 대해 성립합니다. 특정 $p_{1}\left(x_{1}\right) p_{2}\left(x_{2}\right)$에 대해, 이 영역은 그림 15.14에 묘사되어 있습니다.

그림 15.14. 고정된 입력 분포에 대한 다중 접속 채널의 달성 가능 영역.
<!-- Page 559 -->
이제 영역 내의 코너 포인트들을 해석해 보겠습니다. 포인트 A는 송신자 2가 정보를 전송하고 있지 않을 때, 송신자 1로부터 수신자까지 달성 가능한 최대 속도에 해당합니다. 이는 다음과 같습니다.

$$
\max R_{1}=\max _{p_{1}\left(x_{1}\right) p_{2}\left(x_{2}\right)} I\left(X_{1} ; Y \mid X_{2}\right)
$$

이제 임의의 분포 $p_{1}\left(x_{1}\right) p_{2}\left(x_{2}\right)$에 대해,

$$
\begin{aligned}
I\left(X_{1} ; Y \mid X_{2}\right) & =\sum_{x_{2}} p_{2}\left(x_{2}\right) I\left(X_{1} ; Y \mid X_{2}=x_{2}\right) \\
& \leq \max _{x_{2}} I\left(X_{1} ; Y \mid X_{2}=x_{2}\right)
\end{aligned}
$$

평균이 최대값보다 작기 때문입니다. 따라서 (15.76)의 최대값은 $X_{2}=x_{2}$로 설정할 때 달성되며, 여기서 $x_{2}$는 $X_{1}$과 $Y$ 간의 조건부 상호 정보량을 최대화하는 값입니다. $X_{1}$의 분포는 이 상호 정보량을 최대화하도록 선택됩니다. 따라서 $X_{2}$는 $X_{2}=x_{2}$로 설정하여 $X_{1}$의 전송을 용이하게 해야 합니다.

포인트 B는 송신자 1이 자신의 최대 속도로 전송하는 한, 송신자 2가 전송할 수 있는 최대 속도에 해당합니다. 이는 $X_{1}$을 $X_{2}$에서 $Y$로 가는 채널에 대한 잡음으로 간주할 때 얻어지는 속도입니다. 이 경우, 단일 사용자 채널의 결과를 사용하면 $X_{2}$는 $I\left(X_{2} ; Y\right)$의 속도로 전송할 수 있습니다. 수신자는 이제 어떤 $X_{2}$ 코드가 사용되었는지 알고 채널에서 그 효과를 "빼낼" 수 있습니다. 이제 채널을 단일 사용자 채널의 인덱싱된 집합으로 간주할 수 있으며, 여기서 인덱스는 사용된 $X_{2}$ 심볼입니다. 이 경우 달성되는 $X_{1}$ 속도는 이러한 채널에 대한 평균 상호 정보량이며, 각 채널은 해당 $X_{2}$ 심볼이 코드워드에 나타나는 횟수만큼 발생합니다. 따라서 달성되는 속도는 다음과 같습니다.

$$
\sum_{x_{2}} p\left(x_{2}\right) I\left(X_{1} ; Y \mid X_{2}=x_{2}\right)=I\left(X_{1} ; Y \mid X_{2}\right)
$$

포인트 C와 D는 각각 송신자의 역할이 반전된 B와 A에 해당합니다. 코너가 아닌 포인트는 시간 공유를 통해 달성될 수 있습니다. 따라서 다중 접속 채널의 용량 영역에 대한 단일 사용자 해석과 정당성을 제공했습니다.

다른 신호를 잡음의 일부로 간주하고, 하나의 신호를 디코딩한 다음, 수신 신호에서 "빼내는" 아이디어는 매우 유용한 것입니다. 우리는 이와 동일한 개념을 열화된 방송 채널의 용량 계산에서 다시 접하게 될 것입니다.
<!-- Page 560 -->
# 15.3.3 다중 접속 채널의 용량 영역의 볼록성

새로운 확률 변수를 도입하여 볼록 포락선 연산을 고려하도록 다중 접속 채널의 용량 영역을 재구성합니다. 먼저 용량 영역이 볼록함을 증명합니다.

정리 15.3.2 다중 접속 채널의 용량 영역 $\mathcal{C}$는 볼록합니다 [즉, $\left(R_{1}, R_{2}\right) \in \mathcal{C}$이고 $\left(R_{1}^{\prime}, R_{2}^{\prime}\right) \in \mathcal{C}$이면, $0 \leq \lambda \leq 1$에 대해 $\left(\lambda R_{1}+(1-\lambda) R_{1}^{\prime}\right.$, $\left.\lambda R_{2}+(1-\lambda) R_{2}^{\prime}\right) \in \mathcal{C}$입니다].

증명: 아이디어는 시분할입니다. 서로 다른 속도 $\mathbf{R}=\left(R_{1}, R_{2}\right)$ 및 $\mathbf{R}^{\prime}=\left(R_{1}^{\prime}, R_{2}^{\prime}\right)$를 갖는 두 코드열이 주어졌을 때, 첫 번째 $n\lambda$ 심볼에 대해 첫 번째 코드북을 사용하고 마지막 $n(1-\lambda)$ 심볼에 대해 두 번째 코드북을 사용하여 속도 $\lambda \mathbf{R}+(1-\lambda) \mathbf{R}^{\prime}$를 갖는 세 번째 코드북을 구성할 수 있습니다. 새로운 코드의 $X_{1}$ 코드워드 수는 다음과 같습니다.

$$
2^{n \lambda R_{1}} 2^{n(1-\lambda) R_{1}^{\prime}}=2^{n\left(\lambda R_{1}+(1-\lambda) R_{1}^{\prime}\right)}
$$

따라서 새로운 코드의 속도는 $\lambda \mathbf{R}+(1-\lambda) \mathbf{R}^{\prime}$입니다. 전체 오류 확률이 각 부분의 오류 확률의 합보다 작기 때문에, 새로운 코드의 오류 확률은 0으로 가고 속도는 달성 가능합니다.

이제 시분할 확률 변수 $Q$를 사용하여 다중 접속 채널의 용량 영역에 대한 명세서를 재구성할 수 있습니다. 이 결과를 증명하기 전에, 다중 접속 채널의 용량 영역과 같은 선형 부등식으로 정의된 볼록 집합의 속성을 증명해야 합니다. 특히, 선형 제약 조건으로 정의된 두 영역의 볼록 포락선이 제약 조건의 볼록 조합으로 정의된 영역임을 보여주고 싶습니다. 처음에는 이 두 집합의 동등성이 명백해 보이지만, 자세히 살펴보면 일부 제약 조건이 활성화되지 않을 수 있다는 미묘한 어려움이 있습니다. 이는 예시를 통해 가장 잘 설명됩니다. 선형 부등식으로 정의된 다음 두 집합을 고려하십시오.

$$
\begin{aligned}
& C_{1}=\{(x, y): x \geq 0, y \geq 0, x \leq 10, y \leq 10, x+y \leq 100\} \\
& C_{2}=\{(x, y): x \geq 0, y \geq 0, x \leq 20, y \leq 20, x+y \leq 20\}
\end{aligned}
$$

이 경우, 제약 조건의 $\left(\frac{1}{2}, \frac{1}{2}\right)$ 볼록 조합은 다음 영역을 정의합니다.

$$
C=\{(x, y): x \geq 0, y \geq 0, x \leq 15, y \leq 15, x+y \leq 60\}
$$
<!-- Page 561 -->
$C_{1}$ 또는 $C_{2}$ 내의 모든 점은 $x+y<20$을 만족하므로, $C_{1}$과 $C_{2}$의 합집합의 볼록 껍질 내의 모든 점은 이 속성을 만족합니다. 따라서 $C$에 있는 점 $(15,15)$는 $\left(C_{1} \cup C_{2}\right)$의 볼록 껍질에 있지 않습니다. 이 예시는 문제의 원인을 시사합니다. $C_{1}$의 정의에서 제약 조건 $x+y \leq 100$은 활성화되지 않았습니다. 이 제약 조건이 $a \leq 20$인 제약 조건 $x+y \leq a$로 대체된다면, 두 영역의 동일성에 대한 위 결과는 우리가 이제 증명할 것처럼 참이 될 것입니다.

두 사용자 다중 접속 채널의 용량 영역의 구성 요소로 발생하는 오각형 영역으로 제한합니다. 이 경우, 고정된 $p\left(x_{1}\right) p\left(x_{2}\right)$에 대한 용량 영역은 세 개의 상호 정보량, 즉 $I\left(X_{1} ; Y \mid X_{2}\right), I\left(X_{2} ; Y \mid X_{1}\right)$, 및 $I\left(X_{1}, X_{2} ; Y\right)$에 의해 정의되며, 각각 $I_{1}, I_{2}, I_{3}$라고 부르겠습니다. 각 $p\left(x_{1}\right) p\left(x_{2}\right)$에 대해 해당 벡터 $\mathbf{I}=\left(I_{1}, I_{2}, I_{3}\right)$와 다음으로 정의되는 속도 영역이 존재합니다.

$$
C_{\mathbf{I}}=\left\{\left(R_{1}, R_{2}\right): R_{1} \geq 0, R_{2} \geq 0, R_{1} \leq I_{1}, R_{2} \leq I_{2}, R_{1}+R_{2} \leq I_{3}\right\}
$$

또한, 모든 분포 $p\left(x_{1}\right) p\left(x_{2}\right)$에 대해 $I\left(X_{2} ; Y \mid X_{1}\right)=$ $H\left(X_{2} \mid X_{1}\right)-H\left(X_{2} \mid Y, X_{1}\right)=H\left(X_{2}\right)-H\left(X_{2} \mid Y, X_{1}\right)=I\left(X_{2} ; Y, X_{1}\right)=$ $I\left(X_{2} ; Y\right)+I\left(X_{2} ; X_{1} \mid Y\right) \geq I\left(X_{2} ; Y\right)$이고, 따라서 $I\left(X_{1} ; Y \mid X_{2}\right)+$ $I\left(X_{2} ; Y \mid X_{1}\right) \geq I\left(X_{1} ; Y \mid X_{2}\right)+I\left(X_{2} ; Y\right)=I\left(X_{1}, X_{2} ; Y\right)$이므로, 모든 벡터 $I$에 대해 $I_{1}+I_{2} \geq I_{3}$입니다. 이 속성은 정리에 결정적인 역할을 할 것입니다.

정리 15.3.1 $\mathbf{I}_{1}, \mathbf{I}_{2} \in \mathcal{R}^{3}$는 각각 (15.84)에 주어진 속도 영역 $C_{\mathbf{I}_{1}}$ 및 $C_{\mathbf{I}_{2}}$를 정의하는 두 개의 상호 정보량 벡터입니다. $0 \leq \lambda \leq 1$에 대해 $\mathbf{I}_{\lambda}=\lambda \mathbf{I}_{1}+(1-\lambda) \mathbf{I}_{2}$를 정의하고, $C_{\mathbf{I}_{\lambda}}$를 $\mathbf{I}_{\lambda}$에 의해 정의된 속도 영역이라고 합시다. 그러면

$$
C_{\mathbf{I}_{\lambda}}=\lambda C_{\mathbf{I}_{1}}+(1-\lambda) C_{\mathbf{I}_{2}}
$$

증명: 이 정리를 두 부분으로 증명하겠습니다. 먼저, 집합 $C_{\mathbf{I}_{1}}$과 $C_{\mathbf{I}_{2}}$의 $(\lambda, 1-\lambda)$ 혼합 내의 모든 점이 $\mathbf{I}_{\lambda}$의 제약 조건을 만족함을 보입니다. 이는 $C_{\mathbf{I}_{1}}$ 내의 모든 점이 $\mathbf{I}_{1}$에 대한 부등식을 만족하고 $C_{\mathbf{I}_{2}}$ 내의 점이 $\mathbf{I}_{2}$에 대한 부등식을 만족하므로, 이러한 점들의 $(\lambda, 1-\lambda)$ 혼합이 제약 조건의 $(\lambda, 1-\lambda)$ 혼합을 만족하기 때문에 명백합니다. 따라서 다음이 성립합니다.

$$
\lambda C_{\mathbf{I}_{1}}+(1-\lambda) C_{\mathbf{I}_{2}} \subseteq C_{\mathbf{I}_{\lambda}}
$$

역포함 관계를 증명하기 위해 오각형 영역의 극점을 고려합니다. (15.84)에 정의된 속도 영역이 항상 오각형 형태이거나 극단적인 경우임을 보는 것은 어렵지 않습니다.
<!-- Page 562 -->
$I_{3}=I_{1}+I_{2}$일 때, 직사각형 형태로 나타납니다. 따라서, 용량 영역 $C_{\mathbf{I}}$는 다섯 개의 점의 볼록 껍질로도 정의될 수 있습니다:

$$
(0,0),\left(I_{1}, 0\right),\left(I_{1}, I_{3}-I_{1}\right),\left(I_{3}-I_{2}, I_{2}\right),\left(0, I_{2}\right)
$$

$\mathbf{I}_{\lambda}$에 의해 정의된 영역을 고려해 보십시오. 이 영역 또한 다섯 개의 점으로 정의됩니다. 점 중 하나, 예를 들어 $\left(I_{3}^{(\lambda)}-I_{2}^{(\lambda)}, I_{2}^{(\lambda)}\right)$를 선택해 보십시오. 이 점은 $\left(I_{3}^{(1)}-I_{2}^{(1)}, I_{2}^{(1)}\right)$와 $\left(I_{3}^{(2)}-I_{2}^{(2)}, I_{2}^{(2)}\right)$ 점들의 $(\lambda, 1-\lambda)$ 혼합으로 표현될 수 있으며, 따라서 $C_{\mathbf{I}_{1}}$과 $C_{\mathbf{I}_{2}}$의 볼록 혼합 안에 존재합니다. 그러므로, 오각형 $C_{\mathbf{I}_{\lambda}}$의 모든 극점은 $C_{\mathbf{I}_{1}}$과 $C_{\mathbf{I}_{2}}$의 볼록 껍질 안에 존재합니다. 즉,

$$
C_{\mathbf{I}_{\lambda}} \subseteq \lambda C_{\mathbf{I}_{1}}+(1-\lambda) C_{\mathbf{I}_{2}}
$$

두 부분을 결합하면 정리를 얻을 수 있습니다.
정리의 증명에서, 우리는 모든 속도 영역이 다섯 개의 극점으로 정의된다는 사실을 암묵적으로 사용했습니다 (최악의 경우, 일부 점들은 동일할 수 있습니다). $\mathbf{I}$ 벡터에 의해 정의된 다섯 개의 점 모두 속도 영역 내에 있었습니다. 만약 $I_{3} \leq I_{1}+I_{2}$ 조건이 만족되지 않으면, (15.87)의 일부 점들이 속도 영역 밖에 있을 수 있으며 증명이 무효화됩니다.

위 보조정리의 즉각적인 결과로 다음 정리를 얻습니다:

정리 15.3.3 개별 $\mathbf{I}$ 벡터에 의해 정의된 속도 영역들의 합집합의 볼록 껍질은 $\mathbf{I}$ 벡터들의 볼록 껍질에 의해 정의된 속도 영역과 같습니다.

속도 영역에 대한 볼록 껍질 연산과 상호 정보의 볼록 혼합 간의 동등성에 대한 이러한 논의는 일반적인 $m$-사용자 다중 접속 채널로 확장될 수 있습니다. 폴리마트로이드 이론을 이용한 이러한 방식의 증명은 Han [271]에 개발되어 있습니다.

정리 15.3.4 이산 메모리 없는 다중 접속 채널의 달성 가능한 속도 집합은 다음을 만족하는 모든 $\left(R_{1}, R_{2}\right)$ 쌍의 폐포로 주어집니다.

$$
\begin{aligned}
R_{1} & <I\left(X_{1} ; Y \mid X_{2}, Q\right) \\
R_{2} & <I\left(X_{2} ; Y \mid X_{1}, Q\right) \\
R_{1}+R_{2} & <I\left(X_{1}, X_{2} ; Y \mid Q\right)
\end{aligned}
$$
<!-- Page 563 -->
$|\mathcal{Q}| \leq 4$인 공동 분포 $p(q) p\left(x_{1} \mid q\right) p\left(x_{2} \mid q\right) p\left(y \mid x_{1}, x_{2}\right)$의 선택에 대해.

증명: (15.89)에 정의된 영역에 속하는 모든 속도 쌍이 달성 가능함(즉, Theorem 15.3.1을 만족하는 속도 쌍의 볼록 폐포에 속함)을 보일 것입니다. 또한 Theorem 15.3.1의 영역의 볼록 폐포에 있는 모든 점이 (15.89)에 정의된 영역에도 속함을 보일 것입니다.

정리 (15.89)의 부등식을 만족하는 속도 점 $\mathbf{R}$을 고려하십시오. 첫 번째 부등식의 우변을 다음과 같이 다시 쓸 수 있습니다.

$$
\begin{aligned}
I\left(X_{1} ; Y \mid X_{2}, Q\right) & =\sum_{q=1}^{m} p(q) I\left(X_{1} ; Y \mid X_{2}, Q=q\right) \\
& =\sum_{q=1}^{m} p(q) I\left(X_{1} ; Y \mid X_{2}\right)_{p_{1 q}, p_{2 q}}
\end{aligned}
$$

여기서 $m$은 $Q$의 지지 집합의 기수입니다. 다른 상호 정보량도 유사하게 확장할 수 있습니다.

표기법의 단순화를 위해 속도 쌍을 벡터로 간주하고 특정 입력 곱 분포 $p_{1 q}\left(x_{1}\right) p_{2 q}\left(x_{2}\right)$에 대해 (15.58)의 부등식을 만족하는 쌍을 $\mathbf{R}_{p_{1}, p_{2}}$로 표시하며, 이를 $\mathbf{R}_{q}$로 표기합니다. 구체적으로, $\mathbf{R}_{q}=$ $\left(R_{1 q}, R_{2 q}\right)$가 다음을 만족하는 속도 쌍이라고 가정합니다.

$$
\begin{aligned}
R_{1 q} & <I\left(X_{1} ; Y \mid X_{2}\right)_{p_{1 q}\left(x_{1}\right) p_{2 q}\left(x_{2}\right)} \\
R_{2 q} & <I\left(X_{2} ; Y \mid X_{1}\right)_{p_{1 q}\left(x_{1}\right) p_{2 q}\left(x_{2}\right)} \\
R_{1 q}+R_{2 q} & <I\left(X_{1}, X_{2} ; Y\right)_{p_{1 q}\left(x_{1}\right) p_{2 q}\left(x_{2}\right)}
\end{aligned}
$$

그러면 Theorem 15.3.1에 의해 $\mathbf{R}_{q}=\left(R_{1 q}, R_{2 q}\right)$는 달성 가능합니다. 그러면 $\mathbf{R}$이 (15.89)를 만족하고 우변을 (15.91)과 같이 확장할 수 있으므로, (15.94)를 만족하는 쌍들의 집합 $\mathbf{R}_{q}$가 존재하여

$$
\mathbf{R}=\sum_{q=1}^{m} p(q) \mathbf{R}_{q}
$$

달성 가능한 속도들의 볼록 조합은 달성 가능하므로, $\mathbf{R}$도 달성 가능합니다. 따라서 정리의 영역에 대한 달성 가능성을 증명했습니다. 동일한 논증을 사용하여 (15.58)의 영역의 볼록 폐포에 있는 모든 점이 (15.94)를 만족하는 점들의 혼합으로 작성될 수 있으며, 따라서 (15.89)의 형태로 작성될 수 있음을 보일 수 있습니다.
<!-- Page 564 -->
다음 절에서 그 역이 증명됩니다. 이 역은 달성 가능한 모든 속도 쌍이 (15.89)의 형태임을 보여주며, 따라서 이것이 다중 접속 채널의 용량 영역임을 확립합니다. 시간 공유 랜덤 변수 $Q$에 대한 기수성 경계는 볼록 집합에 대한 Carathéodory의 정리에 따른 것입니다. 아래 논의를 참조하십시오.

용량 영역의 볼록성 증명은 달성 가능한 속도 쌍의 모든 볼록 조합도 달성 가능하다는 것을 보여줍니다. 이 과정을 계속하여 더 많은 점들의 볼록 조합을 취할 수 있습니다. 임의의 수의 점을 사용해야 합니까? 용량 영역이 증가할까요? 다음 정리가 아니라고 말합니다.

정리 15.3.5 (Carathéodory) $d$차원 유클리드 공간의 컴팩트 집합 $A$의 볼록 폐포 내의 모든 점은 원래 집합 $A$의 $d+1$개 이하의 점들의 볼록 조합으로 표현될 수 있습니다.

증명: 증명은 Eggleston [183] 및 Grünbaum [263]에서 찾을 수 있습니다.

이 정리는 용량 영역을 계산할 때 특정 유한 볼록 조합에 주의를 집중할 수 있도록 합니다. 이것은 중요한 속성인데, 이것이 없다면 더 큰 알파벳 $\mathcal{Q}$를 사용하는 것이 영역을 증가시킬지 여부를 알 수 없기 때문에 (15.89)에서 용량 영역을 계산할 수 없을 것입니다.

다중 접속 채널에서 경계는 3차원에서 연결된 컴팩트 집합을 정의합니다. 따라서 그 폐포 내의 모든 점은 최대 4개의 점의 볼록 조합으로 정의될 수 있습니다. 따라서 용량 영역의 위 정의에서 $Q$의 기수성을 최대 4개로 제한할 수 있습니다.

비고: 많은 기수성 경계는 다른 고려 사항을 도입하여 약간 개선될 수 있습니다. 예를 들어, 용량 정리에서와 같이 $A$의 볼록 껍질의 경계에만 관심이 있다면, 경계의 점은 $A$의 $d$개의 점의 혼합으로 표현될 수 있습니다. 왜냐하면 경계의 점은 $(d-1)$차원 지지 초평면과 $A$의 교차점에 있기 때문입니다.

# 15.3.4 다중 접속 채널에 대한 역

지금까지 용량 영역의 달성 가능성을 증명했습니다. 이 섹션에서는 역을 증명합니다.
<!-- Page 565 -->
증명: (정리 15.3.1 및 15.3.4의 역정리). $P_{e}^{(n)} \rightarrow 0$인 $\left(\left(2^{n R_{1}}, 2^{n R_{2}}\right), n\right)$ 코드열이 주어졌을 때, 다음을 만족해야 함을 보여야 합니다.

$$
\begin{aligned}
R_{1} & \leq I\left(X_{1} ; Y \mid X_{2}, Q\right) \\
R_{2} & \leq I\left(X_{2} ; Y \mid X_{1}, Q\right) \\
R_{1}+R_{2} & \leq I\left(X_{1}, X_{2} ; Y \mid Q\right)
\end{aligned}
$$

여기서 $Q$는 $\{1,2,3,4\}$ 상에서 정의된 확률 변수이며, $p(q) p\left(x_{1} \mid q\right) p\left(x_{2} \mid q\right) p\left(y \mid x_{1}, x_{2}\right)$의 결합 분포를 따릅니다. $n$을 고정합니다. 주어진 블록 길이 $n$의 코드를 고려합니다. $\mathcal{W}_{1} \times \mathcal{W}_{2} \times \mathcal{X}_{1}^{n} \times \mathcal{X}_{2}^{n} \times$ 상의 결합 분포는 잘 정의됩니다. 유일한 무작위성은 $W_{1}$과 $W_{2}$의 무작위 균등 선택과 채널에 의해 유도되는 무작위성 때문입니다. 결합 분포는 다음과 같습니다.

$$
p\left(w_{1}, w_{2}, x_{1}^{n}, x_{2}^{n}, y^{n}\right)=\frac{1}{2^{n R_{1}}} \frac{1}{2^{n R_{2}}} p\left(x_{1}^{n} \mid w_{1}\right) p\left(x_{2}^{n} \mid w_{2}\right) \prod_{i=1}^{n} p\left(y_{i} \mid x_{1 i}, x_{2 i}\right)
$$

여기서 $p\left(x_{1}^{n} \mid w_{1}\right)$은 $x_{1}^{n}=\mathbf{x}_{1}\left(w_{1}\right)$ (즉, $w_{1}$에 해당하는 코드워드)인지 여부에 따라 1 또는 0이며, 마찬가지로 $p\left(x_{2}^{n} \mid w_{2}\right)=1$ 또는 0은 $x_{2}^{n}=\mathbf{x}_{2}\left(w_{2}\right)$인지 여부에 따라 결정됩니다. 이어지는 상호 정보량은 이 분포에 대해 계산됩니다.

코드 구성에 의해, 수신된 시퀀스 $Y^{n}$으로부터 낮은 오류 확률로 $\left(W_{1}, W_{2}\right)$를 추정하는 것이 가능합니다. 따라서 $Y^{n}$이 주어졌을 때 $\left(W_{1}, W_{2}\right)$의 조건부 엔트로피는 작아야 합니다. 파노의 부등식에 의해,

$$
H\left(W_{1}, W_{2} \mid Y^{n}\right) \leq n\left(R_{1}+R_{2}\right) P_{e}^{(n)}+H\left(P_{e}^{(n)}\right) \triangleq n \epsilon_{n}
$$

$\epsilon_{n} \rightarrow 0$은 $P_{e}^{(n)} \rightarrow 0$일 때 명확합니다. 그러면 다음을 얻습니다.

$$
\begin{aligned}
& H\left(W_{1} \mid Y^{n}\right) \leq H\left(W_{1}, W_{2} \mid Y^{n}\right) \leq n \epsilon_{n} \\
& H\left(W_{2} \mid Y^{n}\right) \leq H\left(W_{1}, W_{2} \mid Y^{n}\right) \leq n \epsilon_{n}
\end{aligned}
$$

이제 $R_{1}$의 비율을 다음과 같이 제한할 수 있습니다.

$$
\begin{aligned}
n R_{1} & =H\left(W_{1}\right) \\
& =I\left(W_{1} ; Y^{n}\right)+H\left(W_{1} \mid Y^{n}\right) \\
& \stackrel{(\mathrm{a})}{\leq} I\left(W_{1} ; Y^{n}\right)+n \epsilon_{n}
\end{aligned}
$$
<!-- Page 566 -->
(b)

$$
\begin{aligned}
& \leq I\left(X_{1}^{n}\left(W_{1}\right) ; Y^{n}\right)+n \epsilon_{n} \\
& =H\left(X_{1}^{n}\left(W_{1}\right)\right)-H\left(X_{1}^{n}\left(W_{1}\right) \mid Y^{n}\right)+n \epsilon_{n} \\
& \stackrel{(c)}{\leq} H\left(X_{1}^{n}\left(W_{1}\right) \mid X_{2}^{n}\left(W_{2}\right)\right)-H\left(X_{1}^{n}\left(W_{1}\right) \mid Y^{n}, X_{2}^{n}\left(W_{2}\right)\right)+n \epsilon_{n} \\
& =I\left(X_{1}^{n}\left(W_{1}\right) ; Y^{n} \mid X_{2}^{n}\left(W_{2}\right)\right)+n \epsilon_{n} \\
& =H\left(Y^{n} \mid X_{2}^{n}\left(W_{2}\right)\right)-H\left(Y^{n} \mid X_{1}^{n}\left(W_{1}\right), X_{2}^{n}\left(W_{2}\right)\right)+n \epsilon_{n} \\
& \stackrel{(d)}{=} H\left(Y^{n} \mid X_{2}^{n}\left(W_{2}\right)\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid Y^{i-1}, X_{1}^{n}\left(W_{1}\right), X_{2}^{n}\left(W_{2}\right)\right)+n \epsilon_{n} \\
& \stackrel{(e)}{=} H\left(Y^{n} \mid X_{2}^{n}\left(W_{2}\right)\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{1 i}, X_{2 i}\right)+n \epsilon_{n} \\
& \stackrel{(f)}{\leq} \sum_{i=1}^{n} H\left(Y_{i} \mid X_{2}^{n}\left(W_{2}\right)\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{1 i}, X_{2 i}\right)+n \epsilon_{n} \\
& \stackrel{(g)}{\leq} \sum_{i=1}^{n} H\left(Y_{i} \mid X_{2 i}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{1 i}, X_{2 i}\right)+n \epsilon_{n} \\
& =\sum_{i=1}^{n} I\left(X_{1 i} ; Y_{i} \mid X_{2 i}\right)+n \epsilon_{n}
\end{aligned}
$$

여기서
(a)는 Fano의 부등식으로부터 도출됩니다.
(b)는 데이터 처리 부등식으로부터 도출됩니다.
(c)는 $W_{1}$과 $W_{2}$가 독립이므로 $X_{1}^{n}\left(W_{1}\right)$와 $X_{2}^{n}\left(W_{2}\right)$도 독립이며, 따라서 $H\left(X_{1}^{n}\left(W_{1}\right) \mid X_{2}^{n}\left(W_{2}\right)\right)=H\left(X_{1}^{n}\left(W_{1}\right)\right)$이고, 조건화에 의해 $H\left(X_{1}^{n}\left(W_{1}\right) \mid Y^{n}, X_{2}^{n}\left(W_{2}\right)\right) \leq H\left(X_{1}^{n}\left(W_{1}\right) \mid Y^{n}\right)$이라는 사실로부터 도출됩니다.
(d)는 연쇄 법칙으로부터 도출됩니다.
(e)는 채널의 메모리리스 속성에 의해 $Y_{i}$가 $X_{1 i}$와 $X_{2 i}$에만 의존한다는 사실로부터 도출됩니다.
(f)는 연쇄 법칙과 조건 제거로부터 도출됩니다.
(g)는 조건 제거로부터 도출됩니다.

따라서 다음과 같은 결과를 얻습니다.

$$
R_{1} \leq \frac{1}{n} \sum_{i=1}^{n} I\left(X_{1 i} ; Y_{i} \mid X_{2 i}\right)+\epsilon_{n}
$$
<!-- Page 567 -->
마찬가지로, 다음과 같은 식이 있습니다.

$$
R_{2} \leq \frac{1}{n} \sum_{i=1}^{n} I\left(X_{2 i} ; Y_{i} \mid X_{1 i}\right)+\epsilon_{n}
$$

비율의 합을 제한하기 위해 다음을 사용합니다.

$$
\begin{aligned}
n\left(R_{1}+R_{2}\right) & =H\left(W_{1}, W_{2}\right) \\
& =I\left(W_{1}, W_{2} ; Y^{n}\right)+H\left(W_{1}, W_{2} \mid Y^{n}\right) \\
& \stackrel{(a)}{\leq} I\left(W_{1}, W_{2} ; Y^{n}\right)+n \epsilon_{n} \\
& \stackrel{(b)}{\leq} I\left(X_{1}^{n}\left(W_{1}\right), X_{2}^{n}\left(W_{2}\right) ; Y^{n}\right)+n \epsilon_{n} \\
& =H\left(Y^{n}\right)-H\left(Y^{n} \mid X_{1}^{n}\left(W_{1}\right), X_{2}^{n}\left(W_{2}\right)\right)+n \epsilon_{n} \\
& \stackrel{(\mathrm{c})}{=} H\left(Y^{n}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid Y^{i-1}, X_{1}^{n}\left(W_{1}\right), X_{2}^{n}\left(W_{2}\right)\right)+n \epsilon_{n}
\end{aligned}
$$

$$
\begin{aligned}
& \stackrel{(d)}{=} H\left(Y^{n}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{1 i}, X_{2 i}\right)+n \epsilon_{n} \\
& \stackrel{(e)}{\leq} \sum_{i=1}^{n} H\left(Y_{i}\right)-\sum_{i=1}^{n} H\left(Y_{i} \mid X_{1 i}, X_{2 i}\right)+n \epsilon_{n} \\
& =\sum_{i=1}^{n} I\left(X_{1 i}, X_{2 i} ; Y_{i}\right)+n \epsilon_{n}
\end{aligned}
$$

여기서
(a)는 Fano의 부등식에서 비롯됩니다.
(b)는 데이터 처리 부등식에서 비롯됩니다.
(c)는 연쇄 법칙에서 비롯됩니다.
(d)는 $Y_{i}$가 $X_{1 i}$와 $X_{2 i}$에만 의존하며 다른 모든 것과 조건부로 독립이라는 사실에서 비롯됩니다.
(e)는 연쇄 법칙과 조건 제거에서 비롯됩니다.

따라서 다음과 같은 식이 성립합니다.

$$
R_{1}+R_{2} \leq \frac{1}{n} \sum_{i=1}^{n} I\left(X_{1 i}, X_{2 i} ; Y_{i}\right)+\epsilon_{n}
$$
<!-- Page 568 -->
(15.114), (15.115), 및 (15.125)의 표현식은 코드북의 $i$ 열에 있는 경험적 분포에서 계산된 상호 정보의 평균입니다. $Q=i \in\{1,2, \ldots, n\}$를 확률 $\frac{1}{n}$으로 하는 새로운 변수 $Q$를 사용하여 이 방정식을 다시 작성할 수 있습니다. 방정식은 다음과 같이 됩니다.

$$
\begin{aligned}
R_{1} & \leq \frac{1}{n} \sum_{i=1}^{n} I\left(X_{1 i} ; Y_{i} \mid X_{2 i}\right)+\epsilon_{n} \\
& =\frac{1}{n} \sum_{i=1}^{n} I\left(X_{1 q} ; Y_{q} \mid X_{2 q}, Q=i\right)+\epsilon_{n} \\
& =I\left(X_{1 Q} ; Y_{Q} \mid X_{2 Q}, Q\right)+\epsilon_{n} \\
& =I\left(X_{1} ; Y \mid X_{2}, Q\right)+\epsilon_{n}
\end{aligned}
$$

여기서 $X_{1} \triangleq X_{1 Q}, X_{2} \triangleq X_{2 Q}$, 및 $Y \triangleq Y_{Q}$는 $X_{1 i}, X_{2 i}$ 및 $Y_{i}$의 분포가 $i$에 의존하는 방식과 동일한 방식으로 $Q$에 의존하는 분포를 갖는 새로운 확률 변수입니다. $W_{1}$과 $W_{2}$는 독립이므로 $X_{1 i}\left(W_{1}\right)$와 $X_{2 i}\left(W_{2}\right)$도 독립이며, 따라서

$$
\begin{aligned}
& \operatorname{Pr}\left(X_{1 i}\left(W_{1}\right)=x_{1}, X_{2 i}\left(W_{2}\right)=x_{2}\right) \\
& \quad \triangleq \operatorname{Pr}\left\{X_{1 Q}=x_{1} \mid Q=i\right\} \operatorname{Pr}\left\{X_{2 Q}=x_{2} \mid Q=i\right\}
\end{aligned}
$$

따라서 $n \rightarrow \infty$일 때 극한을 취하면 $P_{e}^{(n)} \rightarrow 0$이 되며, 다음과 같은 반례를 얻습니다.

$$
\begin{aligned}
R_{1} & \leq I\left(X_{1} ; Y \mid X_{2}, Q\right) \\
R_{2} & \leq I\left(X_{2} ; Y \mid X_{1}, Q\right) \\
R_{1}+R_{2} & \leq I\left(X_{1}, X_{2} ; Y \mid Q\right)
\end{aligned}
$$

일부 결합 분포 $p(q) p\left(x_{1} \mid q\right) p\left(x_{2} \mid q\right) p\left(y \mid x_{1}, x_{2}\right)$의 선택에 대해. 15.3.3절과 마찬가지로 $\mathcal{Q}$의 기수(cardinality)를 4로 제한해도 영역은 변경되지 않습니다.

이것으로 반례 증명이 완료되었습니다.
따라서 Theorem 15.3.1의 영역의 달성 가능성은 15.3.1절에서 증명되었습니다. 15.3.3절에서는 (15.96)에 의해 정의된 영역의 모든 점이 달성 가능함을 보였습니다. 반례에서는 (15.96)의 영역이 우리가 할 수 있는 최선임을 보여 채널의 capacity region임을 확립했습니다. 따라서 (15.58)의 영역은
<!-- Page 569 -->
(15.96)의 영역보다 클 수 없으며, 이것이 다중 접속 채널의 용량 영역입니다.

# 15.3.5 m-사용자 다중 접속 채널

이제 두 송신자에 대해 도출된 결과를 $m$명의 송신자, $m \geq 2$로 일반화하겠습니다. 이 경우 다중 접속 채널은 그림 15.15에 나와 있습니다.

송신자 $1, 2, \ldots, m$에서 채널을 통해 독립적인 인덱스 $w_{1}, w_{2}, \ldots, w_{m}$를 전송합니다. 코드, 속도 및 달성 가능성은 두 송신자 경우와 정확히 동일하게 정의됩니다.

$S \subseteq\{1,2, \ldots, m\}$이라고 합시다. $S^{c}$는 $S$의 여집합을 나타냅니다. $R(S)=\sum_{i \in S} R_{i}$이고 $X(S)=\left\{X_{i}: i \in S\right\}$라고 합시다. 그러면 다음과 같은 정리가 있습니다.

정리 15.3.6 $m$-사용자 다중 접속 채널의 용량 영역은 다음을 만족하는 속도 벡터의 볼록 껍질의 폐포입니다.

$$
R(S) \leq I\left(X(S) ; Y \mid X\left(S^{c}\right)\right) \quad \text { 모든 } S \subseteq\{1,2, \ldots, m\} \text { 에 대해 }
$$

어떤 곱 분포 $p_{1}\left(x_{1}\right) p_{2}\left(x_{2}\right) \cdots p_{m}\left(x_{m}\right)$에 대해.
증명: 증명에는 새로운 아이디어가 없습니다. 달성 가능성 증명에서 오류 확률에는 $2^{m}-1$개의 항이 있으며, 역 증명에는 동일한 수의 부등식이 있습니다. 세부 사항은 독자에게 맡깁니다.

일반적으로 (15.132)의 영역은 베벨 상자입니다.

그림 15.15. $m$-사용자 다중 접속 채널.
<!-- Page 570 -->
# 15.3.6 가우시안 다중 접속 채널

이제 15.1.2절의 가우시안 다중 접속 채널에 대해 좀 더 자세히 논의하겠습니다.

두 송신자 $X_{1}$과 $X_{2}$가 단일 수신자 $Y$와 통신합니다. 시간 $i$에서의 수신 신호는 다음과 같습니다.

$$
Y_{i}=X_{1 i}+X_{2 i}+Z_{i}
$$

여기서 $\left\{Z_{i}\right\}$는 분산 $N$을 갖는 독립적이고 동일하게 분포된, 평균이 0인 가우시안 확률 변수의 시퀀스입니다(그림 15.16). 각 송신자에 대해 모든 메시지에 대해 다음을 만족하는 전력 제약 $P_{j}$가 있다고 가정합니다.

$$
\frac{1}{n} \sum_{i=1}^{n} x_{j i}^{2}\left(w_{j}\right) \leq P_{j}, \quad w_{j} \in\left\{1,2, \ldots, 2^{n R_{j}}\right\}, \quad j=1,2
$$

이산 경우(7장)의 채널 용량 증명 가능성이 가우시안 채널(9장)로 확장된 것처럼, 이산 다중 접속 채널에 대한 증명을 가우시안 다중 접속 채널로 확장할 수 있습니다. 역방향도 유사하게 확장될 수 있으므로, 용량 영역은 다음을 만족하는 속도 쌍의 집합의 볼록 껍질일 것으로 예상됩니다.

$$
\begin{aligned}
R_{1} & \leq I\left(X_{1} ; Y \mid X_{2}\right) \\
R_{2} & \leq I\left(X_{2} ; Y \mid X_{1}\right) \\
R_{1}+R_{2} & \leq I\left(X_{1}, X_{2} ; Y\right)
\end{aligned}
$$

이는 $E X_{1}^{2} \leq P_{1}$ 및 $E X_{2}^{2} \leq P_{2}$를 만족하는 일부 입력 분포 $f_{1}\left(x_{1}\right) f_{2}\left(x_{2}\right)$에 대한 것입니다.

그림 15.16. 가우시안 다중 접속 채널.
<!-- Page 571 -->
이제 상호 정보량을 상대 엔트로피로 확장할 수 있으며, 따라서

$$
\begin{aligned}
I\left(X_{1} ; Y \mid X_{2}\right) & =h\left(Y \mid X_{2}\right)-h\left(Y \mid X_{1}, X_{2}\right) \\
& =h\left(X_{1}+X_{2}+Z \mid X_{2}\right)-h\left(X_{1}+X_{2}+Z \mid X_{1}, X_{2}\right) \\
& =h\left(X_{1}+Z \mid X_{2}\right)-h\left(Z \mid X_{1}, X_{2}\right) \\
& =h\left(X_{1}+Z \mid X_{2}\right)-h(Z) \\
& =h\left(X_{1}+Z\right)-h(Z) \\
& =h\left(X_{1}+Z\right)-\frac{1}{2} \log (2 \pi e) N \\
& \leq \frac{1}{2} \log (2 \pi e)\left(P_{1}+N\right)-\frac{1}{2} \log (2 \pi e) N \\
& =\frac{1}{2} \log \left(1+\frac{P_{1}}{N}\right)
\end{aligned}
$$

여기서 (15.141)은 $Z$가 $X_{1}$ 및 $X_{2}$와 독립이라는 사실에서 비롯되며, (15.142)는 $X_{1}$ 및 $X_{2}$의 독립성에서 비롯되며, (15.144)는 주어진 두 번째 모멘트에 대해 정규 분포가 엔트로피를 최대화한다는 사실에서 비롯됩니다. 따라서 최대화하는 분포는 $X_{1} \sim \mathcal{N}\left(0, P_{1}\right)$ 및 $X_{2} \sim \mathcal{N}\left(0, P_{2}\right)$이며 $X_{1}$과 $X_{2}$는 독립입니다. 이 분포는 (15.135)-(15.137)의 상호 정보량 경계를 동시에 최대화합니다.

정의 채널 용량 함수를 다음과 같이 정의합니다.

$$
C(x) \triangleq \frac{1}{2} \log (1+x)
$$

이는 신호 대 잡음비 $x$를 갖는 가우시안 백색 잡음 채널의 채널 용량에 해당합니다(그림 15.17). 그런 다음 $R_{1}$에 대한 경계를 다음과 같이 작성합니다.

$$
R_{1} \leq C\left(\frac{P_{1}}{N}\right)
$$

마찬가지로,

$$
R_{2} \leq C\left(\frac{P_{2}}{N}\right)
$$
<!-- Page 572 -->

그림 15.17. 가우시안 다중 접속 채널 용량.
그리고

$$
R_{1}+R_{2} \leq C\left(\frac{P_{1}+P_{2}}{N}\right)
$$

이러한 상한은 $X_{1} \sim \mathcal{N}\left(0, P_{1}\right)$ 및 $X_{2}=$ $\mathcal{N}\left(0, P_{2}\right)$일 때 달성되며 용량 영역을 정의합니다. 이러한 부등식에 대한 놀라운 사실은 속도의 합이 $C\left(\frac{P_{1}+P_{2}}{N}\right)$만큼 클 수 있다는 것입니다. 이는 전송 전력의 합과 동일한 전력으로 전송하는 단일 송신기가 달성하는 속도입니다.

코너 포인트의 해석은 고정된 입력 분포에 대한 이산 다중 접속 채널의 달성 가능한 속도 쌍의 해석과 매우 유사합니다. 가우시안 채널의 경우, 디코딩을 두 단계 프로세스로 간주할 수 있습니다. 첫 번째 단계에서 수신기는 첫 번째 송신기를 잡음의 일부로 간주하여 두 번째 송신기를 디코딩합니다. 이 디코딩은 $R_{2}<C\left(\frac{P_{2}}{P_{1}+N}\right)$이면 오류 확률이 낮습니다. 두 번째 송신기가 성공적으로 디코딩된 후, 이를 제거할 수 있으며 첫 번째 송신기는 $R_{1}<C\left(\frac{P_{1}}{N}\right)$이면 올바르게 디코딩할 수 있습니다. 따라서 이 논증은 단일 사용자 연산을 통해 용량 영역의 코너 포인트에 있는 속도 쌍을 달성할 수 있음을 보여줍니다. 이 프로세스를 "양파 껍질 벗기기(onion-peeling)"라고 하며, 사용자 수에 관계없이 확장할 수 있습니다.

이를 동일한 전력을 가진 $m$명의 송신기로 일반화하면 총 속도는 $C\left(\frac{m P}{N}\right)$이며, 이는 $m \rightarrow \infty$일 때 $\infty$로 갑니다. 송신기당 평균 속도인 $\frac{1}{m} C\left(\frac{m P}{N}\right)$는 0으로 갑니다. 따라서 송신기의 총 수가 매우 많을 때,

<!-- Page 573 -->
간섭이 많더라도, 개별 송신자당 전송률이 0으로 수렴하더라도 임의로 큰 총 정보량을 보낼 수 있습니다.

위에서 설명한 용량 영역은 코드 분할 다중 접속(CDMA)에 해당하며, 여기서 서로 다른 송신자들은 별도의 코드를 사용하고 수신기는 이를 하나씩 복조합니다. 그러나 실제 많은 상황에서는 주파수 분할 다중화 또는 시분할 다중화와 같은 더 간단한 방식이 사용됩니다. 주파수 분할 다중화에서는 전송률이 각 송신자에게 할당된 대역폭에 따라 달라집니다. 대역폭 $W_{1}$과 $W_{2}$를 사용하고 $W_{1}+W_{2}=W$(총 대역폭)인 두 송신자의 경우를 고려해 보겠습니다. 단일 사용자 대역 제한 채널의 용량 공식을 사용하면 다음과 같은 전송률 쌍을 달성할 수 있습니다.

$$
\begin{aligned}
& R_{1}=W_{1} \log \left(1+\frac{P_{1}}{N W_{1}}\right) \\
& R_{2}=W_{2} \log \left(1+\frac{P_{2}}{N W_{2}}\right)
\end{aligned}
$$

$W_{1}$과 $W_{2}$를 변경함에 따라 그림 15.18에 표시된 곡선을 추적합니다. 이 곡선은 용량 영역의 경계에 한 지점에서 접하며, 이는 각 채널에 채널 내 전력에 비례하는 대역폭을 할당하는 것에 해당합니다. 주파수 대역을 무선국에 할당하는 어떤 방식도 할당된 전력이 대역폭에 비례하지 않으면 최적이 될 수 없다고 결론 내립니다.

시분할 다중 접속(TDMA)에서는 시간이 슬롯으로 분할되며, 각 사용자는 해당 사용자만 송신하고 다른 모든 사용자는 조용히 있는 슬롯을 할당받습니다. 두 명의 사용자가 있고 각각 전력이 $P$라면, 다른 사용자가 침묵할 때 각 사용자가 보내는 전송률은 $C(P / N)$입니다. 이제 시간이 동일한 길이의 슬롯으로 분할되고, 모든 홀수 슬롯이 사용자 1에게 할당되고 모든 짝수 슬롯이 사용자 2에게 할당된다면, 각 사용자가 달성하는 평균 전송률은 $\frac{1}{2} C(P / N)$입니다. 이 시스템을 순진한 시분할 다중 접속(TDMA)이라고 합니다. 그러나 사용자 1이 시간의 절반만 사용하고 있으므로, 동일한 평균 전력 제약을 유지하면서도 송신 중에 두 배의 전력을 사용할 수 있다는 점을 인지하면 더 나은 성능을 달성할 수 있습니다. 이 수정으로 각 사용자는 $\frac{1}{2} C(2 P / N)$의 전송률로 정보를 보낼 수 있습니다. 각 송신자에게 할당된 슬롯의 길이(및 슬롯 동안 사용되는 순간 전력)를 변경함으로써, 서로 다른 대역폭 할당을 가진 FDMA와 동일한 용량 영역을 달성할 수 있습니다.

그림 15.18에서 설명하듯이, 일반적으로 용량 영역은 시분할 또는 주파수 분할 다중화로 달성되는 영역보다 더 큽니다. 그러나 다음 사항에 유의하십시오.
<!-- Page 574 -->

그림 15.18. FDMA 및 TDMA를 이용한 가우시안 다중 접속 채널 용량.
위에 유도된 다중 접속 용량 영역은 모든 송신자를 위한 공통 디코더를 사용하여 달성됩니다. 그러나 양파 껍질 벗기기(onion-peeling)를 통해 용량 영역을 달성하는 것도 가능하며, 이는 공통 디코더의 필요성을 제거하고 대신 일련의 단일 사용자 코드를 사용합니다. CDMA는 전체 용량 영역을 달성하며, 추가적으로 현재 사용자의 코드를 변경하지 않고도 새 사용자를 쉽게 추가할 수 있습니다. 반면에 TDMA 및 FDMA 시스템은 일반적으로 고정된 수의 사용자를 위해 설계되며, 슬롯이 비어 있거나(실제 사용자 수가 슬롯 수보다 적은 경우) 일부 사용자가 제외될 수 있습니다(사용자 수가 슬롯 수보다 많은 경우). 그러나 많은 실제 시스템에서는 설계의 단순성이 중요한 고려 사항이며, 앞에서 제시된 다중 접속 아이디어로 인한 용량 개선이 증가된 복잡성을 정당화하기에 충분하지 않을 수 있습니다.

$m$개의 소스와 각각의 전력 $P_{1}, P_{2}, \ldots, P_{m}$ 및 전력 $N$의 주변 잡음을 갖는 가우시안 다중 접속 시스템에 대해, 임의의 집합 $S$에 대한 가우스 법칙의 등가물을 다음과 같은 형태로 명시할 수 있습니다.

$$
\begin{aligned}
\sum_{i \in S} R_{i} & =\text { 집합 } S \text{로부터의 총 정보 흐름 속도} \\
& \leq C\left(\frac{\sum_{i \in S} P_{i}}{N}\right)
\end{aligned}
$$
<!-- Page 575 -->
# 15.4 상관관계 있는 소스의 인코딩

이제 분산 데이터 압축으로 넘어가겠습니다. 이 문제는 여러 면에서 다중 접속 채널 문제의 데이터 압축 이중입니다. 소스 $X$를 인코딩하는 방법을 알고 있습니다. $R>H(X)$의 비율이면 충분합니다. 이제 두 개의 소스 $(X, Y) \sim p(x, y)$가 있다고 가정해 보겠습니다. 이들을 함께 인코딩한다면 $H(X, Y)$의 비율이면 충분합니다. 하지만 $X$와 $Y$ 소스를 재구성하고자 하는 어떤 사용자를 위해 $X$와 $Y$ 소스를 별도로 설명해야 한다면 어떻게 될까요? 명백히, $X$와 $Y$를 별도로 인코딩함으로써 $R=R_{x}+R_{y}>H(X)+H(Y)$의 비율이면 충분하다는 것을 알 수 있습니다. 그러나 Slepian과 Wolf [502]의 놀랍고 근본적인 논문에서 상관관계 있는 소스의 별도 인코딩에 대해서도 총 비율 $R=H(X, Y)$이면 충분하다는 것이 입증되었습니다.

$\left(X_{1}, Y_{1}\right),\left(X_{2}, Y_{2}\right), \ldots$를 i.i.d. $\sim p(x, y)$로 공동 분포된 확률 변수의 시퀀스라고 가정합니다. $X$ 시퀀스는 위치 $A$에 있고 $Y$ 시퀀스는 위치 $B$에 있다고 가정합니다. 이 상황은 그림 15.19에 설명되어 있습니다.

이 결과에 대한 증명을 진행하기 전에 몇 가지 정의를 내리겠습니다.

정의 A ( $\left(2^{n R_{1}}, 2^{n R_{2}}\right), n$ ) 공동 소스 $(X, Y)$에 대한 분산 소스 코드는 두 개의 인코더 맵으로 구성됩니다.

$$
\begin{aligned}
& f_{1}: \mathcal{X}^{n} \rightarrow\left\{1,2, \ldots, 2^{n R_{1}}\right\} \\
& f_{2}: \mathcal{Y}^{n} \rightarrow\left\{1,2, \ldots, 2^{n R_{2}}\right\}
\end{aligned}
$$

그림 15.19. Slepian-Wolf 코딩.
<!-- Page 576 -->
디코더 맵은 다음과 같습니다.

$$
g:\left\{1,2, \ldots, 2^{n R_{1}}\right\} \times\left\{1,2, \ldots, 2^{n R_{2}}\right\} \rightarrow \mathcal{X}^{n} \times \mathcal{Y}^{n}
$$

여기서 $f_{1}\left(X^{n}\right)$는 $X^{n}$에 해당하는 인덱스이고, $f_{2}\left(Y^{n}\right)$는 $Y^{n}$에 해당하는 인덱스이며, $\left(R_{1}, R_{2}\right)$는 코드의 속도 쌍입니다.

정의 분산 소스 코드의 오류 확률은 다음과 같이 정의됩니다.

$$
P_{e}^{(n)}=P\left(g\left(f_{1}\left(X^{n}\right), f_{2}\left(Y^{n}\right)\right) \neq\left(X^{n}, Y^{n}\right)\right)
$$

정의 속도 쌍 $\left(R_{1}, R_{2}\right)$는 오류 확률 $P_{e}^{(n)} \rightarrow 0$인 $\left(\left(2^{n R_{1}}, 2^{n R_{2}}\right), n\right)$ 분산 소스 코드 시퀀스가 존재할 경우 분산 소스에 대해 달성 가능하다고 합니다. 달성 가능한 속도 영역은 달성 가능한 속도 집합의 폐포입니다.

정리 15.4.1 (Slepian-Wolf) 소스 $(X, Y)$가 i.i.d $\sim p(x, y)$로 추출되는 분산 소스 코딩 문제에 대해 달성 가능한 속도 영역은 다음과 같이 주어집니다.

$$
\begin{aligned}
R_{1} & \geq H(X \mid Y) \\
R_{2} & \geq H(Y \mid X) \\
R_{1}+R_{2} & \geq H(X, Y)
\end{aligned}
$$

몇 가지 예시를 통해 결과를 설명해 보겠습니다.
예시 15.4.1 고담과 메트로폴리스의 날씨를 고려해 보겠습니다. 예시를 위해 고담은 0.5의 확률로 맑고, 메트로폴리스의 날씨는 0.89의 확률로 고담과 같다고 가정합니다. 날씨의 결합 분포는 다음과 같습니다.

|  | 메트로폴리스 |  |
| :--: | :--: | :--: |
| $p(x, y)$ | 비 | 맑음 |
| 고담 |  |  |
| 비 | 0.445 | 0.055 |
| 맑음 | 0.055 | 0.445 |
<!-- Page 577 -->
워싱턴에 있는 국립 기상청 본부로 100일간의 날씨 정보를 전송한다고 가정해 봅시다. 두 장소의 날씨 정보 100비트 모두를 보내면 총 200비트가 됩니다. 정보를 독립적으로 압축하기로 결정했다면 각 장소에서 여전히 $100 H(0.5)=100$ 비트의 정보가 필요하며, 이는 총 200비트입니다. 대신 Slepian-Wolf 인코딩을 사용하면 총 $H(X)+$ $H(Y \mid X)=100 H(0.5)+100 H(0.89)=100+50=150$ 비트만 필요합니다.

예제 15.4.2 다음의 결합 분포를 고려해 보십시오:

| $p(u, v)$ | 0 | 1 |
| :--: | :--: | :--: |
| 0 | $\frac{1}{3}$ | $\frac{1}{3}$ |
| 1 | 0 | $\frac{1}{3}$ |

이 경우, 이 소스의 전송에 필요한 총 속도는 Slepian-Wolf 인코딩 없이 독립적으로 소스를 전송하는 데 필요한 2비트 대신 $H(U)+H(V \mid U)=\log 3=1.58$ 비트입니다.

# 15.4.1 Slepian-Wolf 정리의 달성 가능성

이제 Slepian-Wolf 정리의 속도 달성 가능성을 증명하겠습니다. 증명을 진행하기 전에 무작위 빈을 사용하는 새로운 코딩 절차를 소개합니다. 무작위 빈의 본질적인 아이디어는 해시 함수와 매우 유사합니다: 각 소스 시퀀스에 대해 큰 무작위 인덱스를 선택합니다. 만약 전형적인 소스 시퀀스의 집합이 충분히 작다면 (또는 동등하게, 해시 함수의 범위가 충분히 크다면), 높은 확률로 서로 다른 소스 시퀀스는 서로 다른 인덱스를 가지며, 인덱스로부터 소스 시퀀스를 복구할 수 있습니다.

이 아이디어를 단일 소스의 인코딩에 적용하는 것을 고려해 봅시다. 3장에서는 전형적인 집합의 모든 요소에 인덱스를 부여하고 전형적인 집합 밖의 요소는 신경 쓰지 않는 방법을 고려했습니다. 이제 모든 시퀀스에 인덱스를 부여하지만 나중에 비전형적인 시퀀스를 거부하는 무작위 빈 할당 절차를 설명하겠습니다.

다음 절차를 고려해 보십시오: 각 시퀀스 $X^{n}$에 대해 $\left\{1,2, \ldots, 2^{n R}\right\}$에서 무작위로 인덱스를 뽑습니다. 동일한 인덱스를 갖는 시퀀스 $X^{n}$의 집합은 빈을 형성한다고 합니다. 이는 먼저 일련의 빈을 놓고 $X^{n}$들을 무작위로 빈에 던지는 것으로 볼 수 있습니다. 빈 인덱스로부터 소스를 디코딩하기 위해, 빈에서 전형적인 $X^{n}$ 시퀀스를 찾습니다. 만약 빈에 전형적인 $X^{n}$ 시퀀스가 하나만 있다면, 이를 소스 시퀀스의 추정치 $\hat{X}^{n}$으로 선언합니다. 그렇지 않으면 오류가 선언됩니다.
<!-- Page 578 -->
위 절차는 소스 코드를 정의합니다. 이 코드의 오류 확률을 분석하기 위해 $X^{n}$ 시퀀스를 일반 시퀀스와 비일반 시퀀스의 두 가지 유형으로 나눌 것입니다. 소스 시퀀스가 일반적이라면, 이 소스 시퀀스에 해당하는 빈에는 적어도 하나의 일반 시퀀스(소스 시퀀스 자체)가 포함될 것입니다. 따라서 빈에 일반 시퀀스가 하나 이상 있는 경우에만 오류가 발생합니다. 소스 시퀀스가 비일반적이라면 항상 오류가 발생합니다. 그러나 빈의 수가 일반 시퀀스의 수보다 훨씬 많으면 빈에 일반 시퀀스가 하나 이상 있을 확률은 매우 작으며, 따라서 일반 시퀀스가 오류를 초래할 확률은 매우 작습니다.

형식적으로, $f\left(X^{n}\right)$를 $X^{n}$에 해당하는 빈 인덱스라고 합시다. 디코딩 함수를 $g$라고 부릅시다. 오류 확률(코드 $f$의 무작위 선택에 대해 평균화됨)은 다음과 같습니다.

$$
\begin{aligned}
P(g(f(\mathbf{X})) \neq \mathbf{X}) & \leq P\left(\mathbf{X} \notin A_{\epsilon}^{(n)}\right)+\sum_{\mathbf{x}} P\left(\exists \mathbf{x}^{\prime} \neq \mathbf{x}: \mathbf{x}^{\prime} \in A_{\epsilon}^{(n)}, f\left(\mathbf{x}^{\prime}\right)\right. \\
& =f(\mathbf{x})) p(\mathbf{x}) \\
& \leq \epsilon+\sum_{\mathbf{x}} \sum_{\substack{\mathbf{x}^{\prime} \in A_{\epsilon}^{(n)}} \\
\mathbf{x}^{\prime} \neq \mathbf{x}} P\left(f\left(\mathbf{x}^{\prime}\right)=f(\mathbf{x})\right) p(\mathbf{x}) \\
& \leq \epsilon+\sum_{\mathbf{x}} \sum_{\substack{\mathbf{x}^{\prime} \in A_{\epsilon}^{(n)}} 2^{-n R}} p(\mathbf{x}) \\
& =\epsilon+\sum_{\substack{\mathbf{x}^{\prime} \in A_{\epsilon}^{(n)}} 2^{-n R}} \sum_{\mathbf{x}} p(\mathbf{x}) \\
& \leq \epsilon+\sum_{\substack{\mathbf{x}^{\prime} \in A_{\epsilon}^{(n)}} 2^{-n R}} \\
& \leq \epsilon+2^{n(H(X)+\epsilon)} 2^{-n R} \\
& \leq 2 \epsilon
\end{aligned}
$$

$R>H(X)+\epsilon$이고 $n$이 충분히 크다면 그렇습니다. 따라서 코드의 속도가 엔트로피보다 크면 오류 확률은 임의로 작아지며 코드는 3장에서 설명한 코드와 동일한 결과를 달성합니다.

위의 예는 엔트로피보다 높은 속도에서 낮은 오류 확률을 가진 코드를 구성하는 데 많은 방법이 있다는 사실을 보여줍니다. 범용 소스 코드는 이러한 코드의 또 다른 예입니다.
<!-- Page 579 -->
구체적인 빈 할당 방식은 인코더에서 일반적인 집합(typical set)을 명시적으로 특성화할 필요가 없으며, 디코더에서만 필요합니다. 이 속성은 이 코드가 분산된 소스(distributed source)의 경우에도 계속 작동할 수 있도록 합니다. 이는 정리의 증명에서 설명됩니다.

이제 분산된 소스 코딩(distributed source coding)에 대한 고려 사항으로 돌아가서 Slepian-Wolf 정리의 속도 영역(rate region)의 달성 가능성을 증명합니다.

증명: (정리 15.4.1의 달성 가능성). 증명의 기본 아이디어는 $\mathcal{X}^{n}$의 공간을 $2^{n R_{1}}$개의 빈으로, $\mathcal{Y}^{n}$의 공간을 $2^{n R_{2}}$개의 빈으로 분할하는 것입니다.

랜덤 코드 생성: $\mathcal{X}^{n}$의 모든 $\mathbf{x}$를 $\left\{1,2, \ldots, 2^{n R_{1}}\right\}$에 대한 균등 분포에 따라 독립적으로 $2^{n R_{1}}$개의 빈 중 하나에 할당합니다. 마찬가지로, $\mathcal{Y}^{n}$의 모든 $\mathbf{y}$를 $2^{n R_{2}}$개의 빈 중 하나에 무작위로 할당합니다. 할당 $f_{1}$과 $f_{2}$를 인코더와 디코더 모두에게 공개합니다.

인코딩: 송신자 1은 $\mathbf{X}$가 속한 빈의 인덱스를 보냅니다. 송신자 2는 $\mathbf{Y}$가 속한 빈의 인덱스를 보냅니다.

디코딩: 수신된 인덱스 쌍 $\left(i_{0}, j_{0}\right)$이 주어졌을 때, $f_{1}(\mathbf{x})=i_{0}$, $f_{2}(\mathbf{y})=j_{0}$이고 $(\mathbf{x}, \mathbf{y}) \in A_{e}^{(n)}$인 유일한 시퀀스 쌍 $(\mathbf{x}, \mathbf{y})$이 하나 존재하면 $(\hat{\mathbf{x}}, \hat{\mathbf{y}})=(\mathbf{x}, \mathbf{y})$라고 선언합니다. 그렇지 않으면 오류라고 선언합니다. 이 방식은 그림 15.20에 설명되어 있습니다. $X$ 시퀀스의 집합과 $Y$ 시퀀스의 집합은 인덱스 쌍이 곱 빈(product bin)을 지정하도록 분할됩니다.

그림 15.20. Slepian-Wolf 인코딩: 공동으로 일반적인 쌍(jointly typical pairs)은 곱 빈으로 분리됩니다.
<!-- Page 580 -->
오류 확률: $\left(X_{i}, Y_{i}\right) \sim p(x, y)$라고 정의합니다. 다음 사건들을 정의합니다.

$$
\begin{aligned}
& E_{0}=\left\{(\mathbf{X}, \mathbf{Y}) \notin A_{\epsilon}^{(n)}\right\} \\
& E_{1}=\left\{\exists \mathbf{x}^{\prime} \neq \mathbf{X}: f_{1}\left(\mathbf{x}^{\prime}\right)=f_{1}(\mathbf{X}) \text { and }\left(\mathbf{x}^{\prime}, \mathbf{Y}\right) \in A_{\epsilon}^{(n)}\right\} \\
& E_{2}=\left\{\exists \mathbf{y}^{\prime} \neq \mathbf{Y}: f_{2}\left(\mathbf{y}^{\prime}\right)=f_{2}(\mathbf{Y}) \text { and }\left(\mathbf{X}, \mathbf{y}^{\prime}\right) \in A_{\epsilon}^{(n)}\right\}
\end{aligned}
$$

그리고

$$
\begin{aligned}
E_{12} & =\left\{\exists\left(\mathbf{x}^{\prime}, \mathbf{y}^{\prime}\right): \mathbf{x}^{\prime} \neq \mathbf{X}, \mathbf{y}^{\prime} \neq \mathbf{Y}, f_{1}\left(\mathbf{x}^{\prime}\right)\right. \\
& \left.=f_{1}(\mathbf{X}), f_{2}\left(\mathbf{y}^{\prime}\right)=f_{2}(\mathbf{Y}) \text { and }\left(\mathbf{x}^{\prime}, \mathbf{y}^{\prime}\right) \in A_{\epsilon}^{(n)}\right\}
\end{aligned}
$$

여기서 $\mathbf{X}, \mathbf{Y}, f_{1}$, 그리고 $f_{2}$는 랜덤 변수입니다. $(\mathbf{X}, \mathbf{Y})$가 $A_{\epsilon}^{(n)}$에 속하지 않거나 같은 빈(bin)에 다른 일반적인 쌍(typical pair)이 존재하면 오류가 발생합니다. 따라서 사건들의 합집합(union of events)에 대한 상한을 사용하면,

$$
\begin{aligned}
P_{\epsilon}^{(n)} & =P\left(E_{0} \cup E_{1} \cup E_{2} \cup E_{12}\right) \\
& \leq P\left(E_{0}\right)+P\left(E_{1}\right)+P\left(E_{2}\right)+P\left(E_{12}\right)
\end{aligned}
$$

먼저 $E_{0}$를 고려합니다. AEP에 의해 $P\left(E_{0}\right) \rightarrow 0$이므로, 충분히 큰 $n$에 대해 $P\left(E_{0}\right)<\epsilon$입니다. $P\left(E_{1}\right)$의 상한을 구하기 위해 다음을 얻습니다.

$$
\begin{aligned}
P\left(E_{1}\right) & =P\left\{\exists \mathbf{x}^{\prime} \neq \mathbf{X}: f_{1}\left(\mathbf{x}^{\prime}\right)=f_{1}(\mathbf{X}), \text { and }\left(\mathbf{x}^{\prime}, \mathbf{Y}\right) \in A_{\epsilon}^{(n)}\right\} \\
& =\sum_{(\mathbf{x}, \mathbf{y})} p(\mathbf{x}, \mathbf{y}) P\left\{\exists \mathbf{x}^{\prime} \neq \mathbf{x}: f_{1}\left(\mathbf{x}^{\prime}\right)=f_{1}(\mathbf{x}), \quad\left(\mathbf{x}^{\prime}, \mathbf{y}\right) \in A_{\epsilon}^{(n)}\right\} \\
& \leq \sum_{(\mathbf{x}, \mathbf{y})} p(\mathbf{x}, \mathbf{y}) \sum_{\substack{\mathbf{x}^{\prime} \neq \mathbf{x} \\
\left(\mathbf{x}^{\prime}, \mathbf{y}\right) \in A_{\epsilon}^{(n)}} P\left(f_{1}\left(\mathbf{x}^{\prime}\right)=f_{1}(\mathbf{x})\right) \\
& =\sum_{(\mathbf{x}, \mathbf{y})} p(\mathbf{x}, \mathbf{y}) 2^{-n R_{1}}\left|A_{\epsilon}(X \mid \mathbf{y})\right| \\
& \leq 2^{-n R_{1}} 2^{n(H(X \mid Y)+\epsilon)} \quad \text { (정리 15.2.2에 의해) }
\end{aligned}
$$

이는 $R_{1}>H(X \mid Y)$이면 0으로 수렴합니다. 따라서 충분히 큰 $n$에 대해 $P\left(E_{1}\right)< \epsilon$입니다. 마찬가지로, 충분히 큰 $n$에 대해 $R_{2}>H(Y \mid X)$이면 $P\left(E_{2}\right)<\epsilon$이고, $R_{1}+R_{2}>H(X, Y)$이면 $P\left(E_{12}\right)<\epsilon$입니다. 평균 오류 확률이 $<4 \epsilon$이므로, 오류 확률이 $<4 \epsilon$인 최소한 하나의 코드 $\left(f_{1}^{*}, f_{2}^{*}, g^{*}\right)$가 존재합니다. 따라서 $P_{\epsilon}^{(n)} \rightarrow 0$인 코드의 수열을 구성할 수 있으며, 달성 가능성 증명이 완료됩니다.
<!-- Page 581 -->
# 15.4.2 Slepian-Wolf 정리에 대한 역정리

Slepian-Wolf 정리에 대한 역정리는 단일 소스에 대한 결과로부터 명백하게 도출되지만, 완전성을 위해 제시하겠습니다.

증명: (정리 15.4.1에 대한 역정리). 평소와 같이 Fano의 부등식으로 시작합니다. $f_{1}, f_{2}, g$가 고정되었다고 가정합니다. $I_{0}=f_{1}\left(X^{n}\right)$이고 $J_{0}=f_{2}\left(Y^{n}\right)$라고 하면,

$$
H\left(X^{n}, Y^{n} \mid I_{0}, J_{0}\right) \leq P_{e}^{(n)} n(\log |\mathcal{X}|+\log |\mathcal{Y}|)+1=n \epsilon_{n}
$$

여기서 $\epsilon_{n} \rightarrow 0$은 $n \rightarrow \infty$일 때 성립합니다. 이제 조건부 확률을 추가하면, 다음과 같은 부등식도 얻습니다.

$$
H\left(X^{n} \mid Y^{n}, I_{0}, J_{0}\right) \leq n \epsilon_{n}
$$

그리고

$$
H\left(Y^{n} \mid X^{n}, I_{0}, J_{0}\right) \leq n \epsilon_{n}
$$

부등식 연쇄를 다음과 같이 작성할 수 있습니다.

$$
\begin{aligned}
n\left(R_{1}+R_{2}\right) & \stackrel{(a)}{\geq} H\left(I_{0}, J_{0}\right) \\
& =I\left(X^{n}, Y^{n} ; I_{0}, J_{0}\right)+H\left(I_{0}, J_{0} \mid X^{n}, Y^{n}\right) \\
& \stackrel{(b)}{=} I\left(X^{n}, Y^{n} ; I_{0}, J_{0}\right) \\
& =H\left(X^{n}, Y^{n}\right)-H\left(X^{n}, Y^{n} \mid I_{0}, J_{0}\right) \\
& \stackrel{(c)}{\geq} H\left(X^{n}, Y^{n}\right)-n \epsilon_{n} \\
& \stackrel{(d)}{=} n H(X, Y)-n \epsilon_{n}
\end{aligned}
$$

여기서
(a)는 $I_{0} \in\left\{1,2, \ldots, 2^{n R_{1}}\right\}$이고 $J_{0} \in\left\{1,2, \ldots, 2^{n R_{2}}\right\}$라는 사실로부터 도출됩니다.
(b)는 $I_{0}$가 $X^{n}$의 함수이고 $J_{0}$가 $Y^{n}$의 함수라는 사실로부터 도출됩니다.
(c)는 Fano의 부등식 (15.178)로부터 도출됩니다.
(d)는 연쇄 법칙과 $\left(X_{i}, Y_{i}\right)$가 i.i.d.라는 사실로부터 도출됩니다.
<!-- Page 582 -->
마찬가지로, (15.179)을 사용하여 다음과 같이 나타낼 수 있습니다.

$$
\begin{aligned}
n R_{1} & \stackrel{(a)}{=} H\left(I_{0}\right) \\
& \geq H\left(I_{0} \mid Y^{n}\right) \\
& =I\left(X^{n} ; I_{0} \mid Y^{n}\right)+H\left(I_{0} \mid X^{n}, Y^{n}\right) \\
& \stackrel{(b)}{=} I\left(X^{n} ; I_{0} \mid Y^{n}\right) \\
& =H\left(X^{n} \mid Y^{n}\right)-H\left(X^{n} \mid I_{0}, J_{0}, Y^{n}\right) \\
& \stackrel{(c)}{=} H\left(X^{n} \mid Y^{n}\right)-n \epsilon_{n} \\
& \stackrel{(d)}{=} n H(X \mid Y)-n \epsilon_{n}
\end{aligned}
$$

여기서 이유는 위 방정식들과 동일합니다. 마찬가지로 다음과 같이 보일 수 있습니다.

$$
n R_{2} \geq n H(Y \mid X)-n \epsilon_{n}
$$

이 부등식들을 $n$으로 나누고 $n \rightarrow \infty$로 극한을 취하면 원하는 역정리를 얻습니다.

Slepian-Wolf 정리에 설명된 영역은 그림 15.21에 나와 있습니다.

# 15.4.3 다중 소스에 대한 Slepian-Wolf 정리

15.4.2절의 결과는 다중 소스로 쉽게 일반화될 수 있습니다. 증명은 정확히 동일한 방식으로 진행됩니다.

정리 15.4.2 $\left(X_{1 i}, X_{2 i}, \ldots, X_{m i}\right)$가 i.i.d. $\sim p\left(x_{1}, x_{2}, \ldots, x_{m}\right)$라고 가정합니다. 그러면 별도의 인코더와 공통 디코더를 갖춘 분산 소스 코딩에 대해 달성 가능한 속도 벡터의 집합은 다음과 같이 정의됩니다.

$$
R(S)>H\left(X(S) \mid X\left(S^{c}\right)\right)
$$

모든 $S \subseteq\{1,2, \ldots, m\}$에 대해, 여기서

$$
R(S)=\sum_{i \in S} R_{i}
$$

그리고 $X(S)=\left\{X_{j}: j \in S\right\}$입니다.
<!-- Page 583 -->

그림 15.21. Slepian-Wolf 인코딩을 위한 속도 영역.

증명: 증명은 두 변수의 경우와 동일하며 생략합니다.

Slepian-Wolf 인코딩의 달성 가능성은 i.i.d. 상관 소스에 대해 증명되었지만, 이 증명은 AEP를 만족하는 임의의 결합 소스의 경우로 쉽게 확장될 수 있습니다. 특히, 이는 임의의 결합 에르고딕 소스의 경우로 확장될 수 있습니다 [122]. 이러한 경우 속도 영역 정의의 엔트로피는 해당 엔트로피 속도로 대체됩니다.

# 15.4.4 Slepian-Wolf 코딩의 해석

그래프 색칠의 관점에서 Slepian-Wolf 인코딩의 속도 영역의 모서리 점들을 해석해 보겠습니다. 속도 $R_{1}=H(X), R_{2}=H(Y \mid X)$를 갖는 점을 고려합니다. $n H(X)$ 비트를 사용하여 $X^{n}$을 효율적으로 인코딩할 수 있으므로 디코더는 임의로 낮은 오류 확률로 $X^{n}$을 재구성할 수 있습니다. 하지만 $n H(Y \mid X)$ 비트로 $Y^{n}$을 어떻게 코딩할까요? 일반적인 집합의 관점에서 그림을 보면, 모든 $X^{n}$과 관련하여 주어진 $X^{n}$과 결합적으로 일반적인 $Y^{n}$ 시퀀스의 일반적인 "팬"이 그림 15.22와 같이 있음을 알 수 있습니다.

$Y$ 인코더가 $X^{n}$을 알고 있다면, 인코더는 이 일반적인 팬 내에서 $Y^{n}$의 인덱스를 보낼 수 있습니다. 디코더도 $X^{n}$을 알고 있으므로 이 일반적인 팬을 구성하여 $Y^{n}$을 재구성할 수 있습니다. 그러나 $Y$ 인코더는 $X^{n}$을 알지 못합니다. 따라서 일반적인 팬을 결정하려고 시도하는 대신, 무작위로

<!-- Page 584 -->

그림 15.22. 공동으로 전형적인 팬(fan).
모든 $Y^{n}$ 시퀀스를 $2^{n R_{2}}$개의 색상으로 색칠합니다. 색상의 수가 충분히 많으면, 높은 확률로 특정 팬 내의 모든 색상이 다르고 $Y^{n}$ 시퀀스의 색상이 $X^{n}$ 팬 내에서 $Y^{n}$ 시퀀스를 고유하게 정의할 것입니다. 만약 비율 $R_{2}>H(Y \mid X)$이면, 색상의 수는 팬의 요소 수보다 지수적으로 많으며, 이 방식이 지수적으로 작은 오류 확률을 가질 것임을 보일 수 있습니다.

# 15.5 SLEPIAN-WOLF 인코딩과 다중 접속 채널 간의 이중성

다중 접속 채널에서는 두 개의 입력과 하나의 출력만 있는 채널을 통해 독립적인 메시지를 전송하는 문제를 고려했습니다. Slepian-Wolf 인코딩에서는 상관 관계가 있는 소스를 잡음 없는 채널을 통해 전송하고, 두 소스 모두를 복구하기 위한 공통 디코더를 사용하는 문제를 고려했습니다. 이 섹션에서는 두 시스템 간의 이중성을 탐구합니다.

그림 15.23에서는 두 개의 독립적인 메시지가 채널을 통해 $X_{1}^{n}$ 및 $X_{2}^{n}$ 시퀀스로 전송될 예정입니다. 수신기는 수신된 시퀀스로부터 메시지를 추정합니다. 그림 15.24에서는 상관 관계가 있는 소스가 "독립적인" 메시지 $i$와 $j$로 인코딩됩니다. 수신기는 $i$와 $j$에 대한 지식을 바탕으로 소스 시퀀스를 추정하려고 합니다.

다중 접속 채널의 용량 영역 달성 가능성 증명에서, 우리는 메시지 집합에서

<!-- Page 585 -->

그림 15.23. 다중 접속 채널.

그림 15.24. 상관 관계가 있는 소스 인코딩.
시퀀스 $X_{1}^{n}$ 및 $X_{2}^{n}$입니다. Slepian-Wolf 코딩 증명에서는 시퀀스 $X^{n}$ 및 $Y^{n}$ 집합에서 메시지 집합으로의 랜덤 맵을 사용했습니다. 다중 접속 채널 코딩 정리 증명에서 오류 확률은 다음과 같이 제한되었습니다.

$$
\begin{aligned}
P_{e}^{(n)} & \leq \epsilon+\sum_{\text {codewords }} \operatorname{Pr}(\text { codeword jointly typical with sequence received }) \\
& =\epsilon+\sum_{2^{n R_{1}} \text { terms }} 2^{-n I_{1}}+\sum_{2^{n R_{2}} \text { terms }} 2^{-n I_{2}}+\sum_{2^{n\left(R_{1}+R_{2}\right)} \text { terms }} 2^{-n I_{3}}
\end{aligned}
$$
<!-- Page 586 -->
여기서 $\epsilon$은 시퀀스가 일반적이지 않을 확률이며, $R_{i}$는 오류 확률에 기여할 수 있는 코드워드 수를 나타내는 비율이고, $I_{i}$는 코드워드가 수신 시퀀스와 공동으로 일반적인 확률에 해당하는 상호 정보입니다.

Slepian-Wolf 인코딩의 경우, 오류 확률에 대한 해당 식은 다음과 같습니다.

$$
\begin{aligned}
P_{e}^{(n)} & \leq \epsilon+\sum_{\text {jointly typical sequences }} \operatorname{Pr} \text { ( have the same codeword) } \\
& =\epsilon+\sum_{2^{n H_{1}} \text { terms }} 2^{-n R_{1}}+\sum_{2^{n H_{2}} \text { terms }} 2^{-n R_{2}}+\sum_{2^{n H_{3}} \text { terms }} 2^{-n\left(R_{1}+R_{2}\right)}
\end{aligned}
$$

여기서 AEP의 제약 조건이 만족되지 않을 확률은 다시 $\epsilon$으로 제한되며, 다른 항들은 다른 시퀀스 쌍이 공동으로 일반적이고 주어진 소스 쌍과 동일한 빈에 있을 수 있는 다양한 방법을 나타냅니다.

다중 접속 채널과 상관 소스 인코딩의 이중성은 이제 명확합니다. 이 두 시스템이 서로의 이중이라는 것은 다소 놀랍습니다. 브로드캐스트 채널과 다중 접속 채널 간의 이중성을 예상했을 것입니다.

# 15.6 브로드캐스트 채널

브로드캐스트 채널은 하나의 송신기와 두 개 이상의 수신기가 있는 통신 채널입니다. 이는 그림 15.25에 나와 있습니다. 기본적인 문제는 브로드캐스트 채널에서 통신하기 위한 동시에 달성 가능한 속도 집합을 찾는 것입니다. 분석을 시작하기 전에 몇 가지 예를 고려해 보겠습니다.

예제 15.6.1 (TV 방송국) 브로드캐스트 채널의 가장 간단한 예는 라디오 또는 TV 방송국입니다. 그러나 이 예는 정상적으로 방송국이 접속 중인 모든 사람에게 동일한 정보를 보내려고 한다는 점에서 약간 퇴화되었습니다. 용량은 본질적으로 $\max _{p(x)}$ $\min _{i} I\left(X ; Y_{i}\right)$이며, 이는 최악의 수신기의 용량보다 작을 수 있습니다. 그러나 더 나은 수신기가 추가 정보를 받아 더 나은 그림이나 소리를 생성하는 동시에 최악의 수신기는 더 기본적인 정보를 계속 받는 방식으로 정보를 구성하고 싶을 수 있습니다. TV 방송국이 고화질 TV(HDTV)를 도입함에 따라, 좋지 않은 수신기가

<!-- Page 587 -->

그림 15.25. 방송 채널.
일반 TV 신호이고, 좋은 수신기는 고화질 신호에 대한 추가 정보를 수신할 것입니다. 이러한 방법을 달성하는 방법은 방송 채널에 대한 논의에서 설명될 것입니다.

예제 15.6.2 (교실의 강연자) 강연자가 교실에서 학생들에게 정보를 전달하고 있습니다. 학생들 간의 차이로 인해 학생들은 다양한 양의 정보를 받습니다. 일부 학생들은 대부분의 정보를 받고, 다른 학생들은 적은 양의 정보만 받습니다. 이상적인 상황에서는 강연자가 우수한 학생들은 더 많은 정보를 받고, 부족한 학생들은 최소한의 정보량을 받도록 강의를 맞춤화할 수 있을 것입니다. 그러나 준비가 부족한 강의는 가장 약한 학생의 속도로 진행됩니다. 이 상황은 방송 채널의 또 다른 예입니다.

예제 15.6.3 (직교 방송 채널) 가장 간단한 방송 채널은 두 수신기에 대한 두 개의 독립적인 채널로 구성됩니다. 여기서 우리는 두 채널 모두에 독립적인 정보를 보낼 수 있으며, $R_{1}<C_{1}$이고 $R_{2}<C_{2}$이면 수신기 1에 대해 속도 $R_{1}$과 수신기 2에 대해 속도 $R_{2}$를 달성할 수 있습니다. 용량 영역은 그림 15.26에 표시된 사각형입니다.

예제 15.6.4 (스페인어 및 네덜란드어 화자) 중첩의 개념을 설명하기 위해 스페인어와 네덜란드어를 모두 구사할 수 있는 화자의 단순화된 예를 고려할 것입니다. 두 명의 청취자가 있습니다. 한 명은 스페인어만 이해하고 다른 한 명은 네덜란드어만 이해합니다. 각 언어의 어휘가 $2^{20}$ 단어이고 화자가 각 언어로 초당 1단어의 속도로 말한다고 가정합니다. 그러면 그는
<!-- Page 588 -->

그림 15.26. 두 개의 직교 방송 채널에 대한 용량 영역.
수신자 1에게 항상 말함으로써 초당 20비트의 정보를 전송할 수 있습니다. 이 경우 수신자 2에게는 정보를 보내지 않습니다. 마찬가지로 수신자 1에게는 정보를 보내지 않고 초당 20비트의 정보를 수신자 2에게 보낼 수 있습니다. 따라서 단순 시분할을 통해 $R_{1}+R_{2}=20$인 모든 속도 쌍을 달성할 수 있습니다. 하지만 더 나은 결과를 얻을 수 있을까요?

스페인어를 이해하지 못하는 네덜란드 청취자도 스페인어 단어가 언제 사용되는지 인식할 수 있다는 점을 기억하십시오. 마찬가지로 스페인어 청취자는 네덜란드어가 언제 발생하는지 인식할 수 있습니다. 화자는 이를 사용하여 정보를 전달할 수 있습니다. 예를 들어, 각 언어를 사용하는 시간의 비율이 50%라면 100단어 시퀀스에서 약 50단어는 네덜란드어이고 약 50단어는 스페인어입니다. 그러나 스페인어와 네덜란드어 단어의 순서에는 여러 가지 방법이 있습니다. 실제로 단어 순서에는 약 $\left({ }_{50}^{100}\right) \approx 2^{100 H\left(\frac{1}{2}\right)}$ 가지 방법이 있습니다. 이러한 순서 중 하나를 선택하면 두 청취자 모두에게 정보를 전달합니다. 이 방법은 화자가 네덜란드 수신자에게 초당 10비트, 스페인 수신자에게 초당 10비트, 두 수신자 모두에게 초당 1비트의 공통 정보를 보낼 수 있게 하여 단순 시분할로 달성할 수 있는 것보다 많은 총 21비트의 정보를 보낼 수 있게 합니다. 이것은 정보의 중첩의 예입니다.

방송 채널의 결과는 분포가 알려지지 않은 단일 사용자 채널의 경우에도 적용될 수 있습니다. 이 경우 목표는 채널이 좋지 않을 때 최소한의 정보를 전달하고 채널이 좋을 때 추가 정보를 전달하는 것입니다. 방송 채널의 경우와 동일한 중첩 논증을 사용하여 정보를 보낼 수 있는 속도를 찾을 수 있습니다.
<!-- Page 589 -->
# 15.6.1 브로드캐스트 채널에 대한 정의

정의 브로드캐스트 채널은 입력 알파벳 $\mathcal{X}$와 두 개의 출력 알파벳 $\mathcal{Y}_{1}$, $\mathcal{Y}_{2}$, 그리고 확률 전이 함수 $p\left(y_{1}, y_{2} \mid x\right)$로 구성됩니다. 브로드캐스트 채널은 $p\left(y_{1}^{n}, y_{2}^{n} \mid x^{n}\right)=\prod_{i=1}^{n} p\left(y_{1 i}, y_{2 i} \mid x_{i}\right)$인 경우 메모리리스(memoryless)라고 합니다.

브로드캐스트 채널에 대한 코드, 오류 확률, 달성 가능성 및 용량 영역을 다중 접속 채널(multiple-access channel)에서와 같이 정의합니다. 독립적인 정보를 위한 브로드캐스트 채널의 $\left(\left(2^{n R_{1}}, 2^{n R_{2}}\right), n\right)$ 코드는 인코더,

$$
X:\left(\left\{1,2, \ldots, 2^{n R_{1}}\right\} \times\left\{1,2, \ldots, 2^{n R_{2}}\right\}\right) \rightarrow \mathcal{X}^{n}
$$

그리고 두 개의 디코더,

$$
g_{1}: \mathcal{Y}_{1}^{n} \rightarrow\left\{1,2, \ldots, 2^{n R_{1}}\right\}
$$

및

$$
g_{2}: \mathcal{Y}_{2}^{n} \rightarrow\left\{1,2, \ldots, 2^{n R_{2}}\right\}
$$

로 구성됩니다.

평균 오류 확률을 디코딩된 메시지가 전송된 메시지와 같지 않을 확률로 정의합니다. 즉,

$$
P_{e}^{(n)}=P\left(g_{1}\left(Y_{1}^{n}\right) \neq W_{1} \quad \text { or } \quad g_{2}\left(Y_{2}^{n}\right) \neq W_{2}\right)
$$

여기서 $\left(W_{1}, W_{2}\right)$는 $2^{n R_{1}} \times 2^{n R_{2}}$에 균일하게 분포한다고 가정합니다.
정의 속도 쌍 $\left(R_{1}, R_{2}\right)$는 $P_{e}^{(n)} \rightarrow 0$인 $\left(\left(2^{n R_{1}}, 2^{n R_{2}}\right), n\right)$ 코드 시퀀스가 존재할 때 브로드캐스트 채널에 대해 달성 가능하다고 합니다.

이제 두 수신자 모두에게 전송될 공통 정보가 있는 경우의 속도를 정의하겠습니다. 공통 정보가 있는 브로드캐스트 채널의 $\left(\left(2^{n R_{0}}, 2^{n R_{1}}, 2^{n R_{2}}\right), n\right)$ 코드는 인코더,

$$
X:\left(\left\{1,2, \ldots, 2^{n R_{0}}\right\} \times\left\{1,2, \ldots, 2^{n R_{1}}\right\} \times\left\{1,2, \ldots, 2^{n R_{2}}\right\}\right) \rightarrow \mathcal{X}^{n}
$$

그리고 두 개의 디코더,

$$
g_{1}: \mathcal{Y}_{1}^{n} \rightarrow\left\{1,2, \ldots, 2^{n R_{0}}\right\} \times\left\{1,2, \ldots, 2^{n R_{1}}\right\}
$$

및

$$
g_{2}: \mathcal{Y}_{2}^{n} \rightarrow\left\{1,2, \ldots, 2^{n R_{0}}\right\} \times\left\{1,2, \ldots, 2^{n R_{2}}\right\}
$$

로 구성됩니다.
<!-- Page 590 -->
$\left(W_{0}, W_{1}, W_{2}\right)$의 분포가 균일하다고 가정하면, 오류 확률을 복호화된 메시지가 전송된 메시지와 같지 않을 확률로 정의할 수 있습니다.

$$
P_{e}^{(n)}=P\left(g_{1}\left(Y_{1}^{n}\right) \neq\left(W_{0}, W_{1}\right) \text { or } g_{2}\left(Z^{n}\right) \neq\left(W_{0}, W_{2}\right)\right)
$$

정의 공통 정보를 갖는 방송 채널에 대해 달성 가능한 속도 트리플 $\left(R_{0}, R_{1}, R_{2}\right)$는 $P_{e}^{(n)} \rightarrow 0$인 $\left(\left(2^{n R_{0}}, 2^{n R_{1}}, 2^{n R_{2}}\right), n\right)$ 코드 시퀀스가 존재한다고 말합니다.

정의 방송 채널의 용량 영역은 달성 가능한 속도 집합의 폐포입니다.

수신자 $Y_{1}^{n}$에 대한 오류는 결합 분포 $p\left(x^{n}, y_{1}^{n}, y_{2}^{n}\right)$가 아닌 분포 $p\left(x^{n}, y_{1}^{n}\right)$에만 의존한다는 것을 관찰합니다. 따라서 다음과 같은 정리가 있습니다.

정리 15.6.1 방송 채널의 용량 영역은 조건부 주변 분포 $p\left(y_{1} \mid x\right)$와 $p\left(y_{2} \mid x\right)$에만 의존합니다.

증명: 문제들을 참조하십시오.

# 15.6.2 저하된 방송 채널

정의 방송 채널이 물리적으로 저하되었다고 말하는 것은 $p\left(y_{1}, y_{2} \mid x\right)=p\left(y_{1} \mid x\right) p\left(y_{2} \mid y_{1}\right)$인 경우입니다.

정의 방송 채널이 확률적으로 저하되었다고 말하는 것은 물리적으로 저하된 방송 채널의 조건부 주변 분포와 동일하다는 것을 의미합니다. 즉, 분포 $p^{\prime}\left(y_{2} \mid y_{1}\right)$가 존재하여

$$
p\left(y_{2} \mid x\right)=\sum_{y_{1}} p\left(y_{1} \mid x\right) p^{\prime}\left(y_{2} \mid y_{1}\right)
$$

방송 채널의 용량은 조건부 주변 분포에만 의존하므로, 확률적으로 저하된 방송 채널의 용량 영역은 해당 물리적으로 저하된 채널의 용량 영역과 동일하다는 점에 유의하십시오. 따라서 이후의 많은 내용에서 채널이 물리적으로 저하되었다고 가정합니다.
<!-- Page 591 -->
# 15.6.3 용량 영역: 열화된 방송 채널

이제 $Y_{1}$에 대한 속도 $R_{1}$과 $Y_{2}$에 대한 속도 $R_{2}$로 열화된 방송 채널을 통해 독립적인 정보를 전송하는 것을 고려합니다.

정리 15.6.2 열화된 방송 채널 $X \rightarrow Y_{1} \rightarrow Y_{2}$을 통해 독립적인 정보를 전송하기 위한 용량 영역은 다음을 만족하는 모든 $\left(R_{1}, R_{2}\right)$의 폐포의 볼록 껍질입니다.

$$
\begin{aligned}
& R_{2} \leq I\left(U ; Y_{2}\right) \\
& R_{1} \leq I\left(X ; Y_{1} \mid U\right)
\end{aligned}
$$

보조 확률 변수 $U$가 $|\mathcal{U}| \leq \min \left\{|\mathcal{X}|,\left|\mathcal{Y}_{1}\right|,\left|\mathcal{Y}_{2}\right|\right\}$의 기수를 갖는 임의의 결합 분포 $p(u) p(x \mid u) p\left(y_{1}, y_{2} \mid x\right)$에 대해.

증명: (보조 확률 변수 $U$의 기수 제약은 볼록 집합 이론의 표준적인 방법으로 유도되며 여기서는 다루지 않습니다.) 먼저 방송 채널에 대한 중첩 코딩의 기본 아이디어 개요를 제공합니다. 보조 확률 변수 $U$는 두 수신기 $Y_{1}$과 $Y_{2}$ 모두 구별할 수 있는 클라우드 중심 역할을 합니다. 각 클라우드는 수신기 $Y_{1}$이 구별할 수 있는 $2^{n R_{1}}$개의 코드워드로 구성됩니다. 최악의 수신기는 클라우드만 볼 수 있고, 더 나은 수신기는 클라우드 내의 개별 코드워드를 볼 수 있습니다. 이 영역의 달성 가능성에 대한 형식적인 증명은 랜덤 코딩 논증을 사용합니다: $p(u)$와 $p(x \mid u)$를 고정합니다.

랜덤 코드북 생성: $\prod_{i=1}^{n} p\left(u_{i}\right)$에 따라 $2^{n R_{2}}$개의 독립적인 길이 $n$ 코드워드 $\mathbf{U}\left(w_{2}\right), w_{2} \in\left\{1,2, \ldots, 2^{n R_{2}}\right\}$를 생성합니다. 각 코드워드 $\mathbf{U}\left(w_{2}\right)$에 대해 $\prod_{i=1}^{n} p\left(x_{i} \mid u_{i}\left(w_{2}\right)\right)$에 따라 $2^{n R_{1}}$개의 독립적인 코드워드 $\mathbf{X}\left(w_{1}, w_{2}\right)$를 생성합니다. 여기서 $\mathbf{u}(i)$는 두 수신기 $Y_{1}$과 $Y_{2}$ 모두 이해할 수 있는 클라우드 중심 역할을 하고, $\mathbf{x}(i, j)$는 $i$번째 클라우드의 $j$번째 위성 코드워드입니다.

인코딩: 쌍 $\left(W_{1}, W_{2}\right)$를 전송하기 위해 해당 코드워드 $\mathbf{X}\left(W_{1}, W_{2}\right)$를 전송합니다.

디코딩: 수신기 2는 $\left(\mathbf{U}\left(\stackrel{*}{W}_{2}\right)\right.$, $\mathbf{Y}_{2}$ ) $\in A_{\epsilon}^{(n)}$인 고유한 $\stackrel{*}{W}_{2}$를 결정합니다. 이러한 것이 없거나 하나보다 많으면 오류가 선언됩니다.

수신기 1은 $\left(\mathbf{U}\left(\hat{W}_{2}\right), \mathbf{X}\left(\hat{W}_{1}, \hat{W}_{2}\right)\right.$, $\mathbf{Y}_{1}$ ) $\in A_{\epsilon}^{(n)}$인 고유한 $\left(\hat{W}_{1}, \hat{W}_{2}\right)$를 찾습니다. 이러한 것이 없거나 하나보다 많으면 오류가 선언됩니다.

오류 확률 분석: 코드 생성의 대칭성으로 인해 오류 확률은 어떤 코드워드가 전송되었는지에 따라 달라지지 않습니다.
<!-- Page 592 -->
전송되었다고 가정할 수 있습니다. $P(\cdot)$는 $(1,1)$이 전송되었다는 조건 하에 어떤 사건의 확률을 나타냅니다.

$U$에서 $Y_{2}$로 가는 단일 사용자 채널이므로, $R_{2}<I\left(U ; Y_{2}\right)$이면 낮은 오류 확률로 $U$ 코드를 복호화할 수 있습니다. 이를 증명하기 위해 다음과 같은 사건들을 정의합니다.

$$
E_{Y i}=\left\{\left(\mathbf{U}(i), \mathbf{Y}_{2}\right) \in A_{\epsilon}^{(n)}\right\}
$$

그러면 수신기 2에서의 오류 확률은 다음과 같습니다.

$$
\begin{aligned}
P_{e}^{(n)}(2) & =P\left(E_{Y 1}^{c} \bigcup \bigcup_{i \neq 1} E_{Y i}\right) \\
& \leq P\left(E_{Y 1}^{c}\right)+\sum_{i \neq 1} P\left(E_{Y i}\right) \\
& \leq \epsilon+2^{n R_{2}} 2^{-n\left(I\left(U ; Y_{2}\right)-2 \epsilon\right)} \\
& \leq 2 \epsilon
\end{aligned}
$$

$n$이 충분히 크고 $R_{2}<I\left(U ; Y_{2}\right)$인 경우, 여기서 (15.215)는 AEP로부터 나옵니다. 마찬가지로, 수신기 1의 복호화를 위해 다음과 같은 사건들을 정의합니다.

$$
\begin{aligned}
\tilde{E}_{Y i} & =\left\{\left(\mathbf{U}(i), \mathbf{Y}_{1}\right) \in A_{\epsilon}^{(n)}\right\} \\
\tilde{E}_{Y i j} & =\left\{\left(\mathbf{U}(i), \mathbf{X}(i, j), \mathbf{Y}_{1}\right) \in A_{\epsilon}^{(n)}\right\}
\end{aligned}
$$

여기서 물결표(tilde)는 수신기 1에서 정의된 사건들을 나타냅니다. 그러면 오류 확률을 다음과 같이 제한할 수 있습니다.

$$
\begin{aligned}
P_{e}^{(n)}(1) & =P\left(\tilde{E}_{Y 1}^{c} \bigcup \tilde{E}_{Y 11}^{c} \bigcup \bigcup_{i \neq 1} \tilde{E}_{Y i} \bigcup \bigcup_{j \neq 1} \tilde{E}_{Y 1 j}\right) \\
& \leq P\left(\tilde{E}_{Y 1}^{c}\right)+P\left(\tilde{E}_{Y 11}^{c}\right)+\sum_{i \neq 1} P\left(\tilde{E}_{Y i}\right)+\sum_{j \neq 1} P\left(\tilde{E}_{Y 1 j}\right)
\end{aligned}
$$

수신기 2에 대한 동일한 논증을 통해 $P\left(\tilde{E}_{Y i}\right) \leq 2^{-n\left(I\left(U ; Y_{1}\right)-3 \epsilon\right)}$로 제한할 수 있습니다. 따라서 세 번째 항은 $R_{2}<I\left(U ; Y_{1}\right)$이면 0으로 수렴합니다. 그러나 데이터 처리 부등식과 채널의 열화된 특성에 의해 $I\left(U ; Y_{1}\right) \geq I\left(U ; Y_{2}\right)$이므로, 정리의 조건은 다음을 의미합니다.
<!-- Page 593 -->
세 번째 항이 0으로 간다는 것을 알 수 있습니다. 오류 확률에 대한 네 번째 항 또한 다음과 같이 제한할 수 있습니다.

$$
\begin{aligned}
P\left(\tilde{E}_{Y 1 j}\right) & =P\left(\left(\mathbf{U}(1), \mathbf{X}(1, j), \mathbf{Y}_{1}\right) \in A_{\epsilon}^{(n)}\right) \\
& =\sum_{\left(\mathbf{U}, \mathbf{X}, \mathbf{Y}_{1}\right) \in A_{\epsilon}^{(n)}} P\left(\left(\mathbf{U}(1), \mathbf{X}(1, j), \mathbf{Y}_{1}\right)\right) \\
& =\sum_{\left(\mathbf{U}, \mathbf{X}, \mathbf{Y}_{1}\right) \in A_{\epsilon}^{(n)}} P(\mathbf{U}(1)) P(\mathbf{X}(1, j) \mid \mathbf{U}(1)) P\left(\mathbf{Y}_{1} \mid \mathbf{U}(1)\right) \\
& \leq \sum_{\left(\mathbf{U}, \mathbf{X}, \mathbf{Y}_{1}\right) \in A_{\epsilon}^{(n)}} 2^{-n(H(U)-\epsilon)} 2^{-n(H(X \mid U)-\epsilon)} 2^{-n\left(H\left(Y_{1} \mid U\right)-\epsilon\right)} \\
& \leq 2^{n\left(H\left(U, X, Y_{1}\right)+\epsilon\right)} 2^{-n(H(U)-\epsilon)} 2^{-n(H(X \mid U)-\epsilon)} 2^{-n\left(H\left(Y_{1} \mid U\right)-\epsilon\right)} \\
& =2^{-n\left(I\left(X ; Y_{1} \mid U\right)-4 \epsilon\right)}
\end{aligned}
$$

따라서 $R_{1}<I\left(X ; Y_{1} \mid U\right)$이면 오류 확률의 네 번째 항은 0으로 갑니다. 그러므로 오류 확률을 다음과 같이 제한할 수 있습니다.

$$
\begin{aligned}
P_{e}^{(n)}(1) & \leq \epsilon+\epsilon+2^{n R_{2}} 2^{-n\left(I\left(U ; Y_{1}\right)-3 \epsilon\right)}+2^{n R_{1}} 2^{-n\left(I\left(X ; Y_{1} \mid U\right)-4 \epsilon\right)} \\
& \leq 4 \epsilon
\end{aligned}
$$

$n$이 충분히 크고 $R_{2}<I\left(U ; Y_{1}\right)$ 및 $R_{1}<I\left(X ; Y_{1} \mid U\right)$인 경우입니다. 위의 제한은 총 오류 확률이 0으로 가는 메시지를 디코딩할 수 있음을 보여줍니다. 따라서 오류 확률이 0으로 가는 좋은 $\left(\left(2^{n R_{1}}\right.\right.$, $\left.2^{n R_{2}}\right), n$) 코드 $\mathcal{C}_{n}^{*}$의 시퀀스가 존재합니다. 이를 통해 퇴화된 브로드캐스트 채널의 용량 영역 달성 가능성에 대한 증명을 완료합니다. 역증명에 대한 Gallager의 증명[225]은 문제 15.11에 개략적으로 설명되어 있습니다.

지금까지 각 수신자에게 독립적인 정보를 보내는 것을 고려했습니다. 하지만 특정 상황에서는 두 수신자 모두에게 공통 정보를 보내고자 할 수 있습니다. 공통 정보를 보내는 속도를 $R_{0}$이라고 하면 다음과 같은 명백한 정리가 있습니다.

정리 15.6.3 독립 정보가 있는 브로드캐스트 채널에 대해 달성 가능한 속도 쌍 $\left(R_{1}, R_{2}\right)$는 공통 속도 $R_{0}$에 대해 속도 삼중항 $\left(R_{0}, R_{1}-R_{0}, R_{2}-\right.$ $\left.R_{0}\right)$이 달성 가능합니다. 단, $R_{0} \leq$ $\min \left(R_{1}, R_{2}\right)$입니다.
<!-- Page 594 -->
방송 채널이 열화된 경우, 우리는 더 나은 결과를 얻을 수 있습니다. 우리의 코딩 방식에 따라 더 나은 수신자는 항상 최악의 수신자에게 전송된 모든 정보를 디코딩하므로, 공통 정보가 있을 때 더 나은 수신자에게 전송되는 정보의 양을 줄일 필요가 없습니다. 따라서 다음과 같은 정리가 있습니다.

정리 15.6.4 만약 비율 쌍 $\left(R_{1}, R_{2}\right)$이 열화된 방송 채널에 대해 달성 가능하다면, 공통 정보를 가진 채널에 대해 비율 삼중항 $\left(R_{0}, R_{1}, R_{2}-R_{0}\right)$이 달성 가능합니다. 단, $R_{0}<R_{2}$라고 가정합니다.

이 섹션을 이진 대칭 방송 채널의 예시를 고려하며 마무리합니다.

예시 15.6.5 그림 15.27에 표시된 방송 채널을 형성하는 매개변수 $p_{1}$ 및 $p_{2}$를 가진 이진 대칭 채널 쌍을 고려합니다. 용량 계산에서 일반성을 잃지 않고, 이 채널을 물리적으로 열화된 채널로 재구성할 수 있습니다. $p_{1}<p_{2}<\frac{1}{2}$라고 가정합니다. 그러면 매개변수 $p_{2}$를 가진 이진 대칭 채널을 매개변수 $p_{1}$을 가진 이진 대칭 채널과 다른 이진 대칭 채널의 연쇄로 표현할 수 있습니다. 새로운 채널의 교차 확률을 $\alpha$라고 하면, 다음을 만족해야 합니다.

$$
p_{1}(1-\alpha)+\left(1-p_{1}\right) \alpha=p_{2}
$$

그림 15.27. 이진 대칭 방송 채널.
<!-- Page 595 -->

그림 15.28. 물리적으로 열화된 이진 대칭 방송 채널.
또는

$$
\alpha=\frac{p_{2}-p_{1}}{1-2 p_{1}}
$$

이제 용량 영역의 정의에서 보조 확률 변수를 고려합니다. 이 경우, $U$의 기수성은 정리의 하한으로부터 이진입니다. 대칭에 의해, 그림 15.28에 설명된 대로 매개변수 $\beta$를 갖는 또 다른 이진 대칭 채널을 통해 $U$를 $X$에 연결합니다.

이제 용량 영역의 비율을 계산할 수 있습니다. 비율을 최대화하는 $U$의 분포는 $\{0,1\}$에 대한 균등 분포라는 것이 대칭에 의해 명확하므로,

$$
\begin{aligned}
I\left(U ; Y_{2}\right) & =H\left(Y_{2}\right)-H\left(Y_{2} \mid U\right) \\
& =1-H\left(\beta * p_{2}\right)
\end{aligned}
$$

여기서

$$
\beta * p_{2}=\beta\left(1-p_{2}\right)+(1-\beta) p_{2}
$$

마찬가지로,

$$
\begin{aligned}
I\left(X ; Y_{1} \mid U\right) & =H\left(Y_{1} \mid U\right)-H\left(Y_{1} \mid X, U\right) \\
& =H\left(Y_{1} \mid U\right)-H\left(Y_{1} \mid X\right) \\
& =H\left(\beta * p_{1}\right)-H\left(p_{1}\right)
\end{aligned}
$$

여기서

$$
\beta * p_{1}=\beta\left(1-p_{1}\right)+(1-\beta) p_{1}
$$

이 점들을 $\beta$의 함수로 플로팅하면 그림 15.29에서 용량 영역을 얻습니다. $\beta=0$일 때, $Y_{2}$로의 최대 정보 전송이 있습니다 [즉, $R_{2}=1-H\left(p_{2}\right)$이고 $R_{1}=0$]. $\beta=\frac{1}{2}$일 때, $Y_{1}$로의 최대 정보 전송 [즉, $R_{1}=1-H\left(p_{1}\right)$]이 있고 $Y_{2}$로의 정보 전송은 없습니다. 이러한 $\beta$ 값은 비율 영역의 꼭짓점을 제공합니다.
<!-- Page 596 -->

그림 15.29. 이진 대칭 방송 채널의 용량 영역.

그림 15.30. 가우시안 방송 채널.

예제 15.6.6 (가우시안 방송 채널) 가우시안 방송 채널은 그림 15.30에 나와 있습니다. 우리는 한 출력이 다른 출력의 열화된 버전인 경우를 보여주었습니다. 문제 15.10의 결과에 따르면, 모든 스칼라 가우시안 방송 채널은 이 유형의 열화된 채널과 동등하다는 것을 알 수 있습니다.

$$
\begin{aligned}
& Y_{1}=X+Z_{1} \\
& Y_{2}=X+Z_{2}=Y_{1}+Z_{2}^{\prime}
\end{aligned}
$$

여기서 $Z_{1} \sim \mathcal{N}\left(0, N_{1}\right)$이고 $Z_{2}^{\prime} \sim \mathcal{N}\left(0, N_{2}-N_{1}\right)$입니다.
이 섹션의 결과를 가우시안 경우로 확장하면, 이 채널의 용량 영역은 다음과 같이 주어짐을 보일 수 있습니다.

$$
\begin{aligned}
& R_{1}<C\left(\frac{\alpha P}{N_{1}}\right) \\
& R_{2}<C\left(\frac{(1-\alpha) P}{\alpha P+N_{2}}\right)
\end{aligned}
$$
<!-- Page 597 -->
여기서 $\alpha$는 임의로 선택될 수 있습니다 $(0 \leq \alpha \leq 1)$. 이 용량 영역을 달성하는 코딩 방식은 15.1.3절에 요약되어 있습니다.

# 15.7 릴레이 채널

릴레이 채널은 하나의 송신자와 하나의 수신자가 있으며, 송신자에서 수신자로의 통신을 돕는 여러 개의 중간 노드가 릴레이 역할을 하는 채널입니다. 가장 간단한 릴레이 채널은 단 하나의 중간 또는 릴레이 노드를 가집니다. 이 경우 채널은 네 개의 유한 집합 $\mathcal{X}, \mathcal{X}_{1}, \mathcal{Y}$, 및 $\mathcal{Y}_{1}$와, 각 $\left(x, x_{1}\right) \in \mathcal{X} \times \mathcal{X}_{1}$에 대해 $\mathcal{Y} \times \mathcal{Y}_{1}$ 상의 확률 질량 함수들의 모음 $p\left(y, y_{1} \mid x, x_{1}\right)$으로 구성됩니다. 여기서 $x$는 채널의 입력이고 $y$는 채널의 출력이며, $y_{1}$은 릴레이의 관측값이고, $x_{1}$은 릴레이가 선택한 입력 심볼임을 의미하며, 이는 그림 15.31에 나타나 있습니다. 문제는 송신자 $X$와 수신자 $Y$ 사이의 채널 용량을 찾는 것입니다.

릴레이 채널은 방송 채널 ($X$에서 $Y$와 $Y_{1}$로)과 다중 접속 채널 ($X$와 $X_{1}$에서 $Y$로)을 결합합니다. 물리적으로 열화된 릴레이 채널의 특수한 경우에 대한 용량은 알려져 있습니다. 먼저 일반적인 릴레이 채널의 용량에 대한 외부 경계를 증명하고, 나중에 열화된 릴레이 채널에 대한 달성 가능한 영역을 확립할 것입니다.

정의 릴레이 채널에 대한 $\left(2^{n R}, n\right)$ 코드는 정수 집합 $\mathcal{W}=\left\{1,2, \ldots, 2^{n R}\right\}$, 인코딩 함수

$$
X:\left\{1,2, \ldots, 2^{n R}\right\} \rightarrow \mathcal{X}^{n}
$$

릴레이 함수 집합 $\left\{f_{i}\right\}_{i=1}^{n}$로서

$$
x_{1 i}=f_{i}\left(Y_{11}, Y_{12}, \ldots, Y_{1 i-1}\right), \quad 1 \leq i \leq n
$$

및 디코딩 함수

$$
g: \mathcal{Y}^{n} \rightarrow\left\{1,2, \ldots, 2^{n R}\right\}
$$

그림 15.31. 릴레이 채널.
<!-- Page 598 -->
인코딩 함수의 정의에는 릴레이에 대한 비예측 조건이 포함된다는 점에 유의하십시오. 릴레이 채널 입력은 과거 관찰값 $y_{11}, y_{12}, \ldots, y_{1 i-1}$에만 의존하도록 허용됩니다. 채널은 $\left(Y_{i}, Y_{1 i}\right)$가 현재 전송된 심볼 $\left(X_{i}, X_{1 i}\right)$를 통해서만 과거에 의존한다는 의미에서 메모리리스입니다. 따라서 임의의 선택 $p(w)$, $w \in \mathcal{W}$, 코드 선택 $X:\left\{1,2, \ldots, 2^{n R}\right\} \rightarrow \mathcal{X}_{i}^{n}$ 및 릴레이 함수 $\left\{f_{i}\right\}_{i=1}^{n}$에 대해 $\mathcal{W} \times \mathcal{X}^{n} \times \mathcal{X}_{1}^{n} \times \mathcal{Y}^{n} \times \mathcal{Y}_{1}^{n}$ 상의 결합 확률 질량 함수는 다음과 같이 주어집니다.

$$
\begin{aligned}
p\left(w, \mathbf{x}, \mathbf{x}_{1}, \mathbf{y}, \mathbf{y}_{1}\right)= & p(w) \prod_{i=1}^{n} p\left(x_{i} \mid w\right) p\left(x_{1 i} \mid y_{11}, y_{12}, \ldots, y_{1 i-1}\right) \\
& \times p\left(y_{i}, y_{1 i} \mid x_{i}, x_{1 i}\right)
\end{aligned}
$$

메시지 $w \in\left[1,2^{n R}\right]$가 전송되면,

$$
\lambda(w)=\operatorname{Pr}\{g(\mathbf{Y}) \neq w \mid w \text { sent }\}
$$

를 오류의 조건부 확률로 나타냅니다. 코드의 평균 오류 확률을 다음과 같이 정의합니다.

$$
P_{e}^{(n)}=\frac{1}{2^{n R}} \sum_{w} \lambda(w)
$$

오류 확률은 코드워드 $w \in\left\{1, \ldots, 2^{n R}\right\}$에 대한 균등 분포 하에서 계산됩니다. 속도 $R$은 $P_{e}^{(n)} \rightarrow 0$인 $\left(2^{n R}, n\right)$ 코드의 시퀀스가 존재하면 릴레이 채널에 의해 달성 가능하다고 말합니다. 릴레이 채널의 용량 $C$는 달성 가능한 속도 집합의 상한입니다.

먼저 릴레이 채널의 용량에 대한 상한을 제시합니다.
정리 15.7.1 임의의 릴레이 채널 $\left(\mathcal{X} \times \mathcal{X}_{1}, p\left(y, y_{1} \mid x, x_{1}\right), \mathcal{Y} \times\right.$ $\mathcal{Y}_{1}$ )에 대해, 용량 $C$는 다음과 같이 상한이 정해집니다.

$$
C \leq \sup _{p\left(x, x_{1}\right)} \min \left\{I\left(X, X_{1} ; Y\right), I\left(X ; Y, Y_{1} \mid X_{1}\right)\right\}
$$

증명: 증명은 15.10절에 주어진 더 일반적인 최대 흐름 최소 컷 정리의 직접적인 결과입니다.

이 상한은 좋은 최대 흐름 최소 컷 해석을 가지고 있습니다. (15.248)의 첫 번째 항은 정보 전송의 최대 속도를 상한으로 합니다.
<!-- Page 599 -->
송신자 $X$와 $X_{1}$에서 수신자 $Y$로 가는 두 번째 항은 $X$에서 $Y$와 $Y_{1}$로 가는 속도를 제한합니다.

이제 릴레이 수신자가 아래 정의된 의미에서 최종 수신자 $Y$보다 우수한 릴레이 채널족을 고려합니다. 여기서 (15.248)의 최대-흐름 최소-컷 상한이 달성됩니다.

정의 릴레이 채널 $\left(\mathcal{X} \times \mathcal{X}_{1}, p\left(y, y_{1} \mid x, x_{1}\right), \mathcal{Y} \times \mathcal{Y}_{1}\right)$은 $p\left(y, y_{1} \mid x, x_{1}\right)$이 다음과 같은 형태로 작성될 수 있을 때 물리적으로 열화되었다고 말합니다.

$$
p\left(y, y_{1} \mid x, x_{1}\right)=p\left(y_{1} \mid x, x_{1}\right) p\left(y \mid y_{1}, x_{1}\right)
$$

따라서 $Y$는 릴레이 신호 $Y_{1}$의 무작위 열화입니다.
물리적으로 열화된 릴레이 채널의 경우, 용량은 다음 정리에 의해 주어집니다.

정리 15.7.2 물리적으로 열화된 릴레이 채널의 용량 $C$는 다음과 같이 주어집니다.

$$
C=\sup _{p\left(x, x_{1}\right)} \min \left\{I\left(X, X_{1} ; Y\right), I\left(X ; Y_{1} \mid X_{1}\right)\right\}
$$

여기서 supremum은 $\mathcal{X} \times \mathcal{X}_{1}$에 대한 모든 결합 분포에 대해 취해집니다.

# 증명:

반증: 증명은 정리 15.7.1과 열화성에 따라 달라지며, 열화된 릴레이 채널의 경우 $I\left(X ; Y, Y_{1} \mid X_{1}\right)=I\left(X ; Y_{1} \mid X_{1}\right)$입니다.

달성 가능성: 달성 가능성 증명은 다음 기본 기법들의 조합을 포함합니다: (1) 무작위 코딩, (2) 리스트 코딩, (3) Slepian-Wolf 분할, (4) 협력적 다중 접속 채널 코딩, (5) 중첩 코딩, (6) 릴레이 및 송신에서의 블록 마르코프 인코딩. 증명의 개요만 제공합니다.

달성 가능성 개요: $n$개의 심볼로 구성된 $B$개의 전송 블록을 고려합니다. $B-1$개의 인덱스 시퀀스, $w_{i} \in\left\{1, \ldots, 2^{n R}\right\}, i=1,2, \ldots$, $B-1$은 $n B$번의 전송을 통해 채널을 통해 전송될 것입니다. (고정된 $n$에 대해 $B \rightarrow \infty$일 때, 속도 $R(B-1) / B$는 $R$에 임의로 가까워진다는 점에 유의하십시오.)

이중 인덱스된 코드워드 세트를 정의합니다:

$$
\mathcal{C}=\left\{\mathbf{x}(w \mid s), \mathbf{x}_{1}(s)\right\}: w \in\left\{1,2^{n R}\right\}, s \in\left\{1,2^{n R_{0}}\right\}, \mathbf{x} \in \mathcal{X}^{n}, \mathbf{x}_{1} \in \mathcal{X}_{1}^{n}
$$

또한 분할이 필요합니다.

$$
\mathcal{S}=\left\{S_{1}, S_{2}, \ldots, S_{2^{n R_{0}}}\right\} \text { of } \mathcal{W}=\left\{1,2, \ldots, 2^{n R}\right\}
$$
<!-- Page 600 -->
$2^{n R_{0}}$개의 셀로 분할되며, $S_{i} \cap S_{j}=\phi, i \neq j$, 그리고 $\cup S_{i}=\mathcal{W}$입니다. 이 분할은 Slepian과 Wolf [502]의 방식대로 수신자에게 측면 정보를 전송할 수 있도록 합니다.

랜덤 코드 생성: $p\left(x_{1}\right) p\left(x \mid x_{1}\right)$을 고정합니다.
먼저 $p\left(\mathbf{x}_{1}\right)=\prod_{i=1}^{n} p\left(x_{1 i}\right)$에 따라 $\mathcal{X}_{1}^{n}$에서 $2^{n R_{0}}$개의 i.i.d. $n$-시퀀스를 무작위로 생성합니다. 이들을 $\mathbf{x}_{1}(s), s \in \left\{1,2, \ldots, 2^{n R_{0}}\right\}$로 인덱싱합니다. 각 $\mathbf{x}_{1}(s)$에 대해, $p\left(\mathbf{x} \mid \mathbf{x}_{1}(s)\right)=\prod_{i=1}^{n} p\left(x_{i} \mid x_{1 i}(s)\right)$에 따라 독립적으로 추출된 $2^{n R}$개의 조건부 독립 $n$-시퀀스 $\mathbf{x}(w \mid s), w \in\left\{1,2, \ldots, 2^{n R}\right\}$를 생성합니다. 이것으로 랜덤 코드북 $\mathcal{C}=\left\{\mathbf{x}(w \mid s), \mathbf{x}_{1}(s)\right\}$을 정의합니다. 랜덤 분할 $\mathcal{S}=\left\{S_{1}, S_{2}, \ldots, S_{2^{n R_{0}}}\right\}$은 $\left\{1,2, \ldots, 2^{n R}\right\}$의 부분집합으로 다음과 같이 정의됩니다. 각 정수 $w \in\left\{1,2, \ldots, 2^{n R}\right\}$는 $s=1,2, \ldots, 2^{n R_{0}}$ 인덱스에 대한 균등 분포에 따라 독립적으로 셀 $S_{s}$에 할당됩니다.

인코딩: 보내고자 하는 새로운 인덱스를 블록 $i$에서 $w_{i} \in\left\{1,2, \ldots, 2^{n R}\right\}$라고 하고, $s_{i}$를 $w_{i-1}$에 해당하는 분할(즉, $w_{i-1} \in S_{s_{i}}$)로 정의합니다. 인코더는 $\mathbf{x}\left(w_{i} \mid s_{i}\right)$를 전송합니다. 릴레이는 이전 인덱스 $w_{i-1}$의 추정값 $\hat{\hat{w}}_{i-1}$을 가지고 있습니다. (이는 디코딩 섹션에서 명확해질 것입니다.) $\hat{\hat{w}}_{i-1} \in S_{\hat{s}_{i}}$라고 가정합니다. 릴레이 인코더는 블록 $i$에서 $\mathbf{x}_{1}\left(\hat{\hat{s}}_{i}\right)$를 전송합니다.

디코딩: 블록 $i-1$이 끝날 때 수신자는 $\left(w_{1}, w_{2}, \ldots, w_{i-2}\right)$와 $\left(s_{1}, s_{2}, \ldots, s_{i-1}\right)$를 알고 있으며, 릴레이는 $\left(w_{1}, w_{2}, \ldots, w_{i-1}\right)$와 따라서 $\left(s_{1}, s_{2}, \ldots, s_{i}\right)$를 알고 있다고 가정합니다. 블록 $i$가 끝날 때의 디코딩 절차는 다음과 같습니다.

1. $s_{i}$를 알고 $\mathbf{y}_{1}(i)$를 수신한 후, 릴레이 수신자는 송신자의 메시지 $\hat{\hat{w}}_{i}=w$를 추정합니다. 이는 $\left(\mathbf{x}\left(w \mid s_{i}\right), \mathbf{x}_{1}\left(s_{i}\right), \mathbf{y}_{1}(i)\right)$가 공동으로 $\epsilon$-typical인 유일한 $w$가 존재하는 경우에만 해당합니다. 정리 15.2.3을 사용하면, 만약

$$
R<I\left(X ; Y_{1} \mid X_{1}\right)
$$

이고 $n$이 충분히 크다면, $\hat{\hat{w}}_{i}=w_{i}$를 임의로 작은 오류 확률로 보일 수 있습니다.

2. 수신자는 $\left(\mathbf{x}_{1}(s), \mathbf{y}(i)\right)$가 공동으로 $\epsilon$-typical인 $s$가 하나만 존재하는 경우에만 $\hat{s}_{i}=s$가 전송되었다고 선언합니다. 정리 15.2.1로부터, 만약

$$
R_{0}<I\left(X_{1} ; Y\right)
$$

이고 $n$이 충분히 크다면, $s_{i}$를 임의로 작은 오류 확률로 디코딩할 수 있음을 압니다.
<!-- Page 601 -->
3. 수신단에서 $s_{i}$가 올바르게 복호화되었다고 가정하면, 수신단은 $(i-1)$번째 블록에서 $\mathbf{y}(i-1)$과 공동으로 전형적인 것으로 간주되는 인덱스들의 목록 $\mathrm{E}(\mathbf{y}(i-1))$을 구성합니다. 수신단은 $S_{s_{i}} \cap \mathrm{E}(\mathbf{y}(i-1))$에 유일한 $w$가 존재하면 블록 $i-1$에서 전송된 인덱스로 $\hat{w}_{i-1}=w$를 선언합니다. 만약 $n$이 충분히 크고

$$
R<I\left(X ; Y \mid X_{1}\right)+R_{0}
$$

이면, 오류 확률은 임의로 작게 만들 수 있습니다. 두 제약 조건 (15.254)와 (15.255)를 결합하면, $R_{0}$는 사라지고 다음과 같이 남습니다.

$$
R<I\left(X ; Y \mid X_{1}\right)+I\left(X_{1} ; Y\right)=I\left(X, X_{1} ; Y\right)
$$

오류 확률에 대한 자세한 분석은 Cover와 El Gamal [127]을 참조하십시오.

정리 15.7.2는 다음 종류의 릴레이 채널에 대한 capacity임을 보일 수도 있습니다.

1. 역방향으로 열화된 릴레이 채널, 즉

$$
p\left(y, y_{1} \mid x, x_{1}\right)=p\left(y \mid x, x_{1}\right) p\left(y_{1} \mid y, x_{1}\right)
$$

2. 피드백이 있는 릴레이 채널
3. 결정론적 릴레이 채널,

$$
y_{1}=f\left(x, x_{1}\right), \quad y=g\left(x, x_{1}\right)
$$

# 15.8 측면 정보가 있는 소스 코딩

이제 두 확률 변수 $X$와 $Y$가 별도로 인코딩되지만 $X$만 복구되는 분산 소스 코딩 문제를 고려합니다. 이제 $Y$를 설명하기 위해 $R_{2}$ 비트를 허용할 경우 $X$를 설명하는 데 필요한 비트 수 $R_{1}$을 질문합니다. 만약 $R_{2}>H(Y)$이면, $Y$는 완벽하게 설명될 수 있으며, Slepian-Wolf 코딩 결과에 따라 $R_{1}=H(X \mid Y)$ 비트로 $X$를 설명하기에 충분합니다. 다른 극단적인 경우, 만약 $R_{2}=0$이면, 우리는 어떤 도움도 없이 $X$를 설명해야 하며, $R_{1}=H(X)$ 비트가 $X$를 설명하는 데 필요합니다. 일반적으로, 우리는 근사 버전의 $Y$를 설명하기 위해 $R_{2}=I(Y ; \hat{Y})$ 비트를 사용합니다. 이는 측면 정보 $\hat{Y}$의 존재 하에 $X$를 $H(X \mid \hat{Y})$ 비트를 사용하여 설명할 수 있게 해줄 것입니다. 다음 정리는 이러한 직관과 일치합니다.
<!-- Page 602 -->
정리 15.8.1 $(X, Y) \sim p(x, y)$라고 가정합니다. 만약 $Y$가 $R_{2}$의 비율로 인코딩되고 $X$가 $R_{1}$의 비율로 인코딩된다면, 다음을 만족하는 어떤 결합 확률 질량 함수 $p(x, y) p(u \mid y)$에 대해, $|\mathcal{U}| \leq$ $|\mathcal{Y}|+2$일 때, 우리는 임의로 작은 오류 확률로 $X$를 복구할 수 있습니다.

$$
\begin{aligned}
& R_{1} \geq H(X \mid U) \\
& R_{2} \geq I(Y ; U)
\end{aligned}
$$

이 정리는 두 부분으로 증명합니다. 먼저 역방향 증명을 시작하며, 작은 오류 확률을 갖는 모든 인코딩 방식에 대해 정리와 같은 결합 확률 질량 함수를 갖는 랜덤 변수 $U$를 찾을 수 있음을 보입니다.

증명: (역방향). 그림 15.32에 대한 임의의 소스 코드를 고려합니다. 소스 코드는 $f_{n}\left(X^{n}\right)$와 $g_{n}\left(Y^{n}\right)$의 매핑으로 구성되며, $f_{n}$와 $g_{n}$의 비율은 각각 $R_{1}$과 $R_{2}$보다 작고, 디코딩 매핑 $h_{n}$는 다음을 만족합니다.

$$
P_{e}^{(n)}=\operatorname{Pr}\left\{h_{n}\left(f_{n}\left(X^{n}\right), g_{n}\left(Y^{n}\right)\right) \neq X^{n}\right\}<\epsilon
$$

새로운 랜덤 변수 $S=f_{n}\left(X^{n}\right)$와 $T=g_{n}\left(Y^{n}\right)$를 정의합니다. 그러면 $S$와 $T$로부터 낮은 오류 확률로 $X^{n}$를 복구할 수 있으므로, 파노의 부등식에 의해 다음을 얻습니다.

$$
H\left(X^{n} \mid S, T\right) \leq n \epsilon_{n}
$$

그러면

$$
\begin{aligned}
n R_{2} & \stackrel{(a)}{=} H(T) \\
& \stackrel{(b)}{=} I\left(Y^{n} ; T\right)
\end{aligned}
$$

그림 15.32. 부가 정보가 있는 인코딩.
<!-- Page 603 -->
$$
\begin{aligned}
& =\sum_{i=1}^{n} I\left(Y_{i} ; T \mid Y_{1}, \ldots, Y_{i-1}\right) \\
& \stackrel{(c)}{=} \sum_{i=1}^{n} I\left(Y_{i} ; T, Y_{1}, \ldots, Y_{i-1}\right) \\
& \stackrel{(d)}{=} \sum_{i=1}^{n} I\left(Y_{i} ; U_{i}\right)
\end{aligned}
$$

여기서
(a)는 $g_{n}$의 범위가 $\left\{1,2, \ldots, 2^{n R_{2}}\right\}$라는 사실로부터 도출됩니다.
(b)는 mutual information의 속성으로부터 도출됩니다.
(c)는 연쇄 법칙과 $Y_{i}$가 $Y_{1}, \ldots, Y_{i-1}$와 독립이며 따라서 $I\left(Y_{i} ; Y_{1}, \ldots, Y_{i-1}\right)=0$이라는 사실로부터 도출됩니다.
(d)는 $U_{i}=\left(T, Y_{1}, \ldots, Y_{i-1}\right)$로 정의하면 도출됩니다.

또한 $R_{1}$에 대한 다른 연쇄를 가지고 있습니다.

$$
\begin{aligned}
n R_{1} & \stackrel{(a)}{=} H(S) \\
& \stackrel{(b)}{=} H(S \mid T) \\
& =H(S \mid T)+H\left(X^{n} \mid S, T\right)-H\left(X^{n} \mid S, T\right) \\
& \stackrel{(c)}{\geq} H\left(X^{n}, S \mid T\right)-n \epsilon_{n} \\
& \stackrel{(d)}{=} H\left(X^{n} \mid T\right)-n \epsilon_{n} \\
& \stackrel{(e)}{=} \sum_{i=1}^{n} H\left(X_{i} \mid T, X_{1}, \ldots, X_{i-1}\right)-n \epsilon_{n} \\
& \stackrel{(f)}{=} \sum_{i=1}^{n} H\left(X_{i} \mid T, X^{i-1}, Y^{i-1}\right)-n \epsilon_{n} \\
& \stackrel{(g)}{=} \sum_{i=1}^{n} H\left(X_{i} \mid T, Y^{i-1}\right)-n \epsilon_{n} \\
& \stackrel{(h)}{=} \sum_{i=1}^{n} H\left(X_{i} \mid U_{i}\right)-n \epsilon_{n}
\end{aligned}
$$
<!-- Page 604 -->
(a) $S$의 범위가 $\left\{1,2, \ldots, 2^{n R_{1}}\right\}$이라는 사실로부터 도출됩니다.
(b) 조건화가 엔트로피를 감소시킨다는 사실로부터 도출됩니다.
(c) Fano의 부등식으로부터 도출됩니다.
(d) 연쇄 법칙과 $S$가 $X^{n}$의 함수라는 사실로부터 도출됩니다.
(e) 엔트로피에 대한 연쇄 법칙으로부터 도출됩니다.
(f) 조건화가 엔트로피를 감소시킨다는 사실로부터 도출됩니다.
(g) $X_{i} \rightarrow\left(T, Y^{i-1}\right) \rightarrow X^{i-1}$이 마르코프 연쇄를 형성한다는 (미묘한) 사실로부터 도출됩니다. 왜냐하면 $X_{i}$는 $Y^{i-1}$과 $T$에 있는 정보 외에는 $X^{i-1}$에 대한 정보를 포함하지 않기 때문입니다.
(h) $U$의 정의로부터 도출됩니다.

또한, $X_{i}$가 $Y_{i}$에 존재하는 것보다 더 많은 정보를 $U_{i}$에 대해 포함하지 않으므로, $X_{i} \rightarrow Y_{i} \rightarrow U_{i}$가 마르코프 연쇄를 형성한다고 결론지을 수 있습니다. 따라서 다음과 같은 부등식을 얻습니다.

$$
\begin{aligned}
& R_{1} \geq \frac{1}{n} \sum_{i=1}^{n} H\left(X_{i} \mid U_{i}\right) \\
& R_{2} \geq \frac{1}{n} \sum_{i=1}^{n} I\left(Y_{i} ; U_{i}\right)
\end{aligned}
$$

이제 시분할 랜덤 변수 $Q$를 도입하여 이 방정식들을 다음과 같이 다시 쓸 수 있습니다.

$$
\begin{aligned}
& R_{1} \geq \frac{1}{n} \sum_{i=1}^{n} H\left(X_{i} \mid U_{i}, Q=i\right)=H\left(X_{Q} \mid U_{Q}, Q\right) \\
& R_{2} \geq \frac{1}{n} \sum_{i=1}^{n} I\left(Y_{i} ; U_{i} \mid Q=i\right)=I\left(Y_{Q} ; U_{Q} \mid Q\right)
\end{aligned}
$$

이제 $Q$가 $Y_{Q}$와 독립이므로 ( $Y_{i}$의 분포는 $i$에 의존하지 않습니다), 다음과 같습니다.

$$
I\left(Y_{Q} ; U_{Q} \mid Q\right)=I\left(Y_{Q} ; U_{Q}, Q\right)-I\left(Y_{Q} ; Q\right)=I\left(Y_{Q} ; U_{Q}, Q\right)
$$

이제 $X_{Q}$와 $Y_{Q}$는 정리에서 주어진 결합 분포 $p(x, y)$를 가집니다. $U=\left(U_{Q}, Q\right), X=X_{Q}$, 그리고 $Y=Y_{Q}$를 정의하면, 다음과 같은 랜덤 변수 $U$의 존재를 보여주었습니다.

$$
\begin{aligned}
& R_{1} \geq H(X \mid U) \\
& R_{2} \geq I(Y ; U)
\end{aligned}
$$
<!-- Page 605 -->
오류 확률이 낮은 모든 인코딩 방식에 대해 그렇습니다. 따라서 역도 증명되었습니다.

이러한 비율의 달성 가능성에 대한 증명을 진행하기 전에, 강한 전형성(strong typicality)과 마르코프 연쇄(Markov chain)에 대한 새로운 보조정리가 필요합니다. 세 개의 확률 변수 $X, Y, Z$에 대한 강한 전형성의 정의를 다시 생각해 봅시다. 시퀀스 트리플 $x^{n}, y^{n}, z^{n}$은 다음과 같을 때 $\epsilon$-강한 전형적이라고 합니다.

$$
\left|\frac{1}{n} N\left(a, b, c \mid x^{n}, y^{n}, z^{n}\right)-p(a, b, c)\right|<\frac{\epsilon}{|\mathcal{X}||\mathcal{Y}||\mathcal{Z}|}
$$

특히, 이는 $\left(x^{n}, y^{n}\right)$이 공동으로 강한 전형적이고 $\left(y^{n}, z^{n}\right)$도 공동으로 강한 전형적임을 의미합니다. 그러나 역은 사실이 아닙니다: $\left(x^{n}, y^{n}\right) \in A_{\epsilon}^{*(n)}(X, Y)$이고 $\left(y^{n}, z^{n}\right) \in A_{\epsilon}^{*(n)}(Y, Z)$라는 사실이 일반적으로 $\left(x^{n}, y^{n}, z^{n}\right) \in A_{\epsilon}^{*(n)}(X, Y, Z)$임을 의미하지는 않습니다. 그러나 $X \rightarrow Y \rightarrow Z$가 마르코프 연쇄를 형성하는 경우, 이 함의는 참입니다. 우리는 이를 증명 없이 보조정리로 명시합니다 $[53,149]$.

보조정리 15.8.1 $(X, Y, Z)$가 마르코프 연쇄 $X \rightarrow Y \rightarrow Z$ [즉, $p(x, y, z)=p(x, y) p(z \mid y)]$를 형성한다고 가정합니다. 주어진 $\left(y^{n}, z^{n}\right) \in A_{\epsilon}^{*(n)}(Y, Z)$에 대해 $X^{n}$이 $\sim \prod_{i=1}^{n} p\left(x_{i} \mid y_{i}\right)$로 추출된다면, 충분히 큰 $n$에 대해 $\operatorname{Pr}\left\{\left(X^{n}, y^{n}, z^{n}\right) \in A_{\epsilon}^{*(n)}(X, Y, Z)\right\}>1-\epsilon$입니다.

주석 이 정리는 $X^{n} \sim \prod_{i=1}^{n} p\left(x_{i} \mid y_{i}, z_{i}\right)$인 경우 큰 수의 법칙(strong law of large numbers)으로부터 참입니다. $X \rightarrow Y \rightarrow Z$의 마르코프성은 $X^{n} \sim p\left(x_{i} \mid y_{i}\right)$가 동일한 결론에 대해 충분함을 보여주는 데 사용됩니다.

이제 Theorem 15.8.1의 달성 가능성에 대한 증명을 개략적으로 설명합니다.
증명: (Theorem 15.8.1의 달성 가능성). $p(u \mid y)$를 고정합니다. $p(u) = \sum_{y} p(y) p(u \mid y)$를 계산합니다.

코드북 생성: $\prod_{i=1}^{n} p\left(u_{i}\right)$에 따라 길이 $n$의 독립적인 $2^{n R_{2}}$개의 코드워드, $\mathbf{U}\left(w_{2}\right), w_{2} \in\left\{1,2, \ldots, 2^{n R_{2}}\right\}$를 생성합니다. 각 $X^{n}$에 대해 $\left\{1,2, \ldots, 2^{n R_{1}}\right\}$ 상에서 균등하게 분포된 인덱스 $b$를 독립적으로 생성하여 모든 $X^{n}$ 시퀀스를 $2^{n R_{1}}$개의 빈으로 무작위로 분류합니다. $B(i)$를 빈 $i$에 할당된 $X^{n}$ 시퀀스의 집합이라고 합시다.

인코딩: $X$ 송신자는 $X^{n}$이 속하는 빈의 인덱스 $i$를 보냅니다.
$Y$ 송신자는 $\left(Y^{n}, U^{n}(s)\right) \in A_{\epsilon}^{*(n)}(Y, U)$인 인덱스 $s$를 찾습니다. 이러한 $s$가 여러 개 있으면 가장 작은 것을 보냅니다. 코드북에 이러한 $U^{n}(s)$가 없으면 $s=1$을 보냅니다.

디코딩: 수신자는 $\left(X^{n}, U^{n}(s)\right) \in A_{\epsilon}^{*(n)}(X, U)$인 고유한 $X^{n} \in B(i)$를 찾습니다. 하나도 없거나 여러 개 있으면 오류를 선언합니다.
<!-- Page 606 -->
오류 확률 분석: 다양한 오류 발생원은 다음과 같습니다.

1. 소스에서 생성된 쌍 $\left(X^{n}, Y^{n}\right)$이 일반적이지 않습니다. $n$이 크면 이 확률은 작습니다. 따라서 일반성을 잃지 않고, 소스가 특정 일반 시퀀스 $\left(x^{n}, y^{n}\right) \in A_{\epsilon}^{*(n)}$를 생성하는 사건에 조건화할 수 있습니다.
2. 시퀀스 $Y^{n}$은 일반적이지만, 코드북에 그것과 함께 일반적인 $U^{n}(s)$가 존재하지 않습니다. 10.6절의 논증에 따르면 이 확률은 작습니다. 여기서 우리는 충분한 코드워드가 있다면, 즉

$$
R_{2}>I(Y ; U)
$$

이면 주어진 소스 시퀀스와 함께 일반적인 코드워드를 찾을 가능성이 매우 높다는 것을 보였습니다.
3. 코드워드 $U^{n}(s)$는 $y^{n}$과 함께 일반적이지만 $x^{n}$과는 함께 일반적이지 않습니다. 그러나 보조정리 15.8.1에 따라 $X \rightarrow Y \rightarrow U$가 마르코프 연쇄를 형성하므로 이 확률은 작습니다.
4. 또한, $B(i)$에 속하는 다른 일반적인 $X^{n}$이 $U^{n}(s)$와 함께 일반적이라면 오류가 발생합니다. 다른 $X^{n}$이 $U^{n}(s)$와 함께 일반적일 확률은 $2^{-n(I(U ; X)-3 \epsilon)}$보다 작으므로, 이 종류의 오류 확률은 다음과 같이 상한이 정해집니다.

$$
\left|B(i) \cap A_{\epsilon}^{*(n)}(X)\right| 2^{-n(I(X ; U)-3 \epsilon)} \leq 2^{n(H(X)+\epsilon)} 2^{-n R_{1}} 2^{-n(I(X ; U)-3 \epsilon)}
$$

이는 $R_{1}>H(X \mid U)$이면 0으로 갑니다.
따라서 실제 소스 시퀀스 $X^{n}$이 $U^{n}(s)$와 함께 일반적이고, 같은 빈에 있는 다른 일반적인 소스 시퀀스는 $U^{n}(s)$와 함께 일반적이지 않을 가능성이 높습니다. 적절한 $n$과 $\epsilon$을 선택하면 임의로 낮은 오류 확률을 달성할 수 있으며, 이는 달성 가능성 증명을 완료합니다.

# 15.9 측면 정보가 있는 비율 왜곡

$R(D)$ 비트가 왜곡 $D$ 내에서 $X$를 설명하는 데 충분하다는 것을 알고 있습니다. 이제 측면 정보 $Y$가 주어졌을 때 몇 비트가 필요한지 묻습니다.

몇 가지 정의부터 시작하겠습니다. $\left(X_{i}, Y_{i}\right)$가 i.i.d. $\sim p(x, y)$이고 그림 15.33과 같이 인코딩된다고 가정합니다.
<!-- Page 607 -->

그림 15.33. 부가 정보가 있는 속도 왜곡.

정의 부가 정보 $Y$가 디코더에게 제공될 때 왜곡 $D$를 달성하는 데 필요한 최소 속도로 정의되는 부가 정보가 있는 속도 왜곡 함수 $R_{Y}(D)$는 다음과 같습니다. 정확히 말하면, $R_{Y}(D)$는 다음과 같은 지도 $i_{n}: \mathcal{X}^{n} \rightarrow\left\{1, \ldots, 2^{n R}\right\}$, $g_{n}: \mathcal{Y}^{n} \times\left\{1, \ldots, 2^{n R}\right\} \rightarrow \hat{\mathcal{X}}^{n}$가 존재하는 속도 $R$의 최소값입니다.

$$
\limsup _{n \rightarrow \infty} E d\left(X^{n}, g_{n}\left(Y^{n}, i_{n}\left(X^{n}\right)\right)\right) \leq D
$$

분명히, 부가 정보는 도움이 될 수만 있으므로, 우리는 $R_{Y}(D) \leq R(D)$를 가집니다. 왜곡이 0인 경우, 이는 Slepian-Wolf 문제이며 $H(X \mid Y)$ 비트가 필요합니다. 따라서 $R_{Y}(0)=H(X \mid Y)$입니다. 우리는 전체 곡선 $R_{Y}(D)$를 결정하고자 합니다. 결과는 다음 정리로 표현될 수 있습니다.

정리 15.9.1 (부가 정보가 있는 속도 왜곡 (Wyner와 Ziv)) $(X, Y)$가 i.i.d. $\sim p(x, y)$로 추출되고 $d\left(x^{n}, \hat{x}^{n}\right)$ $=\frac{1}{n} \sum_{i=1}^{n} d\left(x_{i}, \hat{x}_{i}\right)$가 주어진다고 가정합니다. 부가 정보가 있는 속도 왜곡 함수는 다음과 같습니다.

$$
R_{Y}(D)=\min _{p(w \mid x)} \min _{f}(I(X ; W)-I(Y ; W))
$$

여기서 최소화는 모든 함수 $f: \mathcal{Y} \times \mathcal{W} \rightarrow \hat{\mathcal{X}}$와 조건부 확률 질량 함수 $p(w \mid x),|\mathcal{W}| \leq|\mathcal{X}|+1$에 대해 수행되며, 다음을 만족합니다.

$$
\sum_{x} \sum_{w} \sum_{y} p(x, y) p(w \mid x) d(x, f(y, w)) \leq D
$$

정리의 함수 $f$는 $X$ 심볼의 인코딩된 버전과 부가 정보 $Y$를 출력 알파벳으로 매핑하는 디코딩 맵에 해당합니다. 우리는 $W$에 대한 모든 조건부 분포와 함수 $f$에 대해 최소화하여 결합 분포에 대한 기대 왜곡이 $D$보다 작도록 합니다.

먼저 (15.288)에 정의된 함수 $R_{Y}(D)$의 몇 가지 속성을 고려한 후 역방향 증명을 수행합니다.
<!-- Page 608 -->
정리 15.9.1. 부가 정보 $R_{Y}(D)$를 갖는 속도 왜곡 함수는 $D$의 비증가 볼록 함수입니다.

증명: $R_{Y}(D)$의 단조성은 $R_{Y}(D)$의 정의에서 최소화 영역이 $D$와 함께 증가한다는 사실로부터 즉시 도출됩니다. 부가 정보가 없는 속도 왜곡의 경우와 마찬가지로 $R_{Y}(D)$가 볼록할 것으로 예상됩니다. 그러나 볼록성의 증명은 (15.288)에 있는 $R_{Y}(D)$의 정의에서 단일 최소화가 아닌 이중 최소화 때문에 더 복잡합니다. 여기서는 증명을 개략적으로 설명합니다.

두 왜곡 값으로 $D_{1}$과 $D_{2}$를 선택하고, 각각 $R_{Y}\left(D_{1}\right)$ 및 $R_{Y}\left(D_{2}\right)$의 정의에서 최소값을 달성하는 해당 확률 변수 및 함수로 $W_{1}, f_{1}$ 및 $W_{2}, f_{2}$를 선택합니다. $\lambda$의 확률로 1을, $1-\lambda$의 확률로 2를 취하는 $X, Y, W_{1}, W_{2}$와 독립적인 확률 변수 $Q$를 정의합니다.

$W=\left(Q, W_{Q}\right)$를 정의하고 $f(W, Y)=f_{Q}\left(W_{Q}, Y\right)$로 둡니다. 구체적으로, $f(W, Y)=f_{1}\left(W_{1}, Y\right)$는 $\lambda$의 확률로, $f(W, Y)=f_{2}\left(W_{2}, Y\right)$는 $1-\lambda$의 확률로 나타납니다. 그러면 왜곡은 다음과 같이 됩니다.

$$
\begin{aligned}
D & =E d(X, \hat{X}) \\
& =\lambda E d\left(X, f_{1}\left(W_{1}, Y\right)\right)+(1-\lambda) E d\left(X, f_{2}\left(W_{2}, Y\right)\right) \\
& =\lambda D_{1}+(1-\lambda) D_{2}
\end{aligned}
$$

그리고 (15.288)은 다음과 같이 됩니다.

$$
\begin{aligned}
I(W ; X)-I(W ; Y)= & H(X)-H(X \mid W)-H(Y)+H(Y \mid W) \\
= & H(X)-H\left(X \mid W_{Q}, Q\right)-H(Y)+H\left(Y \mid W_{Q}, Q\right) \\
= & H(X)-\lambda H\left(X \mid W_{1}\right)-(1-\lambda) H\left(X \mid W_{2}\right) \\
& -H(Y)+\lambda H\left(Y \mid W_{1}\right)+(1-\lambda) H\left(Y \mid W_{2}\right) \\
= & \lambda\left(I\left(W_{1}, X\right)-I\left(W_{1} ; Y\right)\right) \\
& +(1-\lambda)\left(I\left(W_{2}, X\right)-I\left(W_{2} ; Y\right)\right)
\end{aligned}
$$

따라서

$$
\begin{aligned}
R_{Y}(D) & =\min _{U: E d \leq D}(I(U ; X)-I(U ; Y)) \\
& \leq I(W ; X)-I(W ; Y)
\end{aligned}
$$
<!-- Page 609 -->
$$
\begin{aligned}
& =\lambda\left(I\left(W_{1}, X\right)-I\left(W_{1} ; Y\right)\right)+(1-\lambda)\left(I\left(W_{2}, X\right)-I\left(W_{2} ; Y\right)\right) \\
& =\lambda R_{Y}\left(D_{1}\right)+(1-\lambda) R_{Y}\left(D_{2}\right)
\end{aligned}
$$

$R_{Y}(D)$의 볼록성을 증명합니다.
이제 조건부율-왜곡 이론의 역정리를 증명할 준비가 되었습니다.

증명: (정리 15.9.1의 역정리). 부가 정보가 있는 임의의율-왜곡 코드를 고려하십시오. 인코딩 함수를 $f_{n}: \mathcal{X}^{n} \rightarrow\{1,2, \ldots$, $\left.2^{n R}\right\}$라고 합시다. 디코딩 함수를 $g_{n}: \mathcal{Y}^{n} \times\left\{1,2, \ldots, 2^{n R}\right\} \rightarrow \mathcal{X}^{n}$라고 하고, $g_{n i}: \mathcal{Y}^{n} \times\left\{1,2, \ldots, 2^{n R}\right\} \rightarrow \mathcal{X}$를 디코딩 함수에 의해 생성된 $i$번째 심볼이라고 합시다. $X^{n}$의 인코딩된 버전을 $T=f_{n}\left(X^{n}\right)$라고 합시다. $E d\left(X^{n}, g_{n}\left(Y^{n}, f_{n}\left(X^{n}\right)\right)\right) \leq D$이면 $R \geq R_{Y}(D)$임을 보여야 합니다. 다음과 같은 부등식 연쇄를 가집니다:

$$
\begin{aligned}
n R & \stackrel{(\mathrm{a})}{\geq} H(T) \\
& \stackrel{(\mathrm{b})}{\geq} H\left(T \mid Y^{n}\right) \\
& \geq I\left(X^{n} ; T \mid Y^{n}\right) \\
& \stackrel{(\mathrm{c})}{\equiv} \sum_{i=1}^{n} I\left(X_{i} ; T \mid Y^{n}, X^{i-1}\right) \\
& =\sum_{i=1}^{n} H\left(X_{i} \mid Y^{n}, X^{i-1}\right)-H\left(X_{i} \mid T, Y^{n}, X^{i-1}\right) \\
& \stackrel{(\mathrm{d})}{=} \sum_{i=1}^{n} H\left(X_{i} \mid Y_{i}\right)-H\left(X_{i} \mid T, Y^{i-1}, Y_{i}, Y_{i+1}^{n}, X^{i-1}\right) \\
& \stackrel{(\mathrm{e})}{=} \sum_{i=1}^{n} H\left(X_{i} \mid Y_{i}\right)-H\left(X_{i} \mid T, Y^{i-1}, Y_{i}, Y_{i+1}^{n}\right) \\
& \stackrel{(\mathrm{f})}{=} \sum_{i=1}^{n} H\left(X_{i} \mid Y_{i}\right)-H\left(X_{i} \mid W_{i}, Y_{i}\right) \\
& \stackrel{(\mathrm{g})}{=} \sum_{i=1}^{n} I\left(X_{i} ; W_{i} \mid Y_{i}\right)
\end{aligned}
$$
<!-- Page 610 -->
$$
\begin{aligned}
& =\sum_{i=1}^{n} H\left(W_{i} \mid Y_{i}\right)-H\left(W_{i} \mid X_{i}, Y_{i}\right) \\
& \stackrel{(h)}{=} \sum_{i=1}^{n} H\left(W_{i} \mid Y_{i}\right)-H\left(W_{i} \mid X_{i}\right) \\
& =\sum_{i=1}^{n} H\left(W_{i}\right)-H\left(W_{i} \mid X_{i}\right)-H\left(W_{i}\right)+H\left(W_{i} \mid Y_{i}\right) \\
& =\sum_{i=1}^{n} I\left(W_{i} ; X_{i}\right)-I\left(W_{i} ; Y_{i}\right) \\
& \stackrel{(i)}{\geq} \sum_{i=1}^{n} R_{Y}\left(E d\left(X_{i}, g_{n i}^{\prime}\left(W_{i}, Y_{i}\right)\right)\right) \\
& =n \frac{1}{n} \sum_{i=1}^{n} R_{Y}\left(E d\left(X_{i}, g_{n i}^{\prime}\left(W_{i}, Y_{i}\right)\right)\right) \\
& \stackrel{(j)}{=} n R_{Y}\left(\frac{1}{n} \sum_{i=1}^{n} E d\left(X_{i}, g_{n i}^{\prime}\left(W_{i}, Y_{i}\right)\right)\right) \\
& \stackrel{(k)}{\geq} n R_{Y}(D)
\end{aligned}
$$

where
(a)는 $T$의 범위가 $\left\{1,2, \ldots, 2^{n R}\right\}$라는 사실로부터 도출됩니다.
(b)는 조건화가 엔트로피를 감소시킨다는 사실로부터 도출됩니다.
(c)는 상호 정보량에 대한 연쇄 법칙으로부터 도출됩니다.
(d)는 $X_{i}$가 $Y_{i}$가 주어졌을 때 과거 및 미래의 $Y$ 및 $X$와 독립이라는 사실로부터 도출됩니다.
(e)는 조건화가 엔트로피를 감소시킨다는 사실로부터 도출됩니다.
(f)는 $W_{i}=\left(T, Y^{i-1}, Y_{i+1}^{n}\right)$를 정의함으로써 도출됩니다.
(g)는 상호 정보량의 정의로부터 도출됩니다.
(h)는 $Y_{i}$가 $X_{i}$에만 의존하고 $T$ 및 과거 및 미래의 $Y$와 조건부로 독립이기 때문에 $W_{i} \rightarrow X_{i} \rightarrow Y_{i}$가 마르코프 연쇄를 형성한다는 사실로부터 도출됩니다.
(i)는 (정보) 조건부율 왜곡 함수 $R_{Y}$의 정의로부터 도출됩니다. 왜냐하면 $\hat{X}_{i}=g_{n i}\left(T, Y^{n}\right) \triangleq g_{n i}^{\prime}\left(W_{i}, Y_{i}\right)$이고, 따라서 $I\left(W_{i} ; X_{i}\right)-I\left(W_{i} ; Y_{i}\right) \geq \min _{W: E d(X, \hat{X}) \leq D_{i}} I(W ; X)-$ $I(W ; Y)=R_{Y}\left(D_{i}\right)$이기 때문입니다.
<!-- Page 611 -->
(j)은 젠센 부등식과 조건부율-왜곡 함수(보조정리 15.9.1)의 볼록성으로부터 도출됩니다.
(k)은 $D=E\left[\frac{1}{n} \sum_{i=1}^{n} d\left(X_{i}, \hat{X}_{i}\right)\right]$의 정의로부터 도출됩니다.

이 반례와 부가 정보가 없는 비율-왜곡에 대한 반례(10.4절) 사이의 유사성을 쉽게 알 수 있습니다. 달성 가능성 증명 또한 강한 전형성을 이용한 비율-왜곡 정리에 대한 증명과 유사합니다. 그러나 소스와 함께 전형적인 코드워드의 인덱스를 보내는 대신, 이 코드워드들을 빈으로 나누고 대신 빈 인덱스를 보냅니다. 각 빈의 코드워드 수가 충분히 작다면, 부가 정보는 수신기에서 특정 코드워드를 분리하는 데 사용될 수 있습니다. 따라서 다시 한번 우리는 랜덤 비닝과 비율-왜곡 인코딩을 결합하여 함께 전형적인 재현 코드워드를 찾습니다. 증명의 세부 사항을 아래에 개략적으로 설명합니다.

증명: (정리 15.9.1의 달성 가능성). $p(w \mid x)$와 함수 $f(w, y)$를 고정합니다. $p(w)=\sum_{x} p(x) p(w \mid x)$를 계산합니다.

코드북 생성: $R_{1}=I(X ; W)+\epsilon$으로 설정합니다. $2^{n R}$개의 독립적이고 동일하게 분포된(i.i.d.) 코드워드 $W^{n}(s) \sim \prod_{i=1}^{n} p\left(w_{i}\right)$를 생성하고, 이를 $s \in\left\{1,2, \ldots, 2^{n R_{1}}\right\}$로 인덱싱합니다. $R_{2}=I(X ; W)-I(Y ; W)+5 \epsilon$으로 설정합니다. 인덱스 $s \in\left\{1,2, \ldots, 2^{n R_{1}}\right\}$를 균등 분포를 사용하여 $2^{n R_{2}}$개의 빈 중 하나에 무작위로 할당합니다. $B(i)$는 빈 $i$에 할당된 인덱스를 나타냅니다. 각 빈에는 약 $2^{n\left(R_{1}-R_{2}\right)}$개의 인덱스가 있습니다.

인코딩: 소스 시퀀스 $X^{n}$가 주어지면, 인코더는 $\left(X^{n}, W^{n}(s)\right) \in A_{\epsilon}^{*(n)}$인 코드워드 $W^{n}(s)$를 찾습니다. 그러한 $W^{n}$이 없으면 인코더는 $s=1$로 설정합니다. 그러한 $s$가 여러 개 있으면 인코더는 가장 작은 $s$를 사용합니다. 인코더는 $s$가 속한 빈의 인덱스를 보냅니다.

디코딩: 디코더는 $s \in B(i)$이고 $\left(W^{n}(s), Y^{n}\right) \in A_{\epsilon}^{*(n)}$인 $W^{n}(s)$를 찾습니다. 고유한 $s$를 찾으면 $\hat{X}^{n}$을 계산합니다. 여기서 $\hat{X}_{i}=f\left(W_{i}, Y_{i}\right)$입니다. 그러한 $s$를 찾지 못하거나 여러 개를 찾으면 $\hat{X}^{n}=\hat{x}^{n}$으로 설정합니다. 여기서 $\hat{x}^{n}$은 $\hat{X}^{n}$의 임의의 시퀀스입니다. 어떤 기본 시퀀스를 사용하든 상관없습니다. 이 사건의 확률이 작다는 것을 보일 것입니다.

오류 확률 분석: 평소와 같이 다양한 오류 사건이 있습니다.

1. 쌍 $\left(X^{n}, Y^{n}\right) \notin A_{\epsilon}^{*(n)}$. 이 사건의 확률은 큰 $n$에 대해 약수 법칙에 의해 작습니다.
2. 시퀀스 $X^{n}$는 전형적이지만, $\left(X^{n}, W^{n}(s)\right) \in A_{\epsilon}^{*(n)}$인 $s$가 존재하지 않습니다. 비율-왜곡 정리에 대한 증명에서와 같이,
<!-- Page 612 -->
이 사건의 확률은 작습니다.

$$
R_{1}>I(W ; X)
$$

3. 시퀀스 쌍 $\left(X^{n}, W^{n}(s)\right) \in A_{\epsilon}^{*(n)}$이지만 $\left(W^{n}(s), Y^{n}\right) \notin A_{\epsilon}^{*(n)}$입니다 (즉, 코드가 $Y^{n}$ 시퀀스와 공동으로 типикал하지 않습니다). 마르코프 보조정리(보조정리 15.8.1)에 따르면, $n$이 충분히 크면 이 사건의 확률은 작습니다.
4. 동일한 빈 인덱스를 가진 다른 $s^{\prime}$가 존재하여 $\left(W^{n}\left(s^{\prime}\right)\right.$, $\left.Y^{n}\right) \in A_{\epsilon}^{*(n)}$입니다. 임의로 선택된 $W^{n}$이 $Y^{n}$과 공동으로 типикал할 확률이 $\approx 2^{-n I(Y ; W)}$이므로, 동일한 빈에 있는 다른 $W^{n}$이 $Y^{n}$과 типикал할 확률은 빈에 있는 코드워드 수에 공동 типикал 확률을 곱한 값으로 제한됩니다. 즉,

$$
\operatorname{Pr}\left(\exists s^{\prime} \in B(i):\left(W^{n}\left(s^{\prime}\right), Y^{n}\right) \in A_{\epsilon}^{*(n)}\right) \leq 2^{n\left(R_{1}-R_{2}\right)} 2^{-n(I(W ; Y)-3 \epsilon)}
$$

이는 $R_{1}-R_{2}<I(Y ; W)-3 \epsilon$이므로 0으로 갑니다.
5. 인덱스 $s$가 올바르게 디코딩되면 $\left(X^{n}, W^{n}(s)\right) \in A_{\epsilon}^{*(n)}$입니다. 항목 1에 따라 $\left(X^{n}, Y^{n}\right) \in A_{\epsilon}^{*(n)}$이라고 가정할 수 있습니다. 따라서 마르코프 보조정리에 의해 $\left(X^{n}, Y^{n}, W^{n}\right) \in A_{\epsilon}^{*(n)}$이며, 따라서 경험적 결합 분포는 우리가 시작했던 원래 분포 $p(x, y) p(w \mid x)$에 가깝습니다. 그러므로 $\left(X^{n}, \hat{X}^{n}\right)$은 왜곡 $D$를 달성하는 분포에 가까운 결합 분포를 갖게 됩니다.

따라서 높은 확률로 디코더는 왜곡이 $n D$에 가까운 $\hat{X}^{n}$을 생성합니다. 이것으로 정리의 증명이 완료됩니다.

증명의 세부 사항은 Wyner와 Ziv [574]를 참조하십시오. 분산된 데이터 압축의 다양한 상황에 대한 논의 후에 문제가 거의 완전히 해결되었다고 예상할 수 있지만, 불행히도 그렇지 않습니다. 위의 모든 문제의 직접적인 일반화는 그림 15.34에 설명된 상관 소스에 대한 rate distortion 문제입니다. 이것은 본질적으로 왜곡이 $X$와 $Y$ 모두에 있는 Slepian-Wolf 문제입니다. 위에서 고려한 세 가지 분산 소스 코딩 문제가 모두 이 설정의 특수한 경우임을 쉽게 알 수 있습니다. 그러나 이전 문제와 달리 이 문제는 아직 해결되지 않았으며 일반적인 rate distortion 영역은 알려져 있지 않습니다.
<!-- Page 613 -->

그림 15.34. 두 상관 관계가 있는 소스에 대한 속도 왜곡.

# 15.10 일반 다중 단자 네트워크

이 장을 마치면서 송신자와 수신자로 구성된 일반 다중 단자 네트워크를 고려하고 해당 네트워크에서 통신을 달성할 수 있는 속도에 대한 몇 가지 경계를 도출할 것입니다. 일반 다중 단자 네트워크는 그림 15.35에 나와 있습니다. 이 섹션에서는 위 첨자가 노드 인덱스를 나타내고 아래 첨자가 시간 인덱스를 나타냅니다. $m$개의 노드가 있으며, 노드 $i$는 전송 변수 $X^{(i)}$와 수신 변수 $Y^{(i)}$를 가지고 있습니다.

그림 15.35. 일반 다중 단자 네트워크.
<!-- Page 614 -->
노드 $i$는 노드 $j$로 정보율 $R^{(i j)}$로 정보를 전송합니다. 노드 $i$에서 노드 $j$로 전송되는 모든 메시지 $W^{(i j)}$는 독립적이며 각자의 범위 $\left\{1,2, \ldots, 2^{n R^{(i j)}}\right\}$에 대해 균등하게 분포되어 있다고 가정합니다.

채널은 채널 전이 함수 $p\left(y^{(1)}, \ldots, y^{(m)} \mid x^{(1)}, \ldots, x^{(m)}\right)$로 표현되며, 이는 입력이 주어졌을 때 출력의 조건부 확률 질량 함수입니다. 이 확률 전이 함수는 네트워크의 잡음 및 간섭 효과를 포착합니다. 채널은 메모리리스(즉, 임의의 시간 인스턴트에서의 출력은 현재 입력에만 의존하며 과거 입력과 조건부로 독립적임)라고 가정합니다.

각 송신-수신 노드 쌍에 대응하여 메시지 $W^{(i j)} \in\left\{1,2, \ldots, 2^{n R^{(i j)}}\right\}$가 있습니다. 노드 $i$에서의 입력 심볼 $X^{(i)}$는 $W^{(i j)}, j \in\{1, \ldots, m\}$ 및 수신 심볼 $Y^{(i)}$의 과거 값에 따라 달라집니다. 따라서 블록 길이 $n$의 인코딩 스킴은 각 노드에 대한 인코딩 및 디코딩 함수의 집합으로 구성됩니다.

- 인코더: $X_{k}^{(i)}\left(W^{(i 1)}, W^{(i 2)}, \ldots, W^{(i m)}, Y_{1}^{(i)}, Y_{2}^{(i)}, \ldots, Y_{k-1}^{(i)}\right), k=1$, $\ldots, n$. 인코더는 메시지와 과거 수신 심볼을 시간 $k$에 전송되는 심볼 $X_{k}^{(i)}$로 매핑합니다.
- 디코더: $\hat{W}^{(j i)}\left(Y_{1}^{(i)}, \ldots, Y_{n}^{(i)}, W^{(i 1)}, \ldots, W^{(i m)}\right), j=1,2, \ldots, m$. 노드 $i$의 디코더 $j$는 각 블록의 수신 심볼과 자신의 전송 정보를 매핑하여 노드 $j$로부터 자신에게 전달되는 메시지의 추정치를 형성합니다.

모든 노드 쌍에는 해당 정보율과 메시지가 올바르게 디코딩되지 않을 확률이 연관되어 있습니다.

$$
P_{e}^{(n)^{(i j)}}=\operatorname{Pr}\left(\hat{W}^{(i j)}\left(\mathbf{Y}^{(j)}, W^{(j 1)}, \ldots, W^{(j m)}\right) \neq W^{(i j)}\right)
$$

여기서 $P_{e}^{(n)^{(i j)}}$는 모든 메시지가 독립적이고 해당 범위에 대해 균등하게 분포되어 있다는 가정 하에 정의됩니다.

율 집합 $\left\{R^{(i j)}\right\}$는 $n \rightarrow \infty$일 때 모든 $i, j \in$ $\{1,2, \ldots, m\}$에 대해 $P_{e}^{(n)^{(i j)}} \rightarrow 0$이 되는 블록 길이 $n$을 갖는 인코더 및 디코더가 존재하는 경우 달성 가능하다고 말합니다. 이 공식을 사용하여 다중 단말 네트워크의 정보 흐름에 대한 상한을 도출합니다. 노드를 두 집합 $S$와 그 여집합 $S^{c}$로 나눕니다. 이제 $S$의 노드에서 $S^{c}$의 노드로의 정보 흐름율을 제한합니다. [514] 참조
<!-- Page 615 -->
정리 15.10.1 만약 정보율 $\left\{R^{(i j)}\right\}$이 달성 가능하다면, 다음을 만족하는 어떤 결합 확률 분포 $p\left(x^{(1)}, x^{(2)}, \ldots, x^{(m)}\right)$가 존재합니다.

$$
\sum_{i \in S, j \in S^{c}} R^{(i j)} \leq I\left(X^{(S)} ; Y^{\left(S^{c}\right)} \mid X^{\left(S^{c}\right)}\right)
$$

모든 $S \subset\{1,2, \ldots, m\}$에 대하여. 따라서, 컷셋을 가로지르는 정보의 총 흐름율은 조건부 상호 정보량에 의해 제한됩니다.

증명: 증명은 다중 접속 채널의 역정리 증명과 동일한 방식으로 진행됩니다. $T=\left\{(i, j): i \in S, j \in S^{c}\right\}$를 $S$에서 $S^{c}$로 교차하는 링크들의 집합이라고 하고, $T^{c}$를 네트워크의 나머지 모든 링크라고 합시다. 그러면

$$
\begin{aligned}
& n \sum_{i \in S, j \in S^{c}} R^{(i j)} \\
& \stackrel{(\underline{\mathrm{a}})}{=} \sum_{i \in S, j \in S^{c}} H\left(W^{(i j)}\right) \\
& \stackrel{(\mathrm{b})}{=} H\left(W^{(T)}\right) \\
& \stackrel{(\mathrm{C})}{=} H\left(W^{(T)} \mid W^{\left(T^{c}\right)}\right) \\
& =I\left(W^{(T)} ; Y_{1}^{\left(S^{c}\right)}, \ldots, Y_{n}^{\left(S^{c}\right)} \mid W^{\left(T^{c}\right)}\right) \\
& \quad+H\left(W^{(T)} \mid Y_{1}^{\left(S^{c}\right)}, \ldots, Y_{n}^{\left(S^{c}\right)}, W^{\left(T^{c}\right)}\right) \\
& \stackrel{(\mathrm{d})}{=} I\left(W^{(T)} ; Y_{1}^{\left(S^{c}\right)}, \ldots, Y_{n}^{\left(S^{c}\right)} \mid W^{\left(T^{c}\right)}\right)+n \epsilon_{n} \\
& \stackrel{(\mathrm{e})}{=} \sum_{k=1}^{n} I\left(W^{(T)} ; Y_{k}^{\left(S^{c}\right)} \mid Y_{1}^{\left(S^{c}\right)}, \ldots, Y_{k-1}^{\left(S^{c}\right)}, W^{\left(T^{c}\right)}\right)+n \epsilon_{n} \\
& \stackrel{(\mathrm{f})}{=} \sum_{k=1}^{n} H\left(Y_{k}^{\left(S^{c}\right)} \mid Y_{1}^{\left(S^{c}\right)}, \ldots, Y_{k-1}^{\left(S^{c}\right)}, W^{\left(T^{c}\right)}\right) \\
& \quad-H\left(Y_{k}^{\left(S^{c}\right)} \mid Y_{1}^{\left(S^{c}\right)}, \ldots, Y_{k-1}^{\left(S^{c}\right)}, W^{\left(T^{c}\right)}\right)+n \epsilon_{n}
\end{aligned}
$$
<!-- Page 616 -->
$$
\begin{aligned}
& \stackrel{(\mathrm{g})}{\leq} \sum_{k=1}^{n} H\left(Y_{k}^{\left(S^{c}\right)} \mid Y_{1}^{\left(S^{c}\right)}, \ldots, Y_{k-1}^{\left(S^{c}\right)}, W^{\left(T^{c}\right)}, X_{k}^{\left(S^{c}\right)}\right) \\
& \quad-H\left(Y_{k}^{\left(S^{c}\right)} \mid Y_{1}^{\left(S^{c}\right)}, \ldots, Y_{k-1}^{\left(S^{c}\right)}, W^{\left(T^{c}\right)}, W^{(T)}, X_{k}^{(S)}, X_{k}^{\left(S^{c}\right)}\right)+n \epsilon_{n} \\
& \text { (h) } \sum_{k=1}^{n} H\left(Y_{k}^{\left(S^{c}\right)} \mid X_{k}^{\left(S^{c}\right)}\right)-H\left(Y_{k}^{\left(S^{c}\right)} \mid X_{k}^{\left(S^{c}\right)}, X_{k}^{(S)}\right)+n \epsilon_{n} \\
& =\sum_{k=1}^{n} I\left(X_{k}^{(S)} ; Y_{k}^{\left(S^{c}\right)} \mid X_{k}^{\left(S^{c}\right)}\right)+n \epsilon_{n} \\
& \text { (i) } n \frac{1}{n} \sum_{k=1}^{n} I\left(X_{Q}^{(S)} ; Y_{Q}^{\left(S^{c}\right)} \mid X_{Q}^{\left(S^{c}\right)}, Q=k\right)+n \epsilon_{n} \\
& \stackrel{(\mathrm{j})}{=} n I\left(X_{Q}^{(S)} ; Y_{Q}^{\left(S^{c}\right)} \mid X_{Q}^{\left(S^{c}\right)}, Q\right)+n \epsilon_{n} \\
& =n\left(H\left(Y_{Q}^{\left(S^{c}\right)} \mid X_{Q}^{\left(S^{c}\right)}, Q\right)-H\left(Y_{Q}^{\left(S^{c}\right)} \mid X_{Q}^{(S)}, X_{Q}^{\left(S^{c}\right)}, Q\right)\right)+n \epsilon_{n} \\
& \stackrel{(\mathrm{k})}{\leq} n\left(H\left(Y_{Q}^{\left(S^{c}\right)} \mid X_{Q}^{\left(S^{c}\right)}\right)-H\left(Y_{Q}^{\left(S^{c}\right)} \mid X_{Q}^{(S)}, X_{Q}^{\left(S^{c}\right)}, Q\right)\right)+n \epsilon_{n} \\
& \stackrel{(\mathrm{l})}{=} n\left(H\left(Y_{Q}^{\left(S^{c}\right)} \mid X_{Q}^{\left(S^{c}\right)}\right)-H\left(Y_{Q}^{\left(S^{c}\right)} \mid X_{Q}^{(S)}, X_{Q}^{\left(S^{c}\right)}\right)\right)+n \epsilon_{n} \\
& =n I\left(X_{Q}^{(S)} ; Y_{Q}^{\left(S^{c}\right)} \mid X_{Q}^{\left(S^{c}\right)}\right)+n \epsilon_{n}
\end{aligned}
$$

where
(a) follows from the fact that the messages $W^{(i j)}$ are uniformly distributed over their respective ranges $\left\{1,2, \ldots, 2^{n R^{(i j)}}\right\}$
(b) follows from the definition of $W^{(T)}=\left\{W^{(i j)}: i \in S, j \in S^{c}\right\}$ and the fact that the messages are independent
(c) follows from the independence of the messages for $T$ and $T^{c}$
(d) follows from Fano's inequality since the messages $W^{(T)}$ can be decoded from $Y^{(S)}$ and $W^{\left(T^{c}\right)}$
(e) is the chain rule for mutual information
(f) follows from the definition of mutual information
(g) follows from the fact that $X_{k}^{\left(S^{c}\right)}$ is a function of the past received symbols $Y^{\left(S^{c}\right)}$ and the messages $W^{\left(T^{c}\right)}$ and the fact that adding conditioning reduces the second term
<!-- Page 617 -->
(h)은 $Y_{k}^{\left(S^{c}\right)}$가 현재 입력 심볼 $X_{k}^{(S)}$ 및 $X_{k}^{\left(S^{c}\right)}$에만 의존한다는 사실로부터 도출됩니다.
(i)은 $\{1,2, \ldots, n\}$ 상에서 균등하게 분포된 새로운 시분할 랜덤 변수 $Q$를 도입한 후에 도출됩니다.
(j)은 상호 정보량의 정의로부터 도출됩니다.
(k)은 조건화가 엔트로피를 감소시킨다는 사실로부터 도출됩니다.
(1)은 $Y_{Q}^{\left(S^{c}\right)}$가 입력 $X_{Q}^{(S)}$ 및 $X_{Q}^{\left(S^{c}\right)}$에만 의존하고 $Q$와 조건부로 독립이라는 사실로부터 도출됩니다.

따라서, 임의의 결합 분포를 갖는 랜덤 변수 $X^{(S)}$와 $X^{\left(S^{c}\right)}$가 존재하여 정리의 부등식을 만족합니다.

이 정리는 간단한 최대 흐름-최소 컷 해석을 갖습니다. 임의의 경계를 가로지르는 정보 흐름의 속도는 경계의 한쪽에 있는 입력과 다른 쪽의 출력 간의 상호 정보량보다 작으며, 다른 쪽의 입력에 조건화됩니다.

만약 정리의 경계가 달성 가능하다면 네트워크에서의 정보 흐름 문제는 해결될 것입니다. 그러나 불행히도, 이러한 경계는 일부 간단한 채널에 대해서도 달성 가능하지 않습니다. 이제 이 경계를 이전에 고려했던 몇 가지 채널에 적용해 보겠습니다.

- 다중 접속 채널. 다중 접속 채널은 여러 개의 입력 노드와 하나의 출력 노드를 갖는 네트워크입니다. 2 사용자 다중 접속 채널의 경우, 정리 15.10.1의 경계는 다음과 같이 축소됩니다.

$$
\begin{aligned}
R_{1} & \leq I\left(X_{1} ; Y \mid X_{2}\right) \\
R_{2} & \leq I\left(X_{2} ; Y \mid X_{1}\right) \\
R_{1}+R_{2} & \leq I\left(X_{1}, X_{2} ; Y\right)
\end{aligned}
$$

일부 결합 분포 $p\left(x_{1}, x_{2}\right) p\left(y \mid x_{1}, x_{2}\right)$에 대해. 이러한 경계는 입력 분포를 곱 분포로 제한하고 볼록 껍질을 취하면 (정리 15.3.1) 용량 영역과 일치합니다.

- 릴레이 채널. 릴레이 채널의 경우, 이러한 경계는 그림 15.36에 표시된 대로 부분집합의 다른 선택으로 정리 15.7.1의 상한을 제공합니다. 따라서,

$$
C \leq \sup _{p\left(x, x_{1}\right)} \min \left\{I\left(X, X_{1} ; Y\right), I\left(X ; Y, Y_{1} \mid X_{1}\right)\right\}
$$

이 상한은 물리적으로 열화된 릴레이 채널의 용량이며, 피드백이 있는 릴레이 채널 [127]의 용량입니다.
<!-- Page 618 -->

그림 15.36. 릴레이 채널.

그림 15.37. 다중 접속 채널에서의 상관 관계가 있는 소스의 전송.

일반적인 네트워크에 대한 논의를 보완하기 위해, 단일 사용자 채널의 두 가지 특징 중 다중 사용자 네트워크에는 적용되지 않는 두 가지 특징을 언급해야 합니다.

- 소스-채널 분리 정리. 7.13절에서 우리는 소스-채널 분리 정리에 대해 논의했습니다. 이 정리는 엔트로피율이 채널 용량보다 작을 때에만 소스를 채널을 통해 잡음 없이 전송할 수 있음을 증명합니다. 이를 통해 우리는 단일 숫자(엔트로피율)로 소스를, 단일 숫자(용량)로 채널을 특징지을 수 있습니다. 다중 사용자 경우는 어떻습니까? 분산된 소스는 소스의 무잡음 코딩을 위한 속도 영역이 채널의 용량 영역 내에 있을 때에만 채널을 통해 전송될 수 있다고 예상할 것입니다. 구체적으로, 그림 15.37에 표시된 것처럼 다중 접속 채널을 통해 분산된 소스를 전송하는 것을 고려해 봅시다. Slepian-Wolf 인코딩 결과와 다중 접속 채널의 용량 결과를 결합하면, 소스를 채널을 통해 전송하고 낮은 오류 확률로 복구할 수 있음을 보일 수 있습니다.

$$
H(U \mid V) \leq I\left(X_{1} ; Y \mid X_{2}, Q\right)
$$
<!-- Page 619 -->
$$
\begin{aligned}
& H(V \mid U) \leq I\left(X_{2} ; Y \mid X_{1}, Q\right) \\
& H(U, V) \leq I\left(X_{1}, X_{2} ; Y \mid Q\right)
\end{aligned}
$$

어떤 분포 $p(q) p\left(x_{1} \mid q\right) p\left(x_{2} \mid q\right) p\left(y \mid x_{1}, x_{2}\right)$에 대해. 이 조건은 소스의 Slepian-Wolf율 영역이 다중 접속 채널의 용량 영역과 공집합이 아닌 교집합을 갖는다고 말하는 것과 동등합니다.
하지만 이 조건이 필요조건이기도 할까요? 아닙니다. 간단한 예시가 이를 보여줍니다. 예제 15.4.2의 소스를 이진 삭제 다중 접속 채널(예제 15.3.3)로 전송하는 것을 고려해 봅시다. Slepian-Wolf 영역은 용량 영역과 교집합을 이루지 않지만, 소스를 채널을 통해 전송할 수 있도록 하는 방안을 고안하는 것은 간단합니다. 우리는 단지 $X_{1}=U$와 $X_{2}=V$로 설정하고, $Y$의 값은 오류 없이 쌍 $(U, V)$를 알려줄 것입니다. 따라서 조건 (15.345)는 필요조건이 아닙니다.

소스-채널 분리 정리가 실패하는 이유는 다중 접속 채널의 용량이 채널 입력 간의 상관관계가 증가함에 따라 증가하기 때문입니다. 따라서 용량을 최대화하기 위해서는 채널 입력 간의 상관관계를 보존해야 합니다. 반면에 Slepian-Wolf 인코딩은 상관관계를 제거합니다. Cover 등 [129]은 상관관계를 보존한다는 아이디어에 기반하여 상관관계가 있는 소스를 다중 접속 채널로 전송하기 위한 달성 가능한 영역을 제안했습니다. Han과 Costa [273]는 상관관계가 있는 소스를 방송 채널로 전송하기 위한 유사한 영역을 제안했습니다.

- 피드백이 있는 용량 영역. 정리 7.12.1은 피드백이 단일 사용자 이산 메모리리스 채널의 용량을 증가시키지 않음을 보여줍니다. 반면에 메모리가 있는 채널의 경우, 피드백은 송신자가 잡음에 대해 무언가를 예측하고 이를 더 효과적으로 처리할 수 있게 하여 용량을 증가시킵니다.
다중 사용자 채널은 어떻습니까? 놀랍게도, 채널이 메모리리스이더라도 피드백은 다중 사용자 채널의 용량 영역을 증가시킵니다. 이는 Gaarder와 Wolf [220]에 의해 처음으로 입증되었으며, 피드백이 이진 삭제 다중 접속 채널의 용량을 증가시키는 데 어떻게 도움이 되는지를 보여주었습니다. 본질적으로, 수신자로부터 두 송신자로의 피드백은 두 송신자 간의 별도 채널 역할을 합니다. 송신자들은 수신자가 디코딩하기 전에 서로의 전송을 디코딩할 수 있습니다. 그런 다음 그들은 협력하여 수신자의 불확실성을 해결하고, 비협력 용량보다는 더 높은 협력 용량으로 정보를 전송합니다. 이 방안을 사용하여 Cover와 Leung [133]은 다중 접속에 대한 달성 가능한 영역을 확립했습니다.
<!-- Page 620 -->
피드백이 있는 채널입니다. Willems [557]은 이 영역이 이진 삭제 다중 접속 채널을 포함하는 다중 접속 채널 클래스의 용량임을 보여주었습니다. Ozarow [410]는 두 사용자 가우시안 다중 접속 채널의 용량 영역을 확립했습니다. 피드백이 있는 다중 접속 채널의 용량 영역을 찾는 문제는 공통 출력이 있는 양방향 채널의 용량과 밀접하게 관련되어 있습니다.

아직 네트워크 정보 흐름에 대한 통일된 이론은 없습니다. 그러나 통신 네트워크에 대한 완전한 이론은 통신 및 계산 이론에 광범위한 영향을 미칠 것이라는 데에는 의심의 여지가 없습니다.

# 요약

다중 접속 채널. 다중 접속 채널 $\left(\mathcal{X}_{1} \times \mathcal{X}_{2}, p\left(y \mid x_{1}, x_{2}\right), \mathcal{Y}\right)$의 용량은 다음을 만족하는 모든 $\left(R_{1}, R_{2}\right)$의 볼록 껍질의 폐포입니다.

$$
\begin{aligned}
R_{1} & <I\left(X_{1} ; Y \mid X_{2}\right) \\
R_{2} & <I\left(X_{2} ; Y \mid X_{1}\right) \\
R_{1}+R_{2} & <I\left(X_{1}, X_{2} ; Y\right)
\end{aligned}
$$

$\mathcal{X}_{1} \times \mathcal{X}_{2}$에 대한 일부 분포 $p_{1}\left(x_{1}\right) p_{2}\left(x_{2}\right)$에 대해.
$m$ 사용자 다중 접속 채널의 용량 영역은 다음을 만족하는 속도 벡터의 볼록 껍질의 폐포입니다.

$$
R(S) \leq I\left(X(S) ; Y \mid X\left(S^{c}\right)\right) \quad \text { 모든 } S \subseteq\{1,2, \ldots, m\} \text { 에 대해 }
$$

일부 곱 분포 $p_{1}\left(x_{1}\right) p_{2}\left(x_{2}\right) \cdots p_{m}\left(x_{m}\right)$에 대해.
가우시안 다중 접속 채널. 두 사용자 가우시안 다중 접속 채널의 용량 영역은 다음과 같습니다.

$$
\begin{aligned}
& R_{1} \leq C\left(\frac{P_{1}}{N}\right) \\
& R_{2} \leq C\left(\frac{P_{2}}{N}\right)
\end{aligned}
$$
<!-- Page 621 -->
$$
R_{1}+R_{2} \leq C\left(\frac{P_{1}+P_{2}}{N}\right)
$$

여기서

$$
C(x)=\frac{1}{2} \log (1+x)
$$

Slepian-Wolf 코딩. 상관관계가 있는 소스 $X$와 $Y$는 각각 $R_{1}$ 및 $R_{2}$의 속도로 별도로 설명될 수 있으며, 다음을 만족하는 경우에만 임의로 낮은 오류 확률로 공통 디코더에 의해 복구될 수 있습니다.

$$
\begin{aligned}
R_{1} & \geq H(X \mid Y) \\
R_{2} & \geq H(Y \mid X) \\
R_{1}+R_{2} & \geq H(X, Y)
\end{aligned}
$$

브로드캐스트 채널. 열화된 브로드캐스트 채널 $X \rightarrow Y_{1} \rightarrow Y_{2}$의 용량 영역은 다음을 만족하는 모든 $\left(R_{1}, R_{2}\right)$의 폐포의 볼록 껍질입니다.

$$
\begin{aligned}
& R_{2} \leq I\left(U ; Y_{2}\right) \\
& R_{1} \leq I\left(X ; Y_{1} \mid U\right)
\end{aligned}
$$

일부 결합 분포 $p(u) p(x \mid u) p\left(y_{1}, y_{2} \mid x\right)$에 대해.
릴레이 채널. 물리적으로 열화된 릴레이 채널 $p\left(y, y_{1} \mid x, x_{1}\right)$의 용량 $C$는 다음과 같이 주어집니다.

$$
C=\sup _{p\left(x, x_{1}\right)} \min \left\{I\left(X, X_{1} ; Y\right), I\left(X ; Y_{1} \mid X_{1}\right)\right\}
$$

여기서 supremum은 $\mathcal{X} \times \mathcal{X}_{1}$에 대한 모든 결합 분포에 대해 취해집니다.
측면 정보가 있는 소스 코딩. $(X, Y) \sim p(x, y)$라고 가정합니다. $Y$가 속도 $R_{2}$로 인코딩되고 $X$가 속도 $R_{1}$로 인코딩되면, 다음을 만족하는 일부 분포 $p(y, u)$에 대해 $X \rightarrow Y \rightarrow U$인 경우에만 임의로 작은 오류 확률로 $X$를 복구할 수 있습니다.

$$
\begin{aligned}
& R_{1} \geq H(X \mid U) \\
& R_{2} \geq I(Y ; U)
\end{aligned}
$$
<!-- Page 622 -->
측면 정보가 있는 속도 왜곡. $(X, Y) \sim p(x, y)$라고 가정합니다. 측면 정보가 있는 속도 왜곡 함수는 다음과 같이 주어집니다.

$$
R_{Y}(D)=\min _{p(w \mid x)} \min _{f: \mathcal{Y} \times \mathcal{W} \rightarrow \hat{X}} I(X ; W)-I(Y ; W)
$$

여기서 최소화는 모든 함수 $f$와 조건부 분포 $p(w \mid x),|\mathcal{W}| \leq|\mathcal{X}|+1$에 대해 수행되며, 다음 조건을 만족합니다.

$$
\sum_{x} \sum_{w} \sum_{y} p(x, y) p(w \mid x) d(x, f(y, w)) \leq D
$$

# 문제

15.1 다중 접속 채널의 협력적 용량

(a) $X_{1}$와 $X_{2}$가 두 인덱스 $W_{1} \in$ $\left\{1,2^{n R}\right\}, W_{2} \in\left\{1,2^{n R_{2}}\right\}$에 접근할 수 있다고 가정합니다. 따라서 코드워드 $\mathbf{X}_{1}\left(W_{1}\right.$, $\left.W_{2}\right), \mathbf{X}_{2}\left(W_{1}, W_{2}\right)$는 두 인덱스 모두에 의존합니다. 용량 영역을 찾으십시오.
(b) 이진 삭제 다중 접속 채널 $Y=X_{1}+X_{2}, X_{i} \in\{0,1\}$에 대해 이 영역을 평가하십시오. 비협력적 영역과 비교하십시오.
15.2 다중 접속 채널의 용량. 다음 각 다중 접속 채널에 대한 용량 영역을 찾으십시오.
(a) 모듈로 2 다중 접속 채널 덧셈. $X_{1} \in\{0,1\}$, $X_{2} \in\{0,1\}, Y=X_{1} \oplus X_{2}$
(b) 다중 접속 채널 곱셈. $X_{1} \in\{-1,1\}$, $X_{2} \in\{-1,1\}, Y=X_{1} \cdot X_{2}$
<!-- Page 623 -->
15.3 다중 접속 채널의 용량 영역에 대한 컷셋 해석. 다중 접속 채널에 대해, $\left(R_{1}, R_{2}\right)$가 달성 가능하다는 것을 압니다.

$$
\begin{gathered}
R_{1}<I\left(X_{1} ; Y \mid X_{2}\right) \\
R_{2}<I\left(X_{2} ; Y \mid X_{1}\right) \\
R_{1}+R_{2}<I\left(X_{1}, X_{2} ; Y\right)
\end{gathered}
$$

$X_{1}, X_{2}$가 독립일 때. $X_{1}, X_{2}$가 독립일 때 다음을 보이십시오.

$$
I\left(X_{1} ; Y \mid X_{2}\right)=I\left(X_{1} ; Y, X_{2}\right)
$$

정보 경계를 컷셋 $S_{1}, S_{2}$, 및 $S_{3}$를 가로지르는 흐름의 경계로 해석하십시오.
15.4 가우시안 다중 접속 채널 용량. AWGN 다중 접속 채널에 대해, 일반적인 시퀀스를 사용하여, 다음을 만족하는 모든 속도 쌍 $\left(R_{1}, R_{2}\right)$의 달성 가능성을 증명하십시오.

$$
\begin{aligned}
R_{1} & <\frac{1}{2} \log \left(1+\frac{P_{1}}{N}\right) \\
R_{2} & <\frac{1}{2} \log \left(1+\frac{P_{2}}{N}\right) \\
R_{1}+R_{2} & <\frac{1}{2} \log \left(1+\frac{P_{1}+P_{2}}{N}\right)
\end{aligned}
$$
<!-- Page 624 -->
증명은 이산 다중 접속 채널에 대한 증명을 단일 사용자 가우시안 채널에 대한 증명이 이산 단일 사용자 채널에 대한 증명을 확장하는 방식과 동일한 방식으로 확장합니다.
15.5 가우시안 다중 접속 채널에 대한 역정리. 이산 경우의 역정리를 확장하여 코드워드에 대한 전력 제약을 고려함으로써 가우시안 다중 접속 채널에 대한 역정리를 증명하십시오.
15.6 특이한 다중 접속 채널. 다음 다중 접속 채널을 고려하십시오: $\mathcal{X}_{1}=\mathcal{X}_{2}=\mathcal{Y}=\{0,1\}$. 만약 $\left(X_{1}, X_{2}\right)=$ $(0,0)$이면, $Y=0$입니다. 만약 $\left(X_{1}, X_{2}\right)=(0,1)$이면, $Y=1$입니다. 만약 $\left(X_{1}, X_{2}\right)=$ $(1,0)$이면, $Y=1$입니다. 만약 $\left(X_{1}, X_{2}\right)=$ $(1,1)$이면, $Y=0$은 확률 $\frac{1}{2}$로, $Y=1$은 확률 $\frac{1}{2}$로 발생합니다.
(a) 속도 쌍 $(1,0)$과 $(0,1)$이 달성 가능함을 보이십시오.
(b) 비퇴화 분포 $p\left(x_{1}\right) p\left(x_{2}\right)$에 대해 $I\left(X_{1}, X_{2} ; Y\right)<1$임을 보이십시오.
(c) 이 다중 접속 채널의 용량 영역에는 시간 공유를 통해서만 달성될 수 있는 점들이 있음을 논증하십시오. 즉, 채널에 대한 용량 영역에 속하지만 다음으로 정의된 영역에는 속하지 않는 달성 가능한 속도 쌍 $\left(R_{1}, R_{2}\right)$이 존재함을 보이십시오.

$$
\begin{aligned}
R_{1} & \leq I\left(X_{1} ; Y \mid X_{2}\right) \\
R_{2} & \leq I\left(X_{2} ; Y \mid X_{1}\right) \\
R_{1}+R_{2} & \leq I\left(X_{1}, X_{2} ; Y\right)
\end{aligned}
$$

모든 곱 분포 $p\left(x_{1}\right) p\left(x_{2}\right)$에 대해. 따라서 볼록화 연산은 용량 영역을 엄격하게 확장합니다. 이 채널은 Csiszár와 Körner [149] 및 Bierbaum과 Wallmeier [59]에 의해 독립적으로 소개되었습니다.
15.7 방송 채널의 용량 영역의 볼록성. 모든 달성 가능한 속도 쌍 $\mathbf{R}=\left(R_{1}, R_{2}\right)$에 대한 용량 영역을 $\mathbf{C} \subseteq \mathbf{R}^{2}$라고 합시다. 시간 공유 논증을 사용하여 $\mathbf{C}$가 볼록 집합임을 보이십시오. 구체적으로, $\mathbf{R}^{(1)}$과 $\mathbf{R}^{(2)}$가 달성 가능하면, $0 \leq \lambda \leq 1$에 대해 $\lambda \mathbf{R}^{(1)}+(1-\lambda) \mathbf{R}^{(2)}$가 달성 가능함을 보이십시오.
15.8 결정론적으로 관련된 소스에 대한 Slepian-Wolf. $(X, Y)$의 동시 데이터 압축에 대한 Slepian-Wolf 속도 영역을 찾고 스케치하십시오. 여기서 $y=f(x)$는 $x$의 어떤 결정론적 함수입니다.
<!-- Page 625 -->
15.9 Slepian-Wolf 문제. $X_{i}$를 i.i.d. Bernoulli $(p)$라고 합시다. $Z_{i}$를 i.i.d. $\sim$ Bernoulli $(r)$라고 하고, $\mathbf{Z}$는 $\mathbf{X}$와 독립이라고 합시다. 마지막으로, $\mathbf{Y}=$ $\mathbf{X} \oplus \mathbf{Z}(\bmod 2$ 덧셈)이라고 합시다. $\mathbf{X}$를 rate $R_{1}$으로, $\mathbf{Y}$를 rate $R_{2}$으로 기술한다고 할 때, 오류 확률이 0으로 수렴하도록 $\mathbf{X}, \mathbf{Y}$를 복구할 수 있는 rate 영역은 무엇입니까?
15.10 Broadcast capacity는 조건부 주변 확률에만 의존합니다. 일반적인 broadcast channel $\left(X, Y_{1} \times Y_{2}, p\left(y_{1}, y_{2} \mid x\right)\right)$를 고려하십시오. capacity region이 $p\left(y_{1} \mid x\right)$와 $p\left(y_{2} \mid\right.$ $x$ )에만 의존함을 보이십시오. 이를 위해, 임의의 주어진 $\left(\left(2^{n R_{1}}, 2^{n R_{2}}\right), n\right)$ 코드에 대해 다음을 정의합니다.

$$
\begin{aligned}
& P_{1}^{(n)}=P\left\{\hat{W}_{1}\left(\mathbf{Y}_{1}\right) \neq W_{1}\right\} \\
& P_{2}^{(n)}=P\left\{\hat{W}_{2}\left(\mathbf{Y}_{2}\right) \neq W_{2}\right\} \\
& P^{(n)}=P\left\{\left(\hat{W}_{1}, \hat{W}_{2}\right) \neq\left(W_{1}, W_{2}\right)\right\}
\end{aligned}
$$

그런 다음 다음을 보이십시오.

$$
\max \left\{P_{1}^{(n)}, P_{2}^{(n)}\right\} \leq P^{(n)} \leq P_{1}^{(n)}+P_{2}^{(n)}
$$

결과는 간단한 논증으로 얻어집니다. (참고: 오류 확률 $P^{(n)}$은 조건부 결합 분포 $p\left(y_{1}, y_{2} \mid x\right)$에 의존합니다. 그러나 $P^{(n)}$이 0으로 줄어들 수 있는지 여부는 [조건부 주변 확률 $\left.p\left(y_{1} \mid x\right), p\left(y_{2} \mid x\right)\right]$을 통해서만 가능합니다.)
15.11 Degraded broadcast channel에 대한 converse. 다음 부등식 연쇄는 degraded discrete memoryless broadcast channel에 대한 converse를 증명합니다. 각 레이블이 붙은 부등식에 대한 이유를 제시하십시오.
Degraded broadcast channel capacity에 대한 converse 설정:

$$
\left(W_{1}, W_{2}\right)_{\text {indep. }} \rightarrow X^{n}\left(W_{1}, W_{2}\right) \rightarrow Y_{1}^{n} \rightarrow Y_{2}^{n}
$$

- Encoding $f_{n}: 2^{n R_{1}} \times 2^{n R_{2}} \rightarrow \mathcal{X}^{n}$
- Decoding: $\quad g_{n}: \mathcal{Y}_{1}^{n} \rightarrow 2^{n R_{1}}, h_{n}: \mathcal{Y}_{2}^{n} \rightarrow 2^{n R_{2}} . \quad$ 그러면 $\quad U_{i}=$ $\left(W_{2}, Y_{1}^{i-1}\right)$라고 합시다.

$$
\begin{aligned}
n R_{2} & \leq_{\text {Fano }} I\left(W_{2} ; Y_{2}^{n}\right) \\
& \stackrel{(\text { a) }}{=} \sum_{i=1}^{n} I\left(W_{2} ; Y_{2 i} \mid Y_{2}^{i-1}\right)
\end{aligned}
$$
<!-- Page 626 -->
$$
\begin{aligned}
& \stackrel{(\mathrm{b})}{=} \sum_{i}\left(H\left(Y_{2 i} \mid Y_{2}^{i-1}\right)-H\left(Y_{2 i} \mid W_{2}, Y_{2}^{i-1}\right)\right) \\
& \stackrel{(\mathrm{c})}{=} \sum_{i}\left(H\left(Y_{2 i}\right)-H\left(Y_{2 i} \mid W_{2}, Y_{2}^{i-1}, Y_{1}^{i-1}\right)\right) \\
& \stackrel{(\mathrm{d})}{=} \sum_{i}\left(H\left(Y_{2 i}\right)-H\left(Y_{2 i} \mid W_{2}, Y_{1}^{i-1}\right)\right) \\
& \stackrel{(\mathrm{e})}{=} \sum_{i=1}^{n} I\left(U_{i} ; Y_{2 i}\right)
\end{aligned}
$$

Converse의 연속: 레이블이 지정된 부등식에 대한 이유를 제시하십시오:

$$
\begin{aligned}
& n R_{1} \leq_{\text {Fano }} I\left(W_{1} ; Y_{1}^{n}\right) \\
& \stackrel{(\mathrm{f})}{=} I\left(W_{1} ; Y_{1}^{n}, W_{2}\right) \\
& \stackrel{(\mathrm{g})}{=} I\left(W_{1} ; Y_{1}^{n} \mid W_{2}\right) \\
& \stackrel{(\mathrm{h})}{=} \sum_{i-1}^{n} I\left(W_{1} ; Y_{1 i} \mid Y_{1}^{i-1}, W_{2}\right) \\
& \stackrel{(\mathrm{i})}{=} \sum_{i=1}^{n} I\left(X_{i} ; Y_{1 i} \mid U_{i}\right) .
\end{aligned}
$$

이제 $Q$를 $\operatorname{Pr}(Q=i)=1 / n, i=1,2, \ldots, n$인 시간 공유 랜덤 변수라고 합시다. 다음을 정당화하십시오:

$$
\begin{aligned}
& R_{1} \leq I\left(X_{Q} ; Y_{1 Q} \mid U_{Q}, Q\right) \\
& R_{2} \leq I\left(U_{Q} ; Y_{2 Q} \mid Q\right)
\end{aligned}
$$

일부 분포 $p(q) p(u \mid q) p(x \mid u, q) p\left(y_{1}, y_{2} \mid x\right)$에 대해. $U$를 적절하게 재정의함으로써, 이 영역이 다음 형태의 영역의 볼록 폐포와 같다고 주장하십시오.

$$
\begin{aligned}
& R_{1} \leq I\left(X ; Y_{1} \mid U\right) \\
& R_{2} \leq I\left(U ; Y_{2}\right)
\end{aligned}
$$

일부 결합 분포 $p(u) p(x \mid u) p\left(y_{1}, y_{2} \mid x\right)$에 대해.
<!-- Page 627 -->
15.12 용량 점.
(a) 열화된 방송 채널 $X \rightarrow Y_{1} \rightarrow Y_{2}$에 대해, 용량 영역이 $R_{1}$ 및 $R_{2}$ 축과 만나는 점 $a$와 $b$를 찾으십시오.

(b) $b \leq a$임을 보이십시오.
15.13 열화된 방송 채널. 아래에 표시된 열화된 방송 채널의 용량 영역을 찾으십시오.

15.14 알려지지 않은 매개변수를 가진 채널. 매개변수 $p$를 가진 이진 대칭 채널이 주어졌습니다. 용량은 $C=1-H(p)$입니다. 이제 문제를 약간 변경합니다. 수신자는 $p \in\left\{p_{1}, p_{2}\right\}$ (즉, $p=p_{1}$ 또는 $p=p_{2}$이며, $p_{1}$과 $p_{2}$는 주어진 실수입니다)만 알고 있습니다. 송신자는 $p$의 실제 값을 알고 있습니다. 송신자가 사용할 두 개의 코드를 고안하십시오. 하나는 $p=p_{1}$일 때 사용하고, 다른 하나는 $p=p_{2}$일 때 사용하며, $p=p_{1}$일 때 속도 $\approx C\left(p_{1}\right)$로, $p=p_{2}$일 때 속도 $\approx C\left(p_{2}\right)$로 전송이 이루어지도록 하십시오. (힌트: 점근적 속도에 영향을 주지 않고 수신자에게 $p$를 공개하는 방법을 고안하십시오. 적절한 길이의 1 시퀀스를 코드워드 앞에 붙이는 것이 작동할 것입니다.)
<!-- Page 628 -->
15.15 양방향 채널. 그림 15.6에 표시된 양방향 채널을 고려하십시오. 출력 $Y_{1}$과 $Y_{2}$는 현재 입력 $X_{1}$과 $X_{2}$에만 의존합니다.
(a) 두 송신자에 대해 독립적으로 생성된 코드를 사용하여 다음의 속도 영역이 달성 가능함을 보이십시오:

$$
\begin{aligned}
& R_{1}<I\left(X_{1} ; Y_{2} \mid X_{2}\right) \\
& R_{2}<I\left(X_{2} ; Y_{1} \mid X_{1}\right)
\end{aligned}
$$

일부 곱셈 분포 $p\left(x_{1}\right) p\left(x_{2}\right) p\left(y_{1}, y_{2} \mid x_{1}, x_{2}\right)$에 대해.
(b) 오류 확률이 임의로 작게 되는 양방향 채널에 대한 모든 코드의 속도가 다음을 만족해야 함을 보이십시오:

$$
\begin{aligned}
& R_{1} \leq I\left(X_{1} ; Y_{2} \mid X_{2}\right) \\
& R_{2} \leq I\left(X_{2} ; Y_{1} \mid X_{1}\right)
\end{aligned}
$$

일부 결합 분포 $p\left(x_{1}, x_{2}\right) p\left(y_{1}, y_{2} \mid x_{1}, x_{2}\right)$에 대해.
양방향 채널의 용량에 대한 내부 및 외부 경계는 Shannon [486]에 의해 제시되었습니다. 그는 또한 내부 경계와 외부 경계가 이진 곱셈 채널 $\mathcal{X}_{1}=\mathcal{X}_{2}=\mathcal{Y}_{1}=\mathcal{Y}_{2}=\{0,1\}, Y_{1}=$ $Y_{2}=X_{1} X_{2}$의 경우 일치하지 않음을 보였습니다. 양방향 채널의 용량은 여전히 열린 문제입니다.
15.16 다중 접속 채널. 다중 접속 채널의 출력 $Y$가 다음과 같이 주어진다고 가정합니다:

$$
Y=X_{1}+\operatorname{sgn}\left(X_{2}\right)
$$

여기서 $X_{1}, X_{2}$는 모두 실수이고 전력 제한이 있습니다:

$$
\begin{aligned}
E\left(X_{1}^{2}\right) & \leq P_{1} \\
E\left(X_{2}^{2}\right) & \leq P_{2}
\end{aligned}
$$

그리고 $\operatorname{sgn}(x)=\left\{\begin{array}{rl}1, & x>0, \\ -1, & x \leq 0 .\end{array}\right.$
이 채널에는 간섭은 있지만 잡음은 없다는 점에 유의하십시오.
(a) 용량 영역을 찾으십시오.
(b) 용량 영역을 달성하는 코딩 방식을 설명하십시오.
<!-- Page 629 -->
15.17 Slepian-Wolf. $(X, Y)$의 결합 확률 질량 함수 $p(x, y)$는 다음과 같습니다:

| $p(x, y)$ | 1 | 2 | 3 |
| :--: | :--: | :--: | :--: |
| 1 | $\alpha$ | $\beta$ | $\beta$ |
| 2 | $\beta$ | $\alpha$ | $\beta$ |
| 3 | $\beta$ | $\beta$ | $\alpha$ |

여기서 $\beta=\frac{1}{6}-\frac{\alpha}{2}$ 입니다. (참고: 이것은 결합 확률 질량 함수이지 조건부 확률 질량 함수가 아닙니다.)
(a) 이 소스에 대한 Slepian-Wolf율 영역을 찾으십시오.
(b) $\alpha$에 대한 $\operatorname{Pr}\{X=Y\}$는 무엇입니까?
(c) $\alpha=\frac{1}{3}$일 때의율 영역은 무엇입니까?
(d) $\alpha=\frac{1}{9}$일 때의율 영역은 무엇입니까?
15.18 Square channel. 다음 다중 접속 채널의 capacity는 얼마입니까?

$$
\begin{aligned}
X_{1} & \in\{-1,0,1\} \\
X_{2} & \in\{-1,0,1\} \\
Y & =X_{1}^{2}+X_{2}^{2}
\end{aligned}
$$

(a) capacity 영역을 찾으십시오.
(b) capacity 영역의 경계에 있는 점을 달성하는 $p^{*}\left(x_{1}\right), p^{*}\left(x_{2}\right)$를 설명하십시오.
15.19 Slepian-Wolf. 두 송신자는 각각 랜덤 변수 $U_{1}$와 $U_{2}$를 알고 있습니다. 랜덤 변수 $\left(U_{1}, U_{2}\right)$가 다음의 결합 분포를 갖는다고 가정합니다:

| $U_{1} \backslash U_{2}$ | 0 | 1 | 2 | $\cdots$ | $m-1$ |
| :--: | :--: | :--: | :--: | :--: | :--: |
| 0 | $\alpha$ | $\frac{\beta}{m-1}$ | $\frac{\beta}{m-1}$ | $\cdots$ | $\frac{\beta}{m-1}$ |
| 1 | $\frac{\gamma}{m-1}$ | 0 | 0 | $\cdots$ | 0 |
| 2 | $\frac{\gamma}{m-1}$ | 0 | 0 | $\cdots$ | 0 |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\ddots$ | $\vdots$ |
| $m-1$ | $\frac{\gamma}{m-1}$ | 0 | 0 | $\cdots$ | 0 |

여기서 $\alpha+\beta+\gamma=1$입니다. 공통 수신자가 두 랜덤 변수를 신뢰성 있게 디코딩할 수 있도록 하는율 $\left(R_{1}, R_{2}\right)$의 영역을 찾으십시오.
<!-- Page 630 -->
15.20 다중 접속
(a) 다중 접속 채널에 대한 용량 영역을 찾으십시오.

$$
Y=X_{1}^{X_{2}}
$$

여기서

$$
X_{1} \epsilon\{2,4\}, \quad X_{2} \epsilon\{1,2\}
$$

(b) $X_{1}$의 범위가 $\{1,2\}$라고 가정합니다. 용량 영역이 감소합니까? 그 이유는 무엇입니까?
15.21 방송 채널. 다음의 열화된 방송 채널을 고려하십시오.

(a) $X$에서 $Y_{1}$까지의 채널 용량은 얼마입니까?
(b) $X$에서 $Y_{2}$까지의 채널 용량은 얼마입니까?
(c) 이 방송 채널에 대해 달성 가능한 모든 $\left(R_{1}, R_{2}\right)$의 용량 영역은 무엇입니까? 단순화하고 스케치하십시오.
15.22 스테레오. 오른쪽 및 왼쪽 귀 신호의 합과 차이는 개별적으로 압축되어 공통 수신기로 전송됩니다. $Z_{1}$은 Bernoulli $\left(p_{1}\right)$이고 $Z_{2}$는 Bernoulli $\left(p_{2}\right)$이며 $Z_{1}$과 $Z_{2}$는 독립이라고 가정합니다. $X=Z_{1}+Z_{2}$이고 $Y=Z_{1}-Z_{2}$라고 합니다.
(a) 달성 가능한 $\left(R_{X}, R_{Y}\right)$의 Slepian-Wolf 속도 영역은 무엇입니까?

<!-- Page 631 -->
(b) 이 영역은 $\left(R_{Z_{1}}, R_{Z_{2}}\right)$의 속도 영역보다 더 크습니까, 아니면 더 작습니까? 그 이유는 무엇입니까?

이 부분을 해결하는 간단한 방법이 있습니다.
15.23 곱셈 다중 접속 채널. 다음 곱셈 다중 접속 채널의 용량 영역을 찾고 스케치하십시오:

여기서 $X_{1} \in\{0,1\}, X_{2} \in\{1,2,3\}$이고 $Y=X_{1} X_{2}$입니다.
15.24 분산 데이터 압축. $Z_{1}, Z_{2}, Z_{3}$가 독립적인 Bernoulli $(p)$라고 가정합니다. $\left(X_{1}, X_{2}, X_{3}\right)$의 설명을 위한 Slepian-Wolf 속도 영역을 찾으십시오. 여기서

$$
\begin{aligned}
& X_{1}=Z_{1} \\
& X_{2}=Z_{1}+Z_{2} \\
& X_{3}=Z_{1}+Z_{2}+Z_{3}
\end{aligned}
$$

<!-- Page 632 -->
15.25 잡음 없는 다중 접속 채널. 두 개의 이진 입력 $X_{1}, X_{2} \in\{0,1\}$와 출력 $Y=\left(X_{1}, X_{2}\right)$를 갖는 다음의 다중 접속 채널을 고려하십시오.
(a) 용량 영역을 찾으십시오. 각 송신자가 용량으로 전송할 수 있음을 유의하십시오.
(b) 이제 협력적 용량 영역, $R_{1} \geq 0, R_{2} \geq$ $0, R_{1}+R_{2} \leq \max _{p\left(x_{1}, x_{2}\right)} I\left(X_{1}, X_{2} ; Y\right)$를 고려하십시오. 처리량 $R_{1}+R_{2}$는 증가하지 않지만 용량 영역은 증가함을 논증하십시오.
15.26 무한 대역폭 다중 접속 채널. 무한 대역폭을 갖는 가우시안 다중 접속 채널의 용량 영역을 찾으십시오. 모든 송신자가 개별 용량으로 전송할 수 있음을 논증하십시오 (즉, 무한 대역폭은 간섭을 제거합니다).
15.27 다중 접속 항등식. 가우시안 채널의 채널 용량을 $C(x)=\frac{1}{2} \log (1+x)$로 신호 대 잡음비 $x$로 정의할 때 다음을 보이십시오.

$$
C\left(\frac{P_{1}}{N}\right)+C\left(\frac{P_{2}}{P_{1}+N}\right)=C\left(\frac{P_{1}+P_{2}}{N}\right)
$$

이는 두 독립적인 사용자가 전력을 합친 것처럼 정보를 전송할 수 있음을 시사합니다.
15.28 주파수 분할 다중 접속 (FDMA). 처리량 $\quad R_{1}+R_{2}=W_{1} \log \left(1+\frac{P_{1}}{N W_{1}}\right)+\left(W-W_{1}\right) \log (1+$ $\left.\frac{P_{2}}{N\left(W-W_{1}\right)}\right)$를 $W_{1}$에 대해 최대화하여 FDMA의 대역폭이 전송 전력에 비례해야 함을 보이십시오.
15.29 삼중 언어 화자 방송 채널. 네덜란드어, 스페인어, 프랑스어 화자가 세 사람: $D, S, F$에게 동시에 통신하고자 합니다. $D$는 네덜란드어만 알지만 스페인어 단어와 프랑스어 단어를 구별할 수 있습니다. 마찬가지로 스페인어와 프랑스어만 알고 외국어 단어가 말해졌을 때와 어떤 언어인지 구별할 수 있는 다른 두 사람도 마찬가지입니다. 각 언어, 네덜란드어, 스페인어, 프랑스어에 $M$개의 단어가 있다고 가정합니다: 네덜란드어 $M$개, 프랑스어 $M$개, 스페인어 $M$개.
(a) 삼중 언어 화자가 $D$에게 말할 수 있는 최대 속도는 얼마입니까?
(b) 최대 속도로 $D$에게 말한다면, 동시에 $S$에게 말할 수 있는 최대 속도는 얼마입니까?
<!-- Page 633 -->
(c) 만약 그가 파트 (b)의 결합된 속도로 $D$와 $S$에게 말하고 있다면, 그는 또한 어떤 양의 속도로든 $F$에게 말할 수 있습니까? 만약 그렇다면, 그 속도는 무엇입니까? 그렇지 않다면, 그 이유는 무엇입니까?
15.30 모바일 전화에 대한 병렬 가우시안 채널. 송신자 $X$가 두 개의 고정된 기지국으로 송신한다고 가정합니다. 송신자는 평균 전력 $P$로 제한되는 신호 $X$를 송신한다고 가정합니다. 두 기지국은 신호 $Y_{1}$과 $Y_{2}$를 수신하며, 여기서

$$
\begin{aligned}
& Y_{1}=\alpha_{1} X+Z_{1} \\
& Y_{2}=\alpha_{2} X+Z_{2}
\end{aligned}
$$

여기서 $Z_{i} \sim \mathcal{N}\left(0, N_{1}\right), Z_{2} \sim \mathcal{N}\left(0, N_{2}\right)$이고 $Z_{1}$과 $Z_{2}$는 독립입니다. $\alpha$는 전송된 블록에 대해 상수라고 가정합니다.
(a) 두 신호 $Y_{1}$과 $Y_{2}$가 공통 디코더 $Y=\left(Y_{1}, Y_{2}\right)$에서 사용 가능하다고 가정할 때, 송신자에서 공통 수신자로의 채널 용량은 얼마입니까?
(b) 대신, 두 수신기 $Y_{1}$과 $Y_{2}$가 각각 독립적으로 신호를 디코드한다면, 이는 브로드캐스트 채널이 됩니다. $R_{1}$을 기지국 1로의 속도라고 하고 $R_{2}$를 기지국 2로의 속도라고 합시다. 이 채널의 용량 영역을 찾으십시오.
15.31 가우시안 다중 접속. 각각 전력 $P$를 갖는 $m$명의 사용자 그룹이 용량에 있는 가우시안 다중 접속 채널을 사용하고 있으며, 따라서

$$
\sum_{i=1}^{m} R_{i}=C\left(\frac{m P}{N}\right)
$$

여기서 $C(x)=\frac{1}{2} \log (1+x)$이고 $N$은 수신기 잡음 전력입니다. 전력 $P_{0}$를 갖는 새로운 사용자가 참여하기를 원합니다.
(a) 다른 사용자들을 방해하지 않고 그는 어떤 속도로 보낼 수 있습니까?
(b) 새로운 사용자의 속도가 다른 모든 사용자의 결합 통신 속도 $C(m P / N)$와 같도록 그의 전력 $P_{0}$는 얼마여야 합니까?
15.32 결정론적 브로드캐스트 채널에 대한 반증. 결정론적 브로드캐스트 채널은 입력 $X$와 입력 $X$의 함수인 두 출력 $Y_{1}$ 및 $Y_{2}$에 의해 정의됩니다. 따라서 $Y_{1}=f_{1}(X)$이고 $Y_{2}=f_{2}(X)$입니다. $R_{1}$과 $R_{2}$를 두 수신기로 전송할 수 있는 정보의 속도라고 합시다. 다음을 증명하십시오.

$$
R_{1} \leq H\left(Y_{1}\right)
$$
<!-- Page 634 -->
$$
\begin{aligned}
R_{2} & \leq H\left(Y_{2}\right) \\
R_{1}+R_{2} & \leq H\left(Y_{1}, Y_{2}\right)
\end{aligned}
$$

15.33 다중 접속 채널. 다중 접속 채널 $Y=X_{1}$
$+X_{2}(\bmod 4)$, 여기서 $X_{1} \in\{0,1,2,3\}, X_{2} \in\{0,1\}$을 고려하십시오.
(a) 용량 영역 $\left(R_{1}, R_{2}\right)$을 찾으십시오.
(b) 최대 처리량 $R_{1}+R_{2}$은 얼마입니까?
15.34 분산 소스 압축. 다음을 가정합니다.

$$
\begin{aligned}
& Z_{1}= \begin{cases}1, & p \\
0, & q\end{cases} \\
& Z_{2}= \begin{cases}1, & p \\
0, & q\end{cases}
\end{aligned}
$$

그리고 $U=Z_{1} Z_{2}, V=Z_{1}+Z_{2}$라고 가정합니다. $Z_{1}$과 $Z_{2}$가 독립이라고 가정합니다. 이는 $(U, V)$에 대한 결합 분포를 유도합니다. $\left(U_{i}, V_{i}\right)$가 이 분포에 따라 i.i.d.라고 가정합니다. 송신자 1은 $U^{n}$을 속도 $R_{1}$으로 설명하고, 송신자 2는 $V^{n}$을 속도 $R_{2}$로 설명합니다.
(a) 수신자에서 $\left(U^{n}, V^{n}\right)$을 복구하기 위한 Slepian-Wolf 속도 영역을 찾으십시오.
(b) 수신자가 $\left(X^{n}, Y^{n}\right)$에 대해 갖는 잔여 불확실성(조건부 엔트로피)은 무엇입니까?
15.35 비용을 포함한 다중 접속 채널 용량. 심볼 $x$의 비용은 $r(x)$입니다. 코드워드 $x^{n}$의 비용은 $r\left(x^{n}\right)=$ $\frac{1}{n} \sum_{i=1}^{n} r\left(x_{i}\right)$입니다. $\left(2^{n R}, n\right)$ 코드북은 모든 $w \in 2^{n R}$에 대해 비용 제약 조건 $r\left(x^{n}(w)\right) \leq r$을 만족하면 비용 제약 조건 $r$을 만족합니다.
(a) 비용 제약 조건 $r$을 갖는 이산 메모리리스 채널의 용량 $C(r)$에 대한 표현식을 찾으십시오.
(b) 송신자 $X_{1}$이 비용 제약 조건 $r_{1}$을 갖고 송신자 $X_{2}$가 비용 제약 조건 $r_{2}$를 갖는 경우 $\left(\mathcal{X}_{1} \times \mathcal{X}_{2}, p\left(y \mid x_{1}, x_{2}\right), \mathcal{Y}\right)$에 대한 다중 접속 채널 용량 영역에 대한 표현식을 찾으십시오.
(c) (b) 부분에 대한 역을 증명하십시오.
15.36 Slepian-Wolf. 세 장의 카드로 된 덱에서 세 장의 카드가 송신자 $X_{1}$, 송신자 $X_{2}$, 송신자 $X_{3}$에게 각각 한 장씩 분배됩니다. 그들의 카드 정보를 복구하기 위해 $X_{1}, X_{2}, X_{3}$는 어떤 속도로 수신자에게 통신해야 합니까?
<!-- Page 635 -->

$\left(X_{1 i}, X_{2 i}, X_{3 i}\right)$가 $\{1,2,3\}$의 순열에 대한 균등 분포에서 i.i.d.로 추출되었다고 가정합니다.

# 역사적 고찰

이 장은 El Gamal과 Cover [186]의 리뷰를 기반으로 합니다. 양방향 채널은 1961년 Shannon [486]에 의해 연구되었습니다. 그는 용량 영역에 대한 내부 및 외부 경계를 도출했습니다. Dueck [175]와 Schalkwijk [464, 465]는 Shannon의 내부 경계를 초과하는 속도를 달성하는 양방향 채널에 대한 코딩 방안을 제안했습니다. 이 채널에 대한 외부 경계는 Zhang 등 [596]과 Willems 및 Hekstra [558]에 의해 도출되었습니다.

다중 접속 채널 용량 영역은 Ahlswede [7]와 Liao [355]에 의해 발견되었으며, Slepian과 Wolf [501]에 의해 공통 정보를 갖는 다중 접속 채널의 경우로 확장되었습니다. Gaarder와 Wolf [220]는 피드백이 이산 메모리 없는 다중 접속 채널의 용량을 증가시킨다는 것을 처음으로 보여주었습니다. Cover와 Leung [133]은 피드백이 있는 다중 접속 채널에 대한 달성 가능한 영역을 제안했으며, 이는 Willems [557]에 의해 특정 종류의 다중 접속 채널에 대해 최적임이 입증되었습니다. Ozarow [410]는 피드백이 있는 두 사용자 가우시안 다중 접속 채널의 용량 영역을 결정했습니다. Cover 등 [129]과 Ahlswede 및 Han [12]은 다중 접속 채널을 통한 상관 소스의 전송 문제를 고려했습니다. Slepian-Wolf 정리는 Slepian과 Wolf [502]에 의해 증명되었으며, Cover [122]의 빈화 논증을 통해 공동으로 에르고딕한 소스로 확장되었습니다.

방송 채널에 대한 중첩 코딩은 1972년 Cover [119]에 의해 제안되었습니다. 열화 방송 채널의 용량 영역은 Bergmans [55]와 Gallager [225]에 의해 결정되었습니다. 열화 방송 채널에 대한 중첩 코드는 덜 노이즈가 있는 방송 채널 (Körner와 Marton [324]), 더 능력이 있는 방송 채널 (El Gamal [185]), 그리고 열화 메시지 세트를 갖는 방송 채널 (Körner와 Marton [325])에 대해서도 최적입니다. Van der Meulen [526]과 Cover [121]는 일반 방송 채널에 대한 달성 가능한 영역을 제안했습니다. 결정론적 방송 채널의 용량은 Gelfand와 Pinsker [242, 243, 423] 및 Marton [377]에 의해 발견되었습니다. 현재까지 알려진 가장 좋은
<!-- Page 636 -->
방송 채널에 대한 달성 가능한 영역은 Marton [377]에 의해 제시되었으며, Marton의 영역에 대한 더 간단한 증명은 El Gamal과 Van der Meulen [188]에 의해 제공되었습니다. El Gamal [184]은 피드백이 물리적으로 열화된 방송 채널의 capacity를 증가시키지 않음을 보여주었습니다. Dueck [176]은 메모리 없는 방송 채널에서 피드백이 capacity를 증가시킬 수 있음을 보여주는 예시를 소개했으며, Ozarow와 Leung [411]은 피드백이 있는 가우시안 방송 채널에 대한 코딩 절차를 설명하여 capacity 영역을 증가시켰습니다.

릴레이 채널은 Van der Meulen [528]에 의해 소개되었으며, 열화된 릴레이 채널의 capacity 영역은 Cover와 El Gamal [127]에 의해 결정되었습니다. Carleial [85]은 전력 제약이 있는 가우시안 간섭 채널을 소개하고 매우 강한 간섭이 전혀 없는 간섭과 동등함을 보여주었습니다. Sato와 Tanabe [459]는 Carleial의 연구를 강한 간섭이 있는 이산 간섭 채널로 확장했습니다. Sato [457]와 Benzel [51]은 열화된 간섭 채널을 다루었습니다. 일반 간섭 채널에 대한 가장 좋은 달성 가능한 영역은 Han과 Kobayashi [274]에 의해 제시되었습니다. 이 영역은 Han과 Kobayashi [274] 및 Sato [458]에서 보여준 것처럼 간섭 매개변수가 1보다 큰 가우시안 간섭 채널의 capacity를 제공합니다. Carleial [84]은 간섭 채널의 capacity 영역에 대한 새로운 경계를 증명했습니다.

측면 정보가 있는 코딩 문제는 Wyner와 Ziv [573] 및 Wyner [570]에 의해 소개되었으며, 이 문제에 대한 달성 가능한 영역은 Ahlswede와 Körner [13], Gray와 Wyner [261], Wyner [571],[572]에서 설명되었습니다. 측면 정보가 있는 속도 왜곡 함수를 찾는 문제는 Wyner와 Ziv [574]에 의해 해결되었습니다. 측면 정보가 있는 속도 왜곡의 채널 capacity 대응 문제는 Gelfand와 Pinsker [243]에 의해 해결되었으며, 두 결과 간의 쌍대성은 Cover와 Chiang [113]에서 탐구됩니다. 다중 설명 문제는 El Gamal과 Cover [187]에서 다루어집니다.

두 확률 변수의 함수의 인코딩이라는 특별한 문제는 Körner와 Marton [326]에 의해 논의되었으며, 두 이진 확률 변수의 modulo 2 합을 인코딩하는 간단한 방법을 설명했습니다. 소스 네트워크 설명에 대한 일반적인 프레임워크는 Csiszár와 Körner [148],[149]에서 찾을 수 있습니다. Slepian-Wolf 인코딩, 측면 정보가 있는 코딩, 측면 정보가 있는 속도 왜곡을 특별한 경우로 포함하는 일반적인 모델은 Berger와 Yeung [54]에 의해 설명되었습니다.

1989년에 Ahlswede와 Dueck [17]은 통신 채널을 통한 식별 문제를 소개했습니다. 이 문제는 송신자가 수신자에게 정보를 보내지만 각 수신자는 단일 메시지가 전송되었는지 여부만 알면 되는 문제로 볼 수 있습니다. 이 경우 안정적으로 전송될 수 있는 가능한 메시지 집합은 이중 지수적으로 증가합니다.
<!-- Page 637 -->
블록 길이이며, 이 논문의 핵심 결과는 용량 $C$를 가진 임의의 잡음 채널에 대해 $2^{2^{n C}}$개의 메시지를 식별할 수 있음을 보여준 것입니다. 이 문제는 피드백이 있는 채널 및 다중 사용자 채널로의 확장을 포함하여 일련의 논문 [16, 18, 269, 434]을 촉발했습니다.

또 다른 활발한 연구 분야는 MIMO(다중 입력 다중 출력) 시스템 또는 공간-시간 코딩의 분석으로, 이는 무선 시스템을 위한 다중 경로로부터의 다양성 이득을 활용하기 위해 송신기와 수신기 모두에 여러 개의 안테나를 사용합니다. Foschini [217], Teletar [512], Rayleigh 및 Cioffi [246]의 이러한 다중 안테나 시스템 분석은 페이딩 환경에서 여러 안테나를 사용하여 얻은 다양성으로부터의 용량 이득이 기존의 등화 및 인터리빙 기술로 달성된 단일 사용자 용량에 비해 상당할 수 있음을 보여줍니다. IEEE Transactions in Information Theory [70]의 특별호에는 이 기술의 다양한 측면을 다루는 여러 논문이 포함되어 있습니다.

네트워크 정보 이론에 대한 포괄적인 설문 조사는 El Gamal 및 Cover [186], Van der Meulen [526-528], Berger [53], Csiszár 및 Körner [149], Verdu [538], Cover [111], Ephremides 및 Hajek [197]에서 찾을 수 있습니다.
<!-- Page 638 -->
.
<!-- Page 639 -->
# 제 16 장

## 정보 이론과 포트폴리오 이론

주식 시장에서 부의 성장률과 시장의 엔트로피율 사이의 이중성은 매우 두드러집니다. 특히, 우리는 경쟁적으로 최적인 포트폴리오 전략과 성장률 최적인 포트폴리오 전략을 찾을 것입니다. 이 둘은 동일하며, 이는 Shannon 코드(Shannon code)가 경쟁적으로나 기대 설명율(expected description rate)에서나 최적이라는 것과 같습니다. 또한, 우리는 에르고딕(ergodic) 주식 시장 프로세스에 대한 부의 점근적 성장률을 찾습니다. 마지막으로, 우리는 과거 최고의 상수 재조정 포트폴리오(constant rebalanced portfolio)와 동일한 점근적 성장률을 달성할 수 있게 하는 범용 포트폴리오(universal portfolios)에 대해 논의합니다.

섹션 16.8에서는 정상 에르고딕(stationary ergodic) 주식 시장에 대한 최적 포트폴리오의 개념에 의해 동기 부여된, 일반적인 에르고딕 프로세스에 대한 점근적 균등 분할 속성(asymptotic equipartition property)의 "샌드위치" 증명을 제공합니다.

### 16.1 주식 시장: 일부 정의

주식 시장은 $m$개의 주식 벡터 $\mathbf{X}=\left(X_{1}, X_{2}, \ldots, X_{m}\right)$, $X_{i} \geq 0, i=1,2, \ldots, m$으로 표현됩니다. 여기서 $m$은 주식의 수이고, 가격 비율 $X_{i}$는 하루의 끝 가격과 하루의 시작 가격의 비율입니다. 따라서 일반적으로 $X_{i}$는 1에 가깝습니다. 예를 들어, $X_{i}=1.03$은 $i$번째 주식이 그날 3퍼센트 상승했음을 의미합니다.

$\mathbf{X} \sim F(\mathbf{x})$라고 가정합니다. 여기서 $F(\mathbf{x})$는 가격 비율 벡터의 결합 분포입니다. 포트폴리오 $\mathbf{b}=\left(b_{1}, b_{2}, \ldots, b_{m}\right), b_{i} \geq 0, \sum b_{i}=1$는 주식 간의 부의 배분입니다. 여기서 $b_{i}$는 자신의 부에서 주식 $i$에 투자하는 비율입니다. 포트폴리오 $\mathbf{b}$를 사용하고 주식 벡터가 $\mathbf{X}$일 때, 부의 비율(하루의 끝 부와 하루의 시작 부의 비율)은 $S=\mathbf{b}^{\prime} \mathbf{X}=\sum_{i=1}^{m} b_{i} X_{i}$입니다.

우리는 어떤 의미에서 $S$를 최대화하고자 합니다. 그러나 $S$는 확률 변수이며, 그 분포는 포트폴리오 $\mathbf{b}$에 따라 달라지므로 논란의 여지가 있습니다.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (C) 2006 John Wiley \& Sons, Inc.
<!-- Page 640 -->

그림 16.1. 샤프-마코위츠 이론: 달성 가능한 평균-분산 쌍의 집합.
$S$에 대한 최적의 분포 선택에 관한 것입니다. 주식 시장 투자에 대한 표준 이론은 $S$의 첫 번째와 두 번째 모멘트 고려에 기반합니다. 목표는 분산에 대한 제약 조건을 만족하면서 $S$의 기대값을 최대화하는 것입니다. 이러한 모멘트를 계산하기 쉽기 때문에, 이 이론은 $S$의 전체 분포를 다루는 이론보다 더 간단합니다.

평균-분산 접근법은 주식 시장 투자에 대한 샤프-마코위츠 이론의 기초이며, 비즈니스 분석가 등이 사용합니다. 이는 그림 16.1에 설명되어 있습니다. 그림은 다양한 포트폴리오를 사용하여 달성 가능한 평균-분산 쌍의 집합을 보여줍니다. 이 영역의 경계에 있는 포트폴리오 집합은 지배받지 않는 포트폴리오에 해당합니다: 이는 주어진 분산에 대해 가장 높은 평균을 갖는 포트폴리오입니다. 이 경계를 효율적 투자선이라고 하며, 평균과 분산에만 관심이 있다면 이 경계를 따라야 합니다.

일반적으로, 무위험 자산(예: 고정 이자율과 제로 분산을 제공하는 현금 또는 국채)의 도입으로 이론이 단순화됩니다. 이 주식은 그림에서 $Y$ 축의 한 점에 해당합니다. 무위험 자산을 다양한 주식과 결합함으로써, 무위험 자산에서 효율적 투자선으로의 접선 아래의 모든 점을 얻게 됩니다. 이 선은 이제 효율적 투자선의 일부가 됩니다.

효율적 투자선의 개념은 또한 주식의 위험에 해당하는 진정한 가격이 존재함을 의미합니다. 이러한 주가 이론을 자본 자산 가격 결정 모형(CAPM)이라고 하며, 주식의 시장 가격이 너무 높거나 낮은지를 결정하는 데 사용됩니다. 확률 변수의 평균을 살펴보는 것은 장기적인 행동에 대한 정보를 제공합니다.
<!-- Page 641 -->
확률 변수의 i.i.d. 버전의 합입니다. 그러나 주식 시장에서는 일반적으로 매일 재투자하므로 $n$일 후의 자산은 각 시장일에 대한 요인의 곱이 됩니다. 곱의 거동은 기댓값에 의해 결정되는 것이 아니라 로그의 기댓값에 의해 결정됩니다. 이를 통해 성장률을 다음과 같이 정의할 수 있습니다.

정의 주식 시장 포트폴리오 $\mathbf{b}$와 주식 분포 $F(\mathbf{x})$에 대한 성장률은 다음과 같이 정의됩니다.

$$
W(\mathbf{b}, F)=\int \log \mathbf{b}^{t} \mathbf{x} d F(\mathbf{x})=E\left(\log \mathbf{b}^{t} \mathbf{X}\right)
$$

로그가 밑 2인 경우 성장률을 배율 성장률이라고도 합니다.

정의 최적 성장률 $W^{*}(F)$는 다음과 같이 정의됩니다.

$$
W^{*}(F)=\max _{\mathbf{b}} W(\mathbf{b}, F)
$$

여기서 최대값은 모든 가능한 포트폴리오 $b_{i} \geq 0, \sum_{i} b_{i}=1$에 대해 계산됩니다.
정의 최대값 $W(\mathbf{b}, F)$를 달성하는 포트폴리오 $\mathbf{b}^{*}$를 로그 최적 포트폴리오 또는 성장 최적 포트폴리오라고 합니다.

성장률의 정의는 다음 정리에 의해 정당화되며, 이 정리는 자산이 $2^{n W^{*}}$로 성장함을 보여줍니다.

정리 16.1.1 $\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}$이 $F(\mathbf{x})$에 따라 i.i.d.라고 가정합니다. 다음을 정의합니다.

$$
S_{n}^{*}=\prod_{i=1}^{n} \mathbf{b}^{* t} \mathbf{X}_{i}
$$

이는 상수 재조정 포트폴리오 $\mathbf{b}^{*}$를 사용한 $n$일 후의 자산입니다. 그러면

$$
\frac{1}{n} \log S_{n}^{*} \rightarrow W^{*} \quad \text { 확률 } 1 \text { 에서 }
$$

증명: 큰 수의 법칙에 따라

$$
\begin{aligned}
\frac{1}{n} \log S_{n}^{*} & =\frac{1}{n} \sum_{i=1}^{n} \log \mathbf{b}^{* t} \mathbf{X}_{i} \\
& \rightarrow W^{*} \quad \text { 확률 } 1 \text { 에서 }
\end{aligned}
$$

따라서 $S_{n}^{*} \doteq 2^{n W^{*}}$입니다.
<!-- Page 642 -->
이제 성장률의 몇 가지 속성을 고려하겠습니다.
정리 16.1.1 $W(\mathbf{b}, F)$는 $\mathbf{b}$에 대해 오목하고 $F$에 대해 선형입니다. $W^{*}(F)$는 $F$에 대해 볼록합니다.

증명: 성장률은 다음과 같습니다.

$$
W(\mathbf{b}, F)=\int \log \mathbf{b}^{t} \mathbf{x} d F(\mathbf{x})
$$

적분은 $F$에 대해 선형이므로 $W(\mathbf{b}, F)$도 $F$에 대해 선형입니다.

$$
\log \left(\lambda \mathbf{b}_{1}+(1-\lambda) \mathbf{b}_{2}\right)^{t} \mathbf{X} \geq \lambda \log \mathbf{b}_{1}^{t} \mathbf{X}+(1-\lambda) \log \mathbf{b}_{2}^{t} \mathbf{X}
$$

로그의 오목성으로 인해 기댓값을 취하면 $W(\mathbf{b}, F)$가 $\mathbf{b}$에 대해 오목하다는 것을 알 수 있습니다. 마지막으로, $F$의 함수로서 $W^{*}(F)$의 볼록성을 증명하기 위해 두 분포를 $F_{1}$과 $F_{2}$로 하고 해당 최적 포트폴리오를 각각 $\mathbf{b}^{*}\left(F_{1}\right)$ 및 $\mathbf{b}^{*}\left(F_{2}\right)$라고 하겠습니다. $\lambda F_{1}+(1-\lambda) F_{2}$에 해당하는 로그 최적 포트폴리오를 $\mathbf{b}^{*}\left(\lambda F_{1}+(1-\lambda) F_{2}\right)$라고 하겠습니다. 그러면 $F$에 대한 $W(\mathbf{b}, F)$의 선형성에 의해 다음이 성립합니다.

$$
\begin{aligned}
& W^{*}\left(\lambda F_{1}+(1-\lambda) F_{2}\right) \\
& \quad=W\left(\mathbf{b}^{*}\left(\lambda F_{1}+(1-\lambda) F_{2}\right), \lambda F_{1}+(1-\lambda) F_{2}\right) \\
& \quad=\lambda W\left(\mathbf{b}^{*}\left(\lambda F_{1}+(1-\lambda) F_{2}\right), F_{1}\right) \\
& \quad+(1-\lambda) W\left(\mathbf{b}^{*}\left(\lambda F_{1}+(1-\lambda) F_{2}\right), F_{2}\right) \\
& \quad \leq \lambda W\left(\mathbf{b}^{*}\left(F_{1}\right), F_{1}\right)+(1-\lambda) W^{*}\left(\mathbf{b}^{*}\left(F_{2}\right), F_{2}\right)
\end{aligned}
$$

$\mathbf{b}^{*}\left(F_{1}\right)$는 $W\left(\mathbf{b}, F_{1}\right)$를 최대화하고 $\mathbf{b}^{*}\left(F_{2}\right)$는 $W\left(\mathbf{b}, F_{2}\right)$를 최대화하기 때문입니다.
정리 16.1.2 주어진 분포에 대한 로그 최적 포트폴리오 집합은 볼록합니다.

증명: $\mathbf{b}_{1}$과 $\mathbf{b}_{2}$가 로그 최적이라고 가정합니다 (즉, $W\left(\mathbf{b}_{1}, F\right)=W\left(\mathbf{b}_{2}, F\right)$ $\left.=W^{*}(F)\right)$. $\mathbf{b}$에 대한 $W(\mathbf{b}, F)$의 오목성에 의해 다음이 성립합니다.

$$
W\left(\lambda \mathbf{b}_{1}+(1-\lambda) \mathbf{b}_{2}, F\right) \geq \lambda W\left(\mathbf{b}_{1}, F\right)+(1-\lambda) W\left(\mathbf{b}_{2}, F\right)=W^{*}(F)
$$

따라서 $\lambda \mathbf{b}_{1}+(1-\lambda) \mathbf{b}_{2}$도 로그 최적입니다.
다음 섹션에서는 이러한 속성을 사용하여 로그 최적 포트폴리오를 특성화합니다.
<!-- Page 643 -->
# 16.2 로그-최적 포트폴리오의 KKT 조건

허용되는 포트폴리오의 집합을 $\mathcal{B}=\left\{\mathbf{b} \in \mathcal{R}^{m}: \mathbf{b}_{i} \geq 0, \sum_{i=1}^{m} \mathbf{b}_{i}=1\right\}$라고 합시다. $W^{*}(F)$를 달성하는 $\mathbf{b}^{*}$의 결정은 볼록 집합 $\mathcal{B}$ 위에서 오목 함수 $W(\mathbf{b}, F)$를 최대화하는 문제입니다. 최대값은 경계에 존재할 수 있습니다. 우리는 표준 KKT 조건을 사용하여 최대값을 특징지을 수 있습니다. 대신, 우리는 이러한 조건을 처음부터 유도합니다.

정리 16.2.1 주식 시장 $\mathbf{X} \sim F$에 대한 로그-최적 포트폴리오 $\mathbf{b}^{*}$ (즉, 성장률 $W(\mathbf{b}, F)$를 최대화하는 포트폴리오)는 다음의 필요충분조건을 만족합니다:

$$
\begin{aligned}
E\left(\frac{X_{i}}{\mathbf{b}^{* t} \mathbf{X}}\right) & =1 & & \text { if } b_{i}^{*}>0 \\
& \leq 1 & & \text { if } b_{i}^{*}=0
\end{aligned}
$$

증명: 성장률 $W(\mathbf{b})=E\left(\ln \mathbf{b}^{t} \mathbf{X}\right)$는 포트폴리오의 단순체에 걸쳐 있는 $\mathbf{b}$에 대해 오목합니다. 따라서 $\mathbf{b}^{*}$는 로그-최적입니다. 왜냐하면 $W(\cdot)$의 방향 미분이 $\mathbf{b}^{*}$에서 임의의 대안 포트폴리오 $\mathbf{b}$ 방향으로 음이 아니기 때문입니다. 따라서, $0 \leq \lambda \leq 1$에 대해 $\mathbf{b}_{\lambda}=(1-\lambda) \mathbf{b}^{*}+\lambda \mathbf{b}$라고 하면,

$$
\left.\frac{d}{d \lambda} W\left(\mathbf{b}_{\lambda}\right)\right|_{\lambda=0+} \leq 0, \quad \mathbf{b} \in \mathcal{B}
$$

이 조건들은 (16.12)로 축소됩니다. 왜냐하면 $W\left(\mathbf{b}_{\lambda}\right)$의 $\lambda=0+$에서의 단측 미분이 다음과 같기 때문입니다:

$$
\begin{aligned}
& \left.\frac{d}{d \lambda} E\left(\ln \left(\mathbf{b}_{\lambda}^{t} \mathbf{X}\right)\right)\right|_{\lambda=0+} \\
& \quad=\lim _{\lambda \downarrow 0} \frac{1}{\lambda} E\left(\ln \left(\frac{(1-\lambda) \mathbf{b}^{* t} \mathbf{X}+\lambda \mathbf{b}^{t} \mathbf{X}}{\mathbf{b}^{* t} \mathbf{X}}\right)\right) \\
& \quad=E\left(\lim _{\lambda \downarrow 0} \frac{1}{\lambda} \ln \left(1+\lambda\left(\frac{\mathbf{b}^{t} \mathbf{X}}{\mathbf{b}^{* t} \mathbf{X}}-1\right)\right)\right) \\
& \quad=E\left(\frac{\mathbf{b}^{t} \mathbf{X}}{\mathbf{b}^{* t} \mathbf{X}}\right)-1
\end{aligned}
$$

여기서 극한과 기댓값의 교환은 지배 수렴 정리 [39]를 사용하여 정당화될 수 있습니다. 따라서 (16.13)은 다음과 같이 축소됩니다:

$$
E\left(\frac{\mathbf{b}^{t} \mathbf{X}}{\mathbf{b}^{* t} \mathbf{X}}\right)-1 \leq 0
$$
<!-- Page 644 -->
모든 $\mathbf{b} \in \mathcal{B}$에 대하여. 만약 $\mathbf{b}$에서 $\mathbf{b}^{*}$까지의 선분이 $\mathbf{b}^{*}$ 너머로 단순체 내에서 연장될 수 있다면, $W\left(\mathbf{b}_{\lambda}\right)$의 $\lambda=0$에서의 양측 미분은 사라지고 (16.17)은 등식으로 성립합니다. 만약 $\mathbf{b}$에 대한 부등식 제약 때문에 $\mathbf{b}$에서 $\mathbf{b}^{*}$까지의 선분이 연장될 수 없다면, (16.17)에는 부등식이 있습니다.

Kuhn-Tucker 조건은 단순체 $\mathcal{B}$의 모든 극점(extreme points)에 대해 성립하면 모든 포트폴리오 $\mathbf{b} \in \mathcal{B}$에 대해 성립할 것입니다. 왜냐하면 $E\left(\mathbf{b}^{i} \mathbf{X} / \mathbf{b}^{* i} \mathbf{X}\right)$는 $\mathbf{b}$에 대해 선형이기 때문입니다. 또한, $j$번째 극점 ( $\mathbf{b}: b_{j}=$ $1, b_{i}=0, i \neq j$ )에서 $\mathbf{b}^{*}$까지의 선분은 $b_{j}^{*}>$ 0일 때만 단순체 내에서 $\mathbf{b}^{*}$ 너머로 연장될 수 있습니다. 따라서, 로그 최적 포트폴리오 $\mathbf{b}^{*}$를 특징짓는 Kuhn-Tucker 조건은 다음의 필요충분조건과 동등합니다:

$$
\begin{aligned}
E\left(\frac{X_{i}}{\mathbf{b}^{* i} \mathbf{X}}\right) & =1 & & \text { if } b_{i}^{*}>0 \\
& \leq 1 & & \text { if } b_{i}^{*}=0
\end{aligned}
$$

이 정리는 몇 가지 즉각적인 결과를 가집니다. 하나의 유용한 동등성은 다음 정리에서 표현됩니다.

정리 16.2.2 로그 최적 포트폴리오 $\mathbf{b}^{*}$로부터 발생하는 랜덤 부(random wealth)를 $S^{*}=\mathbf{b}^{* i} \mathbf{X}$라고 합시다. 다른 포트폴리오 $\mathbf{b}$로부터 발생하는 부를 $S=\mathbf{b}^{t} \mathbf{X}$라고 합시다. 그러면

$$
E \ln \frac{S}{S^{*}} \leq 0 \text { for all } S \Leftrightarrow E \frac{S}{S^{*}} \leq 1 \text { for all } S
$$

증명: 정리 16.2.1로부터 로그 최적 포트폴리오 $\mathbf{b}^{*}$에 대해 다음이 성립합니다.

$$
E\left(\frac{X_{i}}{\mathbf{b}^{* i} \mathbf{X}}\right) \leq 1
$$

모든 $i$에 대하여. 이 방정식을 $b_{i}$로 곱하고 $i$에 대해 합하면 다음과 같습니다.

$$
\sum_{i=1}^{m} b_{i} E\left(\frac{X_{i}}{\mathbf{b}^{* i} \mathbf{X}}\right) \leq \sum_{i=1}^{m} b_{i}=1
$$

이는 다음과 동등합니다.

$$
E \frac{\mathbf{b}^{t} \mathbf{X}}{\mathbf{b}^{* i} \mathbf{X}}=E \frac{S}{S^{*}} \leq 1
$$

역은 Jensen의 부등식으로부터 따릅니다. 왜냐하면

$$
E \log \frac{S}{S^{*}} \leq \log E \frac{S}{S^{*}} \leq \log 1=0
$$
<!-- Page 645 -->
기대 로그를 최대화하는 것은 점근적 성장률에 의해 동기가 부여되었습니다. 그러나 우리는 로그 최적 포트폴리오가 점근적 성장률을 최대화할 뿐만 아니라 하루 동안 기대 부의 상대값 $E\left(S / S^{*}\right)$도 "최대화"한다는 것을 방금 보여주었습니다. 이 포트폴리오의 게임 이론적 최적성을 고려할 때 로그 최적 포트폴리오의 단기 최적성에 대해 더 자세히 설명하겠습니다.

로그 최적 포트폴리오의 쿤-터커(Kuhn-Tucker) 특성화의 또 다른 결과는 로그 최적 포트폴리오 하에서 각 주식에 대한 기대 부의 비율이 매일 변하지 않는다는 사실입니다. 첫날 말에 주식을 고려하십시오. 초기 부 배분은 $\mathbf{b}^{*}$입니다. 하루 말에 주식 $i$의 부 비율은 $\frac{b_{i}^{*} X_{i}}{\mathbf{b}^{* t} \mathbf{X}}$이며, 이 비율의 기대값은 다음과 같습니다.

$$
E \frac{b_{i}^{*} X_{i}}{\mathbf{b}^{* t} \mathbf{X}}=b_{i}^{*} E \frac{X_{i}}{\mathbf{b}^{* t} \mathbf{X}}=b_{i}^{*}
$$

따라서 기대되는 하루 말 주식 $i$의 부 비율은 하루 시작 시 주식 $i$에 투자된 비율과 동일합니다. 이는 켈리 비례 도박(Kelly proportional gambling)에 대한 대응으로, 투자 기간 후 기대값이 변하지 않는 비율에 투자합니다.

# 16.3 로그 최적 포트폴리오의 점근적 최적성

섹션 16.2에서는 로그 최적 포트폴리오를 소개하고 반복적인 독립적인 주식 시장 투자 시퀀스의 장기적 행동 측면에서 그 동기를 설명했습니다. 이 섹션에서는 이 아이디어를 확장하고 조건부 로그 최적 투자자가 인과적 투자 전략을 사용하는 다른 어떤 투자자보다 못하지 않을 것임을 확률 1로 증명합니다.

먼저 i.i.d. 주식 시장(즉, $\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}$은 $F(\mathbf{x})$에 따라 i.i.d.임)을 고려합니다.

$$
S_{n}=\prod_{i=1}^{n} \mathbf{b}_{i}^{t} \mathbf{X}_{i}
$$

는 $i$일에 포트폴리오 $\mathbf{b}_{i}$를 사용하는 투자자의 $n$일 후의 부입니다.

$$
W^{*}=\max _{\mathbf{b}} W(\mathbf{b}, F)=\max _{\mathbf{b}} E \log \mathbf{b}^{t} \mathbf{X}
$$

는 다음과 같습니다.
<!-- Page 646 -->
최대 성장률이 되도록 하고, $\mathbf{b}^{*}$를 최대 성장률을 달성하는 포트폴리오라고 합시다. 우리는 과거에 인과적으로 의존하고 주식 시장의 미래 값과는 독립적인 대안 포트폴리오 $\mathbf{b}_{i}$만을 허용합니다.

정의 비예측 또는 인과적 포트폴리오 전략은 매핑의 시퀀스 $b_{i}: \mathcal{R}^{m(i-1)} \rightarrow \mathcal{B}$이며, 포트폴리오 $b_{i}\left(\mathbf{x}_{1}\right.$, $\left.\ldots, \mathbf{x}_{i-1}\right)$가 $i$일에 사용된다는 해석을 가집니다.

$W^{*}$의 정의로부터 로그 최적 포트폴리오가 최종 자산의 로그 기댓값을 최대화한다는 것이 즉시 도출됩니다. 이는 다음 보조정리에 명시되어 있습니다.

보조정리 16.3.1 i.i.d. 주식에 대해 로그 최적 전략 $\mathbf{b}^{*}$를 사용하여 $n$일 후의 자산을 $S_{n}^{*}$라고 하고, 인과적 포트폴리오 전략 $\mathbf{b}_{i}$를 사용하는 자산을 $S_{n}$이라고 할 때,

$$
E \log S_{n}^{*}=n W^{*} \geq E \log S_{n}
$$

# 증명

$$
\begin{aligned}
\max _{\mathbf{b}_{1}, \mathbf{b}_{2}, \ldots, \mathbf{b}_{n}} E \log S_{n} & =\max _{\mathbf{b}_{1}, \mathbf{b}_{2}, \ldots, \mathbf{b}_{n}} E \sum_{i=1}^{n} \log \mathbf{b}_{i}^{\prime} \mathbf{X}_{i} \\
& =\sum_{i=1}^{n} \max _{\mathbf{b}_{i}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right)} E \log \mathbf{b}_{i}^{\prime}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right) \mathbf{X}_{i} \\
& =\sum_{i=1}^{n} E \log \mathbf{b}^{* t} \mathbf{X}_{i} \\
& =n W^{*}
\end{aligned}
$$

그리고 최대값은 상수 포트폴리오 전략 $\mathbf{b}^{*}$에 의해 달성됩니다.
지금까지 우리는 로그 최적 포트폴리오 정의의 두 가지 간단한 결과를 증명했습니다. 즉, $\mathbf{b}^{*}$((16.12)를 만족하는)가 기대 로그 자산을 최대화하고, 결과 자산 $S_{n}^{*}$이 높은 확률로 지수에서 1차 근사로 $2^{n W^{*}}$와 같다는 것입니다.

이제 우리는 $S_{n}^{*}$이 거의 모든 주식 시장 결과 시퀀스에 대해 다른 모든 투자자의 자산(지수에서 1차 근사로)을 초과한다는 것을 보여주는 훨씬 더 강력한 결과를 증명합니다.

정리 16.3.1 (로그 최적 포트폴리오의 점근적 최적성) $\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}$을 i.i.d. 주식 벡터 시퀀스라고 할 때,
<!-- Page 647 -->
$F(\mathbf{x})$로 정의합니다. $\mathbf{b}^{*}$를 로그 최적 포트폴리오라고 할 때 $S_{n}^{*}=\prod_{i=1}^{n} \mathbf{b}^{* t} \mathbf{X}_{i}$ 이고, 다른 인과적 포트폴리오로부터 발생하는 부를 $S_{n}=\prod_{i=1}^{n} \mathbf{b}_{i}^{t} \mathbf{X}_{i}$ 라고 정의합니다. 그러면

$$
\limsup _{n \rightarrow \infty} \frac{1}{n} \log \frac{S_{n}}{S_{n}^{*}} \leq 0 \quad \text { 확률 1로 }
$$

증명: Kuhn-Tucker 조건과 $S_{n}^{*}$의 로그 최적성으로부터 다음을 얻습니다.

$$
E \frac{S_{n}}{S_{n}^{*}} \leq 1
$$

따라서 Markov 부등식에 의해 다음을 얻습니다.

$$
\operatorname{Pr}\left(S_{n}>t_{n} S_{n}^{*}\right)=\operatorname{Pr}\left(\frac{S_{n}}{S_{n}^{*}}>t_{n}\right)<\frac{1}{t_{n}}
$$

그러므로,

$$
\operatorname{Pr}\left(\frac{1}{n} \log \frac{S_{n}}{S_{n}^{*}}>\frac{1}{n} \log t_{n}\right) \leq \frac{1}{t_{n}}
$$

$t_{n}=n^{2}$으로 설정하고 $n$에 대해 합하면 다음을 얻습니다.

$$
\sum_{n=1}^{\infty} \operatorname{Pr}\left(\frac{1}{n} \log \frac{S_{n}}{S_{n}^{*}}>\frac{2 \log n}{n}\right) \leq \sum_{n=1}^{\infty} \frac{1}{n^{2}}=\frac{\pi^{2}}{6}
$$

그런 다음, Borel-Cantelli 보조정리에 의해 다음을 얻습니다.

$$
\operatorname{Pr}\left(\frac{1}{n} \log \frac{S_{n}}{S_{n}^{*}}>\frac{2 \log n}{n}, \text { 무한히 자주 }\right)=0
$$

이는 주식 시장의 거의 모든 시퀀스에 대해 $N$이 존재하여 모든 $n>N$에 대해 $\frac{1}{n} \log \frac{S_{n}}{S_{n}^{*}}<\frac{2 \log n}{n}$임을 의미합니다. 따라서,

$$
\lim \sup \frac{1}{n} \log \frac{S_{n}}{S_{n}^{*}} \leq 0 \quad \text { 확률 1로 }
$$

이 정리는 로그 최적 포트폴리오가 지수에서 1차적으로 다른 모든 포트폴리오와 같거나 더 나은 성능을 보일 것임을 증명합니다.

# 16.4 부가 정보와 성장률

6장에서 말 경주 $X$에 대한 부가 정보 $Y$가 상호 정보량 $I(X ; Y)$만큼 성장률을 증가시키는 데 사용될 수 있음을 보였습니다.
<!-- Page 648 -->
이제 이 결과를 주식 시장으로 확장합니다. 여기서 $I(X ; Y)$는 성장률 증가에 대한 상한이며, $X$가 경마인 경우 등식이 성립합니다. 먼저 잘못된 분포를 믿음으로써 발생하는 성장률 감소를 고려합니다.

정리 16.4.1 $\mathbf{X} \sim f(\mathbf{x})$라고 가정합니다. $\mathbf{b}_{f}$를 $f(\mathbf{x})$에 해당하는 로그 최적 포트폴리오라고 하고, $\mathbf{b}_{g}$를 다른 밀도 $g(\mathbf{x})$에 해당하는 로그 최적 포트폴리오라고 합니다. 그러면 $\mathbf{b}_{f}$를 $\mathbf{b}_{g}$ 대신 사용함으로써 발생하는 성장률 증가 $\Delta W$는 다음과 같이 제한됩니다.

$$
\Delta W=W\left(\mathbf{b}_{f}, F\right)-W\left(\mathbf{b}_{g}, F\right) \leq D(f \| g)
$$

증명: 다음과 같이 나타낼 수 있습니다.

$$
\begin{aligned}
\Delta W & =\int f(\mathbf{x}) \log \mathbf{b}_{f}^{\prime} \mathbf{x}-\int f(\mathbf{x}) \log \mathbf{b}_{g}^{\prime} \mathbf{x} \\
& =\int f(\mathbf{x}) \log \frac{\mathbf{b}_{f}^{\prime} \mathbf{x}}{\mathbf{b}_{g}^{\prime} \mathbf{x}} \\
& =\int f(\mathbf{x}) \log \frac{\mathbf{b}_{f}^{\prime} \mathbf{x}}{\mathbf{b}_{g}^{\prime} \mathbf{x}} \frac{g(\mathbf{x})}{f(\mathbf{x})} \frac{f(\mathbf{x})}{g(\mathbf{x})} \\
& =\int f(\mathbf{x}) \log \frac{\mathbf{b}_{f}^{\prime} \mathbf{x}}{\mathbf{b}_{g}^{\prime} \mathbf{x}} \frac{g(\mathbf{x})}{f(\mathbf{x})}+D(f \| g) \\
& \stackrel{(a)}{\leq} \log \int f(\mathbf{x}) \frac{\mathbf{b}_{f}^{\prime} \mathbf{x}}{\mathbf{b}_{g}^{\prime} \mathbf{x}} \frac{g(\mathbf{x})}{f(\mathbf{x})}+D(f \| g) \\
& =\log \int g(\mathbf{x}) \frac{\mathbf{b}_{f}^{\prime} \mathbf{x}}{\mathbf{b}_{g}^{\prime} \mathbf{x}}+D(f \| g) \\
& \stackrel{(b)}{\leq} \log 1+D(f \| g) \\
& =D(f \| g),
\end{aligned}
$$

여기서 (a)는 젠센 부등식에 의해, (b)는 쿤-터커 조건과 $\mathbf{b}_{g}$가 $g$에 대해 로그 최적이라는 사실에 의해 도출됩니다.

정리 16.4.2 부가 정보 $Y$로 인한 성장률 증가 $\Delta W$는 다음과 같이 제한됩니다.

$$
\Delta W \leq I(\mathbf{X} ; Y)
$$
<!-- Page 649 -->
증명: $(\mathbf{X}, Y) \sim f(\mathbf{x}, y)$라고 가정합니다. 여기서 $\mathbf{X}$는 시장 벡터이고 $Y$는 관련 부가 정보입니다. 부가 정보 $Y=y$가 주어졌을 때, 로그 최적 투자자는 조건부 분포 $f(\mathbf{x} \mid Y=y)$에 대한 조건부 로그 최적 포트폴리오를 사용합니다. 따라서 $Y=y$에 대한 조건부로, 정리 16.4.1로부터 다음을 얻습니다.

$$
\Delta W_{Y=y} \leq D(f(\mathbf{x} \mid Y=y) \| f(\mathbf{x}))=\int_{\mathbf{x}} f(\mathbf{x} \mid Y=y) \log \frac{f(\mathbf{x} \mid Y=y)}{f(\mathbf{x})} d \mathbf{x}
$$

이를 $Y$의 가능한 값에 대해 평균하면 다음을 얻습니다.

$$
\begin{aligned}
\Delta W & \leq \int_{y} f(y) \int_{\mathbf{x}} f(\mathbf{x} \mid Y=y) \log \frac{f(\mathbf{x} \mid Y=y)}{f(\mathbf{x})} d \mathbf{x} d y \\
& =\int_{y} \int_{\mathbf{x}} f(y) f(\mathbf{x} \mid Y=y) \log \frac{f(\mathbf{x} \mid Y=y)}{f(\mathbf{x})} \frac{f(y)}{f(y)} d \mathbf{x} d y \\
& =\int_{y} \int_{\mathbf{x}} f(\mathbf{x}, y) \log \frac{f(\mathbf{x}, y)}{f(\mathbf{x}) f(y)} d \mathbf{x} d y \\
& =I(\mathbf{X} ; Y)
\end{aligned}
$$

따라서 성장률의 증가는 부가 정보 $Y$와 주식 시장 $\mathbf{X}$ 간의 상호 정보량으로 상한이 결정됩니다.

# 16.5 정상 시장에서의 투자

이제 섹션 16.4의 일부 결과를 i.i.d. 시장에서 시간 종속 시장 프로세스로 확장합니다. $\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}, \ldots$를 벡터값 확률 프로세스로 하고 $\mathbf{X}_{i} \geq 0$이라고 가정합니다. 우리는 시장의 과거 값에 인과적으로 의존하는 투자 전략을 고려합니다 (즉, $\mathbf{b}_{i}$는 $\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}$에 의존할 수 있습니다).

$$
S_{n}=\prod_{i=1}^{n} \mathbf{b}_{i}^{t}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right) \mathbf{X}_{i}
$$

우리의 목표는 모든 인과적 포트폴리오 전략 $\left\{\mathbf{b}_{i}(\cdot)\right\}$에 대해 $E \log S_{n}$을 최대화하는 것입니다. 이제

$$
\begin{aligned}
\max _{\mathbf{b}_{1}, \mathbf{b}_{2}, \ldots, \mathbf{b}_{n}} E \log S_{n} & =\sum_{i=1}^{n} \max _{\mathbf{b}_{i}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right)} E \log \mathbf{b}_{i}^{t} \mathbf{X}_{i} \\
& =\sum_{i=1}^{n} E \log \mathbf{b}_{i}^{* t} \mathbf{X}_{i}
\end{aligned}
$$
<!-- Page 650 -->
여기서 $\mathbf{b}_{i}^{*}$는 $\mathbf{X}_{i}$의 조건부 분포에 대한 로그-최적 포트폴리오이며, 이는 과거 주식 시장 값을 고려한 것입니다. 즉, $\mathbf{b}_{i}^{*}\left(\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{i-1}\right)$는 조건부 최대값을 달성하는 포트폴리오이며, 이는 다음과 같이 표시됩니다.

$$
\begin{aligned}
\max _{\mathbf{b}} E\left[\log \mathbf{b}^{t} \mathbf{X}_{i} \mid\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right)\right. & =\left(\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{i-1}\right) \\
& =W^{*}\left(\mathbf{X}_{i} \mid \mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{i-1}\right)
\end{aligned}
$$

과거에 대한 기댓값을 취하면 다음과 같이 작성할 수 있습니다.

$$
W^{*}\left(\mathbf{X}_{i} \mid \mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right)=E \max _{\mathbf{b}} E\left[\log \mathbf{b}^{t} \mathbf{X}_{i} \mid \mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right]
$$

이는 조건부 최적 성장률이며, 여기서 최대값은 $\mathbf{X}_{1}, \ldots, \mathbf{X}_{i-1}$에 정의된 모든 포트폴리오 값 함수 $\mathbf{b}$에 대한 것입니다. 따라서 가장 높은 기대 로그 수익률은 각 단계에서 조건부 로그-최적 포트폴리오를 사용하여 달성됩니다. 다음을 정의합니다.

$$
W^{*}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}\right)=\max _{\mathbf{b}_{1}, \mathbf{b}_{2}, \ldots, \mathbf{b}_{n}} E \log S_{n}
$$

여기서 최대값은 모든 인과적 포트폴리오 전략에 대한 것입니다. 그러면 $\log S_{n}^{*}=\sum_{i=1}^{m} \log \mathbf{b}_{i}^{* t} \mathbf{X}_{i}$이므로, $W^{*}$에 대한 다음 연쇄 법칙을 얻습니다.

$$
W^{*}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}\right)=\sum_{i=1}^{n} W^{*}\left(\mathbf{X}_{i} \mid \mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right)
$$

이 연쇄 법칙은 형식적으로 $H$의 연쇄 법칙과 동일합니다. 어떤 면에서 $W$는 $H$의 쌍대입니다. 특히, 조건화는 $H$를 감소시키지만 $W$를 증가시킵니다. 이제 시간 의존적 확률 과정에 대한 엔트로피율의 대응물을 정의합니다.

정의 성장률 $W_{\infty}^{*}$는 다음과 같이 정의됩니다.

$$
W_{\infty}^{*}=\lim _{n \rightarrow \infty} \frac{W^{*}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}\right)}{n}
$$

만약 극한이 존재한다면.
정리 16.5.1 정상 시장의 경우 성장률이 존재하며 다음과 같습니다.

$$
W_{\infty}^{*}=\lim _{n \rightarrow \infty} W^{*}\left(\mathbf{X}_{n} \mid \mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n-1}\right)
$$
<!-- Page 651 -->
증명: 정상성(stationarity)에 의해 $W^{*}\left(\mathbf{X}_{n} \mid \mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n-1}\right)$은 $n$에 대해 비감소(nondecreasing)합니다. 따라서, 무한대일 가능성이 있는 극한값을 가집니다. 다음이므로,

$$
\frac{W^{*}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}\right)}{n}=\frac{1}{n} \sum_{i=1}^{n} W^{*}\left(\mathbf{X}_{i} \mid \mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right)
$$

체사로 평균 정리(Cesáro mean theorem, Theorem 4.2.3)에 의해 좌변은 우변 항들의 극한과 동일한 극한을 가집니다. 따라서, $W_{\infty}^{*}$가 존재하며,

$$
W_{\infty}^{*}=\lim _{n \rightarrow \infty} \frac{W^{*}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}\right)}{n}=\lim _{n \rightarrow \infty} W^{*}\left(\mathbf{X}_{n} \mid \mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n-1}\right)
$$

이제 우리는 점근적 최적성(asymptotic optimality) 속성을 정상 시장(stationary markets)으로 확장할 수 있습니다. 다음 정리를 고려합니다.

정리 16.5.2 임의의 확률 과정 $\left\{X_{i}\right\}, X_{i} \in \mathcal{R}_{+}^{m}$, 조건부 로그 최적 포트폴리오(conditionally log-optimal portfolios), $\mathbf{b}_{i}^{*}\left(X^{i-1}\right)$ 및 부(wealth) $S_{n}^{*}$를 고려하십시오. $S_{n}$은 다른 모든 인과적 포트폴리오 전략(causal portfolio strategy) $\mathbf{b}_{i}\left(X^{i-1}\right)$에 의해 생성된 부라고 가정합니다. 그러면 $S_{n} / S_{n}^{*}$는 과거 $X_{1}, X_{2}, \ldots, X_{n}$에 의해 생성된 $\sigma$-필드( $\sigma$-fields)의 순서에 대해 양수 초마팅게일(positive supermartingale)입니다. 결과적으로, 다음을 만족하는 확률 변수 $V$가 존재합니다.

$$
\begin{aligned}
\frac{S_{n}}{S_{n}^{*}} \rightarrow V & \text { 거의 확실하게 (with probability } 1) \\
E V & \leq 1
\end{aligned}
$$

그리고

$$
\operatorname{Pr}\left\{\sup _{n} \frac{S_{n}}{S_{n}^{*}} \geq t\right\} \leq \frac{1}{t}
$$

증명: $S_{n} / S_{n}^{*}$는 양수 초마팅게일입니다. 왜냐하면,

$$
\begin{aligned}
E\left[\left.\frac{S_{n+1}\left(X^{n+1}\right)}{S_{n+1}^{*}\left(X^{n+1}\right)}\right| X^{n}\right] & =E\left[\left.\frac{\left(\mathbf{b}_{n+1}^{t} \mathbf{X}_{n+1}\right) S_{n}\left(X^{n}\right)}{\left(\mathbf{b}_{n+1}^{* t} \mathbf{X}_{n+1}\right) S_{n}^{*}\left(X^{n}\right)}\right| X^{n}\right] \\
& =\frac{S_{n}\left(X^{n}\right)}{S_{n}^{*}\left(X^{n}\right)} E\left[\left.\frac{\mathbf{b}_{n+1}^{t} \mathbf{X}_{n+1}}{\mathbf{b}_{n+1}^{* t} \mathbf{X}_{n+1}}\right| X^{n}\right] \\
& \leq \frac{S_{n}\left(X^{n}\right)}{S_{n}^{*}\left(X^{n}\right)}
\end{aligned}
$$
<!-- Page 652 -->
쿠엔-터커 조건에 따라 조건부 로그 최적 포트폴리오에 대한 것입니다. 따라서 마틴게일 수렴 정리에 의해 $S_{n} / S_{n}^{*}$는 한계값 $V$를 가지며, $E V \leq E\left(S_{0} / S_{0}^{*}\right)=1$입니다. 마지막으로, $\sup \left(S_{n} / S_{n}^{*}\right)$에 대한 결과는 양수 마틴게일에 대한 콜모고로프의 부등식에서 나옵니다.

(16.70)은 $S_{n}^{*}$의 경쟁적 최적성이 얼마나 강한지를 보여줍니다. 명백히, $S_{n}\left(X^{n}\right)$이 $S_{n}^{*}\left(X^{n}\right)$보다 10배 커질 확률은 1/10보다 작습니다. 정상 순환 시장의 경우, 점근적 균등 분할 속성을 확장하여 다음 정리를 증명할 수 있습니다.

정리 16.5.3 (주식 시장에 대한 AEP) $\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}$을 정상 순환 벡터값 확률 과정이라고 가정합니다. $S_{n}^{*}$를 조건부 로그 최적 전략에 대한 시간 $n$에서의 부라고 할 때,

$$
S_{n}^{*}=\prod_{i=1}^{n} \mathbf{b}_{i}^{* t}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right) \mathbf{X}_{i}
$$

그러면

$$
\frac{1}{n} \log S_{n}^{*} \rightarrow W_{\infty}^{*} \quad \text { 확률 } 1 \text{ 로}
$$

증명: 증명은 AEP를 섹션 16.8에서 증명하는 데 사용된 샌드위치 논증 [20]의 일반화를 포함합니다. 증명의 세부 사항 (Algoet 및 Cover [21]에 있음)은 생략합니다.

마지막으로, 경마의 예를 다시 살펴봅니다. 경마는 $m$개의 말에 해당하는 $m$개의 주식이 있는 주식 시장의 특수한 경우입니다. 경주가 끝나면 말 $i$에 대한 주식의 가치는 0이거나 배당률 $o_{i}$입니다. 따라서 $\mathbf{X}$는 우승마에 해당하는 구성 요소에서만 0이 아닙니다.

이 경우, 로그 최적 포트폴리오는 비례 베팅, 즉 켈리 도박 (즉, $b_{i}^{*}=p_{i}$)이며, 균등한 공정한 배당률 (즉, 모든 $i$에 대해 $o_{i}=m$)의 경우,

$$
W^{*}=\log m-H(X)
$$

상관관계가 있는 일련의 경마가 있을 때, 최적 포트폴리오는 조건부 비례 베팅이며 점근적 성장률은 다음과 같습니다.

$$
W_{\infty}^{*}=\log m-H(\mathcal{X})
$$
<!-- Page 653 -->
여기서 $H(\mathcal{X})=\lim \frac{1}{n} H\left(X_{1}, X_{2}, \ldots, X_{n}\right)$ 이며, 극한값이 존재한다고 가정합니다. 그러면 정리 16.5.3은 다음과 같이 주장합니다.

$$
S_{n}^{*} \doteq 2^{n W^{*}}
$$

이는 6장에서의 결과와 일치합니다.

# 16.6 로그-최적 포트폴리오의 경쟁적 최적성

이제 로그-최적 포트폴리오가 주어진 유한 시간 $n$에서 대안 포트폴리오보다 우수한지 여부를 질문합니다. Kuhn-Tucker 조건의 직접적인 결과로, 우리는 다음과 같은 결과를 얻습니다.

$$
E \frac{S_{n}}{S_{n}^{*}} \leq 1
$$

따라서 Markov의 부등식에 의해,

$$
\operatorname{Pr}\left(S_{n}>t S_{n}^{*}\right) \leq \frac{1}{t}
$$

이 결과는 Shannon 코드의 경쟁적 최적성에 대한 5장에서 도출된 결과와 유사합니다.

예시를 고려함으로써, $S_{n}>S_{n}^{*}$일 확률에 대해 더 나은 경계를 얻는 것이 불가능하다는 것을 알 수 있습니다. 두 개의 주식과 두 개의 가능한 결과가 있는 주식 시장을 고려해 봅시다.

$$
\left(X_{1}, X_{2}\right)= \begin{cases}\left(1, \frac{1}{1-\epsilon}\right) & \text { 확률 } 1-\epsilon \\ (1,0) & \text { 확률 } \epsilon\end{cases}
$$

이 시장에서 로그-최적 포트폴리오는 모든 자산을 첫 번째 주식에 투자합니다. [$\mathbf{b}=(1,0)$이 Kuhn-Tucker 조건을 만족한다는 것을 쉽게 확인할 수 있습니다.] 그러나 두 번째 주식에 모든 자산을 투자한 투자자는 확률 $1-\epsilon$로 더 많은 돈을 벌게 됩니다. 따라서 높은 확률로 로그-최적 투자자가 다른 어떤 투자자보다 더 잘할 것이라는 것은 사실이 아닙니다.

로그-최적 투자자가 확률 $\frac{1}{2}$ 이상으로 가장 잘 수행한다는 것을 증명하려고 할 때의 문제는, 위와 같은 예시가 존재하여 대부분의 시간 동안 약간의 차이로 로그-최적 투자자를 능가하는 것이 가능하다는 것입니다. 우리는 각 투자자에게 추가적인 공정한 무작위화를 허용함으로써 이를 극복할 수 있으며, 이는 자산의 작은 차이의 영향을 줄이는 효과가 있습니다.
<!-- Page 654 -->
제16.6.1 정리 (경쟁 최적성) $S^{*}$를 로그-최적 포트폴리오를 가진 주식 시장 $\mathbf{X}$에서의 투자 한 기간 후의 자산이라고 하고, $S$를 다른 어떤 포트폴리오에 의해 유도된 자산이라고 합시다. $U^{*}$를 $[0,2]$ 상에서 균등 분포하는 $\mathbf{X}$와 독립인 난수 변수라고 하고, $V$를 $\mathbf{X}$와 $U^{*}$와 독립이며 $V \geq 0$이고 $E V=1$인 다른 어떤 난수 변수라고 합시다. 그러면

$$
\operatorname{Pr}\left(V S \geq U^{*} S^{*}\right) \leq \frac{1}{2}
$$

비고 여기에서 $U^{*}$와 $V$는 초기 자산의 "공정한" 난수화를 나타냅니다. 초기 자산 $S_{0}=1$을 "공정한" 자산 $U^{*}$로 교환하는 것은 공정한 베팅을 함으로써 실제로 달성될 수 있습니다. 공정한 난수화의 효과는 작은 차이를 난수화하여, 비율 $S / S^{*}$의 유의미한 편차만이 승리 확률에 영향을 미치도록 하는 것입니다.

증명: 우리는 다음과 같이 쓸 수 있습니다.

$$
\begin{aligned}
\operatorname{Pr}\left(V S \geq U^{*} S^{*}\right) & =\operatorname{Pr}\left(\frac{V S}{S^{*}} \geq U^{*}\right) \\
& =\operatorname{Pr}\left(W \geq U^{*}\right)
\end{aligned}
$$

여기서 $W=\frac{V S}{S^{*}}$는 평균이 다음과 같은 음이 아닌 값의 난수 변수입니다.

$$
E W=E(V) E\left(\frac{S_{n}}{S_{n}^{*}}\right) \leq 1
$$

이는 $V$가 $\mathbf{X}$와 독립이고 Kuhn-Tucker 조건에 의해 성립합니다. $W$의 분포 함수를 $F$라고 하면, $U^{*}$가 $[0,2]$ 상에서 균등 분포하므로,

$$
\begin{aligned}
\operatorname{Pr}\left(W \geq U^{*}\right) & =\int_{0}^{2} \operatorname{Pr}(W>w) f_{U^{*}}(w) d w \\
& =\int_{0}^{2} \operatorname{Pr}(W>w) \frac{1}{2} d w \\
& =\int_{0}^{2} \frac{1-F(w)}{2} d w \\
& \leq \int_{0}^{\infty} \frac{1-F(w)}{2} d w \\
& =\frac{1}{2} E W \\
& \leq \frac{1}{2}
\end{aligned}
$$
<!-- Page 655 -->
적분 구간을 부분 적분하여 쉽게 증명되는 사실을 사용하여

$$
E W=\int_{0}^{\infty}(1-F(w)) d w
$$

양수 확률 변수 $W$에 대해. 따라서 우리는 다음과 같이 말할 수 있습니다.

$$
\operatorname{Pr}\left(V S \geq U^{*} S^{*}\right)=\operatorname{Pr}\left(W \geq U^{*}\right) \leq \frac{1}{2}
$$

정리 16.6.1은 로그 최적 포트폴리오 사용에 대한 단기적인 정당성을 제공합니다. 투자자의 유일한 목표가 주식 시장에서 하루가 끝날 때 상대방보다 앞서는 것이고 공정한 무작위화가 허용된다면, 정리 16.6.1은 투자자가 자신의 부를 $[0,2]$ 균등 분포의 부로 교환한 다음 로그 최적 포트폴리오를 사용하여 투자해야 한다고 말합니다. 이것은 주식 시장에서 경쟁적으로 도박하는 문제에 대한 게임 이론적 해결책입니다.

# 16.7 보편 포트폴리오

16.1절에서 로그 최적 포트폴리오 전략의 개발은 주식 벡터의 분포를 알고 따라서 최적 포트폴리오 $\mathbf{b}^{*}$를 계산할 수 있다는 가정에 의존합니다. 그러나 실제로는 분포를 모르는 경우가 많습니다. 이 섹션에서는 개별 시퀀스에서 잘 수행되는 인과적 포트폴리오를 설명합니다. 따라서 시장 시퀀스에 대한 통계적 가정을 하지 않습니다. 주식 시장을 벡터 시퀀스 $\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots \in \mathcal{R}_{+}^{m}$로 표현할 수 있다고 가정합니다. 여기서 $x_{i j}$는 $i$일째 주식 $j$의 가격 비율이고 $\mathbf{x}_{i}$는 $i$일째 모든 주식의 가격 비율 벡터입니다. 유한 기간 문제부터 시작하여 $n$개의 벡터 $\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}$을 다룹니다. 나중에 결과를 무한 기간 사례로 확장합니다.

이러한 주식 시장 결과 시퀀스가 주어졌을 때, 우리가 할 수 있는 최선은 무엇입니까? 현실적인 목표는 최적의 상수 재조정 포트폴리오 전략이 달성한 성장입니다 (즉, 알려진 주식 시장 벡터 시퀀스에서 최적의 상수 재조정 포트폴리오). 상수 재조정 포트폴리오는 알려진 분포를 가진 i.i.d. 주식 시장 시퀀스에 대해 최적이기 때문에 이 포트폴리오 집합은 상당히 자연스럽습니다.

각각 사전에 선택된 상수 재조정 포트폴리오 전략을 따르는 여러 mutual fund가 있다고 가정해 보겠습니다. 우리의 목표는 이 펀드 중 최상의 펀드만큼 잘 수행하는 것입니다. 이 섹션에서는 최상의 상수 재조정 포트폴리오만큼 거의 잘 수행할 수 있음을 보여줍니다.
<!-- Page 656 -->
주식 시장 벡터의 분포에 대한 사전 지식 없이 포트폴리오를 구성합니다.

한 가지 접근 방식은 각기 다른 지속적으로 재조정되는 포트폴리오 전략을 따르는 펀드 매니저들의 연속체에 부를 분배하는 것입니다. 매니저 중 한 명이 다른 매니저들보다 기하급수적으로 더 나은 성과를 낼 것이므로, $n$일 후의 총 부는 가장 큰 항에 의해 결정될 것입니다. 우리는 $n^{\frac{m-1}{2}}$의 인자 내에서 최고의 펀드 매니저의 성과를 달성할 수 있음을 보여줄 것입니다. 이것이 무기한 보편 포트폴리오 전략에 대한 논증의 핵심입니다.

이 문제에 대한 두 번째 접근 방식은 악의적인 상대방 또는 자연과의 게임으로 간주하는 것입니다. 여기서 상대방 또는 자연은 주식 시장 벡터의 순서를 선택할 수 있습니다. 우리는 과거 주식 시장 시퀀스의 값에만 의존하는 인과적(선견지명이 없는) 포트폴리오 전략 $\hat{\mathbf{b}}_{i}\left(\mathbf{x}_{i-1}, \ldots, \mathbf{x}_{1}\right)$를 정의합니다. 그러면 전략 $\hat{\mathbf{b}}_{i}\left(\mathbf{x}^{i-1}\right)$를 알고 있는 자연은 전략의 성과가 해당 주식 시퀀스에 대한 최고의 지속적으로 재조정되는 포트폴리오에 비해 가능한 한 나쁘도록 벡터 시퀀스 $\mathbf{x}_{i}$를 선택합니다. $\mathbf{b}^{*}\left(\mathbf{x}^{n}\right)$를 주식 시장 시퀀스 $\mathbf{x}^{n}$에 대한 최고의 지속적으로 재조정되는 포트폴리오라고 합시다. $\mathbf{b}^{*}\left(\mathbf{x}^{n}\right)$는 벡터가 발생하는 순서가 아니라 시퀀스의 경험적 분포에만 의존한다는 점에 유의하십시오. $n$일이 끝날 때, 지속적으로 재조정되는 포트폴리오 $\mathbf{b}$는 다음과 같은 부를 달성합니다.

$$
S_{n}\left(\mathbf{b}, \mathbf{x}^{n}\right)=\prod_{i=1}^{n} \mathbf{b}^{i} \mathbf{x}_{i}
$$

그리고 최고의 상수 포트폴리오 $\mathbf{b}^{*}\left(\mathbf{x}^{n}\right)$는 다음과 같은 부를 달성합니다.

$$
S_{n}^{*}\left(\mathbf{x}^{n}\right)=\max _{\mathbf{b}} \prod_{i=1}^{n} \mathbf{b}^{i} \mathbf{x}_{i}
$$

반면에 선견지명이 없는 포트폴리오 $\hat{\mathbf{b}}_{i}\left(\mathbf{x}^{i-1}\right)$ 전략은 다음과 같은 부를 달성합니다.

$$
\hat{S}_{n}\left(\mathbf{x}^{n}\right)=\prod_{i=1}^{n} \hat{\mathbf{b}}_{i}^{i}\left(\mathbf{x}^{i-1}\right) \mathbf{x}_{i}
$$

우리의 목표는 $\hat{S}_{n}$과 $S_{n}^{*}$의 비율 측면에서 최악의 경우에 잘 수행되는 선견지명이 없는 포트폴리오 전략 $\hat{\mathbf{b}}(\cdot)=\left(\hat{\mathbf{b}}_{1}\right.$, $\left.\hat{\mathbf{b}}_{2}\left(\mathbf{x}_{1}\right), \ldots, \hat{\mathbf{b}}_{i}\left(\mathbf{x}^{i-1}\right)\right)$를 찾는 것입니다. 우리는 최적의 보편 전략을 찾고 이 전략이 각 주식 시퀀스에 대해 해당 시퀀스의 최고의 지속적으로 재조정되는 포트폴리오가 달성한 부 $S_{n}^{*}$에 $V_{n} \approx n^{-\frac{m-1}{2}}$의 인자 내에 있는 부 $\hat{S}_{n}$를 달성함을 보여줄 것입니다. 이 전략은 기간인 $n$에 의존합니다.
<!-- Page 657 -->
게임입니다. 나중에 유한 구간 게임과 동일한 최악의 점근적 성능을 갖는 구간 없는 결과에 대해 설명합니다.

# 16.7.1 유한 구간 보편 포트폴리오

미리 알려진 $n$ 기간의 주식 시장을 분석하는 것으로 시작하며, $n$개의 가능한 모든 주식 시장 벡터 시퀀스에 대해 잘 작동하는 포트폴리오 전략을 찾으려고 합니다. 주요 결과는 다음 정리로 요약할 수 있습니다.

정리 16.7.1 $m$개의 자산에 대한 길이 $n$의 주식 시장 시퀀스 $\mathbf{x}^{n}=\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}, \mathbf{x}_{i} \in \mathcal{R}_{+}^{m}$에 대해, $\mathbf{x}^{n}$에서 최적의 상수 재조정 포트폴리오로 달성된 부를 $S_{n}^{*}\left(\mathbf{x}^{n}\right)$라고 하고, $\mathbf{x}^{n}$에서 임의의 인과적 포트폴리오 전략 $\hat{\mathbf{b}}_{i}(\cdot)$로 달성된 부를 $\hat{S}_{n}\left(\mathbf{x}^{n}\right)$라고 할 때,

$$
\max _{\hat{\mathbf{b}}_{i}(\cdot)} \min _{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}} \frac{\hat{S}_{n}\left(\mathbf{x}^{n}\right)}{S_{n}^{*}\left(\mathbf{x}^{n}\right)}=V_{n}
$$

여기서

$$
V_{n}=\left[\sum_{n_{1}+\cdots+n_{m}=n}\binom{n}{n_{1}, n_{2}, \ldots, n_{m}} 2^{-n H\left(\frac{n_{1}}{n}, \ldots, \frac{n_{m}}{n}\right)}\right]^{-1}
$$

스털링 근사를 사용하면 $V_{n}$이 $n^{-\frac{m-1}{2}}$의 차수임을 보일 수 있습니다. 따라서 최악의 시퀀스에 대한 보편 포트폴리오의 성장률은 해당 시퀀스에 대한 최적의 상수 재조정 포트폴리오의 성장률과 다항식 인자 이상으로 차이가 납니다. 보편 포트폴리오 $\hat{\mathbf{b}}$의 부의 성장과 최적의 상수 포트폴리오의 부의 성장 비율의 로그는 보편 소스 코드의 중복도와 유사하게 동작합니다. (데이터 압축에서 로그 $V_{n}$이 최소-최대 개별 시퀀스 중복도로 나타나는 Shtarkov [496] 참조.)

먼저 $n=1$에 대한 예시를 통해 주요 결과를 설명합니다. 두 개의 주식과 하루의 경우를 고려해 봅시다. 해당 날짜의 주식 벡터를 $\mathbf{x}=\left(x_{1}, x_{2}\right)$라고 합시다. 만약 $x_{1}>x_{2}$이면, 주식 1에 모든 돈을 투자하는 것이 최적의 포트폴리오이고, 만약 $x_{2}>x_{1}$이면, 주식 2에 모든 돈을 투자하는 것이 최적의 포트폴리오입니다. (만약 $x_{1}=x_{2}$이면, 모든 포트폴리오는 동등합니다.)

이제 우리가 미리 포트폴리오를 선택해야 하고, 우리의 상대방이 우리가 포트폴리오를 선택한 후에 최악의 결과를 초래하도록 주식 시장 시퀀스를 선택할 수 있다고 가정해 봅시다. 우리의 포트폴리오가 주어졌을 때, 상대방은 우리가 더 많은 비중을 둔 주식을 0으로 만들고 다른 주식을 1로 만들어 우리가 가능한 한 최악의 결과를 겪도록 보장할 수 있습니다. 따라서 우리의 최선의 전략은 동일한

<!-- Page 658 -->
양쪽 주식에 가중치를 두어, 최고의 주식 성장률의 절반 이상에 해당하는 성장률을 달성할 것이며, 따라서 끊임없이 재조정되는 최적 포트폴리오의 이득의 절반 이상을 달성할 것입니다. (16.94) 식에서 $n=1$이고 $m=2$일 때 $V_{n}=2$임을 계산하는 것은 어렵지 않습니다.

그러나 이 결과는 오해의 소지가 있는 것처럼 보입니다. 왜냐하면 $n$일 동안 매일 두 주식에 절반씩 투자하는 일정한 균등 포트폴리오를 사용할 것을 시사하는 것처럼 보이기 때문입니다. 만약 상대방이 매일 첫 번째 주식만 1(나머지는 0)이 되도록 주식 시퀀스를 선택한다면, 이 균등 전략은 $1/2^n$의 자산을 달성할 것이고, 우리는 모든 시간 동안 첫 번째 주식에 모든 돈을 투자하는 최적의 상수 포트폴리오와 비교하여 $2^n$의 요인 내에서만 자산을 달성할 것입니다.

정리의 결과는 우리가 훨씬 더 나은 성과를 낼 수 있음을 보여줍니다. 주장의 주요 부분은 주식 벡터 시퀀스를 각 날짜에 하나의 주식만 0이 아닌 극단적인 경우로 줄이는 것입니다. 만약 우리가 그러한 시퀀스에서 좋은 성과를 보장할 수 있다면, 우리는 모든 주식 벡터 시퀀스에서 좋은 성과를 보장하고 정리의 경계를 달성할 수 있습니다.

정리를 증명하기 전에 다음 보조 정리가 필요합니다.
보조 정리 16.7.1 $p_{1}, p_{2}, \ldots, p_{m} \geq 0$이고 $q_{1}, q_{2}, \ldots, q_{m} \geq 0$일 때,

$$
\frac{\sum_{i=1}^{m} p_{i}}{\sum_{i=1}^{m} q_{i}} \geq \min _{i} \frac{p_{i}}{q_{i}}
$$

증명: (16.96)의 우변을 최소화하는 인덱스 $i$를 $I$라고 합시다. $p_{I}>0$이라고 가정합니다 (만약 $p_{I}=0$이면, 보조 정리는 자명하게 참입니다). 또한, $q_{I}=0$이면, (16.96)의 양변은 무한대가 됩니다 (다른 모든 $q_{i}$들도 0이어야 합니다). 이 경우에도 부등식은 성립합니다. 따라서 $q_{I}>0$이라고 가정할 수도 있습니다. 그러면

$$
\frac{\sum_{i=1}^{m} p_{i}}{\sum_{i=1}^{m} q_{i}}=\frac{p_{I}}{q_{I}} \frac{1+\sum_{i \neq I}\left(p_{i} / p_{I}\right)}{1+\sum_{i \neq I}\left(q_{i} / q_{I}\right)} \geq \frac{p_{I}}{q_{I}}
$$

왜냐하면

$$
\frac{p_{i}}{q_{i}} \geq \frac{p_{I}}{q_{I}} \longrightarrow \frac{p_{i}}{p_{I}} \geq \frac{q_{i}}{q_{I}}
$$

모든 $i$에 대해 성립하기 때문입니다.
먼저 $n=1$인 경우를 고려합니다. 첫날 말의 자산은 다음과 같습니다.

$$
\begin{aligned}
& \hat{S}_{1}(\mathbf{x})=\hat{\mathbf{b}}^{t} \mathbf{x} \\
& S_{1}(\mathbf{x})=\mathbf{b}^{t} \mathbf{x}
\end{aligned}
$$
<!-- Page 659 -->
그리고

$$
\frac{\hat{S}_{1}(\mathbf{x})}{S_{1}(\mathbf{x})}=\frac{\sum \hat{b}_{i} x_{i}}{\sum b_{i} x_{i}} \geq \min \left\{\frac{\hat{b}_{i}}{b_{i}}\right\}
$$

우리는 $\max _{\hat{\mathbf{b}}} \min _{\mathbf{b}, \mathbf{x}} \frac{\hat{\mathbf{b}}^{\prime} \mathbf{x}}{\mathbf{b}^{\prime} \mathbf{x}}$를 찾고자 합니다. 자연은 $\frac{\hat{b}_{i}}{b_{i}^{2}}$를 최소화하는 $i$번째 단위 벡터 $\mathbf{e}_{i}$를 선택할 것이며, 투자자는 이 최솟값을 최대화하기 위해 $\hat{\mathbf{b}}$를 선택할 것입니다. 이는 $\hat{\mathbf{b}}=\left(\frac{1}{m}, \frac{1}{m}, \ldots, \frac{1}{m}\right)$를 선택함으로써 달성됩니다.

깨달아야 할 중요한 점은 다음과 같습니다.

$$
\frac{\hat{S}_{n}\left(\mathbf{x}^{n}\right)}{S_{n}\left(\mathbf{x}^{n}\right)}=\frac{\prod_{i=1}^{n} \hat{\mathbf{b}}_{i}^{\prime} \mathbf{x}_{i}}{\prod_{i=1}^{n} \mathbf{b}_{i}^{\prime} \mathbf{x}_{i}}
$$

또한 항들의 비율 형태로 다시 쓸 수 있습니다.

$$
\frac{\hat{S}_{n}\left(\mathbf{x}^{n}\right)}{S_{n}\left(\mathbf{x}^{n}\right)}=\frac{\hat{\mathbf{b}}^{\prime} \mathbf{x}^{\prime}}{\mathbf{b}^{\prime} \mathbf{x}^{\prime}}
$$

여기서 $\hat{\mathbf{b}}, \mathbf{b}, \mathbf{x}^{\prime} \in \mathcal{R}_{+}^{m^{n}}$입니다. 여기서 끊임없이 재조정되는 포트폴리오 $\mathbf{b}$의 $m^{n}$개의 구성 요소는 모두 $b_{1}^{n_{1}} b_{2}^{n_{2}} \cdots b_{m}^{n_{m}}$ 형태의 곱입니다. 끊임없이 재조정되는 포트폴리오에 해당하는 $\mathbf{b}$들에 균일하게 가까운 보편적인 $\hat{\mathbf{b}}$를 찾고자 합니다.

이제 주요 정리(정리 16.7.1)를 증명할 수 있습니다.
정리 16.7.1 증명: $m=2$인 경우에 대해 정리를 증명하겠습니다. 증명은 $m>2$인 경우에도 간단하게 확장됩니다. 주식을 1과 2로 표시합니다. 핵심 아이디어는 시간 $n$에서의 자산,

$$
S_{n}\left(\mathbf{x}^{n}\right)=\prod_{i=1}^{n} \mathbf{b}_{i}^{\prime} \mathbf{x}_{i}
$$

을 합의 곱 형태로 변환하는 것입니다. 합의 각 항은 주식 1 또는 주식 2의 가격 상대값의 시퀀스에 전략이 시간 $i$에서 주식 1 또는 주식 2에 할당하는 비율 $b_{i 1}$ 또는 $b_{i 2}$를 곱한 것에 해당합니다. 따라서 자산 $S_{n}$을 모든 $2^{n}$개의 가능한 $n$-시퀀스(1 또는 2)에 대한 포트폴리오 비율의 곱과 주식 가격 상대값의 곱의 합으로 볼 수 있습니다.

$$
S_{n}\left(\mathbf{x}^{n}\right)=\sum_{j^{n} \in\{1,2\}^{n}} \prod_{i=1}^{n} b_{i j_{i}} x_{i j_{i}}=\sum_{j^{n} \in\{1,2\}^{n}} \prod_{i=1}^{n} b_{i j_{i}} \prod_{i=1}^{n} x_{i j_{i}}
$$
<!-- Page 660 -->
$w\left(j^{n}\right)$를 $i=1$부터 $n$까지 $\prod_{i=1}^{n} b_{i j_{i}}$로 정의하고, 이는 시퀀스 $j^{n}$에 투자된 총 자산의 비율이며, 해당 시퀀스의 수익률인

$$
x\left(j^{n}\right)=\prod_{i=1}^{n} x_{i j_{i}}
$$

를 정의하면 다음과 같이 쓸 수 있습니다.

$$
S_{n}\left(\mathbf{x}^{n}\right)=\sum_{j^{n} \in\{1,2\}^{n}} w\left(j^{n}\right) x\left(j^{n}\right)
$$

유사한 표현이 최적의 상수 재조정 포트폴리오와 범용 포트폴리오 전략 모두에 적용됩니다. 따라서 다음과 같이 쓸 수 있습니다.

$$
\frac{\hat{S}_{n}\left(\mathbf{x}^{n}\right)}{S_{n}^{*}\left(\mathbf{x}^{n}\right)}=\frac{\sum_{j^{n} \in\{1,2\}^{n}} \hat{w}\left(j^{n}\right) x\left(j^{n}\right)}{\sum_{j^{n} \in\{1,2\}^{n}} w^{*}\left(j^{n}\right) x\left(j^{n}\right)}
$$

여기서 $\hat{w}^{n}$은 범용 비예측 전략이 시퀀스 $j^{n}$에 투자한 자산의 양이고, $w^{*}\left(j^{n}\right)$은 최적의 상수 재조정 포트폴리오 전략이 투자한 양입니다. 이제 Lemma 16.7.1을 적용하면 다음과 같습니다.

$$
\frac{\hat{S}_{n}\left(\mathbf{x}^{n}\right)}{S_{n}^{*}\left(\mathbf{x}^{n}\right)} \geq \min _{j^{n}} \frac{\hat{w}\left(j^{n}\right) x\left(j^{n}\right)}{w^{*}\left(j^{n}\right) x\left(j^{n}\right)}=\min _{j^{n}} \frac{\hat{w}\left(j^{n}\right)}{w^{*}\left(j^{n}\right)}
$$

따라서 성능 비율 $\hat{S}_{n} / S_{n}^{*}$을 최대화하는 문제는 범용 포트폴리오가 주식 시퀀스에 베팅하는 금액의 비율이 $\mathbf{b}^{*}$이 베팅하는 비율에 균일하게 가깝도록 보장하는 것으로 축소됩니다. 이제 명백할 수 있듯이, $S_{n}$의 이러한 공식은 $n$기간 주식 시장을 단일 기간 주식 시장의 특수한 경우로 축소합니다. 즉, $2^{n}$개의 주식이 있고, 주식 $j^{n}$에 $w\left(j^{n}\right)$을 투자하고 주식 $j^{n}$에 대한 수익률 $x\left(j^{n}\right)$을 얻으며, 총 자산 $S_{n}$은 $\sum_{j^{n}} w\left(j^{n}\right) x\left(j^{n}\right)$입니다.

먼저 최적의 상수 재조정 포트폴리오 $\mathbf{b}^{*}$와 관련된 가중치 $w^{*}\left(j^{n}\right)$을 계산합니다. 상수 재조정 포트폴리오 $\mathbf{b}$는 다음과 같은 결과를 가져옵니다.

$$
w\left(j^{n}\right)=\prod_{i=1}^{n} b_{i j_{i}}=b^{k}(1-b)^{n-k}
$$

여기서 $k$는 시퀀스 $j^{n}$에서 1이 나타나는 횟수입니다. 따라서 $w\left(j^{n}\right)$은 $j^{n}$에서 1의 개수인 $k$에만 의존합니다. $j^{n}$에 주의를 고정하면,
<!-- Page 661 -->
$b$에 대해 미분하여 최대값을 찾으면 다음과 같습니다.

$$
\begin{aligned}
w^{*}\left(j^{n}\right) & =\max _{0 \leq b \leq 1} b^{k}(1-b)^{n-k} \\
& =\left(\frac{k}{n}\right)^{k}\left(\frac{n-k}{n}\right)^{n-k}
\end{aligned}
$$

이는 다음을 통해 달성됩니다.

$$
\mathbf{b}^{*}=\left(\frac{k}{n}, \frac{n-k}{n}\right)
$$

$\sum w^{*}\left(j^{n}\right)>1$임을 주목하십시오. 이는 $j^{n}$에 대해 "베팅"된 금액이 사후적으로 선택되므로, 사후 투자자가 자신의 투자 $w^{*}\left(j^{n}\right)$의 합이 1이 되도록 할 필요가 없음을 반영합니다. 인과적 투자자는 그러한 사치를 누릴 수 없습니다. 인과적 투자자는 모든 가능한 $j^{n}$과 사후 결정된 $w^{*}\left(j^{n}\right)$로부터 자신을 보호하기 위해 초기 투자 $\hat{w}\left(j^{n}\right), \sum \hat{w}\left(j^{n}\right)=1$을 어떻게 선택할 수 있습니까? 답은 $\hat{w}\left(j^{n}\right)$을 $w^{*}\left(j^{n}\right)$에 비례하도록 선택하는 것입니다. 그러면 $\hat{w}\left(j^{n}\right) / w^{*}\left(j^{n}\right)$의 최악의 경우 비율이 최대화될 것입니다. 진행하기 위해 $V_{n}$을 다음과 같이 정의합니다.

$$
\begin{aligned}
\frac{1}{V_{n}} & =\sum_{j^{n}}\left(\frac{k\left(j^{n}\right)}{n}\right)^{k\left(j^{n}\right)}\left(\frac{n-k\left(j^{n}\right)}{n}\right)^{n-k\left(j^{n}\right)} \\
& =\sum_{k=0}^{n}\binom{n}{k}\left(\frac{k}{n}\right)^{k}\left(\frac{n-k}{n}\right)^{n-k}
\end{aligned}
$$

그리고 다음과 같이 설정합니다.

$$
\hat{w}\left(j^{n}\right)=V_{n}\left(\frac{k\left(j^{n}\right)}{n}\right)^{k\left(j^{n}\right)}\left(\frac{n-k\left(j^{n}\right)}{n}\right)^{n-k\left(j^{n}\right)}
$$

$\hat{w}\left(j^{n}\right)$이 $2^{n}$개의 주식 시퀀스에 대한 합법적인 부의 분포임을 명확히 알 수 있습니다 (즉, $\hat{w}\left(j^{n}\right) \geq 0$ 및 $\sum_{j^{n}} \hat{w}\left(j^{n}\right)=1$). 여기서 $V_{n}$은 $\hat{w}\left(j^{n}\right)$을 확률 질량 함수로 만드는 정규화 상수입니다. 또한 (16.109) 및 (16.113)으로부터 모든 시퀀스 $\mathbf{x}^{n}$에 대해 다음과 같습니다.

$$
\begin{aligned}
\frac{\hat{S}_{n}\left(\mathbf{x}^{n}\right)}{S_{n}^{*}\left(\mathbf{x}^{n}\right)} & \geq \min _{j^{n}} \frac{\hat{w}\left(j^{n}\right)}{w^{*}\left(j^{n}\right)} \\
& =\min _{k} \frac{V_{n}\left(\frac{k}{n}\right)^{k}\left(\frac{n-k}{n}\right)^{n-k}}{b^{* k}\left(1-b^{*}\right)^{n-k}} \\
& \geq V_{n}
\end{aligned}
$$
<!-- Page 662 -->
(16.117)은 (16.109)에서 유도되고, (16.119)는 (16.112)에서 유도됩니다. 따라서 다음을 얻습니다.

$$
\max _{\hat{\mathbf{b}}} \min _{\mathbf{x}^{n}} \frac{\hat{S}_{n}\left(\mathbf{x}^{n}\right)}{S_{n}^{*}\left(\mathbf{x}^{n}\right)} \geq V_{n}
$$

이로써 $2^{n}$개의 가능한 길이 $n$ 시퀀스에 대한 포트폴리오를 시연했으며, 이는 최적의 상수 재조정 포트폴리오가 인지한 부 $S_{n}^{*}\left(\mathbf{x}^{n}\right)$에 대해 $V_{n}$의 비율 내에서 부 $\hat{S}_{n}\left(\mathbf{x}^{n}\right)$를 달성합니다. 정리의 증명을 완료하기 위해, 어떤 비예측 포트폴리오 $\mathbf{b}_{i}\left(\mathbf{x}^{i-1}\right)$도 최악의 경우(즉, $\mathbf{x}^{n}$의 최악의 선택)에 대해 $V_{n}$보다 나은 성능을 낼 수 없음을 보입니다. 이를 증명하기 위해 극단적인 주식 시장 시퀀스 집합을 구성하고, 어떤 비예측 포트폴리오 전략의 성능도 이러한 시퀀스 중 적어도 하나에 대해 $V_{n}$으로 제한됨을 보여 최악의 경우 경계를 증명합니다.

각 $j^{n} \in\{1,2\}^{n}$에 대해, 해당 극단적인 주식 시장 벡터 $\mathbf{x}^{n}\left(j^{n}\right)$를 다음과 같이 정의합니다.

$$
\mathbf{x}_{i}\left(j_{i}\right)= \begin{cases}(1,0)^{t} & \text { if } j_{i}=1 \\ (0,1)^{t} & \text { if } j_{i}=2\end{cases}
$$

표준 기저 벡터를 $\mathbf{e}_{1}=(1,0)^{t}, \mathbf{e}_{2}=(0,1)^{t}$로 둡니다.

$$
\mathcal{K}=\left\{\mathbf{x}\left(j^{n}\right): j^{n} \in\{1,2\}^{n}, \mathbf{x}_{i j_{i}}=\mathbf{e}_{j_{i}}\right\}
$$

를 극단적인 시퀀스의 집합으로 둡니다. 이러한 극단적인 시퀀스는 $2^{n}$개이며, 각 시퀀스의 각 시점에서 비영 수익률을 제공하는 주식은 하나뿐입니다. 다른 주식에 투자된 부는 손실됩니다. 따라서 극단적인 시퀀스 $\mathbf{x}^{n}\left(j^{n}\right)$에 대한 $n$ 기간 말의 부는 주식 $j_{1}, j_{2}, \ldots, j_{n}$에 투자된 금액의 곱입니다. [즉, $\left.S_{n}\left(\mathbf{x}^{n}\left(j^{n}\right)\right)=\prod_{i} b_{j_{i}}=w\left(j^{n}\right)\right]$. 다시 말해, 이를 길이 $n$ 시퀀스에 대한 투자로 볼 수 있으며, 수익률의 0-1 특성을 고려할 때 $\mathbf{x}^{n} \in \mathcal{K}$에 대해 다음이 성립함을 쉽게 알 수 있습니다.

$$
\sum_{j^{n}} S_{n}\left(\mathbf{x}^{n}\left(j^{n}\right)\right)=1
$$

모든 극단적인 시퀀스 $\mathbf{x}^{n}\left(j^{n}\right) \in \mathcal{K}$에 대해, 최적의 상수 재조정 포트폴리오는 다음과 같습니다.

$$
\mathbf{b}^{*}\left(\mathbf{x}^{n}\left(j^{n}\right)\right)=\left(\frac{n_{1}\left(j^{n}\right)}{n}, \quad \frac{n_{2}\left(j^{n}\right)}{n}\right)^{t}
$$
<!-- Page 663 -->
여기서 $n_{1}\left(j^{n}\right)$은 시퀀스 $j^{n}$에서 1의 발생 횟수입니다. $n$ 기간 종료 시점의 해당 자산은 다음과 같습니다.

$$
S_{n}^{*}\left(\mathbf{x}^{n}\left(j^{n}\right)\right)=\left(\frac{n_{1}\left(j^{n}\right)}{n}\right)^{n_{1}\left(j^{n}\right)}\left(\frac{n_{2}\left(j^{n}\right)}{n}\right)^{n_{2}\left(j^{n}\right)}=\frac{\hat{w}\left(j^{n}\right)}{V_{n}}
$$

(16.116)에 따라 다음과 같은 결과가 나옵니다.

$$
\sum_{\mathbf{x}^{n} \in \mathcal{K}} S_{n}^{*}\left(\mathbf{x}^{n}\right)=\frac{1}{V_{n}} \sum_{j^{n}} \hat{w}\left(j^{n}\right)=\frac{1}{V_{n}}
$$

그러면 (16.104)에 정의된 $S_{n}\left(\mathbf{x}^{n}\right)$에 대해 모든 포트폴리오 시퀀스 $\left\{\mathbf{b}_{i}\right\}_{i=1}^{n}$에 대해 다음 부등식이 성립합니다.

$$
\begin{aligned}
\min _{\mathbf{x}^{n} \in \mathcal{K}} \frac{S_{n}\left(\mathbf{x}^{n}\right)}{S_{n}^{*}\left(\mathbf{x}^{n}\right)} & \leq \sum_{\tilde{\mathbf{x}}^{n} \in \mathcal{K}} \frac{S_{n}^{*}\left(\tilde{\mathbf{x}}^{n}\right)}{\sum_{\mathbf{x}^{n} \in \mathcal{K}} S_{n}^{*}\left(\mathbf{x}^{n}\right)} \frac{S_{n}\left(\tilde{\mathbf{x}}^{n}\right)}{S_{n}^{*}\left(\tilde{\mathbf{x}}^{n}\right)} \\
& =\sum_{\tilde{\mathbf{x}}^{n} \in \mathcal{K}} \frac{S_{n}\left(\tilde{\mathbf{x}}^{n}\right)}{\sum_{\mathbf{x}^{n} \in \mathcal{K}} S_{n}^{*}\left(\mathbf{x}^{n}\right)} \\
& =\frac{1}{\sum_{\mathbf{x}^{n} \in \mathcal{K}} S_{n}^{*}\left(\mathbf{x}^{n}\right)} \\
& =V_{n}
\end{aligned}
$$

여기서 부등식은 최소값이 평균보다 작다는 사실에서 비롯됩니다. 따라서,

$$
\max _{\mathbf{b}} \min _{\mathbf{x}^{n} \in \mathcal{K}} \frac{S_{n}\left(\mathbf{x}^{n}\right)}{S_{n}^{*}\left(\mathbf{x}^{n}\right)} \leq V_{n}
$$

정리에서 설명된 전략은 길이가 $n$인 모든 시퀀스에 질량을 할당하며 명확하게 $n$에 의존합니다. 이 전략을 점진적인 용어(즉, 시간 1에 주식 1과 주식 2에 베팅한 금액)로 재구성한 다음, 시간 1의 결과에 따라 시간 2에 각 주식에 베팅한 금액 등을 고려할 수 있습니다. 이전 주식 벡터 시퀀스 $\mathbf{x}^{i-1}$가 주어졌을 때 알고리즘이 시간 $i$에 주식 1에 할당하는 가중치 $\hat{b}_{i, 1}$을 고려해 보겠습니다. 이는 $i$번째 위치에 1을 포함하는 모든 시퀀스 $j^{n}$에 대해 합산하여 계산할 수 있습니다.

$$
\hat{\mathbf{b}}_{i, 1}\left(\mathbf{x}^{i-1}\right)=\frac{\sum_{j^{i-1} \in M^{i-1}} \hat{w}\left(j^{i-1} 1\right) x\left(j^{i-1}\right)}{\sum_{j^{i} \in M^{i}} \hat{w}\left(j^{i}\right) x\left(j^{i-1}\right)}
$$
<!-- Page 664 -->
여기서

$$
\hat{w}\left(j^{i}\right)=\sum_{j^{n}: j^{i} \subseteq j^{n}} w\left(j^{n}\right)
$$

는 $j^{i}$로 시작하는 모든 시퀀스 $j^{n}$에 부여되는 가중치이며,

$$
x\left(j^{i-1}\right)=\prod_{k=1}^{i-1} x_{k j_{k}}
$$

는 (16.106)에 정의된 해당 시퀀스들의 수익률입니다.
$V_{n}$의 점근적 분석을 조사하면 [401, 496] 다음과 같은 결과를 얻습니다.

$$
V_{n} \sim\left(\sqrt{\frac{2}{n}}\right)^{m-1} \Gamma(m / 2) / \sqrt{\pi}
$$

이는 $m$개의 자산에 대한 것입니다. 특히, $m=2$개의 자산의 경우,

$$
V_{n} \sim \sqrt{\frac{2}{\pi n}}
$$

이며,

$$
\frac{1}{2 \sqrt{n+1}} \leq V_{n} \leq \frac{2}{\sqrt{n+1}}
$$

는 모든 $n$에 대해 성립합니다 [400]. 따라서, $m=2$개의 주식의 경우, (16.132)에 주어진 인과적 포트폴리오 전략 $\hat{\mathbf{b}}_{i}\left(\mathbf{x}^{i-1}\right)$는 다음과 같은 부의 $\hat{S}_{n}\left(x^{n}\right)$를 달성합니다.

$$
\frac{\hat{S}_{n}\left(x^{n}\right)}{S_{n}^{*}\left(x^{n}\right)} \geq V_{n} \geq \frac{1}{2 \sqrt{n+1}}
$$

이는 모든 시장 시퀀스 $x^{n}$에 대해 성립합니다.

# 16.7.2 무기한 보편 포트폴리오

무기한 전략을 다양한 포트폴리오 전략의 가중치로 설명합니다. 앞서 설명한 바와 같이, 각 상수 재조정 포트폴리오 $\mathbf{b}$는 $\mathbf{b}$에 따라 $m$개의 자산을 재조정하는 뮤추얼 펀드에 해당한다고 볼 수 있습니다. 초기에는 상수 재조정 포트폴리오 $\mathbf{b}$의 근방 $d \mathbf{b}$에 있는 포트폴리오에 투자되는 부의 양을 $d \mu(\mathbf{b})$라고 할 때, 이러한 펀드들에 부를 분배합니다.
<!-- Page 665 -->
다음과 같이 상수 재조정 포트폴리오 $\mathbf{b}$가 주식 시퀀스 $\mathbf{x}^{n}$에 대해 생성하는 부를 $S_{n}\left(\mathbf{b}, \mathbf{x}^{n}\right)=\prod_{i=1}^{n} \mathbf{b}^{t} \mathbf{x}_{i}$라고 정의합니다.

$$
S_{n}\left(\mathbf{b}, \mathbf{x}^{n}\right)=\prod_{i=1}^{n} \mathbf{b}^{t} \mathbf{x}_{i}
$$

또한,

$$
S_{n}^{*}\left(\mathbf{x}^{n}\right)=\max _{b \in \mathcal{B}} S_{n}\left(\mathbf{b}, \mathbf{x}^{n}\right)
$$

는 사후 검증된 최적의 상수 재조정 포트폴리오의 부입니다.
다음과 같이 인과적 포트폴리오를 조사합니다.

$$
\hat{\mathbf{b}}_{i+1}\left(\mathbf{x}^{i}\right)=\frac{\int_{\mathcal{B}} \mathbf{b} S_{i}\left(\mathbf{b}, \mathbf{x}^{i}\right) d \mu(\mathbf{b})}{\int_{\mathcal{B}} S_{i}\left(\mathbf{b}, \mathbf{x}^{i}\right) d \mu(\mathbf{b})}
$$

다음과 같이 관찰합니다.

$$
\begin{aligned}
\hat{\mathbf{b}}_{i+1}^{t}\left(\mathbf{x}^{i}\right) \mathbf{x}_{i+1} & =\frac{\int_{\mathcal{B}} \mathbf{b}^{t} \mathbf{x}_{i+1} S_{i}\left(\mathbf{b}, \mathbf{x}^{i}\right) d \mu(\mathbf{b})}{\int_{\mathcal{B}} S_{i}\left(\mathbf{b}, \mathbf{x}^{i}\right) d \mu(\mathbf{b})} \\
& =\frac{\int_{\mathcal{B}} S_{i+1}\left(\mathbf{b}, \mathbf{x}^{i+1}\right) d \mu(\mathbf{b})}{\int_{\mathcal{B}} S_{i}\left(\mathbf{b}, \mathbf{x}^{i}\right) d \mu(\mathbf{b})}
\end{aligned}
$$

따라서, 곱 $\prod \hat{\mathbf{b}}_{i}^{t} \mathbf{x}_{i}$는 망원식으로 계산되며, 이 포트폴리오에서 발생하는 부 $\hat{S}_{n}\left(\mathbf{x}^{n}\right)$는 다음과 같이 주어집니다.

$$
\begin{aligned}
\hat{S}_{n}\left(\mathbf{x}^{n}\right) & =\prod_{i=1}^{n} \hat{\mathbf{b}}_{i}^{t}\left(\mathbf{x}^{i-1}\right) \mathbf{x}_{i} \\
& =\int_{b \in \mathcal{B}} S_{n}\left(\mathbf{b}, \mathbf{x}^{n}\right) d \mu(\mathbf{b})
\end{aligned}
$$

(16.145)를 해석하는 다른 방법이 있습니다. 포트폴리오 매니저 $\mathbf{b}$에게 할당된 금액은 $d \mu(\mathbf{b})$이며, 매니저가 $\mathbf{b}$로 재조정할 때 발생하는 성장률은 $S\left(\mathbf{b}, \mathbf{x}^{n}\right)$이고, 이 투자 배치의 총 부는 다음과 같습니다.

$$
\hat{S}_{n}\left(\mathbf{x}^{n}\right)=\int_{\mathcal{B}} S_{n}\left(\mathbf{b}, \mathbf{x}^{n}\right) d \mu(\mathbf{b})
$$

그러면 (16.141)에 정의된 $\hat{\mathbf{b}}_{i+1}$은 개별 포트폴리오 매니저 $\mathbf{b}$의 성과 가중치 총 "매수 주문"입니다.
<!-- Page 666 -->
지금까지 초기 자산을 배분하는 데 어떤 분포 $\mu(\mathbf{b})$를 사용하는지 명시하지 않았습니다. 이제 모든 가능한 포트폴리오에 질량을 두는 분포 $\mu$를 사용하여 주가 벡터의 실제 분포에 대한 최적의 포트폴리오 성능을 근사합니다.

다음 보조정리에서 초기 자산 분포 $\mu(\mathbf{b})$의 함수로서 $\hat{S}_{n} / S_{n}^{*}$를 상한합니다.

보조정리 16.7.2 16.140에서 $S_{n}^{*}\left(\mathbf{x}^{n}\right)$를 최적의 상수 재조정 포트폴리오로 달성한 자산으로, (16.144)에서 $\hat{S}_{n}\left(\mathbf{x}^{n}\right)$를 다음과 같이 주어진 보편 혼합 포트폴리오 $\hat{\mathbf{b}}(\cdot)$로 달성한 자산으로 정의합니다.

$$
\hat{\mathbf{b}}_{i+1}\left(\mathbf{x}^{i}\right)=\frac{\int \mathbf{b} S_{i}\left(\mathbf{b}, \mathbf{x}^{i}\right) d \mu(\mathbf{b})}{\int S_{i}\left(\mathbf{b}, \mathbf{x}^{i}\right) d \mu(\mathbf{b})}
$$

그러면

$$
\frac{\hat{S}_{n}\left(\mathbf{x}^{n}\right)}{S_{n}^{*}\left(\mathbf{x}^{n}\right)} \geq \min _{j^{n}} \frac{\int_{\mathcal{B}} \prod_{i=1}^{n} b_{j_{i}} d \mu(\mathbf{b})}{\prod_{i=1}^{n} b_{j_{i}}^{*}}
$$

증명: 이전과 같이 다음과 같이 쓸 수 있습니다.

$$
S_{n}^{*}\left(\mathbf{x}^{n}\right)=\sum_{j^{n}} w^{*}\left(j^{n}\right) x\left(j^{n}\right)
$$

여기서 $w^{*}\left(j^{n}\right)=\prod_{i=1}^{n} b_{j_{i}}^{*}$는 시퀀스 $j^{n}$에 투자된 금액이고 $x\left(j^{n}\right)=\prod_{i=1}^{n} x_{i j_{i}}$는 해당 수익입니다. 마찬가지로 다음과 같이 쓸 수 있습니다.

$$
\begin{aligned}
\hat{S}_{n}\left(\mathbf{x}^{n}\right) & =\int \prod_{i=1}^{n} \mathbf{b}^{i} \mathbf{x}_{i} d \mu(\mathbf{b}) \\
& =\sum_{j^{n}} \int \prod_{i=1}^{n} b_{j_{i}} x_{i j_{i}} d \mu(\mathbf{b}) \\
& =\sum_{j^{n}} \hat{w}\left(j^{n}\right) x\left(j^{n}\right)
\end{aligned}
$$

여기서 $\hat{w}\left(j^{n}\right)=\int \prod_{i=1}^{n} b_{j_{i}} d \mu(\mathbf{b})$입니다. 이제 보조정리 16.7.1을 적용하면 다음과 같습니다.

$$
\begin{aligned}
\frac{\hat{S}_{n}\left(\mathbf{x}^{n}\right)}{S_{n}^{*}\left(\mathbf{x}^{n}\right)} & =\frac{\sum_{j^{n}} \hat{w}\left(j^{n}\right) x\left(j^{n}\right)}{\sum_{j^{n}} w^{*}\left(j^{n}\right) x\left(j^{n}\right)} \\
& \geq \min _{j^{n}} \frac{\hat{w}\left(j^{n}\right) x\left(j^{n}\right)}{w^{*}\left(j^{n}\right) x\left(j^{n}\right)} \\
& =\min _{j^{n}} \frac{\int_{\mathcal{B}} \prod_{i=1}^{n} b_{j_{i}} d \mu(\mathbf{b})}{\prod_{i=1}^{n} b_{j_{i}}^{*}}
\end{aligned}
$$
<!-- Page 667 -->
이제 $\mu(\mathbf{b})$가 Dirichlet $\left(\frac{1}{2}\right)$ 분포일 때 이 보조정리를 적용합니다.
정리 16.7.2 (16.141)에 주어진 인과적 범용 포트폴리오 $\hat{b}_{i}($ ), $i=1,2, \ldots$에 대해, $m=2$개의 주식과 $d \mu(\mathbf{b})$가 Dirichlet $\left(\frac{1}{2}, \frac{1}{2}\right)$ 분포일 때, 우리는 다음을 얻습니다.

$$
\frac{\hat{S}_{n}\left(x^{n}\right)}{S_{n}^{*}\left(x^{n}\right)} \geq \frac{1}{2 \sqrt{n+1}}
$$

모든 $n$과 모든 주식 시퀀스 $x^{n}$에 대해 성립합니다.
증명: (16.112) 앞의 논의와 마찬가지로, 최적의 상수 포트폴리오 $b^{*}$가 시퀀스 $j^{n}$에 부여하는 가중치가 다음과 같음을 보일 수 있습니다.

$$
\prod_{i=1}^{n} b_{j_{i}}^{*}=\left(\frac{k}{n}\right)^{k}\left(\frac{n-k}{n}\right)^{n-k}=2^{-n H(k / n)}
$$

여기서 $k$는 $j_{i}=1$인 인덱스의 개수입니다. 또한 Dirichlet $\left(\frac{1}{2}\right)$ 밀도에 대한 (16.148)의 분자에서 적분을 명시적으로 계산할 수 있습니다. $m$개의 변수에 대해 정의된 밀도는 다음과 같습니다.

$$
d \mu(\mathbf{b})=\frac{\Gamma\left(\frac{m}{2}\right)}{\left[\Gamma\left(\frac{1}{2}\right)\right]^{m}} \prod_{j=1}^{m} b_{j}^{-\frac{1}{2}} d \mathbf{b}
$$

여기서 $\Gamma(x)=\int_{0}^{\infty} e^{-t} t^{x-1} d t$는 감마 함수를 나타냅니다. 단순화를 위해 두 개의 주식에 대한 경우를 고려하면 다음과 같습니다.

$$
d \mu(b)=\frac{1}{\pi} \frac{1}{\sqrt{b(1-b)}} d b, \quad 0 \leq b \leq 1
$$

여기서 $b$는 주식 1에 투자된 부의 비율입니다. 이제 임의의 시퀀스 $j^{n} \in\{1,2\}^{n}$를 고려하고 해당 시퀀스에 투자된 금액을 고려합니다.

$$
b\left(j^{n}\right)=\prod_{i=1}^{n} b_{j_{i}}=b^{l}(1-b)^{n-l}
$$

여기서 $l$은 $j_{i}=1$인 인덱스의 개수입니다. 그러면

$$
\int b\left(j^{n}\right) d \mu(\mathbf{b})=\int b^{l}(1-b)^{n-l} \frac{1}{\pi} \frac{1}{\sqrt{b(1-b)}} d b
$$
<!-- Page 668 -->
$$
\begin{aligned}
& =\frac{1}{\pi} \int b^{l-\frac{1}{2}}(1-b)^{n-l-\frac{1}{2}} d b \\
& \triangleq \frac{1}{\pi} B\left(l+\frac{1}{2}, n-l+\frac{1}{2}\right)
\end{aligned}
$$

여기서 $B\left(\lambda_{1}, \lambda_{2}\right)$는 베타 함수이며 다음과 같이 정의됩니다.

$$
\begin{aligned}
B\left(\lambda_{1}, \lambda_{2}\right) & =\int_{0}^{1} x^{\lambda_{1}-1}(1-x)^{\lambda_{2}-1} d x \\
& =\frac{\Gamma\left(\lambda_{1}\right) \Gamma\left(\lambda_{2}\right)}{\Gamma\left(\lambda_{1}+\lambda_{2}\right)}
\end{aligned}
$$

그리고

$$
\Gamma(\lambda)=\int_{0}^{\infty} x^{\lambda-1} e^{-x} d x
$$

모든 정수 $n$에 대해 $\Gamma(n+1)=n$ !이고 $\Gamma\left(n+\frac{1}{2}\right)=\frac{1.3 .5 \ldots(2 n-1)}{2^{n}} \sqrt{\pi}$임을 유의하십시오.
적분 부분을 이용한 간단한 재귀를 통해 $B\left(l+\frac{1}{2}, n-l+\frac{1}{2}\right)$를 계산할 수 있습니다. 또는 (16.164)를 사용하여 다음과 같이 얻습니다.

$$
B\left(l+\frac{1}{2}, n-l+\frac{1}{2}\right)=\frac{\pi}{2^{2 n}} \frac{\binom{2 n}{n}\binom{n}{l}}{\binom{2 n}{2 l}}
$$

모든 결과를 Lemma 16.7.2와 결합하면 다음과 같습니다.

$$
\begin{aligned}
\frac{\hat{S}_{n}\left(\mathbf{x}^{n}\right)}{S_{n}^{*}\left(\mathbf{x}^{n}\right)} & \geq \min _{j^{n}} \frac{\int_{I S} \prod_{i=1}^{n} b_{j_{i}} d \mu(\mathbf{b})}{\prod_{i=1}^{n} b_{j_{i}}^{*}} \\
& \geq \min _{l} \frac{\frac{1}{\pi} B\left(l+\frac{1}{2}, n-l+\frac{1}{2}\right)}{2^{-n H(l / n)}} \\
& \geq \frac{1}{2 \sqrt{n+1}}
\end{aligned}
$$

[135, Theorem 2]의 결과를 사용합니다.
$m=2$개의 주식에 대해 다음과 같습니다.

$$
\frac{\hat{S}_{n}}{S_{n}^{*}} \geq \frac{1}{\sqrt{2 \pi}} V_{n}
$$
<!-- Page 669 -->
모든 $n$과 모든 시장 시퀀스 $x_{1}, x_{2}, \ldots, x_{n}$에 대해. 따라서 모든 $n$에 대한 좋은 minimax 성능은 고정된 기간 minimax 포트폴리오보다 최대 $\sqrt{2 \pi}$의 추가적인 요인을 소모합니다. 보편성의 비용은 $V_{n}$이며, 이는 다음과 같은 의미에서 성장률에서 점근적으로 무시할 수 있습니다.

$$
\frac{1}{n} \ln \hat{S}_{n}\left(\mathbf{x}^{n}\right)-\frac{1}{n} \ln S_{n}^{*}\left(\mathbf{x}^{n}\right) \geq \frac{1}{n} \ln \frac{V_{n}}{\sqrt{2 \pi}} \rightarrow 0
$$

따라서, 보편적인 인과 포트폴리오는 최상의 hindsight 포트폴리오와 동일한 부의 점근적 성장률을 달성합니다.

이제 이 포트폴리오 algorithm이 두 개의 실제 주식에서 어떻게 수행되는지 고려해 보겠습니다. 우리는 14년 기간(2004년 종료)과 두 주식, Hewlett-Packard와 Altria(이전에는 Phillip Morris)를 고려할 것입니다. 이 두 주식 모두 Dow Jones Index의 구성 요소입니다. 이 14년 동안 HP는 11.8배 상승한 반면, Altria는 11.5배 상승했습니다. HP와 Altria를 포함하는 다양한 지속적으로 재조정된 포트폴리오의 성능은 그림 16.2에 나와 있습니다. 최상의 지속적으로 재조정된 포트폴리오(이는 오직 hindsight로만 계산될 수 있습니다)는 약 51% HP와 49% Altria의 혼합을 사용하여 18.7배의 성장을 달성합니다. 이 섹션에서 설명된 보편 포트폴리오 전략은 사전 지식 없이 15.7배의 성장 계수를 달성합니다.

그림 16.2. HP와 Altria에 대한 다양한 지속적으로 재조정된 포트폴리오 $\mathbf{b}$의 성능.
<!-- Page 670 -->
# 16.8 섀넌-맥밀런-브레이먼 정리 (일반 AEP)

에르고딕 과정에 대한 AEP는 섀넌-맥밀런-브레이먼 정리로 알려지게 되었습니다. 3장에서는 i.i.d. 과정에 대한 AEP를 증명했습니다. 이 절에서는 일반 에르고딕 과정에 대한 정리의 증명을 제공합니다. 두 개의 에르고딕 시퀀스를 샌드위치하여 $\frac{1}{n} \log p\left(X^{n}\right)$의 수렴을 증명합니다.

어떤 의미에서 에르고딕 과정은 대수의 법칙이 성립하는 가장 일반적인 종속 과정입니다. 유한 알파벳 과정의 경우, 에르고딕성은 모든 $k$에 대해 $k$차 경험 분포가 주변 분포로 수렴하는 것과 동등합니다.

기술적인 정의는 확률 이론의 일부 아이디어를 필요로 합니다. 정확히 말하면, 에르고딕 소스는 확률 공간 $(\Omega, \mathcal{B}, P)$ 상에서 정의되며, 여기서 $\mathcal{B}$는 $\Omega$의 부분집합에 대한 $\sigma$-대수이고 $P$는 확률 측도입니다. 확률 변수 $X$는 확률 공간 상의 함수 $X(\omega), \omega \in \Omega$로 정의됩니다. 또한 시간 이동 역할을 하는 변환 $T: \Omega \rightarrow \Omega$가 있습니다. 변환이 정상적이라는 것은 모든 $A \in \mathcal{B}$에 대해 $P(T A)=P(A)$이면 그렇게 말합니다. 변환이 에르고딕이라는 것은 모든 집합 A에 대해 $T A=A$, 거의 확실하게, $P(A)=0$ 또는 1을 만족하면 그렇게 말합니다. 만약 $T$가 정상적이고 에르고딕이면, $X_{n}(\omega)=X\left(T^{n} \omega\right)$로 정의된 과정이 정상적이고 에르고딕이라고 말합니다. 정상 에르고딕 소스의 경우, 버코프의 에르고딕 정리는 다음과 같이 말합니다.

$$
\frac{1}{n} \sum_{i=1}^{n} X_{i}(\omega) \rightarrow E X=\int X d P \quad \text { 확률 1로 }
$$

따라서 대수의 법칙은 에르고딕 과정에 대해 성립합니다.
에르고딕 정리를 사용하여 다음과 같이 결론짓고 싶습니다.

$$
\begin{aligned}
-\frac{1}{n} \log p\left(X_{0}, X_{1}, \ldots, X_{n-1}\right) & =-\frac{1}{n} \sum_{i=0}^{n-1} \log p\left(X_{i} \mid X_{0}^{i-1}\right) \\
& \rightarrow \lim _{n \rightarrow \infty} E\left[-\log p\left(X_{n} \mid X_{0}^{n-1}\right)\right]
\end{aligned}
$$

그러나 확률적 시퀀스 $p\left(X_{i} \mid X_{0}^{i-1}\right)$는 에르고딕이 아닙니다. 하지만 밀접하게 관련된 양인 $p\left(X_{i} \mid X_{i-k}^{i-1}\right)$와 $p\left(X_{i} \mid X_{-\infty}^{i-1}\right)$는 에르고딕이며 엔트로피율로 쉽게 식별되는 기댓값을 가집니다. $p\left(X_{i} \mid X_{0}^{i-1}\right)$를 이 두 개의 더 다루기 쉬운 과정 사이에 샌드위치할 계획입니다.
<!-- Page 671 -->
$k$차 엔트로피 $H^{k}$를 다음과 같이 정의합니다.

$$
\begin{aligned}
H^{k} & =E\left\{-\log p\left(X_{k} \mid X_{k-1}, X_{k-2}, \ldots, X_{0}\right)\right\} \\
& =E\left\{-\log p\left(X_{0} \mid X_{-1}, X_{-2}, \ldots, X_{-k}\right)\right\}
\end{aligned}
$$

여기서 마지막 등식은 정상성(stationarity)으로부터 유도됩니다. 엔트로피율(entropy rate)은 다음과 같이 주어집니다.

$$
\begin{aligned}
H & =\lim _{k \rightarrow \infty} H^{k} \\
& =\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{k=0}^{n-1} H^{k}
\end{aligned}
$$

물론, 정상성과 조건화가 엔트로피를 증가시키지 않는다는 사실에 의해 $H^{k} \searrow H$입니다. $H^{k} \searrow H=H^{\infty}$라는 사실이 중요할 것입니다. 여기서

$$
H^{\infty}=E\left\{-\log p\left(X_{0} \mid X_{-1}, X_{-2}, \ldots\right)\right\}
$$

$H^{\infty}=H$라는 증명은 기댓값과 극한의 교환을 포함합니다.
증명의 주요 아이디어는 (조건부) 비례 도박(proportional gambling)의 아이디어로 거슬러 올라갑니다. 과거 $k$개의 정보를 아는 상태에서 균등한 배당률을 받는 도박꾼은 부의 성장률이 $\log |\mathcal{X}|-H^{k}$가 될 것이고, 무한한 과거 정보를 아는 도박꾼은 부의 성장률이 $\log |\mathcal{X}|-H^{\infty}$가 될 것입니다. 우리는 과거 $X_{0}^{n}$에 대한 지식이 증가하는 도박꾼의 부의 성장률을 알지 못하지만, 그것은 확실히 $\log |\mathcal{X}|-H^{k}$와 $\log |\mathcal{X}|-H^{\infty}$ 사이에 끼어 있습니다. 그러나 $H^{k} \searrow H=H^{\infty}$입니다. 따라서 샌드위치가 닫히고 성장률은 $\log |\mathcal{X}|-H$여야 합니다.

증명에 이어질 보조정리들을 바탕으로 정리를 증명할 것입니다.
정리 16.8.1 (AEP: Shannon-McMillan-Breiman Theorem) 만약 $H$가 유한값(finite-valued)을 갖는 정상 에르고딕 과정(stationary ergodic process) $\left\{X_{n}\right\}$의 엔트로피율이라면,

$$
-\frac{1}{n} \log p\left(X_{0}, \ldots, X_{n-1}\right) \rightarrow H \quad \text { 확률 } 1 \text { 로 }
$$

증명: 유한 알파벳 $\mathcal{X}$에 대해 이를 증명합니다. 가산 알파벳(countable alphabets)과 밀도(densities)에 대한 증명은 Algoet와 Cover [20]에 제시되어 있습니다. 우리는 확률 변수들의 수열 $-\frac{1}{n} \log p\left(X_{0}^{n-1}\right)$이 모든 $k \geq 0$에 대해 상한 $H^{k}$와 하한 $H^{\infty}$ 사이에 점근적으로 끼어 있다고 주장합니다. AEP는 $H^{k} \rightarrow H^{\infty}$이고 $H^{\infty}=H$이므로 따라올 것입니다. 확률에 대한 $k$차 마르코프 근사(Markov approximation)는 $n \geq k$에 대해 다음과 같이 정의됩니다.

$$
p^{k}\left(X_{0}^{n-1}\right)=p\left(X_{0}^{k-1}\right) \prod_{i=k}^{n-1} p\left(X_{i} \mid X_{i-k}^{i-1}\right)
$$
<!-- Page 672 -->
Lemma 16.8.3으로부터 다음을 얻습니다.

$$
\limsup _{n \rightarrow \infty} \frac{1}{n} \log \frac{p^{k}\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1}\right)} \leq 0
$$

이는 극한값 $\frac{1}{n} \log p^{k}\left(X_{0}^{n}\right)$의 존재를 고려하여 (Lemma 16.8.1), 다음과 같이 다시 작성할 수 있습니다.

$$
\limsup _{n \rightarrow \infty} \frac{1}{n} \log \frac{1}{p\left(X_{0}^{n-1}\right)} \leq \lim _{n \rightarrow \infty} \frac{1}{n} \log \frac{1}{p^{k}\left(X_{0}^{n-1}\right)}=H^{k}
$$

$k=1,2, \ldots$에 대해. 또한, Lemma 16.8.3으로부터 다음을 얻습니다.

$$
\limsup _{n \rightarrow \infty} \frac{1}{n} \log \frac{p\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1} \mid X_{-\infty}^{-1}\right)} \leq 0
$$

이는 다음과 같이 다시 작성할 수 있습니다.

$$
\liminf \frac{1}{n} \log \frac{1}{p\left(X_{0}^{n-1}\right)} \geq \lim \frac{1}{n} \log \frac{1}{p\left(X_{0}^{n-1} \mid X_{-\infty}^{-1}\right)}=H^{\infty}
$$

Lemma 16.8.1의 $H^{\infty}$ 정의로부터.
(16.182)와 (16.184)를 결합하면 다음을 얻습니다.

$$
\begin{aligned}
H^{\infty} & \leq \liminf -\frac{1}{n} \log p\left(X_{0}^{n-1}\right) \leq \lim \sup -\frac{1}{n} \log p\left(X_{0}^{n-1}\right) \\
& \leq H^{k} \quad \text { 모든 } k \text{에 대해}
\end{aligned}
$$

그러나 Lemma 16.8.2에 의해, $H^{k} \rightarrow H^{\infty}=H$입니다. 따라서,

$$
\lim -\frac{1}{n} \log p\left(X_{0}^{n}\right)=H
$$

이제 주요 증명에서 사용된 lemma들을 증명합니다. 첫 번째 lemma는 ergodic theorem을 사용합니다.

Lemma 16.8.1 (Markov 근사) 정상 ergodic 확률 과정 $\left\{X_{n}\right\}$에 대해,

$$
\begin{aligned}
-\frac{1}{n} \log p^{k}\left(X_{0}^{n-1}\right) & \rightarrow H^{k} \quad \text { 확률 } 1 \text{로} \\
-\frac{1}{n} \log p\left(X_{0}^{n-1} \mid X_{-\infty}^{-1}\right) & \rightarrow H^{\infty} \quad \text { 확률 } 1 \text{로}
\end{aligned}
$$
<!-- Page 673 -->
증명: 에르고딕 과정 $\left\{X_{i}\right\}$의 함수 $Y_{n}=f\left(X_{-\infty}^{n}\right)$는 에르고딕 과정입니다. 따라서 $\log p\left(X_{n} \mid X_{n-k}^{n-1}\right)$와 $\log p\left(X_{n} \mid X_{n-1}, X_{n-2}, \ldots,\right)$ 또한 에르고딕 과정이며,

$$
\begin{aligned}
-\frac{1}{n} \log p^{k}\left(X_{0}^{n-1}\right) & =-\frac{1}{n} \log p\left(X_{0}^{k-1}\right)-\frac{1}{n} \sum_{i=k}^{n-1} \log p\left(X_{i} \mid X_{i-k}^{i-1}\right) \\
& \rightarrow 0+H^{k} \quad \text { 확률 } 1 \text{로}
\end{aligned}
$$

에르고딕 정리에 의해. 유사하게, 에르고딕 정리에 의해,

$$
\begin{aligned}
-\frac{1}{n} \log p\left(X_{0}^{n-1} \mid X_{-1}, X_{-2}, \ldots\right) & =-\frac{1}{n} \sum_{i=0}^{n-1} \log p\left(X_{i} \mid X_{i-1}, X_{i-2}, \ldots\right) \\
& \rightarrow H^{\infty} \quad \text { 확률 } 1 \text{로}
\end{aligned}
$$

정리 16.8.2 (간격 없음) $\quad H^{k} \searrow H^{\infty}$이고 $H=H^{\infty}$입니다.
증명: 정상 과정에 대해 $H^{k} \searrow H$임을 알고 있으므로, $H^{k} \searrow H^{\infty}$임을 보이는 것만 남았으며, 이는 $H=H^{\infty}$를 유도합니다. 조건부 확률에 대한 Levy의 마틴게일 수렴 정리는 다음과 같이 주장합니다.

$$
p\left(x_{0} \mid X_{-k}^{-1}\right) \rightarrow p\left(x_{0} \mid X_{-\infty}^{-1}\right) \quad \text { 확률 } 1 \text{로}
$$

모든 $x_{0} \in \mathcal{X}$에 대해. $\mathcal{X}$가 유한하고 $p \log p$가 $0 \leq p \leq 1$인 모든 $p$에 대해 유계이고 $p$에 대해 연속이므로, 유계 수렴 정리는 기댓값과 극한의 교환을 허용하여 다음을 유도합니다.

$$
\begin{aligned}
\lim _{k \rightarrow \infty} H^{k} & =\lim _{k \rightarrow \infty} E\left\{-\sum_{x_{0} \in \mathcal{X}} p\left(x_{0} \mid X_{-k}^{-1}\right) \log p\left(x_{0} \mid X_{-k}^{-1}\right)\right\} \\
& =E\left\{-\sum_{x_{0} \in \mathcal{X}} p\left(x_{0} \mid X_{-\infty}^{-1}\right) \log p\left(x_{0} \mid X_{-\infty}^{-1}\right)\right\} \\
& =H^{\infty}
\end{aligned}
$$

따라서 $H^{k} \searrow H=H^{\infty}$입니다.
<!-- Page 674 -->
Lemma 16.8.3 (Sandwich)

$$
\begin{aligned}
& \limsup _{n \rightarrow \infty} \frac{1}{n} \log \frac{p^{k}\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1}\right)} \leq 0 \\
& \limsup \frac{1}{n} \log \frac{p\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1} \mid X_{-\infty}^{-1}\right)} \leq 0
\end{aligned}
$$

Proof: Let $A$ be the support set of $p\left(X_{0}^{n-1}\right)$. Then

$$
\begin{aligned}
E\left\{\frac{p^{k}\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1}\right)}\right\} & =\sum_{x_{0}^{n-1} \in A} p\left(x_{0}^{n-1}\right) \frac{p^{k}\left(x_{0}^{n-1}\right)}{p\left(x_{0}^{n-1}\right)} \\
& =\sum_{x_{0}^{n-1} \in A} p^{k}\left(x_{0}^{n-1}\right) \\
& =p^{k}(A) \\
& \leq 1
\end{aligned}
$$

Similarly, let $B\left(X_{-\infty}^{-1}\right)$ denote the support set of $p\left(\cdot \mid X_{-\infty}^{-1}\right)$. Then we have

$$
\begin{aligned}
E\left\{\frac{p\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1} \mid X_{-\infty}^{-1}\right)}\right\} & =E\left[\left.E\left\{\left.\frac{p\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1} \mid X_{-\infty}^{-1}\right)} \right\rvert\, X_{-\infty}^{-1}\right\}\right]\right. \\
& =E\left[\sum_{x^{n} \in B\left(X_{-\infty}^{-1}\right)} \frac{p\left(x^{n}\right)}{p\left(x^{n} \mid X_{-\infty}^{-1}\right)} p\left(x^{n} \mid X_{-\infty}^{-1}\right)\right] \\
& =E\left[\sum_{x^{n} \in B\left(X_{-\infty}^{-1}\right)} p\left(x^{n}\right)\right] \\
& \leq 1
\end{aligned}
$$

By Markov's inequality and (16.202), we have

$$
\operatorname{Pr}\left\{\frac{p^{k}\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1}\right)} \geq t_{n}\right\} \leq \frac{1}{t_{n}}
$$
<!-- Page 675 -->
$$
\operatorname{Pr}\left\{\frac{1}{n} \log \frac{p^{k}\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1}\right)} \geq \frac{1}{n} \log t_{n}\right\} \leq \frac{1}{t_{n}}
$$

$t_{n}=n^{2}$로 놓고 $\sum_{n=1}^{\infty} \frac{1}{n^{2}}<\infty$임을 고려하면, 보렐-칸텔리 보조정리에 의해 다음 사건은 확률 1로 유한하게 발생합니다.

$$
\left\{\frac{1}{n} \log \frac{p^{k}\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1}\right)} \geq \frac{1}{n} \log t_{n}\right\}
$$

따라서,

$$
\limsup \frac{1}{n} \log \frac{p^{k}\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1}\right)} \leq 0 \quad \text { 확률 1로 }
$$

마찬가지로, 마르코프 부등식을 (16.206)에 적용하여 동일한 논증을 수행하면 다음을 얻습니다.

$$
\limsup \frac{1}{n} \log \frac{p\left(X_{0}^{n-1}\right)}{p\left(X_{0}^{n-1} \mid X_{-\infty}^{-1}\right)} \leq 0 \quad \text { 확률 1로 }
$$

이는 보조정리를 증명합니다.
증명에 사용된 논증은 주식 시장에 대한 AEP(정리 16.5.3)를 증명하기 위해 확장될 수 있습니다.

# 요약

성장률. 분포 $F(\mathbf{x})$에 대한 주식 시장 포트폴리오 $\mathbf{b}$의 성장률은 다음과 같이 정의됩니다.

$$
W(\mathbf{b}, F)=\int \log \mathbf{b}^{t} \mathbf{x} d F(\mathbf{x})=E\left(\log \mathbf{b}^{t} \mathbf{x}\right)
$$

로그-최적 포트폴리오. 분포 $F(x)$에 대한 최적 성장률은 다음과 같습니다.

$$
W^{*}(F)=\max _{\mathbf{b}} W(\mathbf{b}, F)
$$
<!-- Page 676 -->
최대값 $W(\mathbf{b}, F)$를 달성하는 포트폴리오 $\mathbf{b}^{*}$를 로그-최적 포트폴리오라고 합니다.

오목성. $W(\mathbf{b}, F)$는 $\mathbf{b}$에 대해 오목하고 $F$에 대해 선형입니다. $W^{*}(F)$는 $F$에 대해 볼록합니다.

최적성 조건. 포트폴리오 $\mathbf{b}^{*}$는 다음의 필요충분조건으로 로그-최적입니다.

$$
\begin{aligned}
E\left(\frac{X_{i}}{\mathbf{b}^{* t} \mathbf{X}}\right) & =1 \quad \text { if } b_{i}^{*}>0 \\
& \leq 1 \quad \text { if } b_{i}^{*}=0
\end{aligned}
$$

기대 비율 최적성. $S_{n}^{*}=\prod_{i=1}^{n} \mathbf{b}^{* t} \mathbf{X}_{i}, S_{n}=\prod_{i=1}^{n} \mathbf{b}_{i}^{t} \mathbf{X}_{i}$이면,

$$
E \frac{S_{n}}{S_{n}^{*}} \leq 1 \quad \text { if and only if } \quad E \ln \frac{S_{n}}{S_{n}^{*}} \leq 0
$$

# 성장률 (AEP)

$$
\frac{1}{n} \log S_{n}^{*} \rightarrow W^{*}(F) \quad \text { with probability } 1
$$

## 점근적 최적성

$$
\limsup _{n \rightarrow \infty} \frac{1}{n} \log \frac{S_{n}}{S_{n}^{*}} \leq 0 \quad \text { with probability } 1
$$

잘못된 정보. $f$가 참일 때 $g$를 믿는 것은 손실을 야기합니다.

$$
\Delta W=W\left(\mathbf{b}_{f}^{*}, F\right)-W\left(\mathbf{b}_{g}^{*}, F\right) \leq D(f \| g)
$$

부수 정보 $Y$

$$
\Delta W \leq I(\mathbf{X} ; Y)
$$

## 연쇄 법칙

$$
\begin{aligned}
& W^{*}\left(\mathbf{X}_{i} \mid \mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right)=\max _{\mathbf{b}_{i}\left(\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{i-1}\right)} E \log \mathbf{b}_{i}^{t} \mathbf{X}_{i} \\
& W^{*}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}\right)=\sum_{i=1}^{n} W^{*}\left(\mathbf{X}_{i} \mid \mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{i-1}\right)
\end{aligned}
$$
<!-- Page 677 -->
# 고정 시장의 성장률.

$$
\begin{aligned}
W_{\infty}^{*}= & \lim \frac{W^{*}\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}\right)}{n} \\
& \frac{1}{n} \log S_{n}^{*} \rightarrow W_{\infty}^{*}
\end{aligned}
$$

## 로그-최적 포트폴리오의 경쟁적 최적성.

$$
\operatorname{Pr}\left(V S \geq U^{*} S^{*}\right) \leq \frac{1}{2}
$$

## 범용 포트폴리오.

$$
\max _{\hat{\mathbf{b}}_{i}(\cdot)} \min _{\mathbf{x}^{n}, \mathbf{b}} \frac{\prod_{i=1}^{n} \hat{\mathbf{b}}_{i}^{i}\left(\mathbf{x}^{i-1}\right) \mathbf{x}_{i}}{\prod_{i=1}^{n} \mathbf{b}^{i} \mathbf{x}_{i}}=V_{n}
$$

여기서

$$
V_{n}=\left[\sum_{n_{1}+\cdots+n_{m}=n}\binom{n}{n_{1}, n_{2}, \ldots, n_{m}} 2^{-n H\left(n_{1} / n, \ldots, n_{m} / n\right)}\right]^{-1}
$$

$m=2$의 경우,

$$
V_{n} \sim \sqrt{2 / \pi n}
$$

인과 범용 포트폴리오

$$
\hat{\mathbf{b}}_{i+1}\left(\mathbf{x}^{i}\right)=\frac{\int \mathbf{b} S_{i}\left(\mathbf{b}, \mathbf{x}^{i}\right) d \mu(\mathbf{b})}{\int S_{i}\left(\mathbf{b}, \mathbf{x}^{i}\right) d \mu(\mathbf{b})}
$$

는 다음을 달성합니다.

$$
\frac{\hat{S}_{n}\left(\mathbf{x}^{n}\right)}{S_{n}^{*}\left(\mathbf{x}^{n}\right)} \geq \frac{1}{2 \sqrt{n+1}}
$$

모든 $n$과 모든 $\mathbf{x}^{n}$에 대해.
AEP. 만약 $\left\{X_{i}\right\}$가 정상적이고 에르고딕이라면,
$-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right) \rightarrow H(\mathcal{X}) \quad$ 확률 1로. (16.230)
<!-- Page 678 -->
# 문제

16.1 성장률. 다음을 가정합니다.

$$
\mathbf{X}=\left\{\begin{array}{ll}
(1, a) & \text { 확률 } \frac{1}{2} \\
(1,1 / a) & \text { 확률 } \frac{1}{2}
\end{array}\right.
$$

여기서 $a>1$입니다. 이 벡터 $\mathbf{X}$는 현금 대비 인기 주식의 주식 시장 벡터를 나타냅니다. 다음을 정의합니다.

$$
W(\mathbf{b}, F)=E \log \mathbf{b}^{t} \mathbf{X}
$$

그리고

$$
W^{*}=\max _{\mathbf{b}} W(\mathbf{b}, F)
$$

를 성장률이라고 합니다.
(a) 로그 최적 포트폴리오 $\mathbf{b}^{*}$를 찾으십시오.
(b) 성장률 $W^{*}$를 찾으십시오.
(c) 모든 $\mathbf{b}$에 대해 다음의 점근적 행동을 찾으십시오.

$$
S_{n}=\prod_{i=1}^{n} \mathbf{b}^{t} \mathbf{X}_{i}
$$

16.2 부가 정보. 문제 16.1에서 다음을 가정합니다.

$$
\mathbf{Y}=\left\{\begin{array}{ll}
1 & \text { 만약 }\left(X_{1}, X_{2}\right) \geq(1,1) \\
0 & \text { 만약 }\left(X_{1}, X_{2}\right) \leq(1,1)
\end{array}\right.
$$

포트폴리오 $\mathbf{b}$가 $\mathbf{Y}$에 의존한다고 가정합니다. 새로운 성장률 $W^{* *}$를 찾고 다음을 만족하는지 확인하십시오.

$$
\Delta W=W^{* *}-W^{*}
$$

$$
\Delta W \leq I(X ; Y)
$$

16.3 주식 지배. 주식 시장 벡터를 고려합니다.

$$
\mathbf{X}=\left(X_{1}, X_{2}\right)
$$

$X_{1}=2$가 확률 1로 발생한다고 가정합니다. 따라서 첫 번째 주식에 대한 투자는 하루가 끝날 때 두 배가 됩니다.
<!-- Page 679 -->
(a) 주식 $X_{2}$의 분포에 대한 필요충분조건은 로그 최적 포트폴리오 $\mathbf{b}^{*}$가 모든 부를 주식 $X_{2}$에 투자하는 것[즉, $\left.\mathbf{b}^{*}=(0,1)\right]$입니다.
(b) $X_{2}$에 대한 모든 분포에 대해 성장률이 $W^{*} \geq 1$을 만족한다고 주장하십시오.
16.4 전문가 및 상호 펀드 포함. 주식 시장의 가격 상대 벡터를 $\mathbf{X} \sim F(\mathbf{x}), \mathbf{x} \in \mathcal{R}_{+}^{m}$라고 가정합니다. "전문가"가 포트폴리오 b를 제안한다고 가정합니다. 이는 부 요인 $\mathbf{b}^{t} \mathbf{X}$을 초래할 것입니다. 이를 주식 대안에 추가하여 $\tilde{\mathbf{X}}=$ $\left(X_{1}, X_{2}, \ldots, X_{m}, \mathbf{b}^{t} \mathbf{X}\right)$를 형성합니다. 새로운 성장률이 다음과 같음을 보여주십시오.

$$
\tilde{W}^{*}=\max _{b_{1}, \ldots, b_{m}, b_{m+1}} \int \ln \left(\mathbf{b}^{t} \tilde{\mathbf{x}}\right) d F(\tilde{\mathbf{x}})
$$

이전 성장률과 같습니다.

$$
W^{*}=\max _{b_{1}, \ldots, b_{m}} \int \ln \left(\mathbf{b}^{t} \mathbf{x}\right) d F(\mathbf{x})
$$

16.5 대칭 분포에 대한 성장률. 주식 벡터 $\mathbf{X} \sim F(\mathbf{x}), \quad \mathbf{X} \in \mathcal{R}^{m}, \quad \mathbf{X} \geq 0$를 고려합니다. 여기서 구성 주식은 교환 가능합니다. 따라서 모든 순열 $\sigma$에 대해 $F\left(x_{1}, x_{2}, \ldots, x_{m}\right)=F\left(x_{\sigma(1)}, x_{\sigma(2)}, \ldots, x_{\sigma(m)}\right)$입니다.
(a) 성장률을 최적화하는 포트폴리오 $\mathbf{b}^{*}$를 찾고 그 최적성을 확립하십시오. 이제 $\frac{1}{m} \sum_{i=1}^{m} X_{i}=1$이 되도록 $\mathbf{X}$가 정규화되었고, $F$가 이전과 같이 대칭이라고 가정합니다.
(b) 다시 $\mathbf{X}$가 정규화되었다고 가정하고, 모든 대칭 분포 $F$가 $\mathbf{b}^{*}$에 대해 동일한 성장률을 갖는다는 것을 보여주십시오.
(c) 이 성장률을 찾으십시오.
16.6 볼록성. 동일한 최적 포트폴리오를 제공하는 주식 시장 밀도의 집합에 관심이 있습니다. $P_{\mathbf{b}_{0}}$를 $\mathbf{b}_{0}$가 최적인 모든 확률 밀도의 집합이라고 합시다. 따라서 $P_{\mathbf{b}_{0}}=$ $\left\{p(x): \int \ln \left(\mathbf{b}^{t} x\right) p(x) d x\right.$가 $\left.\mathbf{b}=\mathbf{b}_{0}$에 의해 최대화됨\right\}$입니다. $P_{\mathbf{b}_{0}}$가 볼록 집합임을 보여주십시오. 정리 16.2.2를 사용하는 것이 도움이 될 수 있습니다.
16.7 공매도. 다음을 고려하십시오.

$$
X= \begin{cases}(1,2), & p \\ \left(1, \frac{1}{2}\right), & 1-p\end{cases}
$$

$B=\left\{\left(b_{1}, b_{2}\right): b_{1}+b_{2}=1\right\}$라고 합시다. 따라서 이 포트폴리오 집합 $B$는 제약 조건 $b_{i} \geq 0$을 포함하지 않습니다. (이는 공매도를 허용합니다.)
<!-- Page 680 -->
(a) $\mathbf{b}^{*}(p)$에 대한 로그 최적 포트폴리오를 찾으십시오.
(b) 성장률 $W^{*}(p)$를 엔트로피율 $H(p)$와 연관시키십시오.
16.8 x의 정규화. 상대 성장률을 최대화하는 로그 최적 포트폴리오 $\mathbf{b}^{*}$를 다음과 같이 정의한다고 가정합니다.

$$
\int \ln \frac{\mathbf{b}^{t} \mathbf{x}}{\frac{1}{m} \sum_{i=1}^{m} x_{i}} d F\left(x_{1}, \ldots, x_{m}\right)
$$

$\frac{1}{m} \sum X_{i}$의 정규화의 장점은 균일 포트폴리오와 관련된 부로 볼 수 있는데, 이는 성장률 $\int \ln b^{t} x d F(x)$가 유한하지 않을 때에도 상대 성장률이 유한하다는 것입니다. 예를 들어, $X$가 St. Petersburg와 유사한 분포를 가질 때 이는 중요합니다. 따라서 로그 최적 포트폴리오 $\mathbf{b}^{*}$는 무한한 성장률 $W^{*}(F)$를 갖는 분포를 포함한 모든 분포 $F$에 대해 정의됩니다.
(a) $\mathbf{b}$가 $\int \ln \left(\mathbf{b}^{t} \mathbf{x}\right) d F(x)$를 최대화하면, $u=\left(\frac{1}{m}, \frac{1}{m}, \ldots, \frac{1}{m}\right)$에 대해 $\int \ln \frac{\mathbf{b}^{t} \mathbf{x}}{a^{t} x} d F(x)$도 최대화함을 보이십시오.
(b) 다음의 로그 최적 포트폴리오 $\mathbf{b}^{*}$를 찾으십시오.

$$
\mathbf{X}=\left\{\begin{array}{cc}
\left(2^{2^{k}+1}, 2^{2^{k}}\right), & 2^{-(k+1)} \\
\left(2^{2^{k}}, 2^{2^{k}+1}\right), & 2^{-(k+1)}
\end{array}\right.
$$

여기서 $k=1,2, \ldots$
(c) $E X$와 $W^{*}$를 찾으십시오.
(d) $\mathbf{b}^{*}$가 $\operatorname{Pr}\left\{\mathbf{b}^{t} \mathbf{X}>c \mathbf{b}^{* t} \mathbf{X}\right\} \leq \frac{1}{c}$의 의미에서 모든 포트폴리오 $\mathbf{b}$보다 경쟁적으로 우수함을 논증하십시오.
16.9 보편 포트폴리오. $m=2$개 주식에 대해 $\mu(b)$가 균일한 (16.7.2)에서 보편 포트폴리오 구현의 첫 $n=2$ 단계를 조사합니다. 1일차와 2일차의 주식 벡터를 $\mathbf{x}_{1}=\left(1, \frac{1}{2}\right)$, $\mathbf{x}_{2}=(1,2)$라고 하고, 포트폴리오를 $\mathbf{b}=(b, 1-b)$로 나타냅니다.
(a) $S_{2}(\mathbf{b})=\prod_{i=1}^{2} \mathbf{b}^{t} \mathbf{x}_{i}, \quad 0 \leq b \leq 1$을 그래프로 그리십시오.
(b) $S_{2}^{*}=\max _{\mathbf{b}} S_{2}(\mathbf{b})$를 계산하십시오.
(c) $\log S_{2}(\mathbf{b})$가 $\mathbf{b}$에 대해 오목함을 논증하십시오.
(d) (보편) 부 $\hat{S}_{2}=\int_{0}^{1} S_{2}(\mathbf{b}) d \mathbf{b}$를 계산하십시오.
(e) 시간 $n=1$과 $n=2$에서의 보편 포트폴리오를 계산하십시오:

$$
\hat{\mathbf{b}}_{1}=\int_{0}^{1} \mathbf{b} d \mathbf{b}
$$
<!-- Page 681 -->
$$
\hat{\mathbf{b}}_{2}\left(\mathbf{x}_{1}\right)=\frac{\int_{0}^{1} \mathbf{b} S_{1}(\mathbf{b}) d \mathbf{b}}{\int_{0}^{1} S_{1}(\mathbf{b}) d \mathbf{b}}
$$

(f) $S_{2}(\mathbf{b}), S_{2}^{*}, \hat{S}_{2}, \hat{\mathbf{b}}_{2}$ 중 주식 벡터 결과의 출현 순서를 바꾸어도 변하지 않는 것은 무엇입니까? (즉, 시퀀스가 이제 $(1,2),\left(1, \frac{1}{2}\right)$ 이라면?)
16.10 성장 최적. 두 독립 주식의 가격 상대값으로 $X_{1}, X_{2} \geq 0$ 이라고 가정합니다. $E X_{1}>E X_{2}$ 라고 가정합니다. 성장률 최적 포트폴리오 $S(\mathbf{b})=b X_{1}+\bar{b} X_{2}$ 에 항상 $X_{1}$ 의 일부를 포함시키고 싶습니까? 증명하거나 반례를 제시하십시오.
16.11 보편성의 비용. 유한 기간 보편 포트폴리오 논의에서 보편성으로 인한 손실 계수는 다음과 같다고 показано 했습니다.

$$
\frac{1}{V_{n}}=\sum_{k=0}^{n}\binom{n}{k}\left(\frac{k}{n}\right)^{k}\left(\frac{n-k}{n}\right)^{n-k}
$$

$n=1,2,3$ 에 대해 $V_{n}$ 을 평가하십시오.
16.12 볼록 집합. 이 문제는 정리 16.2.2를 일반화합니다. $\mathcal{S}$ 를 확률 변수의 볼록 집합이라고 부르는데, 이는 $S_{1}, S_{2} \in \mathcal{S}$ 이면 $\lambda S_{1}+(1-\lambda) S_{2} \in \mathcal{S}$ 임을 의미합니다. $\mathcal{S}$ 를 닫힌 볼록 집합이라고 가정합니다. 다음이 성립하는 확률 변수 $S^{*} \in \mathcal{S}$ 가 존재함을 보이십시오.

$$
E \ln \left(\frac{S}{S^{*}}\right) \leq 0
$$

모든 $S \in \mathcal{S}$ 에 대해, 다음이 성립할 필요충분조건은 다음과 같습니다.

$$
E\left(\frac{S}{S^{*}}\right) \leq 1
$$

모든 $S \in \mathcal{S}$ 에 대해.

# 역사적 참고 자료

주식 시장 투자에 대한 평균-분산 접근 방식에 대한 광범위한 문헌이 있습니다. 좋은 소개는 Sharpe [491]의 책입니다. 로그 최적 포트폴리오는 Kelly [308] 및 Latané [346]에 의해 도입되었고 Breiman [75]에 의해 일반화되었습니다. 증가에 대한 상한은
<!-- Page 682 -->
상호 정보량에서의 성장률은 Barron과 Cover [31]에 의한 것입니다. 로그 최적 투자에 대한 비판은 Samuelson [453, 454]을 참조하십시오.

로그 최적 포트폴리오의 경쟁적 최적성에 대한 증명은 Bell과 Cover [39, 40]에 의한 것입니다. Breiman [75]은 랜덤 시장 프로세스에 대한 점근적 최적성을 조사했습니다.

AEP는 Shannon에 의해 도입되었습니다. 주식 시장에 대한 AEP와 로그 최적 투자의 점근적 최적성은 Algoet과 Cover [21]에 제시되어 있습니다. AEP에 대한 비교적 간단한 샌드위치 증명은 Algoet과 Cover [20]에 의한 것입니다. 실수 열거 가능한 프로세스에 대한 AEP는 Barron [34]과 Orey [402]에 의해 완전히 일반화되어 증명되었습니다.

유니버설 포트폴리오는 Cover [110]에서 정의되었으며, 유니버설리티에 대한 증명은 Cover [110]에서, 더 정확하게는 Cover와 Ordentlich [135]에서 제시되었습니다. 고정 기간 유니버설리티 비용 $V_{n}$의 정확한 계산은 Ordentlich와 Cover [401]에 제시되어 있습니다. $V_{n}$이라는 양은 Shtarkov [496]의 데이터 압축 작업에서도 나타납니다.
<!-- Page 683 -->
# CHAPTER 17

## 정보 이론에서의 부등식

이 장에서는 이 책 전반에 걸쳐 발견된 부등식들을 요약하고 재구성합니다. 부분집합의 엔트로피율에 대한 몇 가지 새로운 부등식과 엔트로피와 $\mathbb{L}_{p}$ 노름의 관계도 개발됩니다. Fisher 정보와 엔트로피 사이의 밀접한 관계를 탐구하며, 엔트로피 파워 부등식과 Brunn-Minkowski 부등식의 공통 증명으로 이어집니다. 또한 정보 이론의 부등식과 행렬 이론 및 확률 이론과 같은 다른 수학 분야의 부등식 사이의 유사점도 탐구합니다.

### 17.1 정보 이론의 기본 부등식

정보 이론의 많은 기본 부등식은 볼록성에서 직접적으로 도출됩니다.

정의 함수 $f$가 볼록하다고 말하는 것은 다음을 만족할 때입니다.

$$
f\left(\lambda x_{1}+(1-\lambda) x_{2}\right) \leq \lambda f\left(x_{1}\right)+(1-\lambda) f\left(x_{2}\right)
$$

모든 $0 \leq \lambda \leq 1$ 및 모든 $x_{1}$과 $x_{2}$에 대해.
정리 17.1.1 (정리 2.6.2: Jensen의 부등식) $f$가 볼록하면,

$$
f(E X) \leq E f(X)
$$

보조정리 17.1.1 함수 $\log x$는 오목하고 $x \log x$는 볼록합니다 ($0<x<\infty$).

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (c) 2006 John Wiley \& Sons, Inc.
<!-- Page 684 -->
정리 17.1.2 (정리 2.7.1: 로그 합 부등식) 양수 $a_{1}, a_{2}, \ldots, a_{n}$ 및 $b_{1}, b_{2}, \ldots, b_{n}$에 대하여,

$$
\sum_{i=1}^{n} a_{i} \log \frac{a_{i}}{b_{i}} \geq\left(\sum_{i=1}^{n} a_{i}\right) \log \frac{\sum_{i=1}^{n} a_{i}}{\sum_{i=1}^{n} b_{i}}
$$

등호는 $\frac{a_{i}}{b_{i}}=$ 상수일 때 성립합니다.
2.1절에서 entropy의 다음 속성들을 다시 살펴봅니다.
정의 이산 확률 변수 $X$의 entropy $H(X)$는 다음과 같이 정의됩니다.

$$
H(X)=-\sum_{x \in \mathcal{X}} p(x) \log p(x)
$$

정리 17.1.3 (정리 2.1.1, 정리 2.6.4: Entropy 상한)

$$
0 \leq H(X) \leq \log |\mathcal{X}|
$$

정리 17.1.4 (정리 2.6.5: 조건부 entropy 감소) 임의의 두 확률 변수 $X$와 $Y$에 대하여,

$$
H(X \mid Y) \leq H(X)
$$

등호는 $X$와 $Y$가 독립일 때 성립합니다.
정리 17.1.5 (정리 2.5.1과 정리 2.6.6: 연쇄 법칙)

$$
H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right) \leq \sum_{i=1}^{n} H\left(X_{i}\right)
$$

등호는 $X_{1}, X_{2}, \ldots, X_{n}$이 독립일 때 성립합니다.
정리 17.1.6 (정리 2.7.3) $H(p)$는 $p$에 대한 오목 함수입니다.
이제 상대 entropy와 상호 정보량(2.3절)의 몇 가지 속성을 기술합니다.

정의 두 확률 질량 함수 $p(x)$와 $q(x)$ 사이의 상대 entropy 또는 Kullback-Leibler 거리(Kullback-Leibler distance)는 다음과 같이 정의됩니다.

$$
D(p \| q)=\sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}
$$
<!-- Page 685 -->
정의 두 확률 변수 $X$와 $Y$ 간의 mutual information은 다음과 같이 정의됩니다.

$$
I(X ; Y)=\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}=D(p(x, y) \| p(x) p(y))
$$

다음의 기본적인 정보 부등식은 이 장의 다른 많은 부등식을 증명하는 데 사용될 수 있습니다.

정리 17.1.7 (정리 2.6.3: 정보 부등식) 임의의 두 확률 질량 함수 $p$와 $q$에 대해,

$$
D(p \| q) \geq 0
$$

이며, 등호는 모든 $x \in \mathcal{X}$에 대해 $p(x)=q(x)$일 때 성립합니다.
따름 정리 임의의 두 확률 변수 $X$와 $Y$에 대해,

$$
I(X ; Y)=D(p(x, y) \| p(x) p(y)) \geq 0
$$

이며, 등호는 $p(x, y)=p(x) p(y)$ (즉, $X$와 $Y$가 독립일 때) 성립합니다.
정리 17.1.8 (정리 2.7.2: 상대 엔트로피의 볼록성) $D(p \| q)$는 쌍 $(p, q)$에 대해 볼록입니다.

정리 17.1.9 (정리 2.4.1)

$$
\begin{aligned}
& I(X ; Y)=H(X)-H(X \mid Y) \\
& I(X ; Y)=H(Y)-H(Y \mid X) \\
& I(X ; Y)=H(X)+H(Y)-H(X, Y) \\
& I(X ; X)=H(X)
\end{aligned}
$$

정리 17.1.10 (섹션 4.4) Markov chain에 대해:

1. 상대 엔트로피 $D\left(\mu_{n} \| \mu_{n}^{\prime}\right)$는 시간에 따라 감소합니다.
2. 분포와 정상 분포 간의 상대 엔트로피 $D\left(\mu_{n} \| \mu\right)$는 시간에 따라 감소합니다.
3. 정상 분포가 균일 분포일 경우 엔트로피 $H\left(X_{n}\right)$는 증가합니다.
4. 정상 Markov chain에 대해 조건부 엔트로피 $H\left(X_{n} \mid X_{1}\right)$는 시간에 따라 증가합니다.
<!-- Page 686 -->
정리 17.1.11 $X_{1}, X_{2}, \ldots, X_{n}$을 i.i.d. $\sim p(x)$라고 가정합니다. $\hat{p}_{n}$을 $X_{1}, X_{2}, \ldots, X_{n}$의 경험적 확률 질량 함수라고 가정합니다. 그러면

$$
E D\left(\hat{p}_{n}| | p\right) \leq E D\left(\hat{p}_{n-1}| | p\right)
$$

# 17.2 차분 엔트로피

이제 차분 엔트로피(섹션 8.1)의 몇 가지 기본 속성을 검토합니다.

정의 차분 엔트로피 $h\left(X_{1}, X_{2}, \ldots, X_{n}\right)$는 때때로 $h(f)$로 표기되며 다음과 같이 정의됩니다.

$$
h\left(X_{1}, X_{2}, \ldots, X_{n}\right)=-\int f(\mathbf{x}) \log f(\mathbf{x}) d \mathbf{x}
$$

많은 일반적인 밀도의 차분 엔트로피는 표 17.1에 나와 있습니다.

정의 확률 밀도 $f$와 $g$ 사이의 상대 엔트로피는 다음과 같이 정의됩니다.

$$
D(f \| g)=\int f(\mathbf{x}) \log (f(\mathbf{x}) / g(\mathbf{x})) d \mathbf{x}
$$

연속 버전의 상대 엔트로피 속성은 이산 버전과 동일합니다. 반면에 차분 엔트로피는 이산 엔트로피와 다른 몇 가지 속성을 가집니다. 예를 들어, 차분 엔트로피는 음수일 수 있습니다.

이제 차분 엔트로피에 대해 계속 유지되는 몇 가지 정리를 다시 명시합니다.

정리 17.2.1 (정리 8.6.1: 조건부 엔트로피 감소) $h(X \mid Y) \leq h(X)$이며, $X$와 $Y$가 독립일 때 등호가 성립합니다.

정리 17.2.2 (정리 8.6.2: 연쇄 법칙)

$$
h\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} h\left(X_{i} \mid X_{i-1}, X_{i-2}, \ldots, X_{1}\right) \leq \sum_{i=1}^{n} h\left(X_{i}\right)
$$

$X_{1}, X_{2}, \ldots, X_{n}$이 독립일 때 등호가 성립합니다.
보조 정리 17.2.1 $X$와 $Y$가 독립이면 $h(X+Y) \geq h(X)$입니다.
증명: $\quad h(X+Y) \geq h(X+Y \mid Y)=h(X \mid Y)=h(X)$입니다.
<!-- Page 687 -->
TABLE 17.1 차분 엔트로피 ${ }^{a}$

| 분포 |  | 엔트로피 (nats) |
| :--: | :--: | :--: |
| 이름 | 밀도 |  |
| 베타 | $\begin{aligned} & f(x)=\frac{x^{p-1}(1-x)^{q-1}}{B(p, q)} \\ & 0 \leq x \leq 1, p, q>0 \end{aligned}$ | $\begin{aligned} & \ln B(p, q)-(p-1) \\ & \times[\psi(p)-\psi(p+q)] \\ & -(q-1)[\psi(q)-\psi(p+q)] \end{aligned}$ |
| 코시 | $\begin{aligned} & f(x)=\frac{\lambda}{\pi} \frac{1}{\lambda^{2}+x^{2}} \\ & -\infty<x<\infty, \lambda>0 \end{aligned}$ | $\ln (4 \pi \lambda)$ |
| 카이 | $\begin{aligned} & f(x)=\frac{2}{2^{n / 2} \sigma^{n} \Gamma(n / 2)} x^{n-1} e^{-\frac{x^{2}}{2 \sigma^{2}}} \\ & x>0, n>0 \end{aligned}$ | $\ln \frac{\sigma \Gamma(n / 2)}{\sqrt{2}}-\frac{n-1}{2} \psi\left(\frac{n}{2}\right)+\frac{n}{2}$ |
| 카이제곱 | $\begin{aligned} & f(x)=\frac{1}{2^{n / 2} \sigma^{n} \Gamma(n / 2)} x^{\frac{n}{2}}-1 e^{-\frac{x}{2 \sigma^{2}}} \\ & x>0, n>0 \end{aligned}$ | $\begin{aligned} & \ln 2 \sigma^{2} \Gamma\left(\frac{n}{2}\right) \\ & -\left(1-\frac{n}{2}\right) \psi\left(\frac{n}{2}\right)+\frac{n}{2} \end{aligned}$ |
| 얼랑 | $\begin{aligned} & f(x)=\frac{\beta^{n}}{(n-1)!} x^{n-1} e^{-\beta x} \\ & x, \beta>0, n>0 \end{aligned}$ | $(1-n) \psi(n)+\ln \frac{\Gamma(n)}{\beta}+n$ |
| 지수 | $f(x)=\frac{1}{\lambda} e^{-\frac{x}{\lambda}}, x, \lambda>0$ | $1+\ln \lambda$ |
| F | $\begin{aligned} & f(x)=\frac{\frac{n_{1}}{n_{1}^{2}} \frac{n_{2}}{n_{2}^{2}}}{B\left(\frac{n_{1}}{2}, \frac{n_{2}}{2}\right)} \\ &x>0, n_{1}, n_{2}>0 \end{aligned}$ | $\begin{aligned} & \ln \frac{n_{1}}{n_{2}} B\left(\frac{n_{1}}{2}, \frac{n_{2}}{2}\right) \\ & +\left(1-\frac{n_{1}}{2}\right) \psi\left(\frac{n_{1}}{2}\right) \\ & -\left(1-\frac{n_{2}}{2}\right) \psi\left(\frac{n_{2}}{2}\right) \\ & +\frac{n_{1}+n_{2}}{2} \psi\left(\frac{n_{1}+n_{2}}{2}\right) \end{aligned}$ |
| 감마 | $f(x)=\frac{x^{\alpha-1} e^{-\frac{x}{\beta}}}{\beta^{\alpha} \Gamma(\alpha)}, \quad x, \alpha, \beta>0$ | $\ln (\beta \Gamma(\alpha))+(1-\alpha) \psi(\alpha)+\alpha$ |
| 라플라스 | $\begin{aligned} & f(x)=\frac{1}{2 \lambda} e^{-\frac{|x-\theta|}{\lambda}} \\ & -\infty<x, \theta<\infty, \lambda>0 \end{aligned}$ | $1+\ln 2 \lambda$ |
| 로지스틱 | $\begin{aligned} & f(x)=\frac{e^{-x}}{\left(1+e^{-x}\right)^{2}} \\ & -\infty<x<\infty \end{aligned}$ | 2 |
<!-- Page 688 -->
TABLE 17.1 (continued)

| 분포 |  | 엔트로피 (nats) |
| :--: | :--: | :--: |
| 이름 | 밀도 |  |
| 로그정규분포 | $\begin{aligned} & f(x)=\frac{1}{\sigma x \sqrt{2 \pi}} e^{-\frac{\ln (x-m)^{2}}{2 \sigma^{2}}} \\ & x>0,-\infty<m<\infty, \sigma>0 \end{aligned}$ | $m+\frac{1}{2} \ln \left(2 \pi e \sigma^{2}\right)$ |
| 맥스웰- <br> 볼츠만 | $\begin{aligned} & f(x)=4 \pi^{-\frac{1}{2}} \beta^{\frac{3}{2}} x^{2} e^{-\beta x^{2}} \\ & x, \beta>0 \end{aligned}$ | $\frac{1}{2} \ln \frac{\pi}{\beta}+\gamma-\frac{1}{2}$ |
| 정규분포 | $\begin{aligned} & f(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} \\ & -\infty<x, \mu<\infty, \sigma>0 \end{aligned}$ | $\frac{1}{2} \ln \left(2 \pi e \sigma^{2}\right)$ |
| 일반화 정규분포 | $\begin{aligned} & f(x)=\frac{2 \beta^{\frac{\alpha}{2}}}{\Gamma\left(\frac{\alpha}{2}\right)} x^{\alpha-1} e^{-\beta x^{2}} \\ & x, \alpha, \beta>0 \end{aligned}$ | $\ln \frac{\Gamma\left(\frac{\alpha}{2}\right)}{2 \beta^{\frac{1}{2}}}-\frac{\alpha-1}{2} \psi\left(\frac{\alpha}{2}\right)+\frac{\alpha}{2}$ |
| 파레토 분포 | $f(x)=\frac{a k^{a}}{x^{a+1}}, x \geq k>0, a>0$ | $\ln \frac{k}{a}+1+\frac{1}{a}$ |
| 레일리 분포 | $f(x)=\frac{x}{b^{2}} e^{-\frac{x^{2}}{2 b^{2}}}, \quad x, b>0$ | $1+\ln \frac{\beta}{\sqrt{2}}+\frac{\gamma}{2}$ |
| 스튜던트 t 분포 | $\begin{aligned} & f(x)=\frac{\left(1+x^{2} / n\right)^{-(n+1) / 2}}{\sqrt{n} B\left(\frac{1}{2}, \frac{n}{2}\right)} \\ & -\infty<x<\infty, n>0 \end{aligned}$ | $\begin{aligned} & \frac{n+1}{2} \psi\left(\frac{n+1}{2}\right)-\psi\left(\frac{n}{2}\right) \\ & +\ln \sqrt{n} B\left(\frac{1}{2}, \frac{n}{2}\right) \end{aligned}$ |
| 삼각 분포 | $f(x)= \begin{cases}\frac{2 x}{\mu}, & 0 \leq x \leq a \\ \frac{2(1-x)}{1-a}, & a \leq x \leq 1\end{cases}$ | $\frac{1}{2}-\ln 2$ |
| 균등 분포 | $f(x)=\frac{1}{\beta-\alpha}, \alpha \leq x \leq \beta$ | $\ln (\beta-\alpha)$ |
| 와이블 분포 | $f(x)=\frac{c}{\alpha} x^{c-1} e^{-\frac{x^{c}}{\alpha}}, \quad x, c, \alpha>0$ | $\frac{(c-1) \gamma}{c}+\ln \frac{\alpha^{\frac{1}{c}}}{c}+1$ |

[^0]
[^0]:    ${ }^{a}$ 모든 엔트로피는 nats 단위입니다. $\Gamma(z)=\int_{0}^{\infty} e^{-t} t^{z-1} d t ; \psi(z)=\frac{d}{d z} \ln \Gamma(z) ; \gamma=$ 오일러 상수 $=$ $0.57721566 \ldots$
    출처: Lazo and Rathie [543].
<!-- Page 689 -->
정리 17.2.3 (정리 8.6.5) 영평균과 공분산 $K=E \mathbf{X X}^{\prime}$ (즉, $K_{i j}=E X_{i} X_{j}, 1 \leq i, j \leq n$)을 갖는 확률 벡터 $\mathbf{X} \in \mathbf{R}^{n}$가 있다고 가정합니다. 그러면

$$
h(\mathbf{X}) \leq \frac{1}{2} \log (2 \pi e)^{n}|K|
$$

등호는 $\mathbf{X} \sim \mathcal{N}(0, K)$일 때 성립합니다.

# 17.3 엔트로피 및 상대 엔트로피에 대한 경계

이 절에서는 엔트로피 함수에 대한 몇 가지 경계를 다시 살펴봅니다. 가장 유용한 것은 Fano의 부등식으로, 이는 통신 채널의 용량 이상의 속도에서 최적의 복호기 오류 확률을 0에서 멀리 떨어뜨리는 데 사용됩니다.

정리 17.3.1 (정리 2.10.1: Fano의 부등식) 두 확률 변수 $X$와 $Y$가 주어졌을 때, $\hat{X}=g(Y)$를 $Y$에 대한 $X$의 추정량이라고 하고, 오류 확률을 $P_{e}=\operatorname{Pr}(X \neq \hat{X})$라고 하면, 다음이 성립합니다.

$$
H\left(P_{e}\right)+P_{e} \log |\mathcal{X}| \geq H(X \mid \hat{X}) \geq H(X \mid Y)
$$

결과적으로, $H(X \mid Y)>0$이면 $P_{e}>0$입니다.
유사한 결과가 다음 보조정리에서 주어집니다.
보조정리 17.3.1 (보조정리 2.10.1) $X$와 $X^{\prime}$가 엔트로피 $H(X)$를 갖는 i.i.d.라면

$$
\operatorname{Pr}\left(X=X^{\prime}\right) \geq 2^{-H(X)}
$$

등호는 $X$가 균등 분포를 가질 때만 성립합니다.
Fano의 부등식의 연속적인 유사체는 추정량의 평균 제곱 오차를 경계합니다.

정리 17.3.2 (정리 8.6.6) 차분 엔트로피 $h(X)$를 갖는 확률 변수 $X$가 있다고 가정합니다. $\hat{X}$를 $X$의 추정량이라고 하고, 예측 오류의 기댓값을 $E(X-\hat{X})^{2}$라고 하면, 다음이 성립합니다.

$$
E(X-\hat{X})^{2} \geq \frac{1}{2 \pi e} e^{2 h(X)}
$$

부수 정보 $Y$와 추정량 $\hat{X}(Y)$가 주어졌을 때,

$$
E(X-\hat{X}(Y))^{2} \geq \frac{1}{2 \pi e} e^{2 h(X \mid Y)}
$$
<!-- Page 690 -->
정리 17.3.3 ($\mathcal{L}_{1}$ 엔트로피에 대한 상한) $p$와 $q$를 $\mathcal{X}$ 상의 두 확률 질량 함수라고 할 때,

$$
\|p-q\|_{1}=\sum_{x \in \mathcal{X}}|p(x)-q(x)| \leq \frac{1}{2}
$$

이면,

$$
|H(p)-H(q)| \leq-\|p-q\|_{1} \log \frac{\|p-q\|_{1}}{|\mathcal{X}|}
$$

증명: 그림 17.1에 표시된 함수 $f(t)=-t \log t$를 고려하십시오. 미분을 통해 함수 $f(\cdot)$가 오목 함수임을 확인할 수 있습니다. 또한, $f(0)=f(1)=0$입니다. 따라서 함수는 0과 1 사이에서 양수입니다. $t$와 $t+v$ (여기서 $v \leq \frac{1}{2}$) 사이의 함수의 현을 고려하십시오. 현의 최대 절대 기울기는 양 끝( $t=0$ 또는 $1-v$일 때)에 있습니다. 따라서 $0 \leq t \leq 1-v$에 대해 다음이 성립합니다.

$$
|f(t)-f(t+v)| \leq \max \{f(v), f(1-v)\}=-v \log v
$$

$r(x)=|p(x)-q(x)|$라고 하면,

$$
\begin{aligned}
& |H(p)-H(q)|=\left|\sum_{x \in \mathcal{X}}(-p(x) \log p(x)+q(x) \log q(x))\right| \\
& \quad \leq \sum_{x \in \mathcal{X}}\left|(-p(x) \log p(x)+q(x) \log q(x))\right|
\end{aligned}
$$

그림 17.1. 함수 $f(t)=-t \ln t$.
<!-- Page 691 -->
$$
\begin{aligned}
& \leq \sum_{x \in \mathcal{X}}-r(x) \log r(x) \\
& =\|p-q\|_{1} \sum_{x \in \mathcal{X}}-\frac{r(x)}{\|p-q\|_{1}} \log \frac{r(x)}{\|p-q\|_{1}}\|p-q\|_{1} \\
& =-\|p-q\|_{1} \log \|p-q\|_{1}+\|p-q\|_{1} H\left(\frac{r(x)}{\|p-q\|_{1}}\right) \\
& \leq-\|p-q\|_{1} \log \|p-q\|_{1}+\|p-q\|_{1} \log |\mathcal{X}|
\end{aligned}
$$

여기서 (17.30)은 (17.27)으로부터 유도됩니다.
마지막으로, 상대 엔트로피는 다음과 같은 의미에서 $\mathcal{L}_{1}$ 노름보다 더 강력합니다.

정리 17.3.2 (정리 11.6.1)

$$
D\left(p_{1} \| p_{2}\right) \geq \frac{1}{2 \ln 2}\left\|p_{1}-p_{2}\right\|_{1}^{2}
$$

두 확률 질량 함수 $P(x)$와 $Q(x)$ 사이의 상대 엔트로피는 $P=Q$일 때 0입니다. 이 점 주변에서 상대 엔트로피는 이차적인 거동을 보이며, 상대 엔트로피 $D(P \| Q)$를 $P=Q$ 지점 주변에서 테일러 급수 전개했을 때 첫 번째 항은 분포 $P$와 $Q$ 사이의 카이제곱 거리입니다. 다음을 정의합니다.

$$
\chi^{2}(P, Q)=\sum_{x} \frac{(P(x)-Q(x))^{2}}{Q(x)}
$$

정리 17.3.3 $P$가 $Q$에 가까울 때,

$$
D(P \| Q)=\frac{1}{2} \chi^{2}+\cdots
$$

증명: 문제 11.2를 참조하십시오.

# 17.4 타입에 대한 부등식

타입 방법은 대규모 편차 이론 및 오류 지수에서 결과를 증명하는 강력한 도구입니다. 기본 정리를 반복합니다.

정리 17.4.1 (정리 11.1.1) 분모가 $n$인 타입의 수는 다음과 같이 제한됩니다.

$$
\left|\mathcal{P}_{n}\right| \leq(n+1)^{|\mathcal{X}|}
$$
<!-- Page 692 -->
정리 17.4.2 (정리 11.1.2) $X_{1}, X_{2}, \ldots, X_{n}$이 $Q(x)$에 따라 i.i.d.로 추출되었다면, $x^{n}$의 확률은 그 타입에만 의존하며 다음과 같이 주어집니다.

$$
Q^{n}\left(x^{n}\right)=2^{-n\left(H\left(P_{x^{n}}\right)+D\left(P_{x^{n}} \| Q\right)\right)}
$$

정리 17.4.3 (정리 11.1.3: 타입 클래스 $T(P)$의 크기) 임의의 타입 $P \in \mathcal{P}_{n}$에 대해,

$$
\frac{1}{(n+1)^{|\mathcal{X}|}} 2^{n H(P)} \leq|T(P)| \leq 2^{n H(P)}
$$

정리 17.4.4 (정리 11.1.4) 임의의 $P \in \mathcal{P}_{n}$ 및 임의의 분포 $Q$에 대해, $Q^{n}$ 하에서의 타입 클래스 $T(P)$의 확률은 지수에서 첫 번째 근사값으로 $2^{-n D(P \| Q)}$입니다. 더 정확하게는,

$$
\frac{1}{(n+1)^{|\mathcal{X}|}} 2^{-n D(P \| Q)} \leq Q^{n}(T(P)) \leq 2^{-n D(P \| Q)}
$$

# 17.5 엔트로피에 대한 조합적 경계

Wozencraft와 Reiffen [568]의 결과를 사용하여 $k$가 0 또는 $n$이 아닐 때 $\binom{n}{k}$의 크기에 대한 엄격한 경계를 제시합니다.

정리 17.5.1 $0<p<1, q=1-p$이고 $n p$가 정수일 때,

$$
\frac{1}{\sqrt{8 n p q}} \leq\binom{ n}{n p} 2^{-n H(p)} \leq \frac{1}{\sqrt{\pi n p q}}
$$

증명: 우리는 스털링 근사의 강력한 형태 [208]로 시작합니다. 이는 다음과 같이 말합니다.

$$
\sqrt{2 \pi n}\left(\frac{n}{e}\right)^{n} \leq n!\leq \sqrt{2 \pi n}\left(\frac{n}{e}\right)^{n} e^{\frac{1}{12 n}}
$$

이를 적용하여 상한을 구하면 다음과 같습니다.

$$
\begin{aligned}
\binom{n}{n p} & \leq \frac{\sqrt{2 \pi n}\left(\frac{n}{e}\right)^{n} e^{\frac{1}{12 n}}}{\sqrt{2 \pi n p}\left(\frac{n p}{e}\right)^{n p} \sqrt{2 \pi n q}\left(\frac{n q}{e}\right)^{n q}} \\
& =\frac{1}{\sqrt{2 \pi n p q}} \frac{1}{p^{n p} q^{n q}} e^{\frac{1}{12 n}}
\end{aligned}
$$
<!-- Page 693 -->
$$
<\frac{1}{\sqrt{\pi n p q}} 2^{n H(p)}
$$

$e^{\frac{1}{12 n}}<e^{\frac{1}{12}}=1.087<\sqrt{2}$이므로, 상한을 증명합니다.
하한은 유사하게 얻어집니다. Stirling의 공식을 사용하면 다음과 같습니다.

$$
\begin{aligned}
\binom{n}{n p} & \geq \frac{\sqrt{2 \pi n}\left(\frac{n}{e}\right)^{n} e^{-\left(\frac{1}{12 n p}+\frac{1}{12 n q}\right)}}{\sqrt{2 \pi n p}\left(\frac{n p}{e}\right)^{n p} \sqrt{2 \pi n q}\left(\frac{n q}{e}\right)^{n q}} \\
& =\frac{1}{\sqrt{2 \pi n p q}} \frac{1}{p^{n p} q^{n q}} e^{-\left(\frac{1}{12 n p}+\frac{1}{12 n q}\right)} \\
& =\frac{1}{\sqrt{2 \pi n p q}} 2^{n H(p)} e^{-\left(\frac{1}{12 n p}+\frac{1}{12 n q}\right)}
\end{aligned}
$$

$n p \geq 1$이고 $n q \geq 3$이면,

$$
e^{-\left(\frac{1}{12 n p}+\frac{1}{12 n q}\right)} \geq e^{-\frac{1}{9}}=0.8948>\frac{\sqrt{\pi}}{2}=0.8862
$$

그리고 하한은 이를 방정식에 대입하여 직접적으로 얻어집니다. 이 조건의 예외는 $n p=1, n q=1$ 또는 2, 그리고 $n p=2, n q=2$인 경우입니다 ($n p \geq 3, n q=1$ 또는 2인 경우는 $p$와 $q$의 역할을 바꾸어 처리할 수 있습니다). 이 각 경우에 대해

$$
\begin{aligned}
& n p=1, n q=1 \rightarrow n=2, p=\frac{1}{2}, \quad\binom{ n}{n p}=2, \text { bound }=2 \\
& n p=1, n q=2 \rightarrow n=3, p=\frac{1}{3}, \quad\binom{ n}{n p}=3, \text { bound }=2.92 \\
& n p=2, n q=2 \rightarrow n=4, p=\frac{1}{2}, \quad\binom{ n}{n p}=6, \text { bound }=5.66
\end{aligned}
$$

따라서 이러한 특별한 경우에도 바운드는 유효하며, 그러므로 하한은 모든 $p \neq 0,1$에 대해 유효합니다. 하한은 $p=0$ 또는 $p=1$일 때 발산하므로 유효하지 않다는 점에 유의하십시오.

# 17.6 부분집합의 엔트로피율

이제 미분 엔트로피에 대한 연쇄 법칙을 일반화합니다. 연쇄 법칙은 각 확률 변수의 엔트로피를 사용하여 확률 변수 모음의 엔트로피율에 대한 바운드를 제공합니다.

$$
h\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq \sum_{i=1}^{n} h\left(X_{i}\right)
$$
<!-- Page 694 -->
이것은 무작위 변수 집합의 부분 집합에 대한 요소당 entropy가 부분 집합의 크기가 증가함에 따라 감소함을 보여주기 위해 확장됩니다. 이것은 각 부분 집합에 대해 참이 아니라, 정리 17.6.1에서 표현된 대로 부분 집합에 대한 평균으로 참입니다.

정의: $\left(X_{1}, X_{2}, \ldots, X_{n}\right)$가 밀도를 가지며, 모든 $S \subseteq$ $\{1,2, \ldots, n\}$에 대해 $X(S)$를 $\left\{X_{i}: i \in S\right)$의 부분 집합으로 나타냅니다. 다음을 정의합니다.

$$
h_{k}^{(n)}=\frac{1}{\binom{n}{k}} \sum_{S:|S|=k} \frac{h(X(S))}{k}
$$

여기서 $h_{k}^{(n)}$은 $\left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$에서 무작위로 추출된 $k$개 요소 부분 집합의 기호당 평균 entropy(비트 단위)입니다.

Han [270]의 다음 정리는 평균 entropy가 부분 집합의 크기에 따라 단조롭게 감소한다고 말합니다.

# 정리 17.6.1

$$
h_{1}^{(n)} \geq h_{2}^{(n)} \geq \cdots \geq h_{n}^{(n)}
$$

증명: 먼저 마지막 부등식, 즉 $h_{n}^{(n)} \leq h_{n-1}^{(n)}$을 증명합니다. 다음을 작성합니다.

$$
\begin{aligned}
h\left(X_{1}, X_{2}, \ldots, X_{n}\right)= & h\left(X_{1}, X_{2}, \ldots, X_{n-1}\right)+h\left(X_{n} \mid X_{1}, X_{2}, \ldots, X_{n-1}\right) \\
h\left(X_{1}, X_{2}, \ldots, X_{n}\right)= & h\left(X_{1}, X_{2}, \ldots, X_{n-2}, X_{n}\right) \\
& +h\left(X_{n-1} \mid X_{1}, X_{2}, \ldots, X_{n-2}, X_{n}\right) \\
\leq & h\left(X_{1}, X_{2}, \ldots, X_{n-2}, X_{n}\right) \\
& +h\left(X_{n-1} \mid X_{1}, X_{2}, \ldots, X_{n-2}\right) \\
& \vdots \\
h\left(X_{1}, X_{2}, \ldots, X_{n}\right) & \leq h\left(X_{2}, X_{3}, \ldots, X_{n}\right)+h\left(X_{1}\right)
\end{aligned}
$$

이 $n$개의 부등식을 더하고 연쇄 법칙을 사용하면 다음을 얻습니다.

$$
\begin{aligned}
n h\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq & \sum_{i=1}^{n} h\left(X_{1}, X_{2}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{n}\right) \\
& +h\left(X_{1}, X_{2}, \ldots, X_{n}\right)
\end{aligned}
$$

또는

$$
\frac{1}{n} h\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq \frac{1}{n} \sum_{i=1}^{n} \frac{h\left(X_{1}, X_{2}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{n}\right)}{n-1}
$$
<!-- Page 695 -->
원하는 결과는 $h_{n}^{(n)} \leq h_{n-1}^{(n)}$ 입니다. 이제 $k \leq n$인 모든 $k$에 대해 $h_{k}^{(n)} \leq h_{k-1}^{(n)}$ 임을 증명할 것입니다. 먼저 $k$개의 원소로 이루어진 부분집합에 대해 조건부 확률을 계산하고, 그 다음 해당 부분집합의 $(k-1)$개의 원소로 이루어진 부분집합들에 대해 균등하게 선택합니다. 각 $k$개의 원소로 이루어진 부분집합에 대해 $h_{k}^{(k)} \leq h_{k-1}^{(k)}$ 이므로, $n$개의 원소에서 균등하게 선택된 모든 $k$개의 원소로 이루어진 부분집합에 대한 기댓값을 취한 후에도 이 부등식은 성립합니다.

정리 17.6.2 $r>0$이라고 가정하고 다음과 같이 정의합니다.

$$
t_{k}^{(n)}=\frac{1}{\binom{n}{k}} \sum_{S:|S|=k} e^{\frac{r h(X(S))}{k}}
$$

그러면 다음과 같습니다.

$$
t_{1}^{(n)} \geq t_{2}^{(n)} \geq \cdots \geq t_{n}^{(n)}
$$

증명: (17.54)에서 시작하여 양변에 $r$을 곱하고 지수 함수를 적용한 후, 산술 평균-기하 평균 부등식을 적용하여 다음과 같이 얻습니다.

$$
\begin{aligned}
e^{\frac{1}{n}} r h
& \left(X_{1}, X_{2}, \ldots, X_{n}\right) \\
& \leq e^{\frac{1}{n} \sum_{i=1}^{n} \frac{r h\left(X_{1}, X_{2}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{n}\right)}{(n-1)}} \\
& \leq \frac{1}{n} \sum_{i=1}^{n} e^{\frac{r h\left(X_{1}, X_{2}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{n}\right)}{(n-1)}} \quad \text { 모든 } r \geq 0 \text{에 대해}
\end{aligned}
$$

이는 $t_{n}^{(n)} \leq t_{n-1}^{(n)}$과 동등합니다. 이제 정리 17.6.1과 동일한 논증을 사용하여 모든 부분집합에 대한 평균을 취함으로써 모든 $k \leq n$에 대해 $t_{k}^{(n)} \leq t_{k-1}^{(n)}$임을 증명합니다.

정의 $\{1,2, \ldots, n\}$의 $k$개의 원소로 이루어진 부분집합에 대한 모든 부분집합의 평균 조건부 엔트로피율은 $k$개의 원소로 이루어진 부분집합에 대한 위 양들의 평균입니다.

$$
g_{k}^{(n)}=\frac{1}{\binom{n}{k}} \sum_{S:|S|=k} \frac{h\left(X(S) \mid X\left(S^{c}\right)\right)}{k}
$$

여기서 $g_{k}(S)$는 집합 $S$의 엔트로피율이며, 이는 집합 $S^{c}$의 원소들에 대한 조건부 확률입니다. 집합 $S$의 크기가 증가할 때, 집합 $S$ 내의 원소들 간의 의존성이 증가할 것으로 예상할 수 있으며, 이는 정리 17.6.1을 설명합니다.
<!-- Page 696 -->
조건부 엔트로피를 요소당으로 볼 때, $k$가 증가함에 따라 조건 집합 $S^{c}$의 크기는 감소하고 집합 $S$의 엔트로피는 증가합니다. Han [270]의 다음 정리에 의해 알 수 있듯이, 추가적인 요소 간의 의존성으로 인한 감소보다 조건화 감소로 인한 요소당 엔트로피 증가는 더 큽니다. 다음 정리의 조건부 엔트로피 순서는 정리 17.6.1의 무조건부 엔트로피 순서와 반대임을 유의하십시오.

# 정리 17.6.3

$$
g_{1}^{(n)} \leq g_{2}^{(n)} \leq \cdots \leq g_{n}^{(n)}
$$

증명: 증명은 무작위 부분집합에 대한 요소당 무조건부 엔트로피에 대한 정리의 증명과 매우 유사한 방식으로 진행됩니다. 먼저 $g_{n}^{(n)} \geq g_{n-1}^{(n)}$임을 증명한 다음, 이를 사용하여 나머지 부등식을 증명합니다. 연쇄 법칙에 의해, 무작위 변수 집합의 엔트로피는 엔트로피의 합보다 작습니다:

$$
h\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq \sum_{i=1}^{n} h\left(X_{i}\right)
$$

이 부등식의 양변에서 $n h\left(X_{1}, X_{2}, \ldots, X_{n}\right)$를 빼면 다음과 같습니다.

$$
\begin{aligned}
(n-1) h\left(X_{1}, X_{2}, \ldots, X_{n}\right) & \geq \sum_{i=1}^{n}\left(h\left(X_{1}, X_{2}, \ldots, X_{n}\right)-h\left(X_{i}\right)\right) \\
& =\sum_{i=1}^{n} h\left(X_{1}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{n} \mid X_{i}\right)
\end{aligned}
$$

이를 $n(n-1)$로 나누면 다음과 같습니다.

$$
\frac{h\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n} \geq \frac{1}{n} \sum_{i=1}^{n} \frac{h\left(X_{1}, X_{2}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{n} \mid X_{i}\right)}{n-1}
$$

이는 $g_{n}^{(n)} \geq g_{n-1}^{(n)}$과 동등합니다. 이제 먼저 $k$개 요소 부분집합에 조건화한 다음, 그 $(k-1)$개 요소 부분집합에 대해 균등하게 선택함으로써 모든 $k \leq n$에 대해 $g_{k}^{(n)} \geq g_{k-1}^{(n)}$임을 증명합니다. 각 $k$개 요소 부분집합에 대해 $g_{k}^{(k)} \geq g_{k-1}^{(k)}$이므로, $n$개의 요소에서 균등하게 선택된 모든 $k$개 요소 부분집합에 대한 기대를 취한 후에도 부등식은 참으로 유지됩니다.
<!-- Page 697 -->
정리 17.6.4

$$
f_{k}^{(n)}=\frac{1}{\binom{n}{k}} \sum_{S:|S|=k} \frac{I\left(X(S) ; X\left(S^{c}\right)\right)}{k}
$$

라고 하면

$$
f_{1}^{(n)} \geq f_{2}^{(n)} \geq \cdots \geq f_{n}^{(n)}
$$

입니다.

증명: 이 정리는 항등식 $I\left(X(S) ; X\left(S^{c}\right)\right)=$ $h(X(S))-h\left(X(S) \mid X\left(S^{c}\right)\right)$ 와 정리 17.6.1 및 17.6.3으로부터 유도됩니다.

# 17.7 엔트로피와 피셔 정보

확률 변수의 미분 엔트로피는 그 기술적 복잡성의 척도입니다. 피셔 정보는 분포의 모수를 추정할 때의 최소 오차의 척도입니다. 이 절에서는 이 두 가지 기본적인 양 사이의 관계를 유도하고 이를 엔트로피 파워 부등식을 유도하는 데 사용합니다.

밀도 $f(x)$를 갖는 임의의 확률 변수 $X$를 고려합니다. 위치 모수 $\theta$를 도입하고 모수적 형태로 밀도를 $f(x-\theta)$로 표기합니다. $\theta$에 대한 피셔 정보(11.10절)는 다음과 같이 주어집니다.

$$
J(\theta)=\int_{-\infty}^{\infty} f(x-\theta)\left[\frac{\partial}{\partial \theta} \ln f(x-\theta)\right]^{2} d x
$$

이 경우 $x$에 대한 미분은 $\theta$에 대한 미분과 동일합니다. 따라서 피셔 정보를 다음과 같이 쓸 수 있습니다.

$$
\begin{aligned}
J(X) & =\int_{-\infty}^{\infty} f(x-\theta)\left[\frac{\partial}{\partial x} \ln f(x-\theta)\right]^{2} d x \\
& =\int_{-\infty}^{\infty} f(x)\left[\frac{\partial}{\partial x} \ln f(x)\right]^{2} d x
\end{aligned}
$$

이를 다음과 같이 다시 쓸 수 있습니다.

$$
J(X)=\int_{-\infty}^{\infty} f(x)\left[\frac{\frac{\partial}{\partial x} f(x)}{f(x)}\right]^{2} d x
$$

이를 $X$ 분포의 피셔 정보라고 부르겠습니다. 엔트로피와 마찬가지로 밀도의 함수임을 주목하십시오.

피셔 정보의 중요성은 다음 정리에서 설명됩니다.
<!-- Page 698 -->
정리 17.7.1 (정리 11.10.1: Cramér-Rao 부등식) 모수 $\theta$의 모든 불편 추정량 $T(X)$의 평균 제곱 오차는 피셔 정보의 역수보다 작지 않습니다:

$$
\operatorname{var}(T) \geq \frac{1}{J(\theta)}
$$

이제 미분 엔트로피와 피셔 정보 사이의 기본적인 관계를 증명합니다:
정리 17.7.2 (de Bruijn의 항등식: 엔트로피와 피셔 정보) 유한한 분산을 가지는 임의의 확률 변수 $X$와 밀도 $f(x)$가 있다고 가정합니다. 평균이 0이고 분산이 1인 독립적인 정규 분포 확률 변수 $Z$를 가정합니다. 그러면

$$
\frac{\partial}{\partial t} h_{e}(X+\sqrt{t} Z)=\frac{1}{2} J(X+\sqrt{t} Z)
$$

여기서 $h_{e}$는 밑이 $e$인 미분 엔트로피입니다. 특히, $t \rightarrow 0$일 때 극한이 존재하면,

$$
\left.\frac{\partial}{\partial t} h_{e}(X+\sqrt{t} Z)\right|_{t=0}=\frac{1}{2} J(X)
$$

증명: $Y_{t}=X+\sqrt{t} Z$라고 합시다. 그러면 $Y_{t}$의 밀도는 다음과 같습니다.

$$
g_{t}(y)=\int_{-\infty}^{\infty} f(x) \frac{1}{\sqrt{2 \pi t}} e^{-\frac{(y-x)^{2}}{2 t}} d x
$$

그러면

$$
\begin{aligned}
\frac{\partial}{\partial t} g_{t}(y)= & \int_{-\infty}^{\infty} f(x) \frac{\partial}{\partial t}\left[\frac{1}{\sqrt{2 \pi t}} e^{-\frac{(y-x)^{2}}{2 t}}\right] d x \\
= & \int_{-\infty}^{\infty} f(x)\left[-\frac{1}{2 t} \frac{1}{\sqrt{2 \pi t}} e^{-\frac{(y-x)^{2}}{2 t}}\right. \\
& \left.+\frac{(y-x)^{2}}{2 t^{2}} \frac{1}{\sqrt{2 \pi t}} e^{-\frac{(y-x)^{2}}{2 t}}\right] d x
\end{aligned}
$$

또한 다음과 같이 계산합니다.

$$
\begin{aligned}
\frac{\partial}{\partial y} g_{t}(y) & =\int_{-\infty}^{\infty} f(x) \frac{1}{\sqrt{2 \pi t}} \frac{\partial}{\partial y}\left[e^{-\frac{(y-x)^{2}}{2 t}}\right] d x \\
& =\int_{-\infty}^{\infty} f(x) \frac{1}{\sqrt{2 \pi t}}\left[-\frac{y-x}{t} e^{-\frac{(y-x)^{2}}{2 t}}\right] d x
\end{aligned}
$$
<!-- Page 699 -->
그리고

$$
\begin{aligned}
\frac{\partial^{2}}{\partial y^{2}} g_{t}(y) & =\int_{-\infty}^{\infty} f(x) \frac{1}{\sqrt{2 \pi t}} \frac{\partial}{\partial y}\left[-\frac{y-x}{t} e^{-\frac{(y-x)^{2}}{2 t}}\right] d x \\
& =\int_{-\infty}^{\infty} f(x) \frac{1}{\sqrt{2 \pi t}}\left[-\frac{1}{t} e^{-\frac{(y-x)^{2}}{2 t}}+\frac{(y-x)^{2}}{t^{2}} e^{-\frac{(y-x)^{2}}{2 t}}\right] d x
\end{aligned}
$$

따라서,

$$
\frac{\partial}{\partial t} g_{t}(y)=\frac{1}{2} \frac{\partial^{2}}{\partial y^{2}} g_{t}(y)
$$

이 관계를 사용하여 $Y_{t}$의 entropy의 미분을 계산할 것입니다. entropy는 다음과 같이 주어집니다.

$$
h_{e}\left(Y_{t}\right)=-\int_{-\infty}^{\infty} g_{t}(y) \ln g_{t}(y) d y
$$

미분하면 다음과 같습니다.

$$
\begin{aligned}
\frac{\partial}{\partial t} h_{e}\left(Y_{t}\right) & =-\int_{-\infty}^{\infty} \frac{\partial}{\partial t} g_{t}(y) d y-\int_{-\infty}^{\infty} \frac{\partial}{\partial t} g_{t}(y) \ln g_{t}(y) d y \\
& =-\frac{\partial}{\partial t} \int_{-\infty}^{\infty} g_{t}(y) d y-\frac{1}{2} \int_{-\infty}^{\infty} \frac{\partial^{2}}{\partial y^{2}} g_{t}(y) \ln g_{t}(y) d y
\end{aligned}
$$

첫 번째 항은 $\int g_{t}(y) d y=1$이므로 0입니다. 두 번째 항은 부분 적분을 통해 다음과 같이 얻을 수 있습니다.

$$
\frac{\partial}{\partial t} h_{e}\left(Y_{t}\right)=-\frac{1}{2}\left[\frac{\partial g_{t}(y)}{\partial y} \ln g_{t}(y)\right]_{-\infty}^{\infty}+\frac{1}{2} \int_{-\infty}^{\infty}\left[\frac{\partial}{\partial y} g_{t}(y)\right]^{2} \frac{1}{g_{t}(y)} d y
$$

(17.84)의 두 번째 항은 $\frac{1}{2} J\left(Y_{t}\right)$입니다. 따라서 (17.84)의 첫 번째 항이 0임을 보이면 증명이 완료됩니다. 첫 번째 항을 다음과 같이 다시 쓸 수 있습니다.

$$
\frac{\partial g_{t}(y)}{\partial y} \ln g_{t}(y)=\left[\frac{\frac{\partial g_{t}(y)}{\partial y}}{\sqrt{g_{t}(y)}}\right]\left[2 \sqrt{g_{t}(y)} \ln \sqrt{g_{t}(y)}\right]
$$

첫 번째 인수의 제곱은 Fisher information으로 적분되므로 $y \rightarrow \pm \infty$일 때 유계여야 합니다. 두 번째 인수는 $x \rightarrow 0$일 때 $x \ln x \rightarrow 0$이고 $y \rightarrow \pm \infty$일 때 $g_{t}(y) \rightarrow 0$이므로 0으로 갑니다. 따라서 첫 번째 항은
<!-- Page 700 -->
(17.84)은 두 극한에서 모두 0으로 가며 정리가 증명됩니다. 증명에서는 (17.74), (17.76), (17.78), (17.82)에서 적분과 미분을 교환했습니다. 이러한 교환의 엄격한 정당성은 유계 수렴 정리와 평균값 정리를 적용해야 하며, 자세한 내용은 Barron [30]에서 찾을 수 있습니다.

이 정리는 독립 확률 변수의 합에 대한 entropy의 하한을 제공하는 entropy power inequality를 증명하는 데 사용될 수 있습니다.

정리 17.7.3 (Entropy power inequality) $\mathbf{X}$와 $\mathbf{Y}$가 밀도를 갖는 독립 확률 $n$-벡터이면 다음이 성립합니다.

$$
2^{\frac{2}{n}} h(\mathbf{X}+\mathbf{Y}) \geq 2^{\frac{2}{n}} h(\mathbf{X})+2^{\frac{2}{n}} h(\mathbf{Y})
$$

Stam [505]와 Blachman [61]의 증명에 대한 기본 단계를 개략적으로 설명합니다. 다른 증명은 17.8절에 나와 있습니다.

Stam의 entropy power inequality 증명은 섭동 논증에 기반합니다. $n=1$이라고 가정합니다. $Z_{1}$과 $Z_{2}$가 독립 $\mathcal{N}(0,1)$ 확률 변수라고 할 때, $X_{t}=X+\sqrt{f(t)} Z_{1}, Y_{t}=Y+\sqrt{g(t)} Z_{2}$라고 정의합니다. 그러면 $n=1$에 대한 entropy power inequality는 $s(0) \leq 1$을 보이는 것으로 축소됩니다. 여기서 다음과 같이 정의합니다.

$$
s(t)=\frac{2^{2 h\left(X_{t}\right)}+2^{2 h\left(Y_{t}\right)}}{2^{2 h\left(X_{t}+Y_{t}\right)}}
$$

$t \rightarrow \infty$일 때 $f(t) \rightarrow \infty$이고 $g(t) \rightarrow \infty$이면 $s(\infty)=1$임을 쉽게 보일 수 있습니다. 추가로 $t \geq 0$에 대해 $s^{\prime}(t) \geq 0$이면 $s(0) \leq 1$이 됩니다. $s^{\prime}(t) \geq 0$이라는 사실의 증명은 함수 $f(t)$와 $g(t)$의 영리한 선택, 정리 17.7.2의 적용, 그리고 Fisher 정보에 대한 컨볼루션 부등식의 사용을 포함합니다.

$$
\frac{1}{J(X+Y)} \geq \frac{1}{J(X)}+\frac{1}{J(Y)}
$$

entropy power inequality는 귀납법을 통해 벡터 경우로 확장될 수 있습니다. 자세한 내용은 Stam [505]와 Blachman [61]의 논문에서 찾을 수 있습니다.

# 17.8 ENTROPY POWER INEQUALITY AND BRUNN-MINKOWSKI INEQUALITY

entropy power inequality는 개별적인 차등 entropy에 대한 두 독립 확률 벡터의 합의 차등 entropy에 대한 하한을 제공합니다. 이 섹션에서는 이를 다시 명시하고 개략적으로 설명합니다.
<!-- Page 701 -->
엔트로피 파워 부등식의 대안적 증명을 제시합니다. 또한 엔트로피 파워 부등식과 브룬-민코프스키 부등식이 공통 증명을 통해 어떻게 관련되는지 보여줍니다.

차원 $n=1$에 대한 엔트로피 파워 부등식을 정규 분포와의 관계를 강조하는 형태로 다시 작성할 수 있습니다. $X$와 $Y$를 두 개의 독립적인 확률 변수라고 하고, $X^{\prime}$와 $Y^{\prime}$를 각각 $X$와 $Y$와 동일한 엔트로피를 갖는 독립적인 정규 분포라고 합시다. 그러면 $2^{2 h(X)}=2^{2 h\left(X^{\prime}\right)}=(2 \pi e) \sigma_{X^{\prime}}^{2}$이고, 마찬가지로 $2^{2 h(Y)}=(2 \pi e) \sigma_{Y^{\prime}}^{2}$입니다. 따라서 엔트로피 파워 부등식은 다음과 같이 다시 작성할 수 있습니다.

$$
2^{2 h(X+Y)} \geq(2 \pi e)\left(\sigma_{X^{\prime}}^{2}+\sigma_{Y^{\prime}}^{2}\right)=2^{2 h\left(X^{\prime}+Y^{\prime}\right)}
$$

$X^{\prime}$와 $Y^{\prime}$는 독립이기 때문입니다. 따라서 엔트로피 파워 부등식의 새로운 명제를 얻습니다.

정리 17.8.1 (엔트로피 파워 부등식 재진술) 두 독립 확률 변수 $X$와 $Y$에 대해,

$$
h(X+Y) \geq h\left(X^{\prime}+Y^{\prime}\right)
$$

여기서 $X^{\prime}$와 $Y^{\prime}$는 $h\left(X^{\prime}\right)=h(X)$이고 $h\left(Y^{\prime}\right)=h(Y)$인 독립적인 정규 확률 변수입니다.

이 형태의 엔트로피 파워 부등식은 집합 합의 부피를 제한하는 브룬-민코프스키 부등식과 놀랍도록 유사합니다.

정의 $A, B \subset \mathcal{R}^{n}$인 두 집합의 집합 합 $A+B$는 $\{x+y: x \in A, y \in B\}$로 정의됩니다.

예제 17.8.1 반지름이 1인 두 구의 집합 합은 반지름이 2인 구입니다.

정리 17.8.2 (브룬-민코프스키 부등식) 두 집합 $A$와 $B$의 집합 합의 부피는 각각 $A$와 $B$와 동일한 부피를 갖는 두 구 $A^{\prime}$와 $B^{\prime}$의 집합 합의 부피보다 큽니다.

$$
V(A+B) \geq V\left(A^{\prime}+B^{\prime}\right)
$$

여기서 $A^{\prime}$와 $B^{\prime}$는 $V\left(A^{\prime}\right)=V(A)$이고 $V\left(B^{\prime}\right)=V(B)$인 구입니다.
두 정리 사이의 유사성은 [104]에서 지적되었습니다. Dembo [162]와 Lieb가 시작점에서 공통 증명을 발견했습니다.
<!-- Page 702 -->
Young의 부등식의 강화된 버전입니다. 동일한 증명을 사용하여 엔트로피 파워 부등식과 Brunn-Minkowski 부등식을 특수한 경우로 포함하는 다양한 부등식을 증명할 수 있습니다. 몇 가지 정의부터 시작하겠습니다.

정의 $f$와 $g$를 $\mathcal{R}^{n}$에 대한 두 밀도로 정의하고, $f * g$를 두 밀도의 컨볼루션으로 정의합니다. 밀도의 $\mathcal{L}_{r}$ 노름을 다음과 같이 정의합니다.

$$
\|f\|_{r}=\left(\int f^{r}(x) d x\right)^{\frac{1}{r}}
$$

정리 17.8.1 (강화된 Young의 부등식) $\mathcal{R}^{n}$에 대한 임의의 두 밀도 $f$와 $g$에 대해,

$$
\|f * g\|_{r} \leq\left(\frac{C_{p} C_{q}}{C_{r}}\right)^{\frac{n}{2}}\|f\|_{p}\|g\|_{q}
$$

여기서

$$
\frac{1}{r}=\frac{1}{p}+\frac{1}{q}-1
$$

그리고

$$
C_{p}=\frac{p^{\frac{1}{p}}}{p^{r \frac{1}{p^{\prime}}}}, \quad \frac{1}{p}+\frac{1}{p^{\prime}}=1
$$

증명: 이 부등식의 증명은 [38] 및 [73]에서 찾을 수 있습니다.
엔트로피의 일반화를 정의합니다.
정의 순서 $r$의 Renyi 엔트로피 $h_{r}(X)$는 다음과 같이 정의됩니다.

$$
h_{r}(X)=\frac{1}{1-r} \log \left[\int f^{r}(x) d x\right]
$$

$0<r<\infty, r \neq 1$인 경우. $r \rightarrow 1$로 극한을 취하면 Shannon 엔트로피 함수를 얻습니다.

$$
h(X)=h_{1}(X)=-\int f(x) \log f(x) d x
$$

$r \rightarrow 0$으로 극한을 취하면 지지 집합의 부피의 로그를 얻습니다.

$$
h_{0}(X)=\log (\mu\{x: f(x)>0\})
$$
<!-- Page 703 -->
따라서 0차 Renyi 엔트로피는 밀도 $f$의 지지 집합의 측도의 로그를 제공하며, Shannon 엔트로피 $h_{1}$은 "유효" 지지 집합의 크기의 로그를 제공합니다(정리 8.2.2). 이제 Renyi 엔트로피에 대한 엔트로피 거듭제곱의 등가물을 정의합니다.
정의 0차 Renyi 엔트로피 거듭제곱 $V_{r}(X)$는 다음과 같이 정의됩니다.

$$
V_{r}(X)=\left\{\begin{array}{ll}
{\left[\int f^{r}(x) d x\right]^{-\frac{2}{n} \frac{r^{\prime}}{r}},} & 0<r \leq \infty, r \neq 1, \frac{1}{r}+\frac{1}{r^{\prime}}=1 \\
\exp \left[\frac{2}{n} h(X)\right], & r=1 \\
\mu(\{x: f(x)>0\})^{\frac{2}{n}}, & r=0
\end{array}\right.
$$

정리 17.8.3 두 독립 확률 변수 $X$와 $Y$ 및 임의의 $0 \leq r<\infty$와 임의의 $0 \leq \lambda \leq 1$에 대해 다음이 성립합니다.

$$
\begin{aligned}
\log V_{r}(X+Y) \geq \lambda & \log V_{p}(X)+(1-\lambda) \log V_{q}(Y)+H(\lambda) \\
& +\frac{1+r}{1-r}\left[H\left(\frac{r+\lambda(1-r)}{1+r}\right)-H\left(\frac{r}{1+r}\right)\right]
\end{aligned}
$$

여기서 $p=\frac{r}{(r+\lambda(1-r))}, q=\frac{r}{(r+(1-\lambda)(1-r))}$이고 $H(\lambda)=-\lambda \log \lambda-(1-\lambda)$ $\log (1-\lambda)$입니다.
증명: Young의 부등식(17.93)의 로그를 취하면 다음과 같습니다.

$$
\begin{aligned}
\frac{1}{r^{\prime}} \log V_{r}(X+Y) \geq \frac{1}{p^{\prime}} \log V_{p}(X) & +\frac{1}{q^{\prime}} \log V_{q}(Y)+\log C_{r} \\
& -\log C_{p}-\log C_{q}
\end{aligned}
$$

$\lambda=r^{\prime} / p^{\prime}$로 설정하고 (17.94)를 사용하면 $1-\lambda=r^{\prime} / q^{\prime}, p=\frac{r}{r+\lambda(1-r)}$ 및 $q=\frac{r}{r+(1-\lambda)(1-r)}$가 됩니다. 따라서 (17.101)은 다음과 같이 됩니다.

$$
\begin{aligned}
\log V_{r}(X+Y) \geq & \lambda \log V_{p}(X)+(1-\lambda) \log V_{q}(Y)+\frac{r^{\prime}}{r} \log r-\log r^{\prime} \\
& -\frac{r^{\prime}}{p} \log p+\frac{r^{\prime}}{p^{\prime}} \log p^{\prime}-\frac{r^{\prime}}{q} \log q+\frac{r^{\prime}}{q^{\prime}} \log q^{\prime} \\
= & \lambda \log V_{p}(X)+(1-\lambda) \log V_{q}(Y) \\
& +\frac{r^{\prime}}{r} \log r-(\lambda+1-\lambda) \log r^{\prime} \\
& -\frac{r^{\prime}}{p} \log p+\lambda \log p^{\prime}-\frac{r^{\prime}}{q} \log q+(1-\lambda) \log q^{\prime}
\end{aligned}
$$
<!-- Page 704 -->
$$
\begin{aligned}
= & \lambda \log V_{p}(X)+(1-\lambda) \log V_{q}(Y)+\frac{1}{r-1} \log r+H(\lambda) \\
& -\frac{r+\lambda(1-r)}{r-1} \log \frac{r}{r+\lambda(1-r)} \\
& -\frac{r+(1-\lambda)(1-r)}{r-1} \log \frac{r}{r+(1-\lambda)(1-r)} \\
= & \lambda \log V_{p}(X)+(1-\lambda) \log V_{q}(Y)+H(\lambda) \\
& +\frac{1+r}{1-r}\left[H\left(\frac{r+\lambda(1-r)}{1+r}\right)-H\left(\frac{r}{1+r}\right)\right]
\end{aligned}
$$

여기서 마지막 단계에 대한 대수적 상세 내용은 생략합니다.
이후 Brunn-Minkowski 부등식과 entropy power inequality는 이 정리의 특수한 경우로 얻어질 수 있습니다.

- Entropy power inequality. (17.100)을 $r \rightarrow 1$로 극한을 취하고

$$
\lambda=\frac{V_{1}(X)}{V_{1}(X)+V_{1}(Y)}
$$

로 설정하면

$$
V_{1}(X+Y) \geq V_{1}(X)+V_{1}(Y)
$$

를 얻게 되는데, 이것이 entropy power inequality입니다.

- Brunn-Minkowski inequality. 유사하게, $r \rightarrow 0$으로 두고

$$
\lambda=\frac{\sqrt{V_{0}(X)}}{\sqrt{V_{0}(X)}+\sqrt{V_{0}(Y)}}
$$

를 선택하면

$$
\sqrt{V_{0}(X+Y)} \geq \sqrt{V_{0}(X)}+\sqrt{V_{0}(Y)}
$$

를 얻게 됩니다.
이제 $A$를 $X$의 지지 집합(support set)으로, $B$를 $Y$의 지지 집합으로 놓으면, $A+B$는 $X+Y$의 지지 집합이 되며, (17.109)는 다음과 같이 축약됩니다.

$$
[\mu(A+B)]^{\frac{1}{n}} \geq[\mu(A)]^{\frac{1}{n}}+[\mu(B)]^{\frac{1}{n}}
$$

이것이 Brunn-Minkowski inequality입니다.
<!-- Page 705 -->
일반 정리는 엔트로피 파워 부등식과 브룬-민코프스키 부등식을 통합하고, 엔트로피 파워 부등식과 브룬-민코프스키 부등식 사이에 위치하는 새로운 부등식들의 연속체를 도입합니다. 이는 엔트로피 파워와 부피 사이의 유사성을 더욱 강화합니다.

# 17.9 행렬식에 대한 부등식

이 장의 나머지 부분에서는 $K$가 음이 아닌 준정부호 대칭 $n \times n$ 행렬이라고 가정합니다. $|K|$는 $K$의 행렬식을 나타냅니다.

먼저 Ky Fan [199]의 결과에 대한 정보 이론적 증명을 제시합니다.

정리 17.9.1 $\log |K|$는 오목합니다.
증명: $X_{1}$과 $X_{2}$를 정규 분포를 따르는 $n$-벡터라고 가정합니다. $\mathbf{X}_{i} \sim \mathcal{N}\left(0, K_{i}\right)$, $i=1,2$. 확률 변수 $\theta$가 다음과 같은 분포를 따른다고 가정합니다.

$$
\begin{aligned}
& \operatorname{Pr}\{\theta=1\}=\lambda \\
& \operatorname{Pr}\{\theta=2\}=1-\lambda
\end{aligned}
$$

여기서 $0 \leq \lambda \leq 1$입니다. $\theta, \mathbf{X}_{1}, \mathbf{X}_{2}$가 독립이고 $\mathbf{Z}=$ $\mathbf{X}_{\theta}$라고 가정합니다. 그러면 $\mathbf{Z}$는 공분산 $K_{Z}=\lambda K_{1}+(1-\lambda) K_{2}$를 가집니다. 그러나 $\mathbf{Z}$는 다변수 정규 분포를 따르지 않을 것입니다. 먼저 정리 17.2.3을 사용하고 이어서 정리 17.2.1을 사용하면 다음과 같습니다.

$$
\begin{aligned}
\frac{1}{2} \log (2 \pi e)^{n}\left|\lambda K_{1}+(1-\lambda) K_{2}\right| & \geq h(\mathbf{Z}) \\
& \geq h(\mathbf{Z} \mid \theta) \\
& =\lambda \frac{1}{2} \log (2 \pi e)^{n}\left|K_{1}\right| \\
& \quad+(1-\lambda) \frac{1}{2} \log (2 \pi e)^{n}\left|K_{2}\right|
\end{aligned}
$$

따라서 다음과 같습니다.

$$
\left|\lambda K_{1}+(1-\lambda) K_{2}\right| \geq\left|K_{1}\right|^{\lambda}\left|K_{2}\right|^{1-\lambda}
$$

원하는 대로입니다.
이제 Hadamard의 부등식을 정보 이론적 증명을 사용하여 제시합니다 [128].
<!-- Page 706 -->
정리 17.9.2 (Hadamard) $\quad|K| \leq \Pi K_{i i}$, 등호는 $K_{i j}=0, i \neq j$ 일 때 성립합니다.

증명: $\mathbf{X} \sim \mathcal{N}(0, K)$라고 합시다. 그러면
$\frac{1}{2} \log (2 \pi e)^{n}|K|=h\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq \sum h\left(X_{i}\right)=\sum_{i=1}^{n} \frac{1}{2} \log 2 \pi e\left|K_{i i}\right|$,
등호는 $X_{1}, X_{2}, \ldots, X_{n}$이 독립일 때 (즉, $K_{i j}=0, i \neq j$) 성립합니다. $\square$
이제 Szasz [391]에 의한 Hadamard 부등식의 일반화를 증명합니다. $K\left(i_{1}, i_{2}, \ldots, i_{k}\right)$를 인덱스 $i_{1}, i_{2}, \ldots, i_{k}$를 가진 행과 열로 형성된 $K$의 $k \times k$ 주 부분 행렬이라고 합시다.

정리 17.9.3 (Szasz) $K$가 양의 정부호 $n \times n$ 행렬이고 $P_{k}$가 $K$의 모든 주 $k$-행 마이너의 행렬식의 곱을 나타낸다면, 즉,

$$
P_{k}=\prod_{1 \leq i_{1}<i_{2}<\cdots<i_{k} \leq n}\left|K\left(i_{1}, i_{2}, \ldots, i_{k}\right)\right|
$$

그러면

$$
P_{1} \geq P_{2}^{\frac{1}{\binom{n-1}{1}}} \geq P_{3}^{\frac{1}{\binom{n-1}{2}}} \geq \cdots \geq P_{n}
$$

증명: $\mathbf{X} \sim \mathcal{N}(0, K)$라고 합시다. 그러면 이 정리는 정리 17.6.1에서 $h_{k}^{(n)}=\frac{1}{2 n\binom{n-1}{k-1}} \log P_{k}+\frac{1}{2} \log 2 \pi e$로 식별하여 직접적으로 얻어집니다.

관련된 정리도 증명할 수 있습니다.
정리 17.9.4 $K$가 양의 정부호 $n \times n$ 행렬이고

$$
S_{k}^{(n)}=\frac{1}{\binom{n}{k}} \sum_{1 \leq i_{1}<i_{2}<\cdots<i_{k} \leq n}\left|K\left(i_{1}, i_{2}, \ldots, i_{k}\right)\right|^{\frac{1}{k}}
$$

그러면

$$
\frac{1}{n} \operatorname{tr}(K)=S_{1}^{(n)} \geq S_{2}^{(n)} \geq \cdots \geq S_{n}^{(n)}=|K|^{\frac{1}{n}}
$$

증명: 이는 정리 17.6.1의 따름정리에서 $t_{k}^{(n)}=(2 \pi e) S_{k}^{(n)}$와 $r=2$로 식별하여 직접적으로 얻어집니다.
<!-- Page 707 -->
정리 17.9.5 다음을 가정합니다.

$$
Q_{k}=\left(\prod_{S:|S|=k} \frac{|K|}{\left|K\left(S^{c}\right)\right|}\right)^{\frac{1}{k\binom{n}{k}}}
$$

그러면 다음이 성립합니다.

$$
\left(\prod_{i=1}^{n} \sigma_{i}^{2}\right)^{\frac{1}{n}}=Q_{1} \leq Q_{2} \leq \cdots \leq Q_{n-1} \leq Q_{n}=|K|^{\frac{1}{n}}
$$

증명: 이 정리는 정리 17.6.3과 다음의 식별로부터 즉시 도출됩니다.

$$
h\left(X(S) \mid X\left(S^{c}\right)\right)=\frac{1}{2} \log (2 \pi e)^{k} \frac{|K|}{\left|K\left(S^{c}\right)\right|}
$$

가장 바깥쪽 부등식인 $Q_{1} \leq Q_{n}$은 다음과 같이 다시 쓸 수 있습니다.

$$
|K| \geq \prod_{i=1}^{n} \sigma_{i}^{2}
$$

여기서

$$
\sigma_{i}^{2}=\frac{|K|}{|K(1,2 \ldots, i-1, i+1, \ldots, n)|}
$$

는 나머지 $X$들로부터 $X_{i}$를 선형 예측하는 최소 평균 제곱 오차입니다. 따라서 $\sigma_{i}^{2}$는 $X_{1}, X_{2}, \ldots, X_{n}$이 결합 정규 분포일 때 나머지 $X_{j}$들이 주어졌을 때 $X_{i}$의 조건부 분산입니다. 이를 Hadamard의 부등식과 결합하면 양의 정부호 행렬의 행렬식에 대한 상한과 하한을 얻을 수 있습니다.

# 따름정리

$$
\prod_{i} K_{i i} \geq|K| \geq \prod_{i} \sigma_{i}^{2}
$$

따라서 공분산 행렬의 행렬식은 확률 변수 $X_{i}$의 무조건부 분산 $K_{i i}$의 곱과 조건부 분산 $\sigma_{i}^{2}$의 곱 사이에 있습니다.

이제 정상 확률 과정의 공분산 행렬로서 중요한 Toeplitz 행렬의 속성을 증명합니다. Toeplitz 행렬 $K$는 $K_{i j}=K_{r s}$가 $|i-j|=|r-s|$이면 성립하는 속성으로 특징지어집니다. $K_{k}$를 주 소행렬식 $K(1,2, \ldots, k)$로 나타냅니다. 이러한 행렬에 대해 다음 속성은 엔트로피 함수의 속성으로부터 쉽게 증명될 수 있습니다.
<!-- Page 708 -->
정리 17.9.6 양의 정부호 행렬 $K$가 토플리츠 행렬이면,

$$
\left|K_{1}\right| \geq\left|K_{2}\right|^{\frac{1}{2}} \geq \cdots \geq\left|K_{n-1}\right|^{\frac{1}{(n-1)}} \geq\left|K_{n}\right|^{\frac{1}{n}}
$$

이고 $\left|K_{k}\right| /\left|K_{k-1}\right|$는 $k$에 대해 감소하며,

$$
\lim _{n \rightarrow \infty}\left|K_{n}\right|^{\frac{1}{n}}=\lim _{n \rightarrow \infty} \frac{\left|K_{n}\right|}{\left|K_{n-1}\right|}
$$

증명: $\left(X_{1}, X_{2}, \ldots, X_{n}\right) \sim \mathcal{N}\left(0, K_{n}\right)$이라고 하자. 다음을 관찰한다.

$$
\begin{aligned}
h\left(X_{k} \mid X_{k-1}, \ldots, X_{1}\right) & =h\left(X^{k}\right)-h\left(X^{k-1}\right) \\
& =\frac{1}{2} \log (2 \pi e) \frac{\left|K_{k}\right|}{\left|K_{k-1}\right|}
\end{aligned}
$$

따라서 $\left|K_{k}\right| /\left|K_{k-1}\right|$의 단조성은 $h\left(X_{k} \mid X_{k-1}, \ldots, X_{1}\right)$의 단조성으로부터 나오며, 이는 다음으로부터 따른다.

$$
\begin{aligned}
h\left(X_{k} \mid X_{k-1}, \ldots, X_{1}\right) & =h\left(X_{k+1} \mid X_{k}, \ldots, X_{2}\right) \\
& \geq h\left(X_{k+1} \mid X_{k}, \ldots, X_{2}, X_{1}\right)
\end{aligned}
$$

여기서 등호는 토플리츠 가정으로부터 오고, 부등호는 조건화가 엔트로피를 감소시킨다는 사실로부터 온다. $h\left(X_{k} \mid X_{k-1}, \ldots, X_{1}\right)$가 감소하므로, 다음의 누적 평균이

$$
\frac{1}{k} h\left(X_{1}, \ldots, X_{k}\right)=\frac{1}{k} \sum_{i=1}^{k} h\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right)
$$

$k$에 대해 감소함을 알 수 있다. 그러면 (17.127)은 $h\left(X_{1}, X_{2}, \ldots, X_{k}\right)=$ $\frac{1}{2} \log (2 \pi e)^{k}\left|K_{k}\right|$로부터 따른다.

마지막으로, $h\left(X_{n} \mid X_{n-1}, \ldots, X_{1}\right)$는 감소하는 수열이므로 극한값을 갖는다. 따라서 Cesáro 평균 정리에 의해,

$$
\begin{aligned}
\lim _{n \rightarrow \infty} \frac{h\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n} & =\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} h\left(X_{k} \mid X_{k-1}, \ldots, X_{1}\right) \\
& =\lim _{n \rightarrow \infty} h\left(X_{n} \mid X_{n-1}, \ldots, X_{1}\right)
\end{aligned}
$$
<!-- Page 709 -->
이를 행렬식으로 번역하면 다음과 같습니다.

$$
\lim _{n \rightarrow \infty}\left|K_{n}\right|^{\frac{1}{n}}=\lim _{n \rightarrow \infty} \frac{\left|K_{n}\right|}{\left|K_{n-1}\right|}
$$

정리 17.9.7 (민코프스키 부등식 [390])

$$
\left|K_{1}+K_{2}\right|^{1 / n} \geq\left|K_{1}\right|^{1 / n}+\left|K_{2}\right|^{1 / n}
$$

증명: $\mathbf{X}_{1}, \mathbf{X}_{2}$가 독립이고 $\mathbf{X}_{i} \sim \mathcal{N}\left(0, K_{i}\right)$라고 가정합니다. $\mathbf{X}_{1}+$ $\mathbf{X}_{2} \sim \mathcal{N}\left(0, K_{1}+K_{2}\right)$임을 인지하고 엔트로피 파워 부등식(정리 17.7.3)을 사용하면 다음과 같습니다.

$$
\begin{aligned}
(2 \pi e)\left|K_{1}+K_{2}\right|^{1 / n} & =2^{\frac{2}{n}} h\left(\mathbf{X}_{1}+\mathbf{X}_{2}\right) \\
& \geq 2^{\frac{2}{n}} h\left(\mathbf{X}_{1}\right)+2^{\frac{2}{n} h\left(\mathbf{X}_{2}\right)} \\
& =(2 \pi e)\left|K_{1}\right|^{1 / n}+(2 \pi e)\left|K_{2}\right|^{1 / n}
\end{aligned}
$$

# 17.10 행렬식 비율에 대한 부등식

이제 행렬식 비율에 대한 유사한 부등식을 증명합니다. 다음 정리를 개발하기 전에 최소 평균 제곱 오차 선형 예측에 대한 관찰을 합니다. 만약 $\left(X_{1}, X_{2}, \ldots, X_{n}\right) \sim \mathcal{N}\left(0, K_{n}\right)$이면, $X_{n}$의 조건부 밀도가 $X_{1}, X_{2}, \ldots, X_{n-1}$에 대한 선형 평균과 조건부 분산 $\sigma_{n}^{2}$을 갖는 단변량 정규 분포임을 압니다. 여기서 $\sigma_{n}^{2}$은 $X_{1}, X_{2}, \ldots, X_{n-1}$에 기반한 모든 선형 추정량 $\hat{X}_{n}$에 대한 최소 평균 제곱 오차 $E\left(X_{n}-\hat{X}_{n}\right)^{2}$입니다.

정리 17.10.1 $\sigma_{n}^{2}=\left|K_{n}\right| /\left|K_{n-1}\right|$.
증명: $X_{n}$의 조건부 정규성을 사용하면 다음과 같습니다.

$$
\begin{aligned}
\frac{1}{2} \log 2 \pi e \sigma_{n}^{2} & =h\left(X_{n} \mid X_{1}, X_{2}, \ldots, X_{n-1}\right) \\
& =h\left(X_{1}, X_{2}, \ldots, X_{n}\right)-h\left(X_{1}, X_{2}, \ldots, X_{n-1}\right) \\
& =\frac{1}{2} \log (2 \pi e)^{n}\left|K_{n}\right|-\frac{1}{2} \log (2 \pi e)^{n-1}\left|K_{n-1}\right| \\
& =\frac{1}{2} \log 2 \pi e\left|K_{n}\right| /\left|K_{n-1}\right|
\end{aligned}
$$
<!-- Page 710 -->
허용된 공분산 행렬 집합 $\left\{K_{n}\right\}$에 대한 $\sigma_{n}^{2}$의 최소화는 다음 정리에 의해 도움을 받습니다. 이러한 문제는 최대 엔트로피 스펙트럼 밀도 추정에서 발생합니다.

정리 17.10.1 (Bergstrøm [42]) $\log \left(\left|K_{n}\right| /\left|K_{n-p}\right|\right)$는 $K_{n}$에 대해 오목합니다.

증명: $\log \left(\left|K_{n}\right| /\left|K_{n-p}\right|\right)$는 두 오목 함수의 차이이므로 정리 17.9.1을 사용할 수 없다는 점을 언급합니다. $\mathbf{Z}=\mathbf{X}_{\theta}$라고 하고, 여기서 $\mathbf{X}_{1} \sim \mathcal{N}\left(0, S_{n}\right), \mathbf{X}_{2} \sim \mathcal{N}\left(0, T_{n}\right), \operatorname{Pr}\{\theta=1\}=\lambda=1-\operatorname{Pr}\{\theta=2\}$이며, $\mathbf{X}_{1}, \mathbf{X}_{2}, \theta$는 독립이라고 가정합니다. $\mathbf{Z}$의 공분산 행렬 $K_{n}$은 다음과 같이 주어집니다.

$$
K_{n}=\lambda S_{n}+(1-\lambda) T_{n}
$$

다음 부등식 연쇄는 정리를 증명합니다.

$$
\begin{aligned}
& \lambda \frac{1}{2} \log (2 \pi e)^{p}\left|S_{n}\right| /\left|S_{n-p}\right|+(1-\lambda) \frac{1}{2} \log (2 \pi e)^{p}\left|T_{n}\right| /\left|T_{n-p}\right| \\
& \stackrel{(\mathbf{a})}{=} \lambda h\left(X_{1, n}, X_{1, n-1}, \ldots, X_{1, n-p+1} \mid X_{1,1}, \ldots, X_{1, n-p}\right) \\
& +(1-\lambda) h\left(X_{2, n}, X_{2, n-1}, \ldots, X_{2, n-p+1} \mid X_{2,1}, \ldots, X_{2, n-p}\right) \\
& =h\left(Z_{n}, Z_{n-1}, \ldots, Z_{n-p+1} \mid Z_{1}, \ldots, Z_{n-p}, \theta\right) \\
& \stackrel{(\mathbf{b})}{\leq} h\left(Z_{n}, Z_{n-1}, \ldots, Z_{n-p+1} \mid Z_{1}, \ldots, Z_{n-p}\right) \\
& \stackrel{(\mathbf{c})}{\leq} \frac{1}{2} \log (2 \pi e)^{p} \frac{\left|K_{n}\right|}{\left|K_{n-p}\right|},
\end{aligned}
$$

여기서 (a)는 $h\left(X_{n}, X_{n-1}, \ldots, X_{n-p+1} \mid X_{1}, \ldots, X_{n-p}\right)=$ $h\left(X_{1}, \ldots, X_{n}\right)-h\left(X_{1}, \ldots, X_{n-p}\right)$로부터, (b)는 조건화 보조정리로부터, (c)는 정리 17.2.3의 조건부 버전으로부터 유도됩니다.

정리 17.10.2 (Bergstrøm [42]) $\left|K_{n}\right| /\left|K_{n-1}\right|$는 $K_{n}$에 대해 오목합니다.
증명: 다시 가우시안 확률 변수의 속성을 사용합니다. 두 개의 독립적인 가우시안 확률 $n$-벡터 $\mathbf{X} \sim \mathcal{N}\left(0, A_{n}\right)$와 $\mathbf{Y} \sim \mathcal{N}\left(0, B_{n}\right)$가 있다고 가정합니다. $\mathbf{Z}=\mathbf{X}+\mathbf{Y}$라고 하면,

$$
\frac{1}{2} \log 2 \pi e \frac{\left|A_{n}+B_{n}\right|}{\left|A_{n-1}+B_{n-1}\right|} \stackrel{(\mathbf{a})}{=} h\left(Z_{n} \mid Z_{n-1}, Z_{n-2}, \ldots, Z_{1}\right)
$$
<!-- Page 711 -->
(b) $h\left(Z_{n} \mid Z_{n-1}, Z_{n-2}, \ldots, Z_{1}, X_{n-1}, X_{n-2}, \ldots, X_{1}, Y_{n-1}, Y_{n-2}, \ldots, Y_{1}\right)$
(c) $h\left(X_{n}+Y_{n} \mid X_{n-1}, X_{n-2}, \ldots, X_{1}, Y_{n-1}, Y_{n-2}, \ldots, Y_{1}\right)$
(d) $E \frac{1}{2} \log \left[2 \pi e \operatorname{Var}\left(X_{n}+Y_{n} \mid X_{n-1}, X_{n-2}, \ldots, X_{1}, Y_{n-1}\right.\right.$ $\left.\left.Y_{n-2}, \ldots, Y_{1}\right)\right]$
(e) $E \frac{1}{2} \log \left[2 \pi e\left(\operatorname{Var}\left(X_{n} \mid X_{n-1}, X_{n-2}, \ldots, X_{1}\right)\right.\right.$

$$
\left.+\operatorname{Var}\left(Y_{n} \mid Y_{n-1}, Y_{n-2}, \ldots, Y_{1}\right)\right)\right]
$$

(ff) $E \frac{1}{2} \log \left(2 \pi e\left(\frac{\left|A_{n}\right|}{\left|A_{n-1}\right|}+\frac{\left|B_{n}\right|}{\left|B_{n-1}\right|}\right)\right)$
$=\frac{1}{2} \log \left(2 \pi e\left(\frac{\left|A_{n}\right|}{\left|A_{n-1}\right|}+\frac{\left|B_{n}\right|}{\left|B_{n-1}\right|}\right)\right)$,
where
(a) follows from Lemma 17.10.1
(b) follows from the fact that the conditioning decreases entropy
(c) follows from the fact that $Z$ is a function of $X$ and $Y$
(d) follows since $X_{n}+Y_{n}$ is Gaussian conditioned on $X_{1}, X_{2}, \ldots$, $X_{n-1}, Y_{1}, Y_{2}, \ldots, Y_{n-1}$, and hence we can express its entropy in terms of its variance
(e) follows from the independence of $X_{n}$ and $Y_{n}$ conditioned on the past $X_{1}, X_{2}, \ldots, X_{n-1}, Y_{1}, Y_{2}, \ldots, Y_{n-1}$
(f) follows from the fact that for a set of jointly Gaussian random variables, the conditional variance is constant, independent of the conditioning variables (Lemma 17.10.1)

Setting $A=\lambda S$ and $B=\bar{\lambda} T$, we obtain

$$
\frac{\left|\lambda S_{n}+\bar{\lambda} T_{n}\right|}{\left|\lambda S_{n-1}+\bar{\lambda} T_{n-1}\right|} \geq \lambda \frac{\left|S_{n}\right|}{\left|S_{n-1}\right|}+\bar{\lambda} \frac{\left|T_{n}\right|}{\left|T_{n-1}\right|}
$$

(i.e., $\left|K_{n}\right| /\left|K_{n-1}\right|$ is concave). Simple examples show that $\left|K_{n}\right| /$ $\left|K_{n-p}\right|$ is not necessarily concave for $p \geq 2$.

A number of other determinant inequalities can be proved by these techniques. A few of them are given as problems.
<!-- Page 712 -->
# 종합 요약

엔트로피. $H(X)=-\sum p(x) \log p(x)$.
상대 엔트로피. $D(p \| q)=\sum p(x) \log \frac{p(x)}{q(x)}$.
상호 정보량. $I(X ; Y)=\sum p(x, y) \log \frac{p(x, y)}{p(x) p(y)}$.
정보 부등식. $D(p \| q) \geq 0$.
점근적 등분할 속성. $-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right) \rightarrow$ $H(\mathcal{X})$.

데이터 압축. $H(X) \leq L^{*}<H(X)+1$.
Kolmogorov 복잡도. $K(x)=\min _{\mathcal{U}(p)=x} l(p)$.
보편적 확률. $\log \frac{1}{P_{\mathcal{U}}(x)} \approx K(x)$.
채널 용량. $C=\max _{p(x)} I(X ; Y)$.

## 데이터 전송

- $R<C$ : 점근적으로 오류 없는 통신 가능
- $R>C$ : 점근적으로 오류 없는 통신 불가능

가우시안 채널 용량. $C=\frac{1}{2} \log \left(1+\frac{P}{N}\right)$.
비율 왜곡. $R(D)=\min I(X ; \hat{X})$ 모든 $p(\hat{x} \mid x)$에 대해 $E_{p(x) p(\hat{x} \mid x)} d(X, \hat{X}) \leq D$를 만족하는.

투자의 성장률. $W^{*}=\max _{\mathbf{b}^{*}} E \log \mathbf{b}^{t} \mathbf{X}$.

## 문제

17.1 양의 정부 행렬의 합. 임의의 두 양의 정부 행렬 $K_{1}$과 $K_{2}$에 대해, $\left|K_{1}+K_{2}\right| \geq\left|K_{1}\right|$임을 보이십시오.
<!-- Page 713 -->
17.2 Fan의 부등식 [200] 행렬식 비율에 대하여. $1 \leq p \leq n$인 모든 경우에 대해, 양의 정부호 행렬 $K=K(1,2, \ldots, n)$에 대하여 다음을 보이십시오.

$$
\frac{|K|}{|K(p+1, p+2, \ldots, n)|} \leq \prod_{i=1}^{p} \frac{|K(i, p+1, p+2, \ldots, n)|}{|K(p+1, p+2, \ldots, n)|}
$$

17.3 행렬식 비율의 볼록성. 양의 정부호 행렬 $K$, $K_{0}$에 대하여 $\ln \left(\left|K+K_{0}\right| /|K|\right)$가 $K$에 대해 볼록함을 보이십시오.
17.4 데이터 처리 부등식. 확률 변수 $X_{1}, X_{2}, X_{3}$, $X_{4}$가 마르코프 연쇄 $X_{1} \rightarrow X_{2} \rightarrow X_{3} \rightarrow X_{4}$를 형성한다고 가정합니다. 다음을 보이십시오.

$$
I\left(X_{1} ; X_{3}\right)+I\left(X_{2} ; X_{4}\right) \leq I\left(X_{1} ; X_{4}\right)+I\left(X_{2} ; X_{3}\right)
$$

17.5 마르코프 연쇄. 확률 변수 $X, Y, Z$, $W$가 마르코프 연쇄를 형성하여 $X \rightarrow Y \rightarrow(Z, W)$ [즉, $p(x, y, z, w)=$ $p(x) p(y \mid x) p(z, w \mid y)]$가 성립한다고 가정합니다. 다음을 보이십시오.

$$
I(X ; Z)+I(X ; W) \leq I(X ; Y)+I(Z ; W)
$$

# 역사적 고찰

엔트로피 파워 부등식은 Shannon [472]에 의해 제시되었으며, 최초의 형식적인 증명은 Stam [505]과 Blachman [61]에 의해 이루어졌습니다. 엔트로피 파워 부등식과 Brunn-Minkowski 부등식의 통합된 증명은 Dembo 외 [164]에서 찾을 수 있습니다.

이 장의 대부분의 행렬 부등식은 Cover와 Thomas [118]에 의해 정보 이론적 방법을 사용하여 유도되었습니다. 엔트로피율에 대한 일부 부분 집합 부등식은 Han [270]에서 찾을 수 있습니다.
<!-- Page 714 -->
.
<!-- Page 715 -->
# BIBLIOGRAPHY

[1] J. Abrahams. Code and parse trees for lossless source encoding. Proc. Compression and Complexity of Sequences 1997, pages 145-171, 1998.
[2] N. Abramson. The ALOHA system—another alternative for computer communications. AFIPS Conf. Proc., pages 281-285, 1970.
[3] N. M. Abramson. Information Theory and Coding. McGraw-Hill, New York, 1963.
[4] Y. S. Abu-Mostafa. Information theory. Complexity, pages 25-28, Nov. 1989.
[5] R. L. Adler, D. Coppersmith, and M. Hassner. Algorithms for sliding block codes: an application of symbolic dynamics to information theory. IEEE Trans. Inf. Theory, IT-29(1):5-22, 1983.
[6] R. Ahlswede. The capacity of a channel with arbitrary varying Gaussian channel probability functions. Trans. 6th Prague Conf. Inf. Theory, pages 13-21, Sept. 1971.
[7] R. Ahlswede. Multi-way communication channels. In Proc. 2nd Int. Symp. Inf. Theory (Tsahkadsor, Armenian S.S.R.), pages 23-52. Hungarian Academy of Sciences, Budapest, 1971.
[8] R. Ahlswede. The capacity region of a channel with two senders and two receivers. Ann. Prob., 2:805-814, 1974.
[9] R. Ahlswede. Elimination of correlation in random codes for arbitrarily varying channels. Z. Wahrscheinlichkeitstheorie und verwandte Gebiete, 33:159-175, 1978.
[10] R. Ahlswede. Coloring hypergraphs: A new approach to multiuser source coding. J. Comb. Inf. Syst. Sci., pages 220-268, 1979.
[11] R. Ahlswede. A method of coding and an application to arbitrarily varying channels. J. Comb. Inf. Syst. Sci., pages 10-35, 1980.
[12] R. Ahlswede and T. S. Han. On source coding with side information via a multiple access channel and related problems in multi-user information theory. IEEE Trans. Inf. Theory, IT-29:396-412, 1983.

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (c) 2006 John Wiley & Sons, Inc.
<!-- Page 716 -->
[13] R. Ahlswede와 J. Körner. Source coding with side information and a converse for the degraded broadcast channel. IEEE Trans. Inf. Theory, IT-21:629-637, 1975.
[14] R. F. Ahlswede. Arbitrarily varying channels with states sequence known to the sender. IEEE Trans. Inf. Theory, pages 621-629, Sept. 1986.
[15] R. F. Ahlswede. The maximal error capacity of arbitrarily varying channels for constant list sizes (corresp.). IEEE Trans. Inf. Theory, pages 1416-1417, July 1993.
[16] R. F. Ahlswede와 G. Dueck. Identification in the presence of feedback: a discovery of new capacity formulas. IEEE Trans. Inf. Theory, pages 30-36, Jan. 1989.
[17] R. F. Ahlswede와 G. Dueck. Identification via channels. IEEE Trans. Inf. Theory, pages 15-29, Jan. 1989.
[18] R. F. Ahlswede, E. H. Yang,와 Z. Zhang. Identification via compressed data. IEEE Trans. Inf. Theory, pages 48-70, Jan. 1997.
[19] H. Akaike. Information theory and an extension of the maximum likelihood principle. Proc. 2nd Int. Symp. Inf. Theory, pages 267-281, 1973.
[20] P. Algoet와 T. M. Cover. A sandwich proof of the Shannon-McMillan-Breiman theorem. Ann. Prob., 16(2):899-909, 1988.
[21] P. Algoet와 T. M. Cover. Asymptotic optimality and asymptotic equipartition property of log-optimal investment. Ann. Prob., 16(2):876-898, 1988.
[22] S. Amari. Differential-Geometrical Methods in Statistics. Springer-Verlag, New York, 1985.
[23] S. I. Amari와 H. Nagaoka. Methods of Information Geometry. Oxford University Press, Oxford, 1999.
[24] V. Anantharam와 S. Verdu. Bits through queues. IEEE Trans. Inf. Theory, pages $4-18$, Jan. 1996.
[25] S. Arimoto. An algorithm for calculating the capacity of an arbitrary discrete memoryless channel. IEEE Trans. Inf. Theory, IT-18:14-20, 1972.
[26] S. Arimoto. On the converse to the coding theorem for discrete memoryless channels. IEEE Trans. Inf. Theory, IT-19:357-359, 1973.
[27] R. B. Ash. Information Theory. Interscience, New York, 1965.
[28] J. Aczél와 Z. Daróczy. On Measures of Information and Their Characterization. Academic Press, New York, 1975.
[29] L. R. Bahl, J. Cocke, F. Jelinek,와 J. Raviv. Optimal decoding of linear codes for minimizing symbol error rate (corresp.). IEEE Trans. Inf. Theory, pages 284-287, March 1974.
[30] A. Barron. Entropy and the central limit theorem. Ann. Prob., 14(1): $336-342,1986$.
[31] A. Barron와 T. M. Cover. A bound on the financial value of information. IEEE Trans. Inf. Theory, IT-34:1097-1100, 1988.
<!-- Page 717 -->
[32] A. Barron과 T. M. Cover. Minimum complexity density estimation. IEEE Trans. Inf. Theory, 37(4):1034-1054, July 1991.
[33] A. R. Barron. Logically smooth density estimation. Ph.D. thesis, Department of Electrical Engineering, Stanford University, Stanford, CA, 1985.
[34] A. R. Barron. The strong ergodic theorem for densities: generalized Shan-non-McMillan-Breiman theorem. Ann. Prob., 13:1292-1303, 1985.
[35] A. R. Barron. Are Bayes' rules consistent in information? Prob. Commun. Computation, pages 85-91, 1987.
[36] A. R. Barron, J. Rissanen, and Bin Yu. The minimum description length principle in coding and modeling. IEEE Trans. Inf. Theory, pages 2743-2760, Oct. 1998.
[37] E. B. Baum. Neural net algorithms that learn in polynomial time from examples and queries. IEEE Trans. Neural Networks, pages 5-19, 1991.
[38] W. Beckner. Inequalities in Fourier analysis. Ann. Math., 102:159-182, 1975.
[39] R. Bell과 T. M. Cover. Competitive optimality of logarithmic investment. Math. Oper. Res., 5(2):161-166, May 1980.
[40] R. Bell과 T. M. Cover. Game-theoretic optimal portfolios. Manage. Sci., 34(6):724-733, 1988.
[41] T. C. Bell, J. G. Cleary, and I. H. Witten. Text Compression. Prentice-Hall, Englewood Cliffs, NJ, 1990.
[42] R. Bellman. Notes on matrix theory. IV: An inequality due to Bergstrøm. Am. Math. Monthly, 62:172-173, 1955.
[43] C. H. Bennett와 G. Brassard. Quantum cryptography: public key distribution and coin tossing. Proc. IEEE Int. Conf. Comput., pages 175-179, 1984.
[44] C. H. Bennett, D. P. DiVincenzo, J. Smolin, and W. K. Wootters. Mixed state entanglement and quantum error correction. Phys. Rev. A, pages $3824-3851,1996$.
[45] C. H. Bennett, D. P. DiVincenzo, and J. A. Smolin. Capacities of quantum erasure channels. Phys. Rev. Lett., pages 3217-3220, 1997.
[46] C. H. Bennett와 S. J. Wiesner. Communication via one- and two-particle operators on Einstein-podolsky-Rosen states. Phys. Rev. Lett., pages $2881-2884,1992$.
[47] C. H. Bennett. Demons, engines and the second law. Sci. Am., 259(5):108-116, Nov. 1987.
[48] C. H. Bennett와 R. Landauer. The fundamental physical limits of computation. Sci. Am., 255(1):48-56, July 1985.
[49] C. H. Bennett와 P. W. Shor. Quantum information theory. IEEE Trans. Inf. Theory, IT-44:2724-2742, Oct. 1998.
[50] J. Bentley, D. Sleator, R. Tarjan, and V. Wei. Locally adaptive data compression scheme. Commun. ACM, pages 320-330, 1986.
<!-- Page 718 -->
[51] R. Benzel. The capacity region of a class of discrete additive degraded interference channels. IEEE Trans. Inf. Theory, IT-25:228-231, 1979.
[52] T. Berger. Rate Distortion Theory: A Mathematical Basis for Data Compression. Prentice-Hall, Englewood Cliffs, NJ, 1971.
[53] T. Berger. Multiterminal source coding. In G. Longo (Ed.), The Information Theory Approach to Communications. Springer-Verlag, New York, 1977.
[54] T. Berger and R. W. Yeung. Multiterminal source encoding with one distortion criterion. IEEE Trans. Inf. Theory, IT-35:228-236, 1989.
[55] P. Bergmans. Random coding theorem for broadcast channels with degraded components. IEEE Trans. Inf. Theory, IT-19:197-207, 1973.
[56] E. R. Berlekamp. Block Coding with Noiseless Feedback. Ph.D. thesis, MIT, Cambridge, MA, 1964.
[57] C. Berrou, A. Glavieux, and P. Thitimajshima. Near Shannon limit errorcorrecting coding and decoding: Turbo codes. Proc. 1993 Int. Conf. Commun., pages 1064-1070, May 1993.
[58] D. Bertsekas and R. Gallager. Data Networks, 2nd ed.. Prentice-Hall, Englewood Cliffs, NJ, 1992.
[59] M. Bierbaum and H. M. Wallmeier. A note on the capacity region of the multiple access channel. IEEE Trans. Inf. Theory, IT-25:484, 1979.
[60] E. Biglieri, J. Proakis, and S. Shamai. Fading channels: information-theoretic and communications aspects. IEEE Trans. Inf. Theory, pages 2619-2692, October 1998.
[61] N. Blachman. The convolution inequality for entropy powers. IEEE Trans. Inf. Theory, IT-11:267-271, Apr. 1965.
[62] D. Blackwell, L. Breiman, and A. J. Thomasian. Proof of Shannon's transmission theorem for finite-state indecomposable channels. Ann. Math. Stat., pages 1209-1220, 1958.
[63] D. Blackwell, L. Breiman, and A. J. Thomasian. The capacity of a class of channels. Ann. Math. Stat., 30:1229-1241, 1959.
[64] D. Blackwell, L. Breiman, and A. J. Thomasian. The capacities of certain channel classes under random coding. Ann. Math. Stat., 31:558-567, 1960 .
[65] R. Blahut. Computation of channel capacity and rate distortion functions. IEEE Trans. Inf. Theory, IT-18:460-473, 1972.
[66] R. E. Blahut. Information bounds of the Fano-Kullback type. IEEE Trans. Inf. Theory, IT-22:410-421, 1976.
[67] R. E. Blahut. Principles and Practice of Information Theory. AddisonWesley, Reading, MA, 1987.
[68] R. E. Blahut. Hypothesis testing and information theory. IEEE Trans. Inf. Theory, IT-20:405-417, 1974.
[69] R. E. Blahut. Theory and Practice of Error Control Codes. Addison-Wesley, Reading, MA, 1983.
<!-- Page 719 -->
[70] B. M. Hochwald, G. Caire, B. Hassibi, 및 T. L. Marzetta (Eds.). IEEE Trans. Inf. Theory, Special Issue on Space-Time Transmission, Reception, Coding and Signal-Processing, Vol. 49, Oct. 2003.
[71] L. Boltzmann. Beziehung Zwischen dem zweiten Hauptsatze der mechanischen Wärmertheorie und der Wahrscheilichkeitsrechnung respektive den Saetzen uber das Wärmegleichgwicht. Wien. Ber., pages 373-435, 1877.
[72] R. C. Bose 및 D. K. Ray-Chaudhuri. On a class of error correcting binary group codes. Inf. Control, 3:68-79, Mar. 1960.
[73] H. J. Brascamp 및 E. J. Lieb. Best constants in Young's inequality, its converse and its generalization to more than three functions. Adv. Math., 20:151-173, 1976.
[74] L. Breiman. The individual ergodic theorems of information theory. Ann. Math. Stat., 28:809-811, 1957. With correction made in 31:809-810.
[75] L. Breiman. Optimal gambling systems for favourable games. In Fourth Berkeley Symposium on Mathematical Statistics and Probability, Vol. 1, pages 65-78. University of California Press, Berkeley, CA, 1961.
[76] L. Breiman, J. H. Friedman, R. A. Olshen, 및 C. J. Stone. Classification and Regression Trees. Wadsworth & Brooks, Pacific Grove, CA, 1984.
[77] L. Brillouin. Science and Information Theory. Academic Press, New York, 1962.
[78] J. A. Bucklew. The source coding theorem via Sanov's theorem. IEEE Trans. Inf. Theory, pages 907-909, Nov. 1987.
[79] J. A. Bucklew. Large Deviation Techniques in Decision, Simulation, and Estimation. WILEY, New York, 1990.
[80] J. P. Burg. Maximum entropy spectral analysis. Ph.D. thesis, Department of Geophysics, Stanford University, Stanford, CA, 1975.
[81] M. Burrows 및 D. J. Wheeler. A Block-Sorting Lossless Data Compression Algorithm (Tech. Rept. 124). Digital Systems Research Center, Palo Alto, CA, May 1994.
[82] A. R. Calderbank. The art of signaling: fifty years of coding theory. IEEE Trans. Inf. Theory, pages 2561-2595, Oct. 1998.
[83] A. R. Calderbank 및 P. W. Shor. Good quantum error-correcting codes exist. Phys. Rev. A, pages 1098-1106, 1995.
[84] A. Carleial. Outer bounds on the capacity of the interference channel. IEEE Trans. Inf. Theory, IT-29:602-606, 1983.
[85] A. B. Carleial. A case where interference does not reduce capacity. IEEE Trans. Inf. Theory, IT-21:569-570, 1975.
[86] G. Chaitin. Information-Theoretic Incompleteness. World Scientific, Singapore, 1992.
[87] G. J. Chaitin. On the length of programs for computing binary sequences. J. ACM, pages 547-569, 1966.
<!-- Page 720 -->
[88] G. J. Chaitin. The limits of mathematics. J. Universal Comput. Sci., 2(5):270-305, 1996.
[89] G. J. Chaitin. On the length of programs for computing binary sequences. J. ACM, 13:547-569, 1966.
[90] G. J. Chaitin. Information theoretical limitations of formal systems. J. ACM, 21:403-424, 1974.
[91] G. J. Chaitin. Randomness and mathematical proof. Sci. Am., 232(5):47-52, May 1975.
[92] G. J. Chaitin. Algorithmic information theory. IBM J. Res. Dev., 21:350-359, 1977.
[93] G. J. Chaitin. Algorithmic Information Theory. Cambridge University Press, Cambridge, 1987.
[94] C. S. Chang and J. A. Thomas. Huffman algebras for independent random variables. Discrete Event Dynam. Syst., 4:23-40, 1994.
[95] C. S. Chang and J. A. Thomas. Effective bandwidth in high speed digital networks. IEEE J. Select. Areas Commun., 13:1091-1114, Aug. 1995.
[96] R. Chellappa. Markov Random Fields: Theory and Applications. Academic Press, San Diego, CA, 1993.
[97] H. Chernoff. A measure of the asymptotic efficiency of tests of a hypothesis based on a sum of observations. Ann. Math. Stat., 23:493-507, 1952.
[98] B. S. Choi and T. M. Cover. An information-theoretic proof of Burg's maximum entropy spectrum. Proc. IEEE, 72:1094-1095, 1984.
[99] N. Chomsky. Three models for the description of language. IEEE Trans. Inf. Theory, pages 113-124, Sept. 1956.
[100] P. A. Chou, M. Effros, and R. M. Gray. A vector quantization approach to universal noiseless coding and quantization. IEEE Trans. Inf. Theory, pages 1109-1138, July 1996.
[101] K. L. Chung. A note on the ergodic theorem of information theory. Ann. Math. Stat., 32:612-614, 1961.
[102] B. S. Clarke and A. R. Barron. Information-theoretic asymptotics of Bayes' methods. IEEE Trans. Inf. Theory, pages 453-471, May 1990.
[103] B. S. Clarke and A. R. Barron. Jeffreys' prior is asymptotically least favorable under entropy risk. J. Stat. Planning Inf., pages 37-60, Aug. 1994.
[104] M. Costa and T. M. Cover. On the similarity of the entropy power inequality and the Brunn-Minkowski inequality. IEEE Trans. Inf. Theory, IT-30:837-839, 1984.
[105] M. H. M. Costa. On the Gaussian interference channel. IEEE Trans. Inf. Theory, pages 607-615, Sept. 1985.
[106] M. H. M. Costa and A. A. El Gamal. The capacity region of the discrete memoryless interference channel with strong interference. IEEE Trans. Inf. Theory, pages 710-711, Sept. 1987.
<!-- Page 721 -->
[107] T. M. Cover. 선형 부등식 시스템의 기하학적 및 통계적 속성과 패턴 인식에의 응용. IEEE Trans. Electron. Computation, 페이지 326-334, 1965.
[108] T. M. Cover. 보편적 도박 계획과 Kolmogorov 및 Chaitin의 복잡성 척도 (기술 보고서 12). 통계학과, 스탠포드 대학교, 스탠포드, CA, 1974년 10월.
[109] T. M. Cover. 정보 이론의 열린 문제들. Proc. Moscow Inf. Theory Workshop, 페이지 35-36, 1975.
[110] T. M. Cover. 보편적 포트폴리오. Math. Finance, 페이지 1-29, 1991년 1월.
[111] T. M. Cover. 방송 채널에 대한 논평. IEEE Trans. Inf. Theory, 페이지 2524-2530, 1998년 10월.
[112] T. M. Cover. Shannon과 투자. IEEE Inf. Theory Newslett (특별 황금 기념호), 페이지 10-11, 1998년 6월.
[113] T. M. Cover 및 M. S. Chiang. 양방향 상태 정보를 갖는 channel capacity와 rate distortion 간의 이중성. IEEE Trans. Inf. Theory, IT-48(6):1629-1638, 2002년 6월.
[114] T. M. Cover, P. Gács, 및 R. M. Gray. 정보 이론 및 알고리즘 복잡성에 대한 Kolmogorov의 기여. Ann. Prob., 페이지 840-865, 1989년 7월.
[115] T. M. Cover, A. A. El Gamal, 및 M. Salehi. 임의로 상관된 소스를 갖는 다중 접속 채널. IEEE Trans. Inf. Theory, 페이지 648-657, 1980년 11월.
[116] T. M. Cover 및 P. E. Hart. 최근접 이웃 패턴 분류. IEEE Trans. Inf. Theory, 페이지 21-27, 1967년 1월.
[117] T. M. Cover 및 S. Pombra. 가우시안 피드백 capacity. IEEE Trans. Inf. Theory, 페이지 37-43, 1989년 1월.
[118] T. M. Cover 및 J. A. Thomas. 정보 이론을 통한 행렬식 부등식. SIAM J. Matrix Anal. and Its Applications, 9(3):384-392, 1988년 7월.
[119] T. M. Cover. 방송 채널. IEEE Trans. Inf. Theory, IT-18:2-14, 1972.
[120] T. M. Cover. 열거적 소스 인코딩. IEEE Trans. Inf. Theory, IT19(1):73-77, 1973년 1월.
[121] T. M. Cover. 방송 채널에 대한 달성 가능한 속도 영역. IEEE Trans. Inf. Theory, IT-21:399-404, 1975.
[122] T. M. Cover. Slepian 및 Wolf의 데이터 압축 정리에 대한 에르고딕 소스 증명. IEEE Trans. Inf. Theory, IT-22:226-228, 1975.
[123] T. M. Cover. 기대 로그 투자 수익을 최대화하는 알고리즘. IEEE Trans. Inf. Theory, IT-30(2):369-373, 1984.
[124] T. M. Cover. Kolmogorov 복잡성, 데이터 압축 및 추론. J. Skwirzynski (편집), The Impact of Processing Techniques on Communications, Vol. 91 of Applied Sciences. Martinus-Nijhoff, Dordrecht, 네덜란드, 1985.
<!-- Page 722 -->
[125] T. M. Cover. Huffman 코드의 경쟁적 최적성에 관하여. IEEE Trans. Inf. Theory, 37(1):172-174, 1991년 1월.
[126] T. M. Cover. Universal 포트폴리오. Math. Finance, 페이지 1-29, 1991년 1월.
[127] T. M. Cover와 A El Gamal. 릴레이 채널의 용량 정리. IEEE Trans. Inf. Theory, IT-25:572-584, 1979.
[128] T. M. Cover와 A. El Gamal. 정보 이론적 증명을 통한 Hadamard 부등식. IEEE Trans. Inf. Theory, IT-29(6):930-931, 1983년 11월.
[129] T. M. Cover, A. El Gamal, 그리고 M. Salehi. 임의로 상관된 소스를 갖는 다중 접속 채널. IEEE Trans. Inf. Theory, IT-26:648-657, 1980.
[130] T. M. Cover. 가장 큰 숫자를 선택하십시오, Open Problems in Communication and Computation. T. M. Cover와 B. Gopinath가 편집, 페이지 152, 뉴욕, 1987.
[131] T. M. Cover와 R. King. 영어의 엔트로피에 대한 수렴 도박 추정. IEEE Trans. Inf. Theory, IT-24:413-421, 1978.
[132] T. M. Cover와 C. S. K. Leung. Shannon 엔트로피와 Kolmogorov 복잡성 간의 몇 가지 동등성. IEEE Trans. Inf. Theory, IT-24:331-338, 1978.
[133] T. M. Cover와 C. S. K. Leung. 피드백이 있는 다중 접속 채널에 대한 달성 가능한 속도 영역. IEEE Trans. Inf. Theory, IT-27:292-298, 1981.
[134] T. M. Cover, R. J. McEliece, 그리고 E. Posner. 비동기 다중 접속 채널 용량. IEEE Trans. Inf. Theory, IT-27:409-413, 1981.
[135] T. M. Cover와 E. Ordentlich. 부가 정보가 있는 Universal 포트폴리오. IEEE Trans. Inf. Theory, IT-42:348-363, 1996년 3월.
[136] T. M. Cover와 S. Pombra. 가우시안 피드백 용량. IEEE Trans. Inf. Theory, IT-35:37-43, 1989.
[137] H. Cramer. Mathematical Methods of Statistics. Princeton University Press, 프린스턴, NJ, 1946.
[138] I. Csiszár. 확률 분포의 차이에 대한 정보 유형 측정 및 간접 관찰. Stud. Sci. Math. Hung., 2:299-318, 1967.
[139] I Csiszár. 속도 왜곡 함수 계산에 관하여. IEEE Trans. Inf. Theory, IT-20:122-124, 1974.
[140] I. Csiszár. 확률 분포의 I-발산 기하학 및 최소화 문제. Ann. Prob., 페이지 146-158, 1975년 2월.
[141] I Csiszár. Sanov 속성, 일반화된 I-투영 및 조건부 극한 정리. Ann. Prob., 12:768-793, 1984.
[142] I. Csiszár. 정보 이론과 에르고딕 이론. Probl. Contr. Inf. Theory, 페이지 3-27, 1987.
[143] I. Csiszár. Darroch와 Ratcliff의 일반화된 반복 스케일링에 대한 기하학적 해석. Ann. Stat., 페이지 1409-1413, 1989.
<!-- Page 723 -->
[144] I. Csiszár. Why least squares and maximum entropy? An axiomatic approach to inference for linear inverse problems. Ann. Stat., pages 2032-2066, Dec. 1991.
[145] I. Csiszár. Arbitrarily varying channels with general alphabets and states. IEEE Trans. Inf. Theory, pages 1725-1742, Nov. 1992.
[146] I. Csiszár. The method of types. IEEE Trans. Inf. Theory, pages 2505-2523, October 1998.
[147] I. Csiszár, T. M. Cover, and B. S. Choi. Conditional limit theorems under Markov conditioning. IEEE Trans. Inf. Theory, IT-33:788-801, 1987.
[148] I. Csiszár and J. Körner. Towards a general theory of source networks. IEEE Trans. Inf. Theory, IT-26:155-165, 1980.
[149] I. Csiszár and J. Körner. Information Theory: Coding Theorems for Discrete Memoryless Systems. Academic Press, New York, 1981.
[150] I. Csiszár and J. Körner. Feedback does not affect the reliability function of a DMC at rates above capacity (corresp.). IEEE Trans. Inf. Theory, pages 92-93, Jan. 1982.
[151] I. Csiszár and J. Körner. Broadcast channels with confidential messages. IEEE Trans. Inf. Theory, pages 339-348, May 1978.
[152] I. Csiszár and J. Körner. Graph decomposition: a new key to coding theorems. IEEE Trans. Inf. Theory, pages 5-12, Jan. 1981.
[153] I. Csiszár and G. Longo. On the Error Exponent for Source Coding and for Testing Simple Statistical Hypotheses. Hungarian Academy of Sciences, Budapest, 1971.
[154] I. Csiszár and P. Narayan. Capacity of the Gaussian arbitrarily varying channel. IEEE Trans. Inf. Theory, pages 18-26, Jan. 1991.
[155] I. Csiszár and G. Tusnády. Information geometry and alternating minimization procedures. Statistics and Decisions, Supplement Issue 1:205-237, 1984.
[156] G. B. Dantzig and D. R. Fulkerson. On the max-flow min-cut theorem of networks. In H. W. Kuhn and A. W. Tucker (Eds.), Linear Inequalities and Related Systems (Vol. 38 of Annals of Mathematics Study), pages 215-221. Princeton University Press, Princeton, NJ, 1956.
[157] J. N. Darroch and D. Ratcliff. Generalized iterative scaling for log-linear models. Ann. Math. Stat., pages 1470-1480, 1972.
[158] I. Daubechies. Ten Lectures on Wavelets. SIAM, Philadelphia, 1992.
[159] L. D. Davisson. Universal noiseless coding. IEEE Trans. Inf. Theory, IT-$19: 783-795,1973$.
[160] L. D. Davisson. Minimax noiseless universal coding for Markov sources. IEEE Trans. Inf. Theory, pages 211-215, Mar. 1983.
[161] L. D. Davisson, R. J. McEliece, M. B. Pursley, and M. S. Wallace. Efficient universal noiseless source codes. IEEE Trans. Inf. Theory, pages 269-279, May 1981.
<!-- Page 724 -->
[162] A. Dembo. 정보 불확실성 원리와 불확실성 원리 (기술 보고서), 통계학과, 스탠포드 대학교, 스탠포드, CA, 1990.
[163] A. Dembo. 정보 불확실성 원리와 측정 집중. Ann. Prob., 페이지 927-939, 1997.
[164] A. Dembo, T. M. Cover, and J. A. Thomas. 정보 이론적 불확실성 원리. IEEE Trans. Inf. Theory, 37(6):1501-1518, 1991년 11월.
[165] A. Dembo and O. Zeitouni. 대규모 편차 기법 및 응용. Jones & Bartlett, 보스턴, 1993.
[166] A. P. Dempster, N. M. Laird, and D. B. Rubin. EM algorithm을 통한 불완전 데이터의 최대 가능도 추정. J. Roy. Stat. Soc. B, 39(1):1-38, 1977.
[167] L. Devroye and L. Gyorfi. 비모수적 밀도 추정: $L_{1}$ 관점. WILEY, 뉴욕, 1985.
[168] L. Devroye, L. Gyorfi, and G. Lugosi. 패턴 인식의 확률 이론. Springer-Verlag, 뉴욕, 1996.
[169] D. P. DiVincenzo, P. W. Shor, and J. A. Smolin. 매우 잡음이 많은 채널의 양자 채널 용량. Phys. Rev. A, 페이지 830-839, 1998.
[170] R.L. Dobrushin. 정보 이론의 Shannon 주요 정리에 대한 일반적인 공식화. Usp. Math. Nauk, 14:3-104, 1959. Am. Math. Soc. Trans., 33:323-438에 번역됨.
[171] R. L. Dobrushin. 소련 정보 이론 연구 조사. IEEE Trans. Inf. Theory, 페이지 703-724, 1972년 11월.
[172] D. L. Donoho. 소프트 임계값을 이용한 노이즈 제거. IEEE Trans. Inf. Theory, 페이지 613-627, 1995년 5월.
[173] R. O. Duda and P. E. Hart. 패턴 분류 및 장면 분석. WILEY, 뉴욕, 1973.
[174] G. Dueck. 다중 사용자 채널의 최대 오차 용량 영역은 평균 오차 용량 영역보다 작습니다. Probl. Contr. Inf. Theory, 페이지 11-19, 1978.
[175] G. Dueck. 양방향 채널의 용량 영역은 내부 경계를 초과할 수 있습니다. Inf. Control, 40:258-266, 1979.
[176] G. Dueck. 양방향 및 방송 채널에 대한 부분 피드백. Inf. Control, 46:1-15, 1980.
[177] G. Dueck and J. Körner. 용량 이상의 속도에서의 이산 메모리 없는 채널의 신뢰도 함수. IEEE Trans. Inf. Theory, IT-25:82-85, 1979.
[178] P. M. Ebert. 피드백이 있는 가우시안 채널의 용량. Bell Syst. Tech. J., 49:1705-1712, 1970년 10월.
[179] P. M. Ebert. 피드백이 있는 가우시안 채널의 용량. Bell Syst. Tech. J., 페이지 1705-1712, 1970년 10월.
[180] K. Eckschlager. 분석 화학의 정보 이론. WILEY, 뉴욕, 1994.
<!-- Page 725 -->
[181] M. Effros, K. Visweswariah, S. R. Kulkarni, and S. Verdu. Universal lossless source coding with the Burrows-Wheeler transform. IEEE Trans. Inf. Theory, IT-48:1061-1081, May 2002.
[182] B. Efron and R. Tibshirani. An Introduction to the Bootstrap. Chapman \& Hall, London, 1993.
[183] H. G. Eggleston. Convexity (Cambridge Tracts in Mathematics and Mathematical Physics, No. 47). Cambridge University Press, Cambridge, 1969.
[184] A. El Gamal. The feedback capacity of degraded broadcast channels. IEEE Trans. Inf. Theory, IT-24:379-381, 1978.
[185] A. El Gamal. The capacity region of a class of broadcast channels. IEEE Trans. Inf. Theory, IT-25:166-169, 1979.
[186] A. El Gamal and T. M. Cover. Multiple user information theory. Proc. IEEE, 68:1466-1483, 1980.
[187] A. El Gamal and T. M. Cover. Achievable rates for multiple descriptions. IEEE Trans. Inf. Theory, IT-28:851-857, 1982.
[188] A. El Gamal and E. C. Van der Meulen. A proof of Marton's coding theorem for the discrete memoryless broadcast channel. IEEE Trans. Inf. Theory, IT-27:120-122, 1981.
[189] P. Elias. Error-free coding. IRE Trans. Inf. Theory, IT-4:29-37, 1954.
[190] P. Elias. Coding for noisy channels. IRE Conv. Rec., Pt. 4, pages 37-46, 1955.
[191] P. Elias. Networks of Gaussian channels with applications to feedback systems. IEEE Trans. Inf. Theory, pages 493-501, July 1967.
[192] P. Elias. The efficient construction of an unbiased random sequence. Ann. Math. Stat., pages 865-870, 1972.
[193] P. Elias. Universal codeword sets and representations of the integers. IEEE Trans. Inf. Theory, pages 194-203, Mar. 1975.
[194] P. Elias. Interval and recency rank source coding: two on-line adaptive variable-length schemes. IEEE Trans. Inf. Theory, pages 3-10, Jan. 1987.
[195] P. Elias, A. Feinstein, and C. E. Shannon. A note on the maximum flow through a network. IEEE Trans. Inf. Theory, pages 117-119, December 1956.
[196] R. S. Ellis. Entropy, Large Deviations, and Statistical Mechanics. SpringerVerlag, New York, 1985.
[197] A. Ephremides and B. Hajek. Information theory and communication networks: an unconsummated union. IEEE Trans. Inf. Theory, pages 2416-2434, Oct. 1998.
[198] W. H. R. Equitz and T. M. Cover. Successive refinement of information. IEEE Trans. Inf. Theory, pages 269-275, Mar. 1991.
[199] Ky Fan. On a theorem of Weyl concerning the eigenvalues of linear transformations II. Proc. Nat. Acad. Sci. USA, 36:31-35, 1950.
<!-- Page 726 -->
[200] Ky Fan. 양의 정부호 행렬에 관한 일부 부등식. Proc. Cambridge Philos. Soc., 51:414-421, 1955.
[201] R. M. Fano. 정보 전송 강의 노트, 6.574 (기술 보고서). MIT, Cambridge, MA, 1952.
[202] R. M. Fano. 정보 전송: 통신에 대한 통계 이론. WILEY, New York, 1961.
[203] M. Feder. Huffman 코드의 경쟁적 최적성에 대한 메모. IEEE Trans. Inf. Theory, 38(2):436-439, 1992년 3월.
[204] M. Feder, N. Merhav, 및 M. Gutman. 개별 시퀀스의 보편적 예측. IEEE Trans. Inf. Theory, 페이지 1258-1270, 1992년 7월.
[205] A. Feinstein. 정보 이론에 대한 새로운 기본 정리. IRE Trans. Inf. Theory, IT-4:2-22, 1954.
[206] A. Feinstein. 정보 이론의 기초. McGraw-Hill, New York, 1958.
[207] A. Feinstein. 유한 메모리 채널에 대한 코딩 정리 및 그 역정리에 관하여. Inf. Control, 2:25-44, 1959.
[208] W. Feller. 확률 이론 및 그 응용 입문, 제2판, 제1권. WILEY, New York, 1957.
[209] R. A. Fisher. 이론 통계학의 수학적 기초에 관하여. Philos. Trans. Roy. Soc., London A, 222:309-368, 1922.
[210] R. A. Fisher. 통계적 추정 이론. Proc. Cambridge Philos. Soc., 22:700-725, 1925.
[211] B. M. Fitingof. 알 수 없거나 가변적인 메시지 통계에 대한 최적 인코딩. Probl. Inf. Transm. (USSR), 페이지 3-11, 1966.
[212] B. M. Fitingof. 이산 정보의 압축. Probl. Inf. Transm. (USSR), 페이지 28-36, 1967.
[213] L. R. Ford 및 D. R. Fulkerson. 네트워크를 통한 최대 흐름. Can. J. Math., 페이지 399-404, 1956.
[214] L. R. Ford 및 D. R. Fulkerson. 네트워크에서의 흐름. Princeton University Press, Princeton, NJ, 1962.
[215] G. D. Forney. 삭제, 리스트 및 결정 피드백 방식에 대한 지수적 오류 경계. IEEE Trans. Inf. Theory, IT-14:549-557, 1968.
[216] G. D. Forney. 정보 이론: 미출판 강의 노트. Stanford University, Stanford, CA, 1972.
[217] G. J. Foschini. 다중 안테나를 사용하는 페이딩 환경에서의 무선 통신을 위한 계층적 시공간 아키텍처. Bell Syst. Tech. J., 1(2):41-59, 1996.
[218] P. Franaszek, P. Tsoucas, 및 J. Thomas. 다중 사전 데이터 압축을 위한 컨텍스트 할당. In Proc. IEEE Int. Symp. Inf. Theory, Trondheim, Norway, 페이지 12, 1994.
[219] P. A. Franaszek. 이산 무잡음 채널에 대한 동기식 가변 길이 코딩에 관하여. Inf. Control, 15:155-164, 1969.
<!-- Page 727 -->
[220] T. Gaarder와 J. K. Wolf. 다중 접속 이산 비메모리 채널의 capacity region은 피드백으로 증가할 수 있습니다. IEEE Trans. Inf. Theory, IT-21:100-102, 1975.
[221] D. Gabor. 통신 이론. J. Inst. Elec. Engg., 페이지 429-457, 1946년 9월.
[222] P. Gacs와 J. Körner. 공통 정보는 mutual information보다 훨씬 적습니다. Probl. Contr. Inf. Theory, 페이지 149-162, 1973.
[223] R. G. Gallager. 부가 정보가 있는 소스 코딩 및 범용 코딩. 미출판 원고, 또한 Int. Symp. Inf. Theory, 1974년 10월에 발표됨.
[224] R. G. Gallager. 코딩 정리의 간단한 유도 및 일부 응용. IEEE Trans. Inf. Theory, IT-11:3-18, 1965.
[225] R. G. Gallager. 열화된 브로드캐스트 채널의 capacity 및 코딩. Probl. Peredachi Inf., 10(3):3-14, 1974.
[226] R. G. Gallager. 데이터 통신 네트워크의 프로토콜 정보에 대한 기본 한계. IEEE Trans. Inf. Theory, 페이지 385-398, 1976년 7월.
[227] R. G. Gallager. 분산 계산을 이용한 최소 지연 라우팅 algorithm. IEEE Trans. Commun., 페이지 73-85, 1977년 1월.
[228] R. G. Gallager. Huffman의 테마에 대한 변형. IEEE Trans. Inf. Theory, 페이지 668-674, 1978년 11월.
[229] R. G. Gallager. 부가 정보가 있는 소스 코딩 및 범용 코딩 (기술 보고서 LIDS-P-937). 정보 결정 시스템 연구소, MIT, 케임브리지, MA, 1979.
[230] R. G. Gallager. 다중 접속 채널에 대한 관점. IEEE Trans. Inf. Theory, 페이지 124-142, 1985년 3월.
[231] R. G. Gallager. 저밀도 패리티 검사 코드. IRE Trans. Inf. Theory, IT-8:21-28, 1962년 1월.
[232] R. G. Gallager. 저밀도 패리티 검사 코드. MIT Press, 케임브리지, MA, 1963.
[233] R. G. Gallager. 정보 이론 및 신뢰할 수 있는 통신. WILEY, 뉴욕, 1968.
[234] A. A. El Gamal와 T. M. Cover. 다중 설명에 대한 달성 가능한 속도. IEEE Trans. Inf. Theory, 페이지 851-857, 1982년 11월.
[235] A. El Gamal. 피드백이 있는 브로드캐스트 채널 및 없는 브로드캐스트 채널. 11차 연례 Asilomar 회로 회의, 페이지 180-183, 1977년 11월.
[236] A. El Gamal. 두 개의 일치하지 않는 브로드캐스트 채널의 곱 및 합의 capacity. Probl. Peredachi Inf., 페이지 3-23, 1980년 1월-3월.
[237] A. A. El Gamal. 열화된 브로드캐스트 채널의 피드백 capacity (통신). IEEE Trans. Inf. Theory, 페이지 379-381, 1978년 5월.
[238] A. A. El Gamal. 한 클래스의 브로드캐스트 채널의 capacity. IEEE Trans. Inf. Theory, 페이지 166-169, 1979년 3월.
<!-- Page 728 -->
[239] A. A. El Gamal. 피지적으로 열화된 가우시안 방송 채널의 피드백을 갖는 용량 (corresp.). IEEE Trans. Inf. Theory, 페이지 508-511, 1981년 7월.
[240] A. A. El Gamal 및 E. C. van der Meulen. 이산 메모리 없는 방송 채널에 대한 Marton의 코딩 정리에 대한 증명. IEEE Trans. Inf. Theory, 페이지 120-122, 1981년 1월.
[241] I. M. Gelfand, A. N. Kolmogorov, 및 A. M. Yaglom. 상호 정보의 일반적인 정의에 대하여. Rept. Acad. Sci. USSR, 페이지 745-748, 1956년.
[242] S. I. Gelfand. 하나의 방송 채널의 용량. Probl. Peredachi Inf., 페이지 106-108, 1977년 7월-9월.
[243] S. I. Gelfand 및 M. S. Pinsker. 하나의 결정론적 구성 요소를 갖는 방송 채널의 용량. Probl. Peredachi Inf., 페이지 24-34, 1980년 1월-3월.
[244] S. I. Gelfand 및 M. S. Pinsker. 무작위 매개변수를 갖는 채널에 대한 코딩. Probl. Contr. Inf. Theory, 페이지 19-31, 1980년.
[245] A. Gersho 및 R. M. Gray. Vector Quantization and Signal Compression. Kluwer, Boston, 1992년.
[246] G. G. Rayleigh 및 J. M. Cioffi. 무선 통신을 위한 시공간 코딩. IEEE Trans. Commun., 46:357-366, 1998년.
[247] J. D. Gibson 및 J. L. Melsa. Introduction to Nonparametric Detection with Applications. IEEE Press, New York, 1996년.
[248] E. N. Gilbert. 부정확한 소스 확률에 기반한 코딩. IEEE Trans. Inf. Theory, 페이지 304-314, 1971년 5월.
[249] E. N. Gilbert 및 E. F. Moore. 가변 길이 이진 인코딩. Bell Syst. Tech. J., 38:933-967, 1959년.
[250] S. Goldman. 레이더 및 통신에서의 잡음 감소 및 범위에 관한 몇 가지 근본적인 고려 사항. Proc. Inst. Elec. Engg., 페이지 $584-594, 1948년.
[251] S. Goldman. Information Theory. Prentice-Hall, Englewood Cliffs, NJ, 1953년.
[252] A. Goldsmith 및 M. Effros. 상호 간섭 및 색상 가우시안 잡음을 갖는 가우시안 방송 채널의 용량 영역. IEEE Trans. Inf. Theory, 47:2-8, 2001년 1월.
[253] S. W. Golomb. 런 길이 인코딩. IEEE Trans. Inf. Theory, 페이지 399-401, 1966년 7월.
[254] S. W. Golomb, R. E. Peile, 및 R. A. Scholtz. Basic Concepts in Information Theory and Coding: The Adventures of Secret Agent 00111 (Applications of Communications Theory). Plenum Publishing, New York, 1994년.
[255] A. J. Grant, B. Rimoldi, R. L. Urbanke, 및 P. A. Whiting. 이산 메모리 없는 채널에 대한 속도 분할 다중 액세스. IEEE Trans. Inf. Theory, 페이지 873-890, 2001년 3월.
<!-- Page 729 -->
[256] R. M. Gray. Source Coding Theory. Kluwer, Boston, 1990.
[257] R. M. Gray and L. D. Davisson, (Eds.). Ergodic and Information Theory. Dowden, Hutchinson \& Ross, Stroudsburg, PA, 1977.
[258] R. M. Gray and Lee D. Davisson. Source coding theorems without the ergodic assumption. IEEE Trans. Inf. Theory, pages 502-516, July 1974.
[259] R. M. Gray. Sliding block source coding. IEEE Trans. Inf. Theory, IT-21:357-368, 1975.
[260] R. M. Gray. Entropy and Information Theory. Springer-Verlag, New York, 1990.
[261] R. M. Gray and A. Wyner. Source coding for a simple network. Bell Syst. Tech. J., 58:1681-1721, 1974.
[262] U. Grenander and G. Szego. Toeplitz Forms and Their Applications. University of California Press, Berkeley, CA, 1958.
[263] B. Grünbaum. Convex Polytopes. WILEY, INTERSCIENCE, New York, 1967.
[264] S. Guiasu. Information Theory with Applications. McGraw-Hill, New York, 1976.
[265] B. E. Hajek and M. B. Pursley. Evaluation of an achievable rate region for the broadcast channel. IEEE Trans. Inf. Theory, pages 36-46, Jan. 1979.
[266] R. V. Hamming. Error detecting and error correcting codes. Bell Syst. Tech. J., 29:147-160, 1950.
[267] T. S. Han. The capacity region for the deterministic broadcast channel with a common message (corresp.). IEEE Trans. Inf. Theory, pages 122-125, Jan. 1981.
[268] T. S. Han and S. I. Amari. Statistical inference under multiterminal data compression. IEEE Trans. Inf. Theory, pages 2300-2324, Oct. 1998.
[269] T. S. Han and S. Verdu. New results in the theory of identification via channels. IEEE Trans. Inf. Theory, pages 14-25, Jan. 1992.
[270] T. S. Han. Nonnegative entropy measures of multivariate symmetric correlations. Inf. Control, 36(2):133-156, 1978.
[271] T. S. Han. The capacity region of a general multiple access channel with certain correlated sources. Inf. Control, 40:37-60, 1979.
[272] T. S. Han. Information-Spectrum Methods in Information Theory. SpringerVerlag, New York, 2002.
[273] T. S. Han and M. H. M. Costa. Broadcast channels with arbitrarily correlated sources. IEEE Trans. Inf. Theory, IT-33:641-650, 1987.
[274] T. S. Han and K. Kobayashi. A new achievable rate region for the interference channel. IEEE Trans. Inf. Theory, IT-27:49-60, 1981.
[275] R. V. Hartley. Transmission of information. Bell Syst. Tech. J., 7:535, 1928.
[276] C. W. Helstrom. Elements of Signal Detection and Estimation. Prentice-Hall, Englewood Cliffs, NJ, 1995.
[277] Y. Hershkovits and J. Ziv. On sliding-window universal data compression with limited memory. IEEE Trans. Inf. Theory, pages 66-78, Jan. 1998.
<!-- Page 730 -->
[278] P. A. Hocquenghem. Codes correcteurs d'erreurs. Chiffres, 2:147-156, 1959.
[279] J. L. Holsinger. Digital Communication over Fixed Time-Continuous Channels with Memory, with Special Application to Telephone Channels (Technical Report). MIT, Cambridge, MA, 1964.
[280] M. L. Honig, U. Madhow, and S. Verdu. Blind adaptive multiuser detection. IEEE Trans. Inf. Theory, pages 944-960, July 1995.
[281] J. E. Hopcroft and J. D. Ullman. Introduction to Automata Theory, Formal Languages and Computation. Addison-Wesley, Reading, MA, 1979.
[282] Y. Horibe. An improved bound for weight-balanced tree. Inf. Control, 34:148-151, 1977.
[283] D. A. Huffman. A method for the construction of minimum redundancy codes. Proc. IRE, 40:1098-1101, 1952.
[284] J. Y. Hui. Switching an Traffic Theory for Integrated Broadband Networks. Kluwer, Boston, 1990.
[285] J. Y. N. Hui and P. A. Humblet. The capacity region of the totally asynchronous multiple-access channel. IEEE Trans. Inf. Theory, pages 207-216, Mar. 1985.
[286] S. Ihara. On the capacity of channels with additive non-Gaussian noise. Inf. Contr., pages 34-39, 1978.
[287] S. Ihara. Information Theory for Continuous Systems. World Scientific, Singapore, 1993.
[288] K. A. Schouhamer Immink, Paul H. Siegel, and Jack K. Wolf. Codes for digital recorders. IEEE Trans. Inf. Theory, pages 2260-2299, Oct. 1998.
[289] N. S. Jayant (Ed.). Waveform Quantization and Coding. IEEE Press, New York, 1976.
[290] N. S. Jayant and P. Noll. Digital Coding of Waveforms. Prentice-Hall, Englewood Cliffs, NJ, 1984.
[291] E. T. Jaynes. Information Theory and statistical mechanics. Phys. Rev., 106:620, 1957.
[292] E. T. Jaynes. Information Theory and statistical mechanics II. Phys. Rev., 108:171, 1957.
[293] E. T. Jaynes. On the rationale of maximum entropy methods. Proc. IEEE, 70:939-952, 1982.
[294] E. T. Jaynes. Papers on Probability, Statistics and Statistical Physics. Reidel, Dordrecht, The Netherlands, 1982.
[295] F. Jelinek. Buffer overflow in variable length encoding of fixed rate sources. IEEE Trans. Inf. Theory, IT-14:490-501, 1968.
[296] F. Jelinek. Evaluation of expurgated error bounds. IEEE Trans. Inf. Theory, IT-14:501-505, 1968.
<!-- Page 731 -->
[297] F. Jelinek. Probabilistic Information Theory. McGraw-Hill, New York, 1968.
[298] F. Jelinek. Statistical Methods for Speech Recognition. MIT Press, Cambridge, MA, 1998.
[299] R Jozsa and B. Schumacher. A new proof of the quantum noiseless coding theorem. J Mod. Opt., pages 2343-2350, 1994.
[300] G. G. Langdon, Jr. A note on the Ziv-Lempel model for compressing individual sequences. IEEE Trans. Inf. Theory, pages 284-287, Mar. 1983.
[301] J. Justesen. A class of constructive asymptotically good algebraic codes. IEEE Trans. Inf. Theory, IT-18:652-656, 1972.
[302] M. Kac. On the notion of recurrence in discrete stochastic processes. Bull. Am. Math. Soc., pages 1002-1010, Oct. 1947.
[303] T. Kailath and J. P. M. Schwalkwijk. A coding scheme for additive noise channels with feedback. Part I: No bandwidth constraints. IEEE Trans. Inf. Theory, IT-12:172-182, 1966.
[304] T. Kailath and H. V. Poor. Detection of stochastic processes. IEEE Trans. Inf. Theory, pages 2230-2259, Oct. 1998.
[305] S. Karlin. Mathematical Methods and Theory in Games, Programming and Economics, Vol. 2. Addison-Wesley, Reading, MA, 1959.
[306] J. Karush. A simple proof of an inequality of McMillan. IRE Trans. Inf. Theory, IT-7:118, 1961.
[307] F. P. Kelly. Notes on effective bandwidth. Stochastic Networks Theory and Applications, pages 141-168, 1996.
[308] J. Kelly. A new interpretation of information rate. Bell Syst. Tech. J, 35:917-926, July 1956.
[309] J. H. B. Kemperman. On the Optimum Rate of Transmitting Information (Lecture Notes in Mathematics), pages 126-169. Springer Verlag, New York, 1967.
[310] M. Kendall and A. Stuart. The Advanced Theory of Statistics. Macmillan, New York, 1977.
[311] A. Y. Khinchin. Mathematical Foundations of Information Theory. Dover, New York, 1957.
[312] J. C. Kieffer. A simple proof of the Moy-Perez generalization of the Shan-non-McMillan theorem. Pacific J. Math., 51:203-206, 1974.
[313] J. C. Kieffer. A survey of the theory of source coding with a fidelity criterion. IEEE Trans. Inf. Theory, pages 1473-1490, Sept. 1993.
[314] Y. H. Kim. Feedback capacity of first-order moving average Gaussian channel. Proc. IEEE Int. Symp. Information Theory, Adelaide, pages 416-420, Sept. 2005.
[315] D. E. Knuth. Dynamic Huffman coding. J. Algorithms, pages 163-180, 1985.
[316] D. E. Knuth. Art of Computer Programming.
<!-- Page 732 -->
[317] D. E. Knuth와 A. C. Yao. 난수 생성의 복잡성. J. F. Traub (편집), 알고리즘 및 복잡성: 최근 결과 및 새로운 방향 (카네기 멜론 대학교 알고리즘 및 복잡성 최신 방향 및 최근 결과 심포지엄 발표집, 1976), 357-428쪽. Academic Press, New York, 1976.
[318] A. N. Kolmogorov. 측지 역학 시스템 및 Lebesgue 공간의 자기 동형의 새로운 메트릭 불변량. Dokl. Akad. Nauk SSSR, 861-864쪽, 1958.
[319] A. N. Kolmogorov. 연속 신호의 경우 정보 전송의 Shannon 이론에 관하여. IRE Trans. Inf. Theory, IT-2:102-108, 1956년 9월.
[320] A. N. Kolmogorov. 측지 역학 시스템의 새로운 불변량. Dokl. Acad. Nauks SSR, 119:861-864, 1958.
[321] A. N. Kolmogorov. 정보의 정량적 정의에 대한 세 가지 접근 방식. Probl. Inf. Transm. (USSR), 1:4-7, 1965.
[322] A. N. Kolmogorov. 정보 이론 및 확률 이론의 논리적 기초. IEEE Trans. Inf. Theory, IT-14:662-664, 1968.
[323] A. N. Kolmogorov. 정보 전송 이론. A. N. Kolmogorov 선집, 제3권: 정보 이론 및 알고리즘 이론, 산업 자동화의 과학적 문제에 대한 세션, 제1권, 전체 발표, Izd. Akad. Nauk SSSR, Moscow, 1957, 66-99쪽. Kluwer, Dordrecht, The Netherlands, 1993.
[324] J. Körner와 K. Marton. 두 개의 노이즈 채널의 비교. I. Csiszár 및 P. Elias (편집), 정보 이론의 주제 (Coll. Math. Soc. J. Bolyai, No. 16), 411-423쪽. North-Holland, Amsterdam, 1977.
[325] J. Körner와 K. Marton. 저하된 메시지 세트를 가진 일반 브로드캐스트 채널. IEEE Trans. Inf. Theory, IT-23:60-64, 1977.
[326] J. Körner와 K. Marton. 두 개의 이진 소스의 modulo 2 합을 인코딩하는 방법. IEEE Trans. Inf. Theory, IT-25:219-221, 1979.
[327] J. Körner와 A. Orlitsky. 제로 오류 정보 이론. IEEE Trans. Inf. Theory, IT-44:2207-2229, 1998년 10월.
[328] V. A. Kotel'nikov. 전기 통신에서 "에테르" 및 와이어의 전송 용량에 관하여. Izd. Red. Upr. Svyazi RKKA, 44, 1933.
[329] V. A. Kotel'nikov. 최적 잡음 내성 이론. McGraw-Hill, New York, 1959.
[330] L. G. Kraft. 진폭 변조 펄스를 양자화, 그룹화 및 코딩하는 장치. 석사 학위 논문, 전기 공학부, MIT, Cambridge, MA, 1949.
[331] R. E. Krichevsky. 라플라스의 계승 법칙 및 범용 인코딩. IEEE Trans. Inf. Theory, 296-303쪽, 1998년 1월.
[332] R. E. Krichevsky. 범용 압축 및 검색. Kluwer, Dordrecht, The Netherlands, 1994.
<!-- Page 733 -->
[333] R. E. Krichevsky 및 V. K. Trofimov. 범용 인코딩의 성능. IEEE Trans. Inf. Theory, 199-207쪽, 1981년 3월.
[334] S. R. Kulkarni, G. Lugosi, 및 S. S. Venkatesh. 패턴 분류 학습: 설문 조사. IEEE Trans. Inf. Theory, 2178-2206쪽, 1998년 10월.
[335] S. Kullback. 정보 이론 및 통계. WILEY, 뉴욕, 1959년.
[336] S. Kullback. 변화량에 따른 판별의 하한. IEEE Trans. Inf. Theory, IT-13:126-127, 1967년.
[337] S. Kullback, J. C. Keegel, 및 J. H. Kullback. 통계 정보 이론의 주제. Springer-Verlag, 베를린, 1987년.
[338] S. Kullback 및 M. A. Khairat. 최소 판별 정보에 대한 참고. Ann. Math. Stat., 279-280쪽, 1966년.
[339] S. Kullback 및 R. A. Leibler. 정보와 충분성에 관하여. Ann. Math. Stat., 22:79-86, 1951년.
[340] H. J. Landau 및 H. O. Pollak. 긴 타원형 구면파 함수, 푸리에 분석 및 불확실성: 파트 II. Bell Syst. Tech. J., 40:65-84, 1961년.
[341] H. J. Landau 및 H. O. Pollak. 긴 타원형 구면파 함수, 푸리에 분석 및 불확실성: 파트 III. Bell Syst. Tech. J., 41:1295-1336, 1962년.
[342] G. G. Langdon. 산술 코딩 소개. IBM J. Res. Dev., 28:135-149, 1984년.
[343] G. G. Langdon 및 J. J. Rissanen. 간단한 일반 이진 소스 코드. IEEE Trans. Inf. Theory, IT-28:800, 1982년.
[344] A. Lapidoth 및 P. Narayan. 채널 불확실성 하에서의 신뢰할 수 있는 통신. IEEE Trans. Inf. Theory, 2148-2177쪽, 1998년 10월.
[345] A. Lapidoth 및 J. Ziv. LZ 기반 디코딩 algorithm의 보편성에 관하여. IEEE Trans. Inf. Theory, 1746-1755쪽, 1998년 9월.
[346] H. A. Latané. 위험한 벤처 간의 선택 기준. J. Polit. Econ., 38:145-155, 1959년 4월.
[347] H. A. Latané 및 D.L. Tuttle. 포트폴리오 구축 기준. J. Finance, 22:359-373, 1967년 9월.
[348] E. A. Lee 및 D. G. Messerschmitt. 디지털 통신, 제2판. Kluwer, 보스턴, 1994년.
[349] J. Leech 및 N. J. A. Sloane. 구체 포장 및 오류 수정 코드. Can. J. Math, 718-745쪽, 1971년.
[350] E. L. Lehmann 및 H. Scheffé. 완전성, 유사 영역 및 편향되지 않은 추정. Sankhya, 10:305-340, 1950년.
[351] A. Lempel 및 J. Ziv. 유한 시퀀스의 복잡성에 관하여. IEEE Trans. Inf. Theory, 75-81쪽, 1976년 1월.
[352] L. A. Levin. 무작위 시퀀스 개념에 관하여. Sov. Math. Dokl., 14:1413-1416, 1973년.
[353] L. A. Levin 및 A. K. Zvonkin. 유한 객체의 복잡성과 정보 및 무작위성 개념의 알고리즘 이론을 통한 발전. Russ. Math. Surv., 25/6:83-124, 1970년.
<!-- Page 734 -->
[354] M. Li와 P. Vitanyi. An Introduction to Kolmogorov Complexity and Its Applications, 2판. Springer-Verlag, New York, 1997.
[355] H. Liao. Multiple access channels. 박사 학위 논문, Department of Electrical Engineering, University of Hawaii, Honolulu, 1972.
[356] S. Lin과 D. J. Costello, Jr. Error Control Coding: Fundamentals and Applications. Prentice-Hall, Englewood Cliffs, NJ, 1983.
[357] D. Lind와 B. Marcus. Symbolic Dynamics and Coding. Cambridge University Press, Cambridge, 1995.
[358] Y. Linde, A. Buzo, R. M. Gray. An algorithm for vector quantizer design. IEEE Trans. Commun., COM-28:84-95, 1980.
[359] T. Linder, G. Lugosi, K. Zeger. Rates of convergence in the source coding theorem in empirical quantizer design. IEEE Trans. Inf. Theory, pages 1728-1740, Nov. 1994.
[360] T. Linder, G. Lugosi, K. Zeger. Fixed-rate universal lossy source coding and rates of convergence for memoryless sources. IEEE Trans. Inf. Theory, pages 665-676, May 1995.
[361] D. Lindley. Boltzmann's Atom: The Great Debate That Launched A Revolution in Physics. Free Press, New York, 2001.
[362] A. Liversidge. Profile of Claude Shannon. N. J. A. Sloane 및 A. D. Wyner (Eds.)의 Claude Elwood Shannon Collected Papers. IEEE Press, Piscataway, NJ, 1993 (Omni magazine, Aug. 1987.)
[363] S. P. Lloyd. Least Squares Quantization in PCM (Technical Report). Bell Lab. Tech. Note, 1957.
[364] G. Louchard 및 Wojciech Szpankowski. On the average redundancy rate of the Lempel-Ziv code. IEEE Trans. Inf. Theory, pages 2-8, Jan. 1997.
[365] L. Lovasz. On the Shannon capacity of a graph. IEEE Trans. Inf. Theory, IT-25:1-7, 1979.
[366] R. W. Lucky. Silicon Dreams: Information, Man and Machine. St. Martin's Press, New York, 1989.
[367] D. J. C. Mackay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, Cambridge, 2003.
[368] D. J. C. MacKay 및 R. M. Neal. Near Shannon limit performance of low-density parity-check codes. Electron. Lett., pages 1645-1646, Mar. 1997.
[369] F. J. MacWilliams 및 N. J. A. Sloane. The Theory of Error-Correcting Codes. North-Holland, Amsterdam, 1977.
[370] B. Marcus. Sofic systems and encoding data. IEEE Trans. Inf. Theory, IT-31(3):366-377, May 1985.
[371] R. J. Marks. Introduction to Shannon Sampling and Interpolation Theory. Springer-Verlag New York, 1991.
<!-- Page 735 -->
[372] A. Marshall 및 I. Olkin. Inequalities: Theory of Majorization and Its Applications. Academic Press, New York, 1979.
[373] A. Marshall 및 I. Olkin. A convexity proof of Hadamard's inequality. Am. Math. Monthly, 89(9):687-688, 1982.
[374] P. Martin-Löf. The definition of random sequences. Inf. Control, 9:602-619, 1966.
[375] K. Marton. Information and information stability of ergodic sources. Probl. Inf. Transm. (VSSR), pages 179-183, 1972.
[376] K. Marton. Error exponent for source coding with a fidelity criterion. IEEE Trans. Inf. Theory, IT-20:197-199, 1974.
[377] K. Marton. A coding theorem for the discrete memoryless broadcast channel. IEEE Trans. Inf. Theory, IT-25:306-311, 1979.
[378] J. L. Massey 및 P. Mathys. The collision channel without feedback. IEEE Trans. Inf. Theory, pages 192-204, Mar. 1985.
[379] R. A. McDonald. Information rates of Gaussian signals under criteria constraining the error spectrum. D. Eng. dissertation, Yale University School of Electrical Engineering, New Haven, CT, 1961.
[380] R. A. McDonald 및 P. M. Schultheiss. Information rates of Gaussian signals under criteria constraining the error spectrum. Proc. IEEE, pages $415-416,1964$.
[381] R. A. McDonald 및 P. M. Schultheiss. Information rates of Gaussian signals under criteria constraining the error spectrum. Proc. IEEE, 52:415-416, 1964.
[382] R. J. McEliece, D. J. C. MacKay, 및 J. F. Cheng. Turbo decoding as an instance of Pearl's belief propagation algorithm. IEEE J. Sel. Areas Commun., pages 140-152, Feb. 1998.
[383] R. J. McEliece. The Theory of Information and Coding. Addison-Wesley, Reading, MA, 1977.
[384] B. McMillan. The basic theorems of information theory. Ann. Math. Stat., 24:196-219, 1953.
[385] B. McMillan. Two inequalities implied by unique decipherability. IEEE Trans. Inf. Theory, IT-2:115-116, 1956.
[386] N. Merhav 및 M. Feder. Universal schemes for sequential decision from individual data sequences. IEEE Trans. Inf. Theory, pages 1280-1292, July 1993.
[387] N. Merhav 및 M. Feder. A strong version of the redundancy-capacity theorem of universal coding. IEEE Trans. Inf. Theory, pages 714-722, May 1995.
[388] N. Merhav 및 M. Feder. Universal prediction. IEEE Trans. Inf. Theory, pages 2124-2147, Oct. 1998.
[389] R. C. Merton 및 P. A. Samuelson. Fallacy of the log-normal approximation to optimal portfolio decision-making over many periods. J. Finan. Econ., $1: 67-94,1974$.
<!-- Page 736 -->
[390] H. Minkowski. Diskontinuitätsbereich für arithmetische Äquivalenz. J. Math., 129:220-274, 1950.
[391] L. Mirsky. On a generalization of Hadamard's determinantal inequality due to Szasz. Arch. Math., VIII:274-275, 1957.
[392] S. C. Moy. Generalizations of the Shannon-McMillan theorem. Pacific J. Math., pages 705-714, 1961.
[393] J. von Neumann and O. Morgenstern. Theory of Games and Economic Behaviour. Princeton University Press, Princeton, NJ, 1980.
[394] J. Neyman and E. S. Pearson. On the problem of the most efficient tests of statistical hypotheses. Philos. Trans. Roy. Soc. London A, 231:289-337, 1933.
[395] M. Nielsen and I. Chuang. Quantum Computation and Quantum Information. Cambridge University Press, Cambridge, 2000.
[396] H. Nyquist. Certain factors affecting telegraph speed. Bell Syst. Tech. J., 3:324, 1924.
[397] H. Nyquist. Certain topics in telegraph transmission theory. AIEE Trans., pages 617-644, Apr. 1928.
[398] J. Omura. A coding theorem for discrete time sources. IEEE Trans. Inf. Theory, IT-19:490-498, 1973.
[399] A. Oppenheim. Inequalities connected with definite Hermitian forms. J. London Math. Soc., 5:114-119, 1930.
[400] E. Ordentlich. On the factor-of-two bound for Gaussian multiple-access channels with feedback. IEEE Trans. Inf. Theory, pages 2231-2235, Nov. 1996.
[401] E. Ordentlich and T. Cover. The cost of achieving the best portfolio in hindsight. Math. Operations Res., 23(4): 960-982, Nov. 1998.
[402] S. Orey. On the Shannon-Perez-Moy theorem. Contemp. Math., 41:319-327, 1985.
[403] A. Orlitsky. Worst-case interactive communication. I: Two messages are almost optimal. IEEE Trans. Inf. Theory, pages 1111-1126, Sept. 1990.
[404] A. Orlitsky. Worst-case interactive communication. II: Two messages are not optimal. IEEE Trans. Inf. Theory, pages 995-1005, July 1991.
[405] A. Orlitsky. Average-case interactive communication. IEEE Trans. Inf. Theory, pages 1534-1547, Sept. 1992.
[406] A. Orlitsky and A. El Gamal. Average and randomized communication complexity. IEEE Trans. Inf. Theory, pages 3-16, Jan. 1990.
[407] D. S. Ornstein. Bernoulli shifts with the same entropy are isomorphic. $A d v$. Math., pages 337-352, 1970.
[408] D. S. Ornstein and B. Weiss. Entropy and data compression schemes. IEEE Trans. Inf. Theory, pages 78-83, Jan. 1993.
[409] D. S. Ornstein. Bernoulli shifts with the same entropy are isomorphic. $A d v$. Math., 4:337-352, 1970.
<!-- Page 737 -->
[410] L. H. Ozarow. 피드백이 있는 화이트 가우시안 다중 접속 채널의 용량. IEEE Trans. Inf. Theory, IT-30:623-629, 1984.
[411] L. H. Ozarow and C. S. K. Leung. 피드백이 있는 가우시안 방송 채널에 대한 달성 가능한 영역 및 외부 경계. IEEE Trans. Inf. Theory, IT-30:667-671, 1984.
[412] H. Pagels. The Dreams of Reason: the Computer and the Rise of the Sciences of Complexity. Simon and Schuster, New York, 1988.
[413] C. Papadimitriou. 정보 이론과 계산 복잡성: 확장되는 인터페이스. IEEE Inf. Theory Newslett. (Special Golden Jubilee Issue), pages 12-13, June 1998.
[414] R. Pasco. 빠른 데이터 압축을 위한 소스 코딩 알고리즘. 박사 학위 논문, 스탠포드 대학교, 스탠포드, CA, 1976.
[415] A. J. Paulraj and C. B. Papadias. 무선 통신을 위한 시공간 처리. IEEE Signal Processing Mag., pages 49-83, Nov. 1997.
[416] W. B. Pennebaker and J. L. Mitchell. JPEG Still Image Data Compression Standard. Van Nostrand Reinhold, New York, 1988.
[417] A. Perez. 더 일반적인 확률 과정에 대한 Shannon-McMillan의 극한 정리 확장. In Trans. Third Prague Conference on Information Theory, Statistical Decision Functions and Random Processes, pages 545-574, Czechoslovak Academy of Sciences, Prague, 1964.
[418] J. R. Pierce. 정보 이론의 초기 시절. IEEE Trans. Inf. Theory, pages 3-8, Jan. 1973.
[419] J. R. Pierce. An Introduction to Information Theory: Symbols, Signals and Noise, 2nd ed. Dover Publications, New York, 1980.
[420] J. T. Pinkston. 코딩 정리에 대한 역설에 대한 속도-왜곡 이론의 적용. IEEE Trans. Inf. Theory, IT-15:66-71, 1969.
[421] M. S. Pinsker. 소련 정보 이론 회의에서의 발표, 1969. 초록은 출판되지 않았습니다.
[422] M. S. Pinsker. Information and Information Stability of Random Variables and Processes. Holden-Day, San Francisco, CA, 1964. (원래 1960년 러시아어로 출판됨.)
[423] M. S. Pinsker. 무음 방송 채널의 용량 영역. Probl. Inf. Transm. (USSR), 14(2):97-102, 1978.
[424] M. S. Pinsker and R. L. Dobrushin. 메모리는 용량을 증가시킵니다. Probl. Inf. Transm. (USSR), pages 94-95, Jan. 1969.
[425] M. S. Pinsker. Information and Stability of Random Variables and Processes. Izd. Akad. Nauk, 1960. A. Feinstein이 번역, 1964.
[426] E. Plotnik, M. Weinberger, and J. Ziv. 유한 상태 소스에서 방출되는 시퀀스의 확률에 대한 상한 및 Lempel-Ziv 알고리즘의 중복도. IEEE Trans. Inf. Theory, IT-38(1):66-72, Jan. 1992.
[427] D. Pollard. Convergence of Stochastic Processes. Springer-Verlag, New York, 1984.
<!-- Page 738 -->
[428] G. S. Poltyrev. 병렬 브로드캐스트 채널의 전송 용량 및 열화된 구성 요소. Probl. Peredachi Inf., 페이지 23-35, 1977년 4월-6월.
[429] S. Pombra 및 T. M. Cover. 피드백이 있는 비백색 가우시안 다중 접속 채널. IEEE Trans. Inf. Theory, 페이지 885-892, 1994년 5월.
[430] H. V. Poor. 신호 탐지 및 추정 입문, 제2판. Springer-Verlag, 뉴욕, 1994.
[431] F. Pratt. 비밀과 긴급. Blue Ribbon Books, Garden City, NY, 1939.
[432] L. R. Rabiner. 은닉 마르코프 모델 및 음성 인식에서의 선택적 응용에 대한 튜토리얼. Proc. IEEE, 페이지 257-286, 1989년 2월.
[433] L. R. Rabiner 및 R. W. Schafer. 음성 신호 디지털 처리. Prentice-Hall, Englewood Cliffs, NJ, 1978.
[434] R. Ahlswede 및 Z. Zhang. 채널을 통한 식별 이론의 새로운 방향. IEEE Trans. Inf. Theory, 41:1040-1050, 1995.
[435] C. R. Rao. 통계 매개변수 추정에서 얻을 수 있는 정보 및 정확도. Bull. Calcutta Math. Soc., 37:81-91, 1945.
[436] I. S. Reed. 1982년 Claude Shannon 강의: 코딩 및 관련 주제에 대한 변환 적용. IEEE Inf. Theory Newslett., 페이지 4-7, 1982년 12월.
[437] F. M. Reza. 정보 이론 입문. McGraw-Hill, 뉴욕, 1961.
[438] S. O. Rice. 랜덤 노이즈의 수학적 분석. Bell Syst. Tech. J., 페이지 282-332, 1945년 1월.
[439] S. O. Rice. 노이즈 존재 하에서의 통신: 두 가지 인코딩 방식의 오류 확률. Bell Syst. Tech. J., 29:60-93, 1950.
[440] B. E. Rimoldi 및 R. Urbanke. 가우시안 다중 접속 채널에 대한 속도 분할 접근 방식. IEEE Trans. Inf. Theory, 페이지 364-375, 1996년 3월.
[441] J. Rissanen. 일반화된 Kraft 부등식 및 산술 코딩. IBM J. Res. Dev., 20:198, 1976.
[442] J. Rissanen. 최단 데이터 설명에 의한 모델링. Automatica, 14:465-471, 1978.
[443] J. Rissanen. 정수에 대한 보편적 사전 및 최소 설명 길이 추정. Ann. Stat., 11:416-431, 1983.
[444] J. Rissanen. 보편적 코딩, 정보, 예측 및 추정. IEEE Trans. Inf. Theory, IT-30:629-636, 1984.
[445] J. Rissanen. 확률적 복잡성 및 모델링. Ann. Stat., 14:1080-1100, 1986.
[446] J. Rissanen. 확률적 복잡성 (토론 포함). J. Roy. Stat. Soc., 49:223-239, 252-265, 1987.
[447] J. Rissanen. 통계적 조사에서의 확률적 복잡성. World Scientific, 싱가포르, 1989.
[448] J. J. Rissanen. 마르코프 소스 클래스에서의 문자열 복잡성. IEEE Trans. Inf. Theory, 페이지 526-532, 1986년 7월.
<!-- Page 739 -->
[449] J. J. Rissanen 및 G. G. Langdon, Jr. Universal modeling and coding. IEEE Trans. Inf. Theory, 페이지 12-23, 1981년 1월.
[450] B. Y. Ryabko. 알 수 없지만 순서가 있는 확률을 가진 소스 인코딩. Probl. Inf. Transm., 페이지 134-139, 1979년 10월.
[451] B. Y. Ryabko. 빠른 온라인 적응형 코드. IEEE Trans. Inf. Theory, 페이지 1400-1404, 1992년 7월.
[452] P. A. Samuelson. 동적 확률 계획법을 이용한 평생 포트폴리오 선택. Rev. Econ. Stat., 페이지 236-239, 1969년.
[453] P. A. Samuelson. 투자 또는 도박의 긴 시퀀스에서 기하 평균을 최대화하는 것의 "오류". Proc. Natl. Acad. Sci. USA, 68:214-224, 1971년 10월.
[454] P. A. Samuelson. 행동할 시간이 길더라도 부의 평균 로그를 크게 만들지 않는 이유. J. Banking and Finance, 3:305-307, 1979년.
[455] I. N. Sanov. 확률 변수의 큰 편차 확률에 대하여. Mat. Sbornik, 42:11-44, 1957년. Sel. Transl. Math. Stat. Prob., Vol. 1, 페이지 213-244, 1961년에 영어 번역됨.
[456] A. A. Sardinas 및 G.W. Patterson. 코딩된 메시지의 고유 분해에 대한 필요충분 조건. IRE Conv. Rec., Pt. 8, 페이지 $104-108, 1953년.
[457] H. Sato. 강한 간섭을 위한 이산 2 사용자 채널의 capacity region에 대하여. IEEE Trans. Inf. Theory, IT-24:377-379, 1978년.
[458] H. Sato. 강한 간섭 하의 가우시안 간섭 채널의 capacity. IEEE Trans. Inf. Theory, IT-27:786-788, 1981년.
[459] H. Sato 및 M. Tanabe. 강한 간섭을 가진 이산 2 사용자 채널. Trans. IECE Jap., 61:880-884, 1978년.
[460] S. A. Savari. Lempel-Ziv 증분 파싱 규칙의 중복성. IEEE Trans. Inf. Theory, 페이지 9-21, 1997년 1월.
[461] S. A. Savari 및 R. G. Gallager. 메모리가 있는 소스를 위한 일반화된 Tunstall 코드. IEEE Trans. Inf. Theory, 페이지 658-668, 1997년 3월.
[462] K. Sayood. 데이터 압축 소개. Morgan Kaufmann, 샌프란시스코, CA, 1996년.
[463] J. P. M. Schalkwijk. 잡음 채널에 대한 코딩 방식 (피드백 포함). II: 대역폭 제한 신호. IEEE Trans. Inf. Theory, 페이지 183-189, 1966년 4월.
[464] J. P. M. Schalkwijk. Shannon의 내부 경계를 넘어서 작동하는 코딩 방식인 이진 곱셈 채널. IEEE Trans. Inf. Theory, IT-28:107-110, 1982년.
[465] J. P. M. Schalkwijk. 이진 곱셈 채널에 대한 달성 가능한 속도 영역의 확장. IEEE Trans. Inf. Theory, IT-29:445-448, 1983년.
[466] C. P. Schnorr. 난수열 정의에 대한 통합적 접근. Math. Syst. Theory, 5:246-258, 1971년.
<!-- Page 740 -->
[467] C. P. Schnorr. Process, complexity and effective random tests. J. Comput. Syst. Sci., 7:376-388, 1973.
[468] C. P. Schnorr. A surview on the theory of random sequences. In R. Butts and J. Hinitikka (Eds.), Logic, Methodology and Philosophy of Science. Reidel, Dordrecht, The Netherlands, 1977.
[469] G. Schwarz. Estimating the dimension of a model. Ann. Stat., 6:461-464, 1978.
[470] S. Shamai and S. Verdu. The empirical distribution of good codes. IEEE Trans. Inf. Theory, pages 836-846, May 1997.
[471] C. E. Shannon. A Mathematical Theory of Cryptography (Tech. Rept. MM 45-110-02). Bell Lab. Tech. Memo., Sept. 1, 1945.
[472] C. E. Shannon. A mathematical theory of communication. Bell Syst. Tech. J., 27:379-423,623-656, 1948.
[473] C. E. Shannon. Some geometrical results in channel capacity. Verh. Dtsch. Elektrotechnik. Fachber., pages 13-15, 1956.
[474] C. E. Shannon. The zero-error capacity of a noisy channel. IRE Trans. Inf. Theory, IT-2:8-19, 1956.
[475] C. E. Shannon. Channels with side information at the transmitter. IBM J. Res. Dev., pages 289-293, 1958.
[476] C. E. Shannon. Probability of error for optimal codes in a Gaussian channel. Bell Syst. Tech. J., pages 611-656, May 1959.
[477] C. E. Shannon. Two-way communication channels. Proc. 4th. Berkeley Symp. Mathematical Statistics and Probability (June 20-July 30, 1960), pages 611-644, 1961.
[478] C. E. Shannon. The wonderful world of feedback. IEEE Int. Symp. Infor. Theory, ser. First Shannon Lecture, Ashkelon, Israel, 1973.
[479] C. E. Shannon. The mind reading machine. In Shannon's Collected Papers, pages 688-689, 1993.
[480] C. E. Shannon. Communication in the presence of noise. Proc. IRE, 37:10-21, January 1949.
[481] C. E. Shannon. Communication theory of secrecy systems. Bell Syst. Tech. J., 28:656-715, 1949.
[482] C. E. Shannon. Prediction and entropy of printed English. Bell Syst. Tech. J., 30:50-64, January 1951.
[483] C. E. Shannon. Certain results in coding theory for noisy channels. Infor. Control, 1:6-25, 1957.
[484] C. E. Shannon. Channels with side information at the transmitter. IBM J. Res. Dev., 2:289-293, 1958.
[485] C. E. Shannon. Coding theorems for a discrete source with a fidelity criterion. IRE Nat. Conv. Rec., Pt. 4, pages 142-163, 1959.
<!-- Page 741 -->
[486] C. E. Shannon. Two-way communication channels. In Proc. 4th Berkeley Symp. Math. Stat. Prob., Vol. 1, pages 611-644. University of California Press, Berkeley, CA, 1961.
[487] C. E. Shannon, R. G. Gallager, and E. R. Berlekamp. Lower bounds to error probability for coding in discrete memoryless channels. I. Inf. Control, 10:65-103, 1967.
[488] C. E. Shannon, R. G. Gallager, and E. R. Berlekamp. Lower bounds to error probability for coding in discrete memoryless channels. II. Inf. Control, 10:522-552, 1967.
[489] C. E. Shannon and W. W. Weaver. The Mathematical Theory of Communication. University of Illinois Press, Urbana, IL, 1949.
[490] C. E. Shannon. General treatment of the problem of coding. IEEE Trans. Inf. Theory, pages 102-104, February 1953.
[491] W. F. Sharpe. Investments, 3rd ed. Prentice-Hall, Englewood Cliffs, NJ, 1985.
[492] P. C. Shields. Universal redundancy rates do not exist. IEEE Trans. Inf. Theory, pages 520-524, Mar. 1993.
[493] P. C. Shields. The interactions between ergodic theory and information theory. IEEE Trans. Inf. Theory, pages 2079-2093, Oct. 1998.
[494] P. C. Shields and B. Weiss. Universal redundancy rates for the class of B-processes do not exist. IEEE Trans. Inf. Theory, pages 508-512, Mar. 1995.
[495] J. E. Shore and R. W. Johnson. Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy. IEEE Trans. Inf. Theory, IT-26:26-37, 1980.
[496] Y. M. Shtarkov. Universal sequential coding of single messages. Probl. Inf. Transm. (USSR), 23(3):3-17, July-Sept. 1987.
[497] A. Shwartz and A. Weiss. Large Deviations for Performance Analysis, Queues, Communication and Computing. Chapman \& Hall, London, 1995.
[498] D. Slepian. Key Papers in the Development of Information Theory. IEEE Press, New York, 1974.
[499] D. Slepian. On bandwidth. Proc. IEEE, pages 292-300, Mar. 1976.
[500] D. Slepian and H. O. Pollak. Prolate spheroidal wave functions, Fourier analysis and uncertainty: Part I. Bell Syst. Tech. J., 40:43-64, 1961.
[501] D. Slepian and J. K. Wolf. A coding theorem for multiple access channels with correlated sources. Bell Syst. Tech. J., 52:1037-1076, 1973.
[502] D. Slepian and J. K. Wolf. Noiseless coding of correlated information sources. IEEE Trans. Inf. Theory, IT-19:471-480, 1973.
[503] D. S. Slepian. Information theory in the fifties. IEEE Trans. Inf. Theory, pages 145-148, Mar. 1973.
<!-- Page 742 -->
[504] R. J. Solomonoff. A formal theory of inductive inference. Inf. Control, $7:1-22,224-254,1964$.
[505] A. Stam. Some inequalities satisfied by the quantities of information of Fisher and Shannon. Inf. Control, 2:101-112, June 1959.
[506] A. Steane. Quantum computing. Rept. Progr. Phys., pages 117-173, Feb. 1998.
[507] J. A. Storer and T. G. Szymanski. Data compression via textual substitution. J. ACM, 29(4):928-951, 1982.
[508] W. Szpankowski. Asymptotic properties of data compression and suffix trees. IEEE Trans. Inf. Theory, pages 1647-1659, Sept. 1993.
[509] W. Szpankowski. Average Case Analysis of Algorithms on Sequences. WileyInterscience, New York, 2001.
[510] D. L. Tang and L. R. Bahl. Block codes for a class of constrained noiseless channels. Inf. Control, 17:436-461, 1970.
[511] I. E. Teletar and R. G. Gallager. Combining queueing theory with information theory for multiaccess. IEEE J. Sel. Areas Commun., pages 963-969, Aug. 1995.
[512] E. Teletar. Capacity of multiple antenna Gaussian channels. Eur. Trans. Telecommип., 10(6):585-595, 1999.
[513] J. A. Thomas. Feedback can at most double Gaussian multiple access channel capacity. IEEE Trans. Inf. Theory, pages 711-716, Sept. 1987.
[514] T. J. Tjalkens and F. M. J. Willems. A universal variable-to-fixed length source code based on Lawrence's algorithm. IEEE Trans. Inf. Theory, pages 247-253, Mar. 1992.
[515] T. J. Tjalkens and F. M. J. Willems. Variable- to fixed-length codes for Markov sources. IEEE Trans. Inf. Theory, pages 246-257, Mar. 1987.
[516] S. C. Tornay. Ockham: Studies and Selections (chapter "Commentarium in Sententias," I, 27). Open Court Publishers, La Salle, IL, 1938.
[517] H. L. Van Trees. Detection, Estimation, and Modulation Theory, Part I. Wiley, New York, 1968.
[518] B. S. Tsybakov. Capacity of a discrete-time Gaussian channel with a filter. Probl. Inf. Transm., pages 253-256, July-Sept. 1970.
[519] B. P. Tunstall. Synthesis of noiseless compression codes. Ph.D. dissertation, Georgia Institute of Technology, Atlanta, GA, Sept. 1967.
[520] G. Ungerboeck. Channel coding with multilevel/phase signals. IEEE Trans. Inf. Theory, pages 55-67, January 1982.
[521] G. Ungerboeck. Trellis-coded modulation with redundant signal sets part I: Introduction. IEEE Commun. Mag., pages 5-11, Feb. 1987.
[522] G. Ungerboeck. Trellis-coded modulation with redundant signal sets part II: State of the art. IEEE Commun. Mag., pages 12-21, Feb. 1987.
[523] I. Vajda. Theory of Statistical Inference and Information. Kluwer, Dordrecht, The Netherlands, 1989.
<!-- Page 743 -->
[524] L. G. Valiant. A theory of the learnable. Commun. ACM, pages 1134-1142, 1984.
[525] J. M. Van Campenhout and T. M. Cover. Maximum entropy and conditional probability. IEEE Trans. Inf. Theory, IT-27:483-489, 1981.
[526] E. Van der Meulen. Random coding theorems for the general discrete memoryless broadcast channel. IEEE Trans. Inf. Theory, IT-21:180-190, 1975.
[527] E. C. van der Meulen. Some reflections on the interference channel. In R. E. Blahut, D. J. Costello, U. Maurer, and T. Mittelholzer, (Eds.), Communications and Cryptography: Two Sides of One Tapestry. Kluwer, Boston, 1994.
[528] E. C. Van der Meulen. A survey of multi-way channels in information theory. IEEE Trans. Inf. Theory, IT-23:1-37, 1977.
[529] E. C. Van der Meulen. Recent coding theorems for multi-way channels. Part I: The broadcast channel (1976-1980). In J. K. Skwyrzinsky (Ed.), New Concepts in Multi-user Communication (NATO Advanced Study Institute Series), pages 15-51. Sijthoff & Noordhoff, Amsterdam, 1981.
[530] E. C. Van der Meulen. Recent coding theorems and converses for multiway channels. Part II: The multiple access channel (1976-1985) (Technical Report). Department Wiskunde, Katholieke Universiteit Leuven, 1985.
[531] V. N. Vapnik. Estimation of Dependencies Based on Empirical Data. Springer-Verlag, New York, 1982.
[532] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, New York, 1991.
[533] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies to their probabilities. Theory Prob. Appl., pages 264-280, 1971.
[534] V. N. Vapnik and A. Y. Chervonenkis. Necessary and sufficient conditions for the uniform convergence of means to their expectations. Theory Prob. Appl., pages 532-553, 1981.
[535] S. Verdu. The capacity region of the symbol-asynchronous Gaussian multiple-access channel. IEEE Trans. Inf. Theory, pages 733-751, July 1989.
[536] S. Verdu. Recent Progress in Multiuser Detection (Advances in Communication and Signal Processing), Springer-Verlag, Berlin, 1989. [Reprinted in N. Abramson (Ed.), Multiple Access Communications, IEEE Press, New York, 1993.]
[537] S. Verdu. The exponential distribution in information theory. Probl. Inf. Transm. (USSR), pages 86-95, Jan.-Mar. 1996.
[538] S. Verdu. Fifty years of Shannon theory. IEEE Trans. Inf. Theory, pages 2057-2078, Oct. 1998.
[539] S. Verdu. Multiuser Detection. Cambridge University Press, New York, 1998.
[540] S. Verdu and T. S. Han. A general formula for channel capacity. IEEE Trans. Inf. Theory, pages 1147-1157, July 1994.
<!-- Page 744 -->
[541] S. Verdu와 T. S. Han. 무잡음 소스 코딩에서 점근적 균등 분할 속성의 역할. IEEE Trans. Inf. Theory, 페이지 847-857, 1997년 5월.
[542] S. Verdu와 S. W. McLaughlin (편집자). 정보 이론: 50년의 발견. WILEY-IEEE Press, 뉴욕, 1999년.
[543] S. Verdu와 V. K. W. Wei. 채널을 통한 식별을 위한 최적 상수 가중치 코드의 명시적 구성. IEEE Trans. Inf. Theory, 페이지 30-36, 1993년 1월.
[544] A. C. G. Verdugo Lazo와 P. N. Rathie. 연속 확률 분포의 entropy에 관하여. IEEE Trans. Inf. Theory, IT-24:120-122, 1978년.
[545] M. Vidyasagar. 학습 및 일반화 이론. Springer-Verlag, 뉴욕, 1997년.
[546] K. Visweswariah, S. R. Kulkarni, S. Verdu. 소스 코드를 난수 생성기로서 사용. IEEE Trans. Inf. Theory, 페이지 462-471, 1998년 3월.
[547] A. J. Viterbi와 J. K. Omura. 디지털 통신 및 코딩의 원리. McGraw-Hill, 뉴욕, 1979년.
[548] J. S. Vitter. 동적 Huffman 코딩. ACM Trans. Math. Software, 페이지 158-167, 1989년 6월.
[549] V. V. V'yugin. 주어진 복잡도 경계가 있는 측도에 대한 유한 객체의 무작위성 결함에 관하여. Theory Prob. Appl., 32(3):508-512, 1987년.
[550] A. Wald. 순차 분석. WILEY, 뉴욕, 1947년.
[551] A. Wald. 최대 우도 추정치의 일관성에 대한 참고. Ann. Math. Stat., 페이지 595-601, 1949년.
[552] M. J. Weinberger, N. Merhav, M. Feder. 개별 시퀀스에 대한 최적 순차 확률 할당. IEEE Trans. Inf. Theory, 페이지 384-396, 1994년 3월.
[553] N. Weiner. 사이버네틱스. MIT Press, 케임브리지, MA, 및 WILEY, 뉴욕, 1948년.
[554] T. A. Welch. 고성능 데이터 압축 기법. Computer, 17(1):8-19, 1984년 1월.
[555] N. Wiener. 정상 시계열의 외삽, 보간 및 평활. MIT Press, 케임브리지, MA, 및 WILEY, 뉴욕, 1949년.
[556] H. J. Wilcox와 D. L. Myers. 르베그 적분 및 푸리에 급수에 대한 소개. R.E. Krieger, 헌팅턴, NY, 1978년.
[557] F. M. J. Willems. 일련의 이산 메모리 없는 다중 접속 channel에 대한 피드백 capacity. IEEE Trans. Inf. Theory, IT-28:93-95, 1982년.
[558] F. M. J. Willems와 A. P. Hekstra. 단일 출력 양방향 channel에 대한 의존성 균형 경계. IEEE Trans. Inf. Theory, IT-35:44-53, 1989년.
[559] F. M. J. Willems. 범용 데이터 압축 및 반복 시간. IEEE Trans. Inf. Theory, 페이지 54-58, 1989년 1월.
<!-- Page 745 -->
[560] F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens. The context-tree weighting method: basic properties. IEEE Trans. Inf. Theory, pages 653-664, May 1995.
[561] F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens. Context weighting for general finite-context sources. IEEE Trans. Inf. Theory, pages 1514-1520, Sept. 1996.
[562] H. S. Witsenhausen. The zero-error side information problem and chromatic numbers. IEEE Trans. Inf. Theory, pages 592-593, Sept. 1976.
[563] H. S. Witsenhausen. Some aspects of convexity useful in information theory. IEEE Trans. Inf. Theory, pages 265-271, May 1980.
[564] I. H. Witten, R. M. Neal, and J. G. Cleary. Arithmetic coding for data compression. Commun. ACM, 30(6):520-540, June 1987.
[565] J. Wolfowitz. The coding of messages subject to chance errors. Ill. J. Math., $1: 591-606,1957$.
[566] J. Wolfowitz. Coding Theorems of Information Theory. Springer-Verlag, Berlin, and Prentice-Hall, Englewood Cliffs, NJ, 1978.
[567] P. M. Woodward. Probability and Information Theory with Applications to Radar. McGraw-Hill, New York, 1953.
[568] J. Wozencraft and B. Reiffen. Sequential Decoding. MIT Press, Cambridge, MA, 1961.
[569] J. M. Wozencraft and I. M. Jacobs. Principles of Communication Engineering. Wiley, New York, 1965.
[570] A. Wyner. A theorem on the entropy of certain binary sequences and applications II. IEEE Trans. Inf. Theory, IT-19:772-777, 1973.
[571] A. Wyner. The common information of two dependent random variables. IEEE Trans. Inf. Theory, IT-21:163-179, 1975.
[572] A. Wyner. On source coding with side information at the decoder. IEEE Trans. Inf. Theory, IT-21:294-300, 1975.
[573] A. Wyner and J. Ziv. A theorem on the entropy of certain binary sequences and applications I. IEEE Trans. Inf. Theory, IT-19:769-771, 1973.
[574] A. Wyner and J. Ziv. The rate distortion function for source coding with side information at the receiver. IEEE Trans. Inf. Theory, IT-22:1-11, 1976.
[575] A. Wyner and J. Ziv. On entropy and data compression. IEEE Trans. Inf. Theory, 1991.
[576] A. D. Wyner. Capacity of the the band-limited Gaussian channel. Bell Syst. Tech. J., 45:359-395, Mar. 1966.
[577] A. D. Wyner. Communication of analog data from a Gaussian source over a noisy channel. Bell Syst. Tech. J., pages 801-812, May-June 1968.
[578] A. D. Wyner. Recent results in the Shannon theory. IEEE Trans. Inf. Theory, pages $2-10$, Jan. 1974.
[579] A. D. Wyner. The wiretap channel. Bell Syst. Tech. J., pages 1355-1387, 1975.
<!-- Page 746 -->
[580] A. D. Wyner. 디코더에서 부가 정보가 있는 소스 코딩을 위한 속도-왜곡 함수. II: 일반 소스. Inf. Control, 페이지 60-80, 1978.
[581] A. D. Wyner. 가우시안 셀룰러 다중 접속 채널에 대한 Shannon 이론적 접근. IEEE Trans. Inf. Theory, 페이지 1713-1727, 1994년 11월.
[582] A. D. Wyner 및 A. J. Wyner. Lempel-Ziv algorithm의 한 버전에 대한 향상된 중복도. IEEE Trans. Inf. Theory, 페이지 723-731, 1995년 5월.
[583] A. D. Wyner 및 J. Ziv. 메모리가 있는 정상 소스의 속도-왜곡 함수에 대한 경계. IEEE Trans. Inf. Theory, 페이지 508-513, 1971년 9월.
[584] A. D. Wyner 및 J. Ziv. 디코더에서 부가 정보가 있는 소스 코딩을 위한 속도-왜곡 함수. IEEE Trans. Inf. Theory, 페이지 1-10, 1976년 1월.
[585] A. D. Wyner 및 J. Ziv. 데이터 압축에 대한 응용과 함께 정상 에르고딕 데이터 소스의 엔트로피의 일부 점근적 속성. IEEE Trans. Inf. Theory, 페이지 1250-1258, 1989년 11월.
[586] A. D. Wyner 및 J. Ziv. 유한 메모리를 사용한 분류. IEEE Trans. Inf. Theory, 페이지 337-347, 1996년 3월.
[587] A. D. Wyner, J. Ziv 및 A. J. Wyner. 정보 이론에서 패턴 매칭의 역할에 대하여. IEEE Trans. Inf. Theory, 페이지 2045-2056, 1998년 10월.
[588] A. J. Wyner. 고정 데이터베이스 Lempel-Ziv algorithm의 구문 길이의 중복도 및 분포. IEEE Trans. Inf. Theory, 페이지 1452-1464, 1997년 9월.
[589] A. D. Wyner. 대역폭 제한 가우시안 채널의 용량. Bell Syst. Tech. J., 45:359-371, 1965.
[590] A. D. Wyner 및 N. J. A. Sloane (편집자) Claude E. Shannon: Collected Papers. WILEY, New York, 1993.
[591] A. D. Wyner 및 J. Ziv. 슬라이딩 윈도우 Lempel-Ziv algorithm은 점근적으로 최적입니다. Proc. IEEE, 82(6):872-877, 1994.
[592] E.-H. Yang 및 J. C. Kieffer. 문자열 매칭 기반 데이터 압축 알고리즘의 성능에 대하여. IEEE Trans. Inf. Theory, 페이지 47-65, 1998년 1월.
[593] R. Yeung. 정보 이론 입문. Kluwer Academic, Boston, 2002.
[594] H. P. Yockey. 정보 이론과 분자 생물학. Cambridge University Press, New York, 1992.
[595] Z. Zhang, T. Berger 및 J. P. M. Schalkwijk. 양방향 채널의 용량 영역에 대한 새로운 외부 경계. IEEE Trans. Inf. Theory, 페이지 383-386, 1986년 5월.
[596] Z. Zhang, T. Berger 및 J. P. M. Schalkwijk. 양방향 채널의 용량 영역에 대한 새로운 외부 경계. IEEE Trans. Inf. Theory, IT-32:383-386, 1986.
[597] J. Ziv. 통계가 알려지지 않은 소스의 코딩. II: 충실도 기준에 대한 왜곡. IEEE Trans. Inf. Theory, IT-18:389-394, 1972.
<!-- Page 747 -->
[598] J. Ziv. 알 수 없는 통계적 특성을 가진 소스의 코딩. II: 충실도 기준에 대한 왜곡. IEEE Trans. Inf. Theory, 페이지 389-394, 1972년 5월.
[599] J. Ziv. 개별 시퀀스에 대한 코딩 정리. IEEE Trans. Inf. Theory, 페이지 405-412, 1978년 7월.
[600] J. Ziv. 개별 시퀀스에 대한 왜곡-비율 이론. IEEE Trans. Inf. Theory, 페이지 137-143, 1980년 3월.
[601] J. Ziv. 유한 상태 채널에 대한 범용 디코딩. IEEE Trans. Inf. Theory, 페이지 453-460, 1985년 7월.
[602] J. Ziv. 마르코프 소스의 경우, 가변 대 고정 길이 코드가 고정 대 가변 길이 코드보다 우수합니다. IEEE Trans. Inf. Theory, 페이지 861-863, 1990년 7월.
[603] J. Ziv 및 A. Lempel. 순차 데이터 압축을 위한 범용 알고리즘. IEEE Trans. Inf. Theory, IT-23:337-343, 1977.
[604] J. Ziv 및 A. Lempel. 가변 비율 코딩을 통한 개별 시퀀스의 압축. IEEE Trans. Inf. Theory, IT-24:530-536, 1978.
[605] J. Ziv 및 N. Merhav. 개별 시퀀스 간의 상대적 엔트로피 측정 및 범용 분류에의 적용. IEEE Trans. Inf. Theory, 페이지 1270-1279, 1993년 7월.
[606] W. H. Zurek. 알고리즘적 무작위성과 물리적 엔트로피. Phys. Rev. A, 40:4731-4751, 1989년 10월 15일.
[607] W. H. Zurek. 계산의 열역학적 비용, 알고리즘 복잡성 및 정보 측정. Nature, 341(6238):119-124, 1989년 9월.
[608] W. H. Zurek (편집). 복잡성, 엔트로피 및 정보의 물리학 (1988년 복잡성, 엔트로피 및 정보의 물리학 워크숍 회의록). Addison-Wesley, Reading, MA, 1990.
<!-- Page 748 -->
.
<!-- Page 749 -->
# 기호 목록

| $X, 14$ | $x^{n}, 61$ |
| :--: | :--: |
| $p(x), 14$ | $B_{\delta}^{(n)}, 62$ |
| $p_{X}(x), 14$ | $\doteq, 63$ |
| $\mathcal{X}, 14$ | $\bar{Z}_{n}, 65$ |
| $H(X), 14$ | $H(\mathcal{X}), 71$ |
| $H(p), 14$ | $P_{i j}, 72$ |
| $H_{b}(X), 14$ | $H^{\prime}(\mathcal{X}), 75$ |
| $E_{p} g(X), 14$ | $C(x), 103$ |
| $\operatorname{Eg}(X), 14$ | $l(x), 103$ |
| $\underline{\text { def }}, 15$ | C, 103 |
| $p(x, y), 17$ | $L(C), 104$ |
| $H(X, Y), 17$ | $\mathcal{D}, 104$ |
| $H(Y \mid X), 17$ | $C^{*}, 105$ |
| $D(p \| q), 20$ | $l_{i}, 107$ |
| $I(X ; Y), 21$ | $l_{\text {max }}, 108$ |
| $I(X ; Y \mid Z), 24$ | $L, 110$ |
| $D(p(y \mid x) \| q(y \mid x)), 25$ | $L^{*}, 111$ |
| $|\mathcal{X}|, 30$ | $H_{D}(X), 111$ |
| $X \rightarrow Y \rightarrow Z, 35$ | $\lceil x\rceil, 113$ |
| $\mathcal{N}\left(\mu, \sigma^{2}\right), 37$ | $\frac{F}{F}(x), 127$ |
| $T(X), 38$ | $\bar{F}(x), 128$ |
| $f_{\theta}(x), 38$ | $\operatorname{sgn}(t), 132$ |
| $P_{e}, 39$ | $p_{i}^{(j)}, 138$ |
| $H(\mathbf{p}), 45$ | $p_{i}, 159$ |
| $\{0,1\}^{*}, 55$ | $o_{i}, 159$ |
| $2^{-n(H \pm \epsilon)}, 58$ | $b_{i}, 160$ |
| $A_{\epsilon}^{(n)}, 59$ | $b(i), 160$ |
| $\mathbf{x}, 60$ | $S_{n}, 160$ |
| $\mathcal{X}^{n}, 60$ | $S(X), 160$ |

[^0]Copyright (c) 2006 John Wiley \& Sons, Inc.

[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (c) 2006 John Wiley \& Sons, Inc.
<!-- Page 750 -->
$W(\mathbf{b}, \mathbf{p}), 160$
b, 160
p, 160
$W^{*}(\mathbf{p}), 161$
$\Delta W, 165$
$W^{*}(X \mid Y), 165$
C, 184
$\hat{W}, 193$
$\left(\mathcal{X}^{n}, p\left(y^{n} \mid x^{n}\right), \mathcal{Y}^{n}\right), 193$
$\lambda_{i}, 194$
$\lambda^{(n)}, 194$
$P_{e}^{(n)}, 194$
$R, 195$
$\left(\left\lceil 2^{n R}\right\rceil, n\right), 195$
$\mathcal{C}, 200$
$\mathcal{E}, 202$
$E_{i}, 203$
$\cup, 203$
$\mathcal{C}^{*}, 204$
$C_{\mathrm{F} B}, 216$
$\oplus, 224$
$F(x), 243$
$f(x), 243$
$h(X), 243$
$S, 243$
$h(f), 243$
$\phi(x), 244$
$\operatorname{Vol}(A), 245$
$X^{\Delta}, 247$
$h(X \mid Y), 249$
$\mathcal{N}_{n}(\mu, K), 249$
$|K|, 249$
$D(f \| g), 250$
X, 253
$P, 261$
W, 270
$F(\omega), 271$
$\operatorname{sinc}(t), 271$
$N_{0}, 272$
$x^{+}, 276$
$K_{X}, 278$
$K_{Z}, 278$
$\operatorname{tr}\left(K_{X}\right), 278$
$B, 282$
$K_{V}, 282$
$C_{n, F B}, 283$
$\{\hat{X}(w)\}, 303$
$\mathcal{X}, 304$
$\mathcal{R}^{+}, 304$
$d_{\max }, 304$
$D, 306$
$\hat{X}^{n}(w), 306$
$R(D), 306$
$D(R), 306$
$R^{(I)}(D), 307$
$A_{\text {d,e }}^{(n)}, 319$
$\bar{D}, 321$
$K\left(x^{n}, \hat{x}^{n}\right), 322$
$N\left(a \mid x^{n}\right), 326$
$A_{e}^{*(n)}, 326$
$N\left(a, b \mid x^{n}, y^{n}\right), 326$
$\phi(D), 337$
$V_{y^{n} \mid x^{n}}(b \mid a), 342$
$T_{V}\left(x^{n}\right), 342$
$A_{e}^{*(n)}\left(Y \mid x^{n}\right), 343$
x, 347
$\mathcal{X}, 347$
$P_{\mathbf{x}}, 348$
$P_{x^{n}}, 348$
$\mathcal{P}_{n}, 348$
$T(P), 348$
$Q^{n}\left(x^{n}\right), 349$
$T_{\Omega}^{e}, 356$
$Q^{n}(E), 361$
$P^{*}, 362$
$\mathcal{L}_{1}, 369$
|| |||, 369
$S_{t}, 372$
$D^{*}, 372$
$\alpha, 375$
$\beta, 376$
$a^{*}, 376$
<!-- Page 751 -->
| $b^{*}, 376$ | $\log ^{*} n, 469$ |
| :--: | :--: |
| $A_{n}, 376$ | $H_{0}(p), 470$ |
| $B_{n}, 376$ | $K(n), 471$ |
| $\phi_{A}(), 376$ | $P_{\mathcal{U}}(x), 481$ |
| $P_{\lambda}, 380$ | $\Omega, 484$ |
| $\lambda^{*}, 380$ | $\Omega_{n}, 484$ |
| $C\left(P_{1}, P_{2}\right), 386$ | $K_{k}\left(x^{n} \mid n\right), 496$ |
| $\psi(s), 392$ | $k^{*}, 497$ |
| $T\left(X_{1}, X_{2}, \ldots, X_{n}\right), 393$ | $p^{*}, 497$ |
| $V, 394$ | $S^{*}, 497$ |
| $J(\theta), 394$ | $S^{* *}, 497$ |
| $b_{T}(\theta), 396$ | $p^{* *}, 497$ |
| $J_{i j}(\theta), 397$ | $C(P / N), 514$ |
| $R(k), 415$ | S, 520 |
| $S(\lambda), 415$ | $X(S), 520$ |
| $\hat{R}(k), 415$ | $A_{i}^{(n)}(S), 521$ |
| $K^{(n)}, 416$ | $a_{n} \doteq 2^{n(b \pm \epsilon)}, 521$ |
| $\sigma_{\infty}^{2}, 417$ | $A_{e}^{(n)}\left(S_{1} \mid \mathbf{s}_{2}\right), 523$ |
| $K_{p}, 420$ | $E_{i j}, 531$ |
| $\Psi(u), 422$ | Q, 534 |
| $p_{\theta}, 428$ | $C_{\mathrm{I}}, 535$ |
| $R\left(p_{\theta}, q\right), 429$ | $R(S), 543$ |
| $R^{*}, 429$ | $S^{c}, 543$ |
| $q_{\pi}, 430$ | $\beta * p_{1}, 569$ |
| $A(n, k), 434$ | b, 613 |
| $q_{\frac{1}{2}}\left(x^{n}\right), 436$ | S, 613 |
| $F_{U}(u), 437$ | $W(\mathbf{b}, F), 615$ |
| $R_{n}\left(X_{0}, X_{1}, \ldots, X_{n-1}\right), 444$ | $W^{*}(F), 615$ |
| $Q_{u}(i), 445$ | $\mathbf{b}^{*}, 615$ |
| $A_{j k}, 445$ | $S_{n}^{*}, 615$ |
| $c(n), 450$ | $W^{*}, 615$ |
| $n_{k}, 450$ | $\mathcal{B}, 617$ |
| $c_{l s}, 452$ | $\Delta W, 622$ |
| $\left\{X_{i}\right\}_{\infty}^{\infty}, 455$ | $W_{\infty}^{*}, 624$ |
| $A_{D}, 460$ | $U^{*}, 628$ |
| $\mathcal{U}, 466$ | $S_{n}^{*}\left(\mathbf{x}^{n}\right), 630$ |
| $\mathcal{U}(p), 466$ | $\widehat{S}^{n}\left(\mathbf{x}^{n}\right), 630$ |
| $p, 466$ | $V_{n}, 631$ |
| $K_{\mathcal{U}}(x), 466$ | $j^{n}, 634$ |
| $K_{\mathcal{U}}(x \mid l(x)), 467$ | $w\left(j^{n}\right), 634$ |
| $K(x \mid l(x)), 468$ | $\mathcal{K}, 636$ |
<!-- Page 752 -->
| $\Gamma(m), 638$ | $J(X), 671$ |
| :--: | :--: |
| $B\left(\lambda_{1}, \lambda_{2}\right), 642$ | $g_{t}(y), 672$ |
| $(\Omega, \mathcal{B}, P), 644$ | $V(A), 675$ |
| $X(\omega), 644$ | $f * g, 676$ |
| $H^{k}, 645$ | $\mathcal{L}_{r}, 676$ |
| $H^{\infty}, 645$ | $\\|f\\|_{r}, 676$ |
| $\tilde{p}_{n}, 660$ | $C_{p}, 676$ |
| $\Gamma(z), 662$ | $h_{r}(X), 676$ |
| $\psi(z), 662$ | $V_{r}(X), 677$ |
| $\gamma, 662$ | $P_{k}, 680$ |
| $X(S), 668$ | $S_{k}^{(n)}, 680$ |
| $h_{k}^{(n)}, 668$ | $K\left(i_{1}, i_{2}, \ldots, i_{k}\right), 680$ |
| $t_{k}^{(n)}, 669$ | $Q_{k}, 681$ |
| $g_{k}^{(n)}, 669$ | $\sigma_{i}^{2}, 681$ |
| $f_{k}^{(n)}, 671$ | $K_{k}, 681$ |
<!-- Page 753 -->
# 색인

Abrahams, J., 689
Abramson, N.M., xxiii, 689
Abu-Mostafa, Y.S., 689
수용 영역, 376, 383
달성 가능한 비율, 195, 268, 306, 538, 546, 550, 598
달성 가능한 비율 왜곡 쌍, 306
달성 가능한 비율 영역, 514, 550
Aczél, J., 690
Adams, K., xxiii
적응형 사전 압축 알고리즘, 441
가산 채널, 229
가산 잡음 채널, 280, 287, 296
가산 백색 가우시안 잡음 (AWGN), 289
Adler, R.L., 158, 689
AEP, xix, xx, 6, 12, 57, 58, 64, 69, 77, 101, 168, 219, 220, 222, 223, 347, $356,381,382,409,460,531,554$, $560,566,644,645,649,656,686$, 또한 Shannon-McMillan-Breiman 정리 참조
연속 확률 변수, 245
이산 확률 변수, $\mathbf{58 - 62}$
왜곡이 일반적인 경우, 319
성장률, 650
결합, 196, 223
곱, 66
상대 엔트로피, 380
샌드위치 증명, 644-649
정상 순환 과정, 644-649
Ahlswede, R., 11, 609, 610, 689, 690, 712
Akaike, H., 690
Algoet, P., xxiii, 69, 626, 645, 656, 690
알고리즘, 442
교대 최소화, 332
산술 코딩, 산술 코딩 참조
Blahut-Arimoto, Blahut-Arimoto 알고리즘 참조
Durbin, 419
Frank-Wolfe, 191
경사 하강법, 191
탐욕적, 127
허프만 코딩, 127, 154, 허프만 코딩 참조
반복적, 332
Lempel-Ziv, Lempel-Ziv 코딩 참조
Levinson, 419
온라인, 427
알고리즘적 복잡도, 1, 3, 463, 464, 466, 507, 508, Kolmogorov 복잡도 참조
알고리즘적으로 무작위적인, 477, 486, 502, 507
거의 확실하게, 58, 69
알파벳, 13, 347
이산, 13
유효 크기, 46, 256
유효 지지 집합, 256
입력, 183
출력, 183
알파벳 코딩, 121
Altria, 643
Amari, S.I., 55, 690, 703
Anantharam, V., 690
안테나, 292, 611
Arimoto, S., 191, 240, 335, 346, 690, 또한 Blahut-Arimoto 알고리즘 참조
임의로 변하는 채널, 689, 690, 697

[^0]
[^0]:    Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas Copyright (c) 2006 John Wiley \& Sons, Inc.
<!-- Page 754 -->
산술 코딩, 130, 158, 171, 427, 428, $435-440,461$
유한 정밀도 산술, 439
산술 평균 기하 평균 부등식, 669
ASCII, 466
Ash, R.B., 690
비대칭 왜곡, 337
점근적 균등 분할 속성, 참조 AEP
점근적 최적성,
로그 최적 포트폴리오, 619
ATM 네트워크, 218
대기권, 412
원자, 137-140, 257, 404
자기 상관, 415,420
자기 회귀 과정, 416
보조 확률 변수, 565,569
평균 복호어 길이, 124, 129, 148
평균 설명 길이, 103
평균 왜곡, 318, 324, 325, 329, 344
평균 전력, 261, 295, 296, 547
평균 오류 확률, 199, 201, 202, 204, 207, 231, 297, 532, 554
AWGN (가산 백색 가우시안 잡음), 289
엔트로피의 공리적 정의, 14,54

Baer, M., xxi
Bahl, L.R., xxiii, 690, 716
대역 제한, 272
대역 통과 필터, 270
대역폭, 272-274, 289, 515, 547, 606
Barron, A.R., xxi, xxiii, 69, 420, 508, 656, $674,691,694$
로그 밑, 14
야구, 389
Baum, E.B., 691
베이즈 오류 지수, 388
베이즈 가설 검정, 384
베이즈 사후 확률, 435
베이즈 오류 확률, 385, 399
BCH (Bose-Chaudhuri-Hocquenghem) 코드, 214
Beckenbach, E.F., 484
Beckner, W., 691
Bell, R., 182, 656, 691
Bell, T.C., 439, 691
벨 부등식, 56
Bellman, R., 691

Bennett, C.H., 241, 691
Bentley, J., 691
Benzel, R., 692
Berger, T., xxiii, 325, 345, 610, 611, 692, 720
Bergmans, P., 609, 692
Bergstrøm 부등식, 684
Berlekamp, E.R., 692, 715
Bernoulli, J., 182
Bernoulli 분포, 434
Bernoulli 과정, 237, 361, 437, 476, 484, 488
Bernoulli 확률 변수, 63
Bernoulli 소스, 307, 338
비율 왜곡, 307
Berrou, C., 692
베리의 역설, 483
Bertsekas, D., 692
베타 분포, 436, 661
베타 함수, 642
베팅, 162, 164, 167, 173, 174, 178, 487, 626
경마, 160
비례, 162, 626
편향, 393, 402
Bierbaum, M., 692
Biglieri, E., 692
이진 엔트로피 함수, 15, 49
그래프, 15,16
이진 삭제 채널, 188, 189, 218, 227, 232, 457
이진 승산기 채널, 234, 602
이진 비율 왜곡 함수, 307
이진 소스, 308, 337
이진 대칭 채널 (BSC), 8, 187-189, 210, 215, 222, 224, 225, 227-229, 231, 232, 237, 238, 262, 308, 568, 601
용량, 187
비닝, 609
무작위, 551, 585
생물 정보학, xv
새, 97
Birkhoff의 에르고딕 정리, 644
주교, 80
비트, 14
Blachman, N., 674, 687, 692
Blackwell, D., 692
<!-- Page 755 -->
Blahut, R.E., 191, 240, 335, 346, 692, see also Blahut-Arimoto algorithm
Blahut-Arimoto algorithm, 191, 334
block code, 226
block length, 195, 204
block Markov encoding, 573
Boltzmann, L., 11, 55, 693, see also Maxwell-Boltzmann distribution
bone, 93
bookie, 163
Borel-Cantelli lemma, 357, 621, 649
Bose, R.C., 214, 693
bottleneck, 47, 48
bounded convergence theorem, 396, 647
bounded distortion, 307, 321
brain, 465
Brascamp, H.J., 693
Brassard, G., 691
Breiman, L., 69, 655, 656, 692, 693, see also Shannon-McMillan-Breiman theorem
Brillouin, L., 56, 693
broadcast channel, 11, 100, 515, 518, 533, 563, 560-571, 593, 595, 599, 601, $604,607,609,610$
capacity region, 564
convexity, 598
common information, 563
converse, 599
degraded, 599, 609
achievablity, 565
converse, 599
with feedback, 610
Gaussian, 515, 610
physically degraded, 564
stochastically degraded, 564
Brunn-Minkowski inequality, xx, 657, $674-676,678,679,687$
BSC, see binary symmetric channel (BSC)
Bucklew, J.A., 693
Burg, J.P., 416, 425, 693
Burg's algorithm, 416
Burg's maximum entropy theorem, 417
Burrows, M., 462, 693
burst error correcting code, 215
Buzo, A., 708

Caire, G., 693
cake, 65
calculus, 103, 111, 410
Calderbank, A.R., 693
Canada, 82
capacity, 223, 428
channel, see channel capacity
capacity region, 11, 509-610
degraded broadcast channel, 565
multiple access channel, 526
capacity theorem,xviii, 215
CAPM (Capital Asset Pricing Model), 614
Carathéodory's theorem, 538
cardinality, 245, 538, 542, 565, 569
cards, 55, 84, 167, 608
Carleial, A.B., 610, 693
cascade, 225
cascade of channels, 568
Castelli, V., xxiii
Cauchy distribution, 661
Cauchy-Schwarz inequality, 393, 395
causal, 516, 623
causal investment strategy, 619, 623, 635
causal portfolio, 620, 621, 624, 629-631, $635,639,641,643$
universal, 651
CCITT, 462
CDMA (Code Division Multiple Access), 548
central limit theorem,xviii, 261, 361
centroid, 303, 312
Cesáro mean, 76, 625, 682
chain rule,xvii, 35, 43, 49, 287, 418, 540, $541,555,578,584,590,624,667$
differential entropy, 253
entropy, 23, 31, 43, 76
growth rate, 650
mutual information, 24, 25, 43
relative entropy, 44,81
Chaitin, G.J., 3, 4, 484, 507, 508, 693, 694
Chang, C.S., xxi, 694
channel,
binary erasure, 188, 222
binary symmetric, see binary symmetric channel (BSC)
broadcast, see broadcast channel
cascade, 225, 568
discrete memoryless, see discrete memoryless channel
exponential noise, 291
extension, 193
<!-- Page 756 -->
피드백, 216
가우시안, 가우시안 채널 참조
간섭, 간섭 채널 참조
다중 접속, 다중 접속 채널 참조
무잡음 이진, 7, 184
병렬, 224, 238
릴레이, 릴레이 채널 참조
대칭, 189, 190, 222
시간 가변, 226, 294
양방향, 양방향 채널 참조
합집합, 236
약한 대칭, 190
메모리 포함, 224, 240
$Z$-채널, 225
채널 용량, xv, xviii, xix, 1, 3, 7-9, 11, 38, 183-241, 291, 297, 298, 307, $333,427,432,458,461,544-546$, $548,592,604,608,686$
달성 가능성, 200
계산, 332
피드백, 223
정보, 184, 263
운영 정의, 184
속성, 222
무오류, 226
채널 용량 정리, 38
채널 코드, 193, 220
채널 코딩, 207, 219, 220, 318, 324, 325,344
달성 가능성, 195
채널 코딩 정리, 9, 199, 207, 210, 223, 230, 321, 324, 347
달성 가능성, 200
역, 207
채널 전이 행렬, 190, 234, 428, 433, 509
메모리 있는 채널, 224, 277
특성 함수, 422
Chellappa, R., 694
Cheng, J.F., 709
Chernoff, H., 694
Chernoff 바운드, 392
Chernoff 정보, 380, 384, 386, 388, 399
Chernoff-Stein 렘마, 347, 376, 380, 383, 399
Chervonenkis, A.Y., 717
체스판, 80, 97
$\chi^{2}$ 거리, 665
$\chi^{2}$ 통계량, 400
카이제곱 분포, 661
Chiang, M.S., xxi, 610, 695
닭, 301
Choi, B.S., 425, 694, 697
Chomsky, N., 694
Chou, P.A., 694
Chuang, I., 241, 710
Chung, K.L., 69, 694
처치의 논제, 465
Cioffi, J.M., 611, 702
암호, 170
치환, 170
Clarke, B.S., 694
교실, 561
Cleary, J.G., 691, 719
닫힌 계, 11
폐포, 363
Cocke, J., 690
칵테일 파티, 515
코드, 10, 20, 61, 62, 98, 104-158, 172, 194-232, 264-281, 302-339, 357, $360,428,429,434,436,438,440$, $443,444,456,460-463,492$, $525-602$
산술, 산술 코딩 참조
블록, 블록 코드 참조
채널, 채널 코드 참조
분산 소스, 분산 소스 코딩 참조
오류 정정, 오류 정정 코드 참조
확장, 105, 126
가우시안 채널, 264
해밍, 해밍 코드 참조
허프만, 허프만 코드 참조
순간, 103, 106, 107, 110, 118, $124,142,143,146,152$, 코드, 접두사 참조
최소 거리, 212
최소 가중치, 212
모스, 모스 코드 참조
비특이, 105, 152
최적, 124, 144
접두사, 106
무작위, 199, 201
자기 구멍 뚫기, 105, 106
섀넌, 섀넌 코드 참조
<!-- Page 757 -->
source, source code 참조
uniquely decodable, 105, 110, 116-118, $127,131,141-143,147,148,150$, $152,157,158$
zero-error, 205
codebook, 204, 206, 212, 266, 268, 321, $322,327,328,513-519,530,534$, $565,574,580$
codelength, 114, 115, 122
code points, 302
codeword, 109, 122, 131
optimal length, 113
Cohn, M., xxi
coin flips, 44, 103, 155
coin tosses, 37, 134, 137, 139, 225, 375
coin weighing, 45
coins,
bent, 48
biased, 96, 375, 407
fair, 134
large deviations, 365
colored noise, 288
coloring,
random, 558
comma, 105, 468
common information, 563, 564, 567, 568
communication channel, 1, 7, 187, 223, 261, 560, 663
communication networks, 509
communication system, 8, 184, 192
communication theory, 1
compact disc, 3, 215
compact set, 432, 538
company mergers, 149
competitive optimality,
log-optimal portfolio, 627
Shannon code, 130
competitively optimal, 103, 613
composition class, 348
compression, data compression 참조
computable, 466, 479, 482, 484, 491
computable probability distribution, 482
computable statistical tests, 479
computation, 4, 5, 12, 68, 190, 438, 463, 466, 594
channel capacity, 191, 332
halting, halting computation 참조
models of, 464
physical limits, 56
rate distortion, 332
computer science, xvii, 1, 463, 483
computer source code, 360
computers, 4, 442, 464, 504
concatenation, 105, 116
concavity, 27, 31, 33, 222, 453, 474, 616, convexity 참조
conditional entropy, 17, 38-40, 75, 77, 88, 89, 206, 417, 669, 670
conditional limit theorem, 366, 371, 375, 389, 398
conditional mutual information, 24, 589
conditional rate distortion, convexity, 583, 585
conditional rate distortion function, 584, 585
conditional relative entropy, 25
conditional type, 342, 366
conditionally typical set, 327, 342
conditioning reduces entropy, 33, 39, 85, $311,313,318,418,578,584,682$
constant rebalanced portfolio, 615, 630
constrained sequences, 94, 101
continuous alphabet, 261, 305
continuous channel, 263
continuous random variable, 21, 37, 243, $245,248,256,301,304,338,500$, differential entropy, rate distortion theory 참조
quantization, quantization 참조
continuous time, 270
convergence of random variables, convergence in mean square, 58
convergence in probability, 58
convergence with probability, 1, 58
converse,
broadcast channel, 599
discrete memoryless channel, 206
with feedback, 216
general multiterminal network, 589
multiple access channel, 538
rate distortion, 315
Slepian-Wolf coding, 555
<!-- Page 758 -->
볼록 폐포, 538
볼록 집합족, 655
볼록 껍질, 526, 530, 532, 534-536, 538, $543,544,565,591,594,595$
볼록 집합, 330, 534
볼록화, 598
볼록성, 26, 28, 32, 42, 432, 616, 657, 또한 오목성 참조
용량 영역,
방송 채널, 598
다중 접속 채널, 534
율 왜곡 함수, 316
엄격한, 26
합곱, 270, 674
합곱 부호, 215
인터리브된, 215
쿠키 커팅, 240
구리, 274
Coppersmith, D., 689
상관된 확률 변수,
부호화, Slepian-Wolf 부호화 참조
상관, 46, 252, 258, 294, 295, 593
Costa, M.H.M., 593, 694, 703
Costello, D.J., 708
위조, 45
공분산 행렬, 249, 277, 279, 280, 284, 292, 397, 416, 417, 681
Cover, T.M., xxi, 69, 158, 182, 240, 299, $425,508,575,593,609-611,626$, $656,687,690,691,694-699,701$, 712,717
CPU, 465
Cramér, H., 696
Cramér-Rao 부등식, 392, 395, 396, 399
편향 포함, 402
누화, 273
암호학, 171
Csiszár, I., 55, 325, 332, 334, 335, 346, $347,358,408,461,610,696,697$
Csiszár-Tusnády 알고리즘, 335
누적 분포 함수, 127, 128, $130,243,437,439$
절단 집합, 512, 589, 597
$D$-진 분포, 112
$D$-진 알파벳, 103, 109
Dantzig, G.B., 697
Darroch, J.N., 697
Daróczy, Z., 690
데이터 압축, xv, xvii, xix, 1, 3, 5, 11, $103,156,163,172,173,184,218$, $221,301,427,442,457,549,656,686$
데이터 처리 부등식, 35, 39, 44, 47, 371,687
데이터 전송, 5, 686
Daubechies, I., 697
Davisson, L.D., 461, 697, 702, 703
de Bruijn의 항등식, 672
결정 함수, 375
결정 이론, 가설 검정 참조
디코더, 194
디코딩, 194
강건한, 296
디코딩 지연, 148
디코딩 함수, 194, 219, 264, 552, 571, 583
퇴화된, 515, 516, 533, 565-570, 595, 599, 601, 604, 609
방송 채널, 방송 채널, 퇴화된 참조
물리적으로 퇴화된, 564
릴레이 채널, 릴레이 채널, 퇴화된 참조
확률적으로 퇴화된, 564
Dembo, A., xxiii, 687, 697, 698
복조, 3
Dempster, A.P., 698
밀도, 243
기술 복잡도, 463
행렬식, 249, 279, 679, 681
행렬식 부등식, xviii, 679-685
결정론적, 53, 173, 204, 511, 607, 609
결정론적 디코딩 규칙, 194
결정론적 함수, 339, 456
Devroye, L., 698
주사위, 364, 375, 411, 412
사전, 442
차분 엔트로피, xix, 243-674
경계, 258
조건부, 249
가우시안 분포, 244
이산 엔트로피와의 관계, 247
표, 661
Diggavi, S., xxi
디지털, 273, 274, 465
차원, 211, 675
디리클레 분포, 436, 641
디리클레 분할, 303
<!-- Page 759 -->
이산, 184
이산 채널, 263, 264, 268
이산 엔트로피, 244, 259, 660
이산 무기억 채널, 183, 184-241, 280, 318, 344, 536
이산 무기억 소스, 92
이산 확률 변수, 14, 49, 54, 134, 243, 245, 249, 251, 252, 258, 347, 371, 658
이산 시간, 261
판별, 55
거리, 20, 301, 325, 332, 369, 432
확률 분포 간의, 13
유클리드, 297, 332, 367
상대 엔트로피, 356, 366
변분, 370
구별 가능한 입력, 10, 222
구별 가능한 신호, 183
왜곡, 10, 301-341, 580-582, 586, 587, 또한 참조율 왜곡 이론
비대칭, 337
해밍, 또한 참조 해밍 왜곡
제곱 오차, 또한 참조 제곱 오차 왜곡
왜곡 함수, 304, 307, 309, 312, 336, 340, 344, 345
왜곡 측정, 301, 302, 304, 305, 307, 314, 319, 321
유계, 304
해밍 왜곡, 304,
이타쿠라-사이토, 305
제곱 오차 왜곡, 305
왜곡율 함수, 306, 341
왜곡 특이, 319, 321, 322, 328
분산 소스 코딩, 509, 511, 550, 556, 586, 또한 참조 슬레피안-울프 코딩
분포, 427, 428, 456
두 질점, 28
발산, 55
DiVincenzo, D.P., 691, 698
DMC, 또한 참조 이산 무기억 채널
Dobrushin, R.L., 698, 711
개, 93
Donoho, D.L., 698
두 배 속도, xvii, 9, 11, 12, 160, $163-167,175,179,180$
이중 확률 행렬, 88
DSL, 274
쌍대성, 610
데이터 압축 및 데이터 전송, 184
데이터 압축 및 도박, 173
성장률 및 엔트로피율, 159, 613
다중 접속 채널 및
슬레피안-울프 코딩, 558
율 왜곡 및 채널 용량, 311
소스 코딩 및 확률 변수 생성, 134
Duda, R.O., 698
Dueck, G., 609, 610, 690, 698
더빈 알고리즘, 419
네덜란드어, 561, 562, 606
네덜란드식 베팅, 164, 180
DVD, 3
이진, 129, 130, 132, 137-139
이진 분포, 151
귀, 219
Ebert, P.M., 299, 698
메아리, 273
Eckschlager, K., 698
경제학, 4
엣지 프로세스, 98
효과적으로 계산 가능한, 465
효율적인 추정기, 396
효율적 투자선, 614
Effros, M., 462, 694, 698, 702
Efron, B., 699
Eggleston, H.G., 538, 699
고유값, 95, 279, 282, 315, 336
아인슈타인, A., xvii
Ekroot, L., xxiii
El Gamal, A., xxiii, 575, 609-611, 694-696, 699, 701, 702, 710
코끼리, 301
Elias, P., 158, 699
Ellis, R.S., 699
EM 알고리즘, 335
경험적, 68, 381, 409, 470, 474, 542, 660
경험적 분포, 68, 168, 209, 347, $356,366,630$
수렴, 68
경험적 엔트로피, 195
경험적 빈도, 474
인코더, 173, 231, 304, 321, 357, 360, 443, $458,459,549,553,557,574,585,588$
<!-- Page 760 -->
인코딩 함수, 193, 264, 305, 359, 571, 583, 599
암호화된 텍스트, 506
에너지, 261, 265, 272, 273, 294, 424
영국, 82
영어, 104, 168-171, 174, 175, 182, 360, 470, 506
엔트로피율, 159, 182
모델, 168
얽힘, 56
엔트로피, xvii, 3, 4, 13-56, 87, 659, 671, 686
평균, 49
공리적 정의, 14, 54
로그 밑, 14, 15
경계, 663
연쇄 법칙, 23
오목성, 33, 34
조건부, 16, 51, 참조 조건부 엔트로피
조건화, 42
교차 엔트로피, 55
미분, 참조 미분 엔트로피
이산, 14
인코딩된 비트, 156
함수, 45
그룹화, 50
독립성 경계, 31
무한, 49
결합, 16, 47, 참조 결합 엔트로피
혼합 증가, 51
혼합, 46
및 상호 정보, 21
속성, 42
상대, 참조 상대 엔트로피
Renyi, 676
합계, 47
열역학, 14
엔트로피 및 상대 엔트로피, 12, 28
엔트로피 파워, xviii, 674, 675, 678, 679, 687
엔트로피 파워 부등식, xx, 298, 657, 674-676, 678, 679, 687
엔트로피율, 4, 74, 71-101, 114, 115, 134, 151, 156, 159, 163, 167, 168, 171, 175, 182, 221, 223, 259, 417, 419, 420, 423-462, 613, 624, 645, 667, 669
미분, 416

영어, 168, 170, 174, 175
가우시안 과정, 416
은닉 마르코프 모델, 86
마르코프 연쇄, 77
부분 집합, 667
봉투, 182
Ephremides, A., 611, 699
Epimenides 거짓말쟁이 역설, 483
등화, 611
Equitz, W., xxiii, 699
삭제, 188, 226, 227, 232, 235, 527, 529, 594
삭제 채널, 219, 235, 433
에르고딕, 69, 96, 167, 168, 175, 297, 360, 443, 444, 455, 462, 557, 613, 626, 644, 646, 647, 651
에르고딕 과정, xx, 11, 77, 168, 444, 446, 451, 453, 644
에르고딕 소스, 428, 644
에르고딕 정리, 644
에르고딕 이론, 11
Erkip, E., xxi, xxiii
Erlang 분포, 661
오류 정정 코드, 205
오류 검출 코드, 211
오류 지수, 4, 376, 380, 384, 385, 388, 399, 403
추정, xviii, 255, 347, 392, 425, 508
스펙트럼, 415
추정기, 39, 40, 52, 255, 392, 393, 395-397, 401, 402, 407, 417, 500, 663
편향, 393
편향된, 401
확률적으로 일관된, 393
지배, 393
효율적인, 396
비편향된, 392, 393, 395-397, 399, 401, 402, 407
유클리드 거리, 514
유클리드 기하학, 378
유클리드 공간, 538
오일러 상수, 153, 662
교환 가능한 주식, 653
기대값, 14, 167, 281, 306, 321, 328, 393, 447, 479, 617, 645, 647, 669, 670
기대 길이, 104
지수 분포, 256, 661
채널 확장, 193
코드 확장, 105
<!-- Page 761 -->
F-distribution, 661
face vase illusion, 505
factorial, 351, 353
Stirling's approximation, 405
fading, 611
fading channel, 291
Fahn, P., xxi
fair odds, 159, 164, 487, 488
fair randomization, 627, 629
Fan, K., 679, 699
Fano, R.M., 56, 158, 240, 699, 700, see also Shannon-Fano-Elias code
Fano's inequality, 13, 38, 39, 41, 44, 52, 56, 206, 208, 221, 255, 268, 283, $539-541,555,576,578,590,663$
FAX, 130
FDMA (Frequency Division Multiple Access), 547, 548, 606
Feder, M., 158, 462, 700, 709, 718
Feder, T., 461
feedback, xix, 189, 193, 216, 218, 238, 280-284, 286-290, 509, 519, 593, $594,610,611$
discrete memoryless channel, 216
Gaussian channel, xv, 280-289
Feinstein, A., 240, 699, 700
Feller, W., 182, 700
Fermat's last theorem, 486
fingers, 143
finite alphabet, 220, 318, 344, 473, 474, 645
finitely often, 649
finitely refutable, 486
first order in the expononent, 63
Fisher, R.A., 56, 700
Fisher information, xviii, xx, 247, 347, 392, 394, 395, 397, 399, 401, 407, 657, $671,673,674$
examples, 401
multiparameter, 397
Fitingof, B.M., 461, 700
fixed rate block code, 357
flag, 61, 442, 460
flow of information, 588, 589
flow of time, 89
flow of water, 511
football, 390, 391
Ford, L.R., 700
Ford-Fulkerson theorem, 511, 512
Forney, G.D., 240, 700

Foschini, G.J., 611, 700
Fourier transform, 271, 415
fractal, 471
Franaszek, P.A., xxi, xxiii, 158, 700
Frank-Wolfe algorithm, 191
French, 606
frequency, 168-170, 270, 274, 315, 404, 547
Friedman, J.H., 693
Fulkerson, D.R., 697, 700
function,
concave, 26
convex, 26
functional, 161, 276, 313, 330
future, 93

Gaarder, T., 593, 609, 700
Gabor, D., 701
Gács, P., 695, 701
Gadsby, 168
Gallager, R.G., xxiii, 215, 240, 299, 430, 461, 609, 692, 701, 713, 715, 716
Galois field theory, 214
gambling, xviii, xx, 11, 13, 159, 171-173, $175,178,181,182,488,507,629$
universal, 487
gambling and data compression, 171
game, 181, 298, 391, 631
20 questions, $6,120,121,143,145,157$, 237
Hi-Lo, 147
mutual information, 298
red and black, 167, 177
Shannon guessing, 174
stock market, 630
game theory, 132
fundamental theorem, 432
game-theoretic optimality, 132, 619
$\gamma$ (Euler's constant), 153, 662
Gamma distribution, 661
gas, 34, 409, 411, 412
Gauss's law, 548
Gauss-Markov process, 417-420
Gaussian, 252, 255, 258, 378, 389, 684, 685
Gaussian channel, xv, xix, 205, 261-299, $324,513,514,519,520,544,546,686$
achievability, 266
AWGN (additive white Gaussian noise), 289
<!-- Page 762 -->
가우시안 채널 (계속)
대역 제한, 270-274
방송, 방송 채널, 가우시안 참조
용량, 264
색소음, 277
반대, 268
피드백, 280-289
간섭, 간섭 채널, 가우시안 참조
메모리 포함, 277, 280
다중 접속, 다중 접속 채널, 가우시안 참조
병렬, 274-280, 292
릴레이, 릴레이 채널, 가우시안 참조
가우시안 분포, 정규 분포 참조
가우시안 프로세스, 272, 279, 417
가우시안 소스, 311, 336
율 왜곡 함수, 311
가우시안 확률 과정, 315, 416, 417, 423
겔판트, I.M., 702
겔판트, S.I., 609, 610, 702
게멜로스, G., xxi
일반 다자간 네트워크, 587
일반 상대성 이론, 490
일반화된 로이드 알고리즘, 303
난수 생성, 134
측지선, 380
기하 분포, 405, 444
기하학, 9, 301, 367
유클리드, 378
지구물리학적 응용, 415
거쇼, A., 702
깁슨, J.D., 702
GIF, 443, 462
길버트, E.N., 158, 702
길, J., xxiii
글라비유, A., 692
괴델의 불완전성 정리, 483
골드바흐의 추측, 486
골드버그, M., xxiii
골드만, S., 702
골드스미스, A., 702
골롬, S.W., 702
구델, K., xxiii
고피나스, R., xxi
고담, 470, 550
경사 하강법, 191
문법, 171
그랜트, A.J., 702
그래프, 73, 78, 79, 97
그래프 색칠, 557
중력, 490
그레이, R.M., 610, 694, 695, 702, 703, 708
인사 전보, 441
그레나더, U., 703
그룹화 규칙, 50
성장률, xix, 4, 159, 178, 180, 182, $615,613-656,686$
연쇄 법칙, 624, 650
경쟁적 최적성, 628
볼록성, 616, 650
최적, 615
부수 정보, 622, 650
성장률 최적, 162, 613
그룬바움, B., 538, 703
귀아수, S., 703
굽타, V., xxi
거트만, M., 462, 700
죄르피, L., 698
gzip, 442

하데마르 부등식, 279, 680, 681
하이예크, B., 611, 699, 703
정지, 484
정지 계산, 466, 486
정지 문제, 483
정지 프로그램, 473
해밍 코드, 205, 212-214
해밍 왜곡, 307, 308, 336, 337
해밍, R.V., 210, 703
한, T.S., xxi, 593, 609, 610, 668, 670, $687,689,703,717,718$
필기, 87
하트, P.E., 695, 698
하틀리, R.V., 55, 703
하산푸르, N., xxi
하시비, B., 693
하스너, M., 689
HDTV, 560
헥스트라, A.P., 609, 718
헬스트롬, C.W., 703
허쉬코비츠, Y., 703
휴렛팩커드, 643
은닉 마르코프 모델 (HMM), 87, 101
<!-- Page 763 -->
고확률 집합, 62
히스토그램, 174
역사적 참고 사항, xv
HMM, 숨겨진 마르코프 모델(HMM) 참조
Hochwald, B.M., 693
Hocquenghem, P.A., 214, 703
Holsinger, J.L., 704
Honig, M.L., 704
Hopcroft, J.E., 704
Horibe, Y., 704
경마, 5, 6, 11, 159-182, 622, 626
허프만 코드, 103, 118-127, 129-131, $137,142,145,146,149,151,155$, 157, 357, 427, 436, 460, 491, 492
경쟁적 최적성, 158
이진 분포, 151
Huffman, D.A., 158, 704
Hui, J.Y., 704
Humblet, P.A., 704
가설 검정, 1, 4, 11, 355, 375, 380, 384, 389
베이즈, 384
최적, Neyman-Pearson lemma 참조
i.i.d. (독립적이고 동일하게 분포된) 소스, 307, 318, 344, 357
식별 용량, 610
Ihara, S., 704
이미지, 305
왜곡 측정, 305
엔트로피율, 171
콜모고로프 복잡성, 499, 505, 506
Immink, K.A.S., 704
압축 불가능한 시퀀스, 477, 479
엔트로피에 대한 독립성 경계, 31
인도, 441
지시 함수, 194, 219, 486, 497, 503
귀납, 95, 123, 127, 674
부등식, xviii-xx, 53, 207, 418, $657-687$
부등식,
산술 평균 기하 평균, 669
Brunn-Minkowski, Brunn-Minkowski 부등식 참조
코시-슈바르츠, 393
체비쇼프, 64
데이터 처리, 데이터 처리 부등식 참조
행렬식, 행렬식 부등식 참조
엔트로피 거듭제곱, 엔트로피 거듭제곱 부등식 참조
파노의, 파노의 부등식 참조
아다마르의, 아다마르의 부등식 참조
정보, 29, 410, 659
젠센의, 젠센의 부등식 참조
크라프트, 크라프트 부등식 참조
$\log$ 합, $\log$ 합 부등식 참조
마르코프의, 마르코프의 부등식 참조
맥밀란의, 맥밀란의 부등식 참조
부분 집합, 부분 집합 부등식 참조
영의, 676
지브의, 450
추론, 1, 3, 4, 463, 484
무한 대역폭, 273
무한히 자주, 621
정보, Fisher 정보, 상호 정보, 자체 정보 참조
정보 용량, 207, 263, 274, 277
정보 채널 용량, 184
정보 발산, 55, 상대 엔트로피 참조
판별을 위한 정보, 55, 상대 엔트로피 참조
정보율 왜곡 함수, 306, 307, 329
혁신, 282
입력 알파벳, 183, 209, 268
입력 분포, 188, 227, 228, 278, 335, $430,431,532,544,546,591$
순간 코드, 순간 코드 참조
정수,
이진 표현, 469
기술적 복잡성, 469
적분 가능성, 248
간섭, xix, 3, 11, 273, 509, 511, 515, $518,519,527,547,588,610$
간섭 채널, 510, 518, 519, 610
퇴화된, 610
가우시안, 518, 519, 610
높은 간섭, 518
강한 간섭, 610
인터리빙, 611
인터넷, 218
심볼 간 간섭, 94
내재적 복잡성, 464
투자, 4, 9, 11, 159, 614, 619, 623, $636,655,656$
투자자, 619, 623, 627, 629, 633, 635
<!-- Page 764 -->
불가분 마르코프 연쇄, 마르코프 연쇄, 불가분 참조
이타쿠라-사이토 거리, 305
반복 복호화, 215
이야가르, G., xxi

제이콥스, I.M., 719
자얀트, N.S., 704
자예스, E.T., 56, 416, 425, 704
젤리넥, F., xxiii, 158, 690, 704, 705
젠센 부등식, 28, 32, 41, 42, 44, 49, $252,253,270,318,447,453,474$, $585,618,622,657$
존슨, R.W., 715
결합 AEP, 202, 203, 267, 329, 520
결합 밀도, 249
결합 분포, 16, 23, 34, 51, 52, 71, $228,268,307,308,323,328,343$, $365,402,537,539,542,550,564$, $565,578,586,595,600,602,608$
결합 엔트로피, 16
결합 소스 채널 코딩 정리, 218
결합 타입, 499
결합 전형성, 195, 222, 240
결합적으로 전형적인, 198-203, 227-230, 240, 266, 267, 319, 327-329, 341, 343, $365,366,520,553,557,559,560$, 575,580
결합적으로 전형적인 시퀀스, 520
결합적으로 전형적인 집합, 227, 228, 319, 327
조즈사, R, 705
JPEG, 130
줄리안, D., xxi
저스텐, J., 215, 705

카츠, M., 443, 705
카츠의 보조정리, 444
카일라스, T., 705
칼린, S., 705
카루쉬, J., 158, 705
카울, A., xxiii
카와바타, B., xxiii
키겔, J.C., 707
켈리, J., 182, 655, 705
켈리, F.P., 705
켈리 도박, 182, 626
켐퍼만, J.H.B., 408, 705
켄달, M., 705
키보드, 480, 482

카이라트, M.A., 707
힌친, A.Y., 705
키퍼, J.C., 69, 705, 720
김, Y.H., xxi, 299, 705
킴버, D., xxiii
운동 에너지, 409
킹, R., 182, 696
크누스, D.E., 153, 705
고바야시, K., 610, 703
콜모고로프, A.N., 3, 345, 417, 463, 507, 702, 706
콜모고로프 복잡성, xv, xviii, xix, 1, $3,4,10-12,428,466,463-508,686$
조건부, 467
및 엔트로피, 473, 502
정수의, 475
하한, 469, 502
보편 확률, 490
상한, 501
콜모고로프 구조 함수, 496, 503, 507
콜모고로프 충분 통계량, 496, 497, 508
콜모고로프 부등식, 626
콘토야니스, Y., xxi
쾨르너, J., 241, 325, 347, 358, 408, 609, $610,690,697,698,701,706$
코텔니코프, V.A., 706
크래프트, L.G., 158, 706
크래프트 부등식, 103, 107-110, 112, 113, $116-118,127,138,141,143,158$, $473,484,494$
크리체프스키, R.E., 706
쿤-터커 조건, 164, 177, 191, $314,331,617,618,621,622$
쿨카르니, S.R., 698, 707, 718
쿨백, J.H., 707
쿨백, S., xix, 55, 408, 707
쿨백-라이블러 거리, 20, 55, 251, 상대 엔트로피 참조
$\mathcal{L}_{1}$ 거리, 369
라그랑주 승수, 110, 153, 161, 276, $313,330,334,335,421$
레이어드, N.M., 698
램핑, J., xxi
란다우, H.J., 272, 299, 707
랜다우어, R., 56, 691
랭던, G.G., 705, 707, 713
<!-- Page 765 -->
Lapidoth, A., xxi, 707
Laplace, P.S., 488, 489
Laplace 분포, 257, 661
Laplace 추정치, 488
대규모 편차 이론, 4, 12, 357, 360
Latané, H.A., 182, 655, 707
Lavenberg, S., xxiii
큰 수의 법칙, 57, 199, 245, 267, $319,326,355-357,361,403,477$, $479,520,522,615$
압축 불가능한 시퀀스, 477, 502
타입 방법, 355
약한 법칙, 57, 58, 65, 196, 245, 361, 380, 479
강사, 561
Lee, E.A., 707
Leech, J., 707
Lehmann, E.L., 56, 707
Leibler, R.A., 55, 707
Lempel, A., 428, 442, 462, 707, 721, Lempel-Ziv 코딩 참조
Lempel-Ziv,
고정 데이터베이스, 459
무한 사전, 458
슬라이딩 윈도우, 443
트리 구조, 448
Lempel-Ziv 알고리즘, xxiii, 441
Lempel-Ziv 코딩, 440-456
Lempel-Ziv 압축, 360
Lempel-Ziv 파싱, 427
문자, 105, 168-171, 174, 175, 209, 210, 224, 226, 233
Leung, C.S.K., 593, 609, 610, 696, 711
Levin, L.A., 507, 707
Levinson 알고리즘, 419
Levy의 마팅게일 수렴 정리, 647
사전식 순서, 327, 472
Li, M., 508, 707
Liao, H., 10, 609, 708
거짓말쟁이 역설, 483
Lieb, E.J., 693
가능도, 20, 365, 377, 404, 482, 508
가능도 비율, 482
가능도 비율 검정, 377, 378, 385, 389
Lin, S., 708
Lind, D., 708
Linde, Y., 708

Linder, T., 708
Lindley, D., 708
선형 대수학, 211
선형 코드, 214
선형 부등식, 534
선형 예측 코딩, 416
리스트 디코딩, 517, 575
Liversidge, A., 708
Lloyd, S.P., 708
Lloyd 알고리즘, 303
국소적 실재론, 56
로그,
밑, 14
로그 정규 분포, 662
로그 가능도, 65, 67, 405
로그 최적 포트폴리오, 616-624, 626-629, $649,653,654,656$
경쟁적 최적성, 627, 651
로그 합 부등식, 31-33, 44
Longo, G., 697
로또, 178
Louchard, G., 708
Lovasz, L., 226, 241, 708
저밀도 패리티 검사 (LDPC) 코드, 215
Lucky, R.W., 170, 171, 708
Lugosi, G., 698, 707, 708
LZ77, 441
LZ78, 441

MacKay, D.J.C., 215, 708, 709
거시 상태, 55, 409, 411, 412
MacWilliams, F.J., 708
Madhow, U., 704
자기 기록, 94, 101, 105, 158
Malone, D., 175
만델브로 집합, 471
Marcus, B., 158, 708
마진, 181
주변 분포, 297, 333
마르코프 근사, 169, 646
마르코프 연쇄, 35, 36, 39, 40, 47, 52, 71-100, 144, 206, 258, 294, 295, 423, 458, 470, 497, 499, 578-580, 584, 659,687
비주기적, 72,78
함수, 84
기약, 72, 78, 98
정상 분포, 73
<!-- Page 766 -->
마르코프 연쇄 (계속)
시간 불변, 72
시간 가역, 81
마르코프 필드, 35
마르코프 보조정리, 586
마르코프 과정, 87, 100, 144, 422, 428, 437, 또한 가우스-마르코프 과정 참조
마르코프 부등식, 49, 64, 157, 238, 392, $460,621,627,648,649$
마코위츠, H., 614
마크스, R.J., 708
마셜, A., 708, 709
화성인, 143
마틴-뢰프, P., 507, 709
마틴게일, 647
마틴게일 수렴 정리, 626
마르톤, K., 609, 610, 706, 709
마르체타, T.L., 693
매시, J.L., 709
수학, xvi
매티스, C., xxi
마티스, P., 709
행렬, 88, 95, 99, 200, 212, 239, 337, 338, $340,342,397,432,458,657,681$, 682,687
채널 전이, 190
이중 확률, 190
패리티 검사, 211
순열, 88
확률 전이, 72
대각합, 278
전이, 77, 88
행렬 부등식, 687
최대 흐름 최소 컷, 512
최대 오차 확률, 204, 207, 264, 268
최대 사후 확률, 388
최대 엔트로피, xviii, 51, 92, 96, 255, 258, 263, 282, 289, 375, 409, $412-415,417,420-425,451$
조건부 극한 정리, 371
예측 오차, 423
스펙트럼 밀도, 419, 421
최대 엔트로피 분포, 30, 364, $375,409,410,412-414$
최대 엔트로피 그래프, 97
최대 엔트로피 과정, 419, 422
최대 우도, 201, 231, 500
최대 우도 복호화, 231
최대 우도 추정, 404
맥스웰-볼츠만 분포, 409, 662
맥스웰의 악마, 507
미로, 97
마조, J., xxiii
맥도널드, R.A., 345, 709
맥엘리스, R.J., 696, 697, 709
맥러플린, S.W., 718
맥밀란, B., 69, 158, 709, 또한 섀넌-맥밀란-브리먼 정리 참조
맥밀란 부등식, 141
MDL (최소 설명 길이), 501
평균값 정리, 247
평균-분산 이론, 614
측도론, xx
중앙값, 257
의학 검사, 375
멜사, J.L., 702
메모리리스, 184, 216, 280, 513, 563, 572, 588, 593, 610, 또한 이산 메모리리스 채널 참조
병합, 149
메르하브, N., 461, 462, 700, 709, 718, 721
머튼, R.C., 709
메서슈미트, D.G., 707
유형별 방법, xv, 347, 357, 361, 665
거리, 46
마이크로프로세서, 468
미시 상태, 55, 409, 411
MIMO (다중 입력 다중 출력), 611
최소 충분 통계량, 38
최소 최대 중복성, 456
최소 설명 길이, 3, 501, 508
최소 거리, 213, 325, 332
볼록 집합 간의, 332
상대 엔트로피, 367
최소 분산, 396
최소 가중치, 212
민코프스키, H., 710
미르스키, L., 710
미첼, J.L., 711
혼합 전략, 391
휴대 전화, 607
계산 모델, 464
모뎀, 273, 442
변조, 3, 263
모듈로 2 산술, 211, 308, 596
<!-- Page 767 -->
분자, 409
모멘트, 255, 414, 614
모나리자, 471, 499
돈, 160, 164, 171, 172, 176-178, 487, 631, 634, 또한 부 참조
원숭이, 480, 482, 504
무어, E.F., 158, 702
모르겐슈테른, O., 710
모렐, M., xxiii
모스 부호, 103, 104
모이, S.C., 69, 710
다중 경로, 292, 611
다중 접속 채널, 10, 518, 524, 589, 594, 609
달성 가능성, 530
이진 삭제 채널, 527
이진 삭제 다중 접속 채널, 594
이진 승수 채널, 527
용량 영역, 526
볼록성, 534
반대, 538
협력적 용량, 596
상관 관계가 있는 소스, 593
Slepian-Wolf 코딩과의 쌍대성, 558
삭제 채널, 529
피드백, 594
가우시안, 514, 598, 607
독립적인 BSC, 526
다중화, 273, 515, 547
다변수 정보 이론, 네트워크 정보 이론 참조
다변수 분포, 411
다변수 정규 분포, 249, 254, 287, 305, $315,413,417,679$
음악, 1, 428
뮤추얼 펀드, 653
상호 정보량, xvii, 12, 20, 159, 252, 656, 686
연쇄 법칙, 24
조건부, 45, 49
연속 확률 변수, 251
음수 아님, 29
속성, 43
마이어스, D.L., 718

나가오카, H., 690
나하무, D., xxiii
나라얀, P., 697, 707
나트, 14, 244, 255, 313
나약, P.P., xxi
닐, R.M., 215, 708, 719
최근접 이웃, 303
최근접 이웃 복호화, 3
이웃, 361, 638
넬슨, R., xxi
네트워크, 11, 270, 273, 274, 509-511, 519, 520, 587, 588, 592, 594
네트워크 정보 이론, xv, xix, 3, 10, $11,509-611$
피드백, 593
폰 노이만, J., 710
뉴턴, I., xvii, 4
뉴턴 물리학, 490
네이만, J., 710
네이만-피어슨 보조 정리, 376, 398
닐슨, M., 241, 710
노벨, A., xxiii
잡음, xvii, xix, 1, 3, 11, 183, 224, 234, 237, 257, 261, 265, 272-274, 276-281, 289, 291-293, 297-299, $324,509,513-516,519,520,533$, $546,548,588$
색깔 있는 잡음, 277
무잡음 채널, 8, 558
잡음 있는 타자기, 186
놀, P., 704
음이 아닌 정부호 행렬, 284, 285
음수 아님,
엔트로피, 15
상호 정보량, 29
상대 엔트로피, 20, 29
무의미, 464, 482, 504
노름, 297
유클리드 노름, 297
정규 분포, 38, 254, 269, 311, 411, 414, 662, 675, 가우시안 채널, 가우시안 소스 참조
일반화된 정규 분포, 662
최대 엔트로피 속성, 254
영 공간, 211
나이퀴스트, H., 270, 272, 710

오컴의 면도날, 1, 4, 463, 481, 488, 490, 500
배당률, 11, 67, 159, 162-164, 176-180, 626, 645
짝수, 159
<!-- Page 768 -->
확률 (계속)
공정한, $159,167,176$
준공정한, 164,176
초공정한, 164
균등, 172
균등 공정한, 163
Olkin, I., 708, 709
Olshen, R.A., 693
$\Omega$, xix, 484, 502
Omura, J.K., 718, 710
양파 껍질 벗기기, 546
Oppenheim, A., 710
광학 채널, 101
최적 코드 길이, 148, 149
최적 복호화, 231, 514
최적 두 배율, $162,165,166$
최적 포트폴리오, $613,626,629,652$
오라클, 485
Ordentlich, E., xxi, xxiii, 656, 696, 710
Orey, S., 69, 656, 710
Orlitsky, A., xxi, xxiii, 241, 706, 710
Ornstein, D.S., 710
Oslick, M., xxiii
출력 알파벳, 143, 183
Ozarow, L.H., 594, 609, 610, 711

Pagels, H., 508, 711
Papadias, C.B., 711
Papadimitriou, C., 711
역설, 482
Berry의 역설, 483
에피메니데스의 거짓말쟁이 역설, 483
상트페테르부르크 역설, 181
병렬 채널, 277, 293
병렬 가우시안 소스, 314
파레토 분포, 662
패리티, 212-214
패리티 검사 코드, 211, 214
패리티 검사 행렬, 211
파싱, 441, 448-450, 452, 455, 456, 458, 459
부분 재귀 함수, 466
분할, 251
Pasco, R., 158, 711
과거, 93
Patterson, G.W., 149, 713
Paulraj, A.J., 711
Pearson, E.S., 710
Peile, R.E., 702

Pennebaker, W.B., 711
Perez, A., 69, 711
수성 근일점, 490
주기ogram, 415
순열, 84, 190, 258
순열 행렬, 88
수직 이등분선, 378
섭동, 674
Phamdo, N., xxi
철학자의 돌, 484
과학 철학, 4
사진 필름, 293
구문, 441-443, 448, 452
물리적으로 열화된, 564, 568, 571, 573, 610
물리학, xvi, xvii, 1, 4, 56, 409, 463, 481
$\pi, 4$
표지 그림, 471
Pierce, J.R., 711
비둘기, 233
Pinkston, J.T., 337, 711
Pinsker, M.S., 299, 609, 610, 702, 711
함정, 483
픽셀, 471
pkzip, 442
Plotnik, E., 711
푸아송 분포, 293
Pollak, H.O., 272, 299, 707, 715
Pollard, D., 711
Poltyrev, G.S., 712
Polya의 항아리 모델, 90
다항식 종류 수, 355, 357, 373
Pombra, S., xxi, 299, 695, 696, 712
Poor, H.V., 705, 712
포트폴리오, 182, 613-654, 656
포트폴리오 전략, 620, 629-631, 634, 636, 643
포트폴리오 이론, xv, 613
양의 정부 행렬, 279, 686, 687
Posner, E., 696
전력, 84, 116, 142, 273, 293, 295, 297, $298,320,324,357,415,513-515$, $517,518,546-548,606,607,610$, 674
전력 제약, 261, 262-264, 266, 268, 270, 274, 277, 278, 281, 289, 291, 292, 296, 513, 547
전력 스펙트럼 밀도, 272, 289, 415
Pratt, F., 712
<!-- Page 769 -->
예측, 11
예측 오류, 423
접두사, 106, 109, 110, 118, 124, 149, 150, 443, 473
접두사 코드, 109, 110, 118, 148, 150
주요 소행렬식, 680, 681
사전, 385, 388, 389, 435, 436
베이즈, 384
Proakis, J., 692
확률 밀도, 243, 250, 420, 425
확률 질량 함수, 5
오류 확률,
베이즈, 385
최대, 195, 204
확률 단체, 348, 359, 362, $378-380,385,386,391,408$
확률 이론, 1, 12
확률 전이 행렬, 7, 72, 73, 226, 524
과정, 183
프로그램 길이, 3, 463
길쭉한 구형 함수, 272
비례 베팅, 487
비례 도박, 162-164, 173, 182, 619, 645
구두점, 168
Pursley, M.B., 697, 703
피타고라스 정리, 367, 368
양자화, 247, 248, 251, 263, 301-303, 312, 363
양자 채널 용량, 56
양자 데이터 압축, 56
양자 정보 이론, 11, 241
양자 역학, 11, 56, 241
퀸, 80

Rabiner, L.R., 712
경주, 경마 참조
라디오, 261, 270, 547, 560
라듐, 257
무작위 상자 크기, 67
무작위 코딩, 3, 201, 204, 230, 324, 565
난수 생성, 134
무작위 과정,
베르누이, 98
무작위 질문, 53
확률 변수, $5,6,13,14,103$
베르누이, 53, 63
생성, 134, 155
무작위 행보, 78
무작위화, 627
계수, 211, 393
Rao, C.R., 712
Ratcliff, D., 697
비율,
달성 가능한, 달성 가능한 비율 참조
엔트로피, 엔트로피 비율 참조
비율 왜곡, xv, 301-347, 582, 585, 586, 596, 610, 686
달성 가능성, 306, 318
베르누이 소스, 307, 336
계산, 332
반례, 316
삭제 왜곡, 338
가우시안 소스, 310, 311, 325, 336
무한 왜곡, 336
다변량 가우시안 소스, 336
운영 정의, 307
병렬 가우시안 소스, 314
섀넌 하한, 337
측면 정보 포함, 580, 596
제곱 오차 왜곡, 310, 338
비율 왜곡 코드, 305, 316, 321, 324, $325,329,341,583$
최적, 339
비율 왜곡 함수, 306-308, 310, 311, $313-316,321,327,333,334$, $337-340,344,596,610$
볼록성, 316
정보, 307
비율 왜곡 영역, 306, 586
비율 왜곡 정리, 307, 310, 324, 325, $336,341,583,585$
비율 왜곡 이론, 10, 301, 303, 307, 357
비율 영역, 535, 536, 557, 569, 592, 593, $602-605,608$
Rathie, P.N., 662, 718
Raviv, J., 690
Ray-Chaudhuri, D.K., 214, 693
Rayleigh, G.G., 611, 702
레일리 분포, 662
재조정된 포트폴리오, 613, 629-632, 634, $636,638,639,643$
수신기, 183
재발, 91, 457, 459, 460
재발 시간, 444, 445
<!-- Page 770 -->
재귀, 90, 95, 123, 469
부의 재분배, 82
중복성, 148, 171, 184, 210, 429, 430, $435,436,456,461,462,631$
미니맥스, 429
Reed, I.S., 214, 712
Reed-Solomon 부호, 214, 215
Reiffen, B., 666, 719
재투자, 181, 615
상대 엔트로피, xvii, xix, 4, 9, 11, 12, 20, $25,30,43,52,68,81,87,112,115$, $151,252,259,305,332,333,362$, $366,368,369,378-384,401,421$, $427,429,545,658-660,665,686$
$\chi^{2}$ 하한, 400
비대칭성, 52
하한, 663
연쇄 법칙, 25
볼록성, 33
및 Fisher 정보, 401
$\mathcal{L}_{1}$ 하한, 398
음수 아님, 29, 50
속성, 43
상대 엔트로피 거리, $82,356,433$
상대 엔트로피 근방, 361
릴레이 채널, 510, 516, 571, 572, 591, 595, 610
달성 가능성, 573
용량, 573
반례, 572
퇴화된, 571, 573, 591, 610
피드백, 591
가우시안, 516
물리적으로 퇴화된, 571, 573, 591
역으로 퇴화된, 575
Renyi 엔트로피, 676, 677
Renyi 엔트로피 거듭제곱, 677
재생점, 302
역 워터필링, 315, 336, 345
Reza, F.M., 712
Rice, S.O., 712
리만 적분 가능성, 248
Rimoldi, B., 702, 712
무위험 자산, 614
Rissanen, J., 158, 420, 462, 508, 691, 707, 712,713
Roche, J., xxi, xxiii
룩, 80
Roy, B., xxi

Rubin, D.B., 698
런렝스 코딩, 49
Ryabko, B.Ya., 430, 461, 713
새들점, 298
Salehi, M., xxiii, 695, 696
Salz, J., xxiii
표본 상관, 415
샘플링 정리, 272
Samuelson, P.A., 656, 709, 713
샌드위치 논증, 69, 644, 648
Sanov, I.N., 408, 713
Sanov 정리, 362, 378, 386, 391, 398, 403
Sardinas, A.A., 149, 713
Sardinas-Patterson 검정, 149
위성, 215, 261, 509, 515, 565
Sato, H., 610, 713
Savari, S.A., 713
Sayood, K., 713
Schafer, R.W., 712
Schalkwijk, J.P.M., 609, 713, 720
Scheffé, H., 56, 707
Schnorr, C.P., 507, 713, 714
Scholtz, R.A., 702
Schrödinger 파동 방정식, xvii
Schultheiss, P.M., 345, 709
Schumacher, B., 705
Schwalkwijk, J.P.M., 705
Schwarz, G., 714
점수 함수, 393, 394
열역학 제2법칙, xviii, 4,11 , $55,81,87,507$, 또한 통계 역학 참조
오목성, 100
자기 정보, 13, 22
자기 보완, 468
자기 참조, 483
열 길이, 55
순차적 투영, 400
집합 합, 675
부호 함수, 132
셰익스피어, 482
Shamai, S., 692, 714
Shannon 부호, 115, 122, 131, 132, 142, $145,463,470,613$
경쟁적 최적성, 130, 132, 142, 158
Shannon 추측 게임, 174
<!-- Page 771 -->
Shannon 하한, 337
Shannon의 첫 번째 정리 (소스 코딩 정리), 115
Shannon의 두 번째 정리 (채널 코딩 정리), 189, 192
Shannon의 세 번째 정리 (비율 왜곡 정리), 307
Shannon, C.E., xv, xviii, 55, 69, 100, 157, 171, 174, 182, 205, 240, 270, 299, $345,609,656,687,699,714,715$, 또한 Shannon 코드, Shannon-Fano-Elias 코드, Shannon-McMillan-Breiman 정리 참조
Shannon-Fano 코드, 158, 491, 또한 Shannon 코드 참조
Shannon-Fano-Elias 코드, 127, 130, 428
Shannon-McMillan-Breiman 정리, 69, $644-649$
Shannon-Nyquist 샘플링 정리, 272
Sharpe, W.F., 614, 715
Sharpe-Markowitz 이론, 614
Shields, P.C., 462, 715
Shimizu, M., xxiii
Shor, P.W., 241, 691, 693, 698
Shore, J.E., 715
공매도, 181
Shtarkov, Y.M., 631, 656, 719
Shtarkov, Y.V., 715
셔플, 84,89
Shwartz, A., 715
부수 정보,
소스 코딩과 함께, 575
부수 정보, xvii, 12, 159, 165, 166, 180, 255, 574, 576, 580-583, 596, $610,623,652$
두 배 속도와 함께, 165,622
Siegel, P.H., 704
$\sigma$ 대수, 644
신호, 1, 171, 192, 199, 234, 258, 262-299, 513, 517, 519, 533, 544, 561, 607
Sigurjonsson, S., xxi
실리콘 드림즈, 170
요오드화은 결정, 293
단순체, 348, 378, 380, 385, 386, 391, 408, 617,618
sinc 함수, 271
Sleator, D., 691
Slepian, D., 272, 299, 549, 609, 715

Slepian-Wolf 코딩, 10, 549-560, 575, 581, 586, 592, 593, 595, 598, $603-605,608-610$
달성 가능성, 551
반례, 555
다중 접속 채널과의 쌍대성, 558
슬라이스 코드, 122
슬라이스 질문, 121
슬라이딩 윈도우 Lempel-Ziv, 441
Sloane, N.J.A., 707, 708, 720
가장 작은 확률 집합, 64
Smolin, J., 691, 698
SNR (신호 대 잡음비), 273, 514, 516
Solomon, G., 214
Solomonoff, R.J., 3, 4, 507, 716
소스, 103, 337
이진, 307
가우시안, 310
소스 채널 코딩 정리, 218, 223
소스 채널 분리, 218, 318, 344, 592, 593
소스 코드, 103, 123, 552, 631
소스 코딩, $60,134,447,473,511$
채널 용량과 함께, 430
부수 정보와 함께, 575, 595
소스 코딩 정리, 144, 158
시공간 코딩, 611
스페인어, 561, 562, 606
스펙트럼 표현 정리, 315
스펙트럼, 271, 279, 280, 315, 415, 417, 419,421
스펙트럼 추정, 415
음성, 1, 87, 101, 171, 218, 305, 416
구, 265, 297, 324, 675
구 덮개, 324
구 패킹, 10, 324, 325
제곱 오차, 302, 393, 423, 683
제곱 오차 왜곡, 336
상트페테르부르크 역설, 181, 182
Stam, A., 674, 687, 716
상태 다이어그램, 95
상태 전이, 73, 465
정상, 4, 69, 114, 168, 220, 221, 279, 297, 415-417, 423, 428, 444, 446, $451,453,455,458,462,613,625$, $626,646,647,651,659,681$
정상 분포, 73, 77-79, 96
정상 에르고딕 과정, 69
<!-- Page 772 -->
stationary ergodic source, 219
stationary market, 624
stationary process, 71, 142, 644
statistic, 36, 38, 400
Kolmogorov sufficient, 496
minimal sufficient, 38
sufficient, 38
statistical mechanics, $4,6,11,55,56,425$
statistics, xvi-xix, 1, 4, 12, 13, 20, 36-38, $169,347,375,497,499$
Steane, A., 716
Stein's lemma, 399
stereo, 604
Stirling's approximation, 351, 353, 405, 411,666
stochastic process, $71,72,74,75,77,78$, $87,88,91,93,94,97,98,100,114$, $166,219,220,223,279,415,417$, $420,423-425,455,625,626,646$
ergodic, see ergodic process
function of, 93
Gaussian, 315
without entropy rate, 75
stock, 9, 613-615, 619, 624, 626, 627, $629-634,636,637,639-641,652$, 653
stock market, xix, 4, 9, 159, 613, 614-617, $619-622,627,629-631,634,636$, $649,652,653,655,656$
Stone, C.J., 693
stopping time, 55
Storer, J.A., 441, 459, 716
strategy, 160, 163, 164, 166, 178, 391, 392, 487
investment, 620
strong converse, 208, 240
strongly jointly typical, 327, 328
strongly typical, 326, 327, 357, 579, 580
strongly typical set, 342, 357
Stuart, A., 705
Student's $t$ distribution, 662
subfair odds, 164
submatrix, 680
subset, xx, 8, 66, 71, 183, 192, 211, 222, $319,347,520,644,657,668-670$
subset inequalities, 668-671
subsets, 505
entropy rate, 667
subspace, 211
sufficient statistic, 13, 36, 37, 38, 44, 56, 209, 497-499
minimal, 38,56
suffix code, 145
superfair odds, 164, 180
supermartingale, 625
superposition coding, 609
support set, 29, 243, 244, 249, 251, 252, 256, 409, 676-678
surface area, 247
Sutivong, A., xxi
Sweetkind-Singer, J., xxi
symbol, 103
symmetric channel, 187, 190
synchronization, 94
Szasz's inequality, 680
Szegö, G., 703
Szpankowski, W., 716, 708
Szymanski, T.G., 441, 459, 716

Tanabe, M., 713
Tang, D.L., 716
Tarjan, R., 691
TDMA (Time Division Multiple Access), 547,548
Telatar, I.E., 716
telegraph, 441
telephone, 261, 270, 273, 274
channel capacity, 273
Teletar, E., 611, 716
temperature, 409,411
ternary, 119, 145, 157, 239, 439, 504, 527
ternary alphabet, 349
ternary channel, 239
ternary code, 145, 152, 157
text, 428
thermodynamics, 1, 4, see also second law of thermodynamics
Thitimajshima, P., 692
Thomas, J.A., xxi, 687, 694, 695, 698, 700, 716
Thomasian, A.J., 692
Tibshirani, R., 699
time symmetry, 100
timesharing, 527, 532-534, 538, 562, 598, 600
Tjalkens, T.J., 716, 719
Toeplitz matrix, 416, 681
Tornay, S.C., 716
<!-- Page 773 -->
추적, 279, 547
전이 행렬, 77, 92, 98, 144, 190
이중 확률, 83, 88, 190
송신기, 266, 294, 296, 299, 515, $517-519,546,573,574,588,601$, 611
재무부 채권, 614
트리,
코드, 107
허프만, 124
무작위, 89
트리 구조 Lempel-Ziv, 441, 442
삼각 부등식, 20, 369
삼각 분포, 662
삼그램 모델, 171
Trofimov, V.K., 706
Trott, M., xxiii
Tseng, C.W., xxiii
Tsoucas, P., 700
Tsybakov, B.S., 716
Tunstall, B.P., 716
Tunstall 코딩, 460
터보 코드, 3, 205, 215
튜링, A., 465
튜링 기계, xix, 465, 466
Tusnády, G., 332, 335, 346, 697
Tuttle, D.L., 182, 707
TV, 509, 560, 561
쌍둥이, 171
두 봉투 문제, 179
2단계 신호, 262
2단계 설명, 496
양방향 채널, 510, 519, 594, 602, 609
유형, 342, 347, 348-350, 353-356, 358, $360,361,366,367,371,373,374$, $378,391,408,474,490,499,570,666$
유형 클래스, 348-351, 353-356, 666
타자기, 74, 192, 224, 235, 482
전형적인 시퀀스, $11,12,57,63,245,381$, 522
전형적인 집합, 57, 59, 61, 62, 64, 68, 77, 196, 220, 227, 245, 247, 258, 319, 321, $356,381,382,384,524,551$
조건부 전형적, 341
데이터 압축, 60
왜곡 전형적, 319
속성, 59, 64, 245
강하게 전형적, 326
부피, 245

Ullman, J.D., 704
불확실성, 5, 6, 11, 13, 15, 20, 22, 24, 31, $53,83,89,170,517,518,593$
Ungerboeck, G., 716
균등 분포, 5, 30, 43, 83, 88, 148, 163, 190, 195, 202, 204, 209, 210, $228,268,338,375,408,411,412$, $434,436,437,553,662,663$
균등 공정 배당률, 163, 166, 176, 626
고유 복호화 가능 코드, 참조 코드, 고유 복호화 가능
범용 컴퓨터, 465, 501
범용 데이터 압축, 333, 457
범용 도박, 487, 488, 507
범용 포트폴리오, 629-643, 651
유한 기간, 631
기간 무관, 638
범용 확률, 481, 487, 489-491, 502, 503, 507, 686
범용 확률 질량 함수, 481
범용 소스, 358
범용 소스 코드, 357, 360, 461
범용 소스 코딩, xv, 355, 427-462
오차 지수, 400
범용 튜링 기계, 465, 480
Unix, 443
Urbanke, R., 702, 712
V. 90, 273

V'yugin, V.V., 507, 718
Vajda, I., 716
Valiant, L.G., 717
Van Campenhout, J.M., 717
Van der Meulen, E., xxiii, 609-611, 699, 702, 717
Van Trees, H.L., 716
Vapnik, V.N., 717
가변 길이 코딩, 460
분산, xx, 36, 37, 64, 65, 255, 261, 265, $272,292,315,325,389,393,394$, $396,513,516,520,544,614,655$, 681, 685, 또한 공분산 행렬 참조
변분 거리, 370
벡터 양자화, 303, 306
Venkata, R., xxi
Venkatesh, S.S., 707
벤 다이어그램, 23, 47, 50, 213
Verdu, S., 690, 698, 703, 704, 714, 717, 718
<!-- Page 774 -->
Verdugo Lazo, A.C.G., 662, 718
비디오, 218
비디오 압축, xv
Vidyasagar, M., 718
Visweswariah, K., 698, 718
비타민, xx
Vitanyi, P., 508, 707
Viterbi, A.J., 718
Vitter, J.S., 718
어휘, 561
볼륨, 67, 149, 244-247, 249, 265, 324, $675,676,679$
Von Neumann, J., 11
보로노이 분할, 303
대기 시간, 99
Wald, A., 718
Wallace, M.S., 697
Wallmeier, H.M., 692
워싱턴, 551
Watanabe, Y., xxiii
워터 필링, 164, 177, 277, 279, 282, 289, 315
파형, 270, 305, 519
약한, 342
약하게 전형적인, 전형적인 참조
부, 175
부 상대, 613, 619
날씨, 470, 471, 550, 551
Weaver, W.W., 715
웹 검색, xv
Wei, V.K.W., 718, 691
Weibull 분포, 662
Weinberger, M.J., 711, 718
Weiner, N., 718
Weiss, A., 715
Weiss, B., 462, 710, 715
Welch, T.A., 462, 718
Wheeler, D.J., 462, 693
백색 잡음, 270, 272, 280, 참조
가우시안 채널도 참조
Whiting, P.A., 702
Wiener, N., 718
Wiesner, S.J., 691
Wilcox, H.J., 718

Willems, F.M.J., xxi, 461, 594, 609, 716, 718,719
창, 442
와인, 153
무선, xv, 215, 611
Witsenhausen, H.S., 719
Witten, I.H., 691, 719
Wolf, J.K., 549, 593, 609, 700, 704, 715
Wolfowitz, J., 240, 408, 719
Woodward, P.M., 719
Wootters, W.K., 691
월드 시리즈, 48
Wozencraft, J.M., 666, 719
Wright, E.V., 168
잘못된 분포, 115
Wyner, A.D., xxiii, 299, 462, 581, 586, $610,703,719,720$
Wyner, A.J., 720

Yaglom, A.M., 702
Yamamoto, H., xxiii
Yang, E.-H., 690, 720
Yao, A.C., 705
Yard, J., xxi
Yeung, R.W., xxi, xxiii, 610, 692, 720
Yockey, H.P., 720
Young의 부등식, 676, 677
Yu, Bin, 691
Yule-Walker 방정식, 418, 419

Zeevi, A., xxi
Zeger, K., 708
Zeitouni, O., 698
무오류, 205, 206, 210, 226, 301
무오류 용량, 210, 226
제로섬 게임, 131
Zhang, Z., 609, 690, 712, 720
Ziv, J., 428, 442, 462, 581, 586, 610, 703, 707, 711, 719-721, Lempel-Ziv 코딩 참조
Ziv의 부등식, 449, 450, 453, 455, 456
Zurek, W.H., 507, 721
Zvonkin, A.K., 507, 707
